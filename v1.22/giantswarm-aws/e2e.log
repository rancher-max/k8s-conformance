I0225 09:27:21.494457      22 e2e.go:129] Starting e2e run "6d994d1c-d9a8-48f3-9b8e-d365eea525f1" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1645781241 - Will randomize all specs
Will run 346 of 6434 specs

Feb 25 09:27:23.947: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
E0225 09:27:23.947625      22 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Feb 25 09:27:23.949: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 25 09:27:27.062: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 25 09:27:27.126: INFO: The status of Pod etcd-cluster-migrator--1-49npf is Succeeded, skipping waiting
Feb 25 09:27:27.126: INFO: 76 / 77 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 25 09:27:27.126: INFO: expected 16 pod replicas in namespace 'kube-system', 16 are Running and Ready.
Feb 25 09:27:27.126: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 25 09:27:27.142: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'aws-node' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'cert-exporter-daemonset' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kiam-agent' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kiam-server' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kiam-watchdog' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'net-exporter' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-exporter-v1-3-1' (0 seconds elapsed)
Feb 25 09:27:27.142: INFO: e2e test version: v1.22.6
Feb 25 09:27:27.144: INFO: kube-apiserver version: v1.22.6
Feb 25 09:27:27.144: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:27:27.151: INFO: Cluster IP family: ipv4
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:27:27.151: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
W0225 09:27:27.197718      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Feb 25 09:27:27.197: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Feb 25 09:27:27.219: INFO: PSP annotation exists on dry run pod: "aws-cni"; assuming PodSecurityPolicy is enabled
W0225 09:27:27.223786      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0225 09:27:27.230471      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Feb 25 09:27:27.240: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-ce2e2974-742b-4074-8e92-08451d4cddec
STEP: Creating a pod to test consume secrets
Feb 25 09:27:27.388: INFO: Waiting up to 5m0s for pod "pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073" in namespace "secrets-916" to be "Succeeded or Failed"
Feb 25 09:27:27.395: INFO: Pod "pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073": Phase="Pending", Reason="", readiness=false. Elapsed: 7.193806ms
Feb 25 09:27:29.404: INFO: Pod "pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016526502s
Feb 25 09:27:31.415: INFO: Pod "pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02714843s
STEP: Saw pod success
Feb 25 09:27:31.415: INFO: Pod "pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073" satisfied condition "Succeeded or Failed"
Feb 25 09:27:31.419: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073 container secret-env-test: <nil>
STEP: delete the pod
Feb 25 09:27:31.453: INFO: Waiting for pod pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073 to disappear
Feb 25 09:27:31.457: INFO: Pod pod-secrets-ce23a0b6-40ac-44cb-a66d-38d6c2d32073 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:27:31.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-916" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":12,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:27:31.472: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-3b9de2ff-9951-4485-9e52-89ebebb9b7fa
STEP: Creating a pod to test consume configMaps
Feb 25 09:27:31.648: INFO: Waiting up to 5m0s for pod "pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761" in namespace "configmap-6277" to be "Succeeded or Failed"
Feb 25 09:27:31.655: INFO: Pod "pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761": Phase="Pending", Reason="", readiness=false. Elapsed: 6.6665ms
Feb 25 09:27:33.666: INFO: Pod "pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017153091s
Feb 25 09:27:35.674: INFO: Pod "pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025472368s
Feb 25 09:27:37.682: INFO: Pod "pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033524346s
STEP: Saw pod success
Feb 25 09:27:37.682: INFO: Pod "pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761" satisfied condition "Succeeded or Failed"
Feb 25 09:27:37.686: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 25 09:27:37.711: INFO: Waiting for pod pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761 to disappear
Feb 25 09:27:37.715: INFO: Pod pod-configmaps-1be6e4fb-7141-4344-aaed-addb65f68761 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:27:37.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6277" for this suite.

• [SLOW TEST:6.257 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":36,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:27:37.730: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 25 09:27:37.908: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 25 09:28:37.965: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:28:37.969: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Feb 25 09:28:42.173: INFO: found a healthy node: ip-10-1-2-67.eu-central-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:28:52.278: INFO: pods created so far: [1 1 1]
Feb 25 09:28:52.278: INFO: length of pods created so far: 3
Feb 25 09:28:54.298: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:01.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4252" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:01.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7291" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:83.707 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":3,"skipped":58,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:01.437: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 25 09:29:01.609: INFO: Waiting up to 5m0s for pod "downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a" in namespace "downward-api-1848" to be "Succeeded or Failed"
Feb 25 09:29:01.616: INFO: Pod "downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.654183ms
Feb 25 09:29:03.626: INFO: Pod "downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016754205s
Feb 25 09:29:05.634: INFO: Pod "downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02422169s
STEP: Saw pod success
Feb 25 09:29:05.634: INFO: Pod "downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a" satisfied condition "Succeeded or Failed"
Feb 25 09:29:05.638: INFO: Trying to get logs from node ip-10-1-2-231.eu-central-1.compute.internal pod downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a container dapi-container: <nil>
STEP: delete the pod
Feb 25 09:29:05.674: INFO: Waiting for pod downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a to disappear
Feb 25 09:29:05.678: INFO: Pod downward-api-11c92936-e9bd-4cf1-810e-7e7ce6e6bb5a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:05.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1848" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":79,"failed":0}

------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:05.692: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 25 09:29:05.851: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6792" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":5,"skipped":79,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:07.762: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:29:07.963: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"87fb363e-fefc-47e8-a862-3e89d58d36ea", Controller:(*bool)(0xc0039fe382), BlockOwnerDeletion:(*bool)(0xc0039fe383)}}
Feb 25 09:29:07.976: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e407c9b4-8c76-4b77-ab00-8bc44238ed1d", Controller:(*bool)(0xc0039fe632), BlockOwnerDeletion:(*bool)(0xc0039fe633)}}
Feb 25 09:29:07.985: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e0e7b8f4-a486-4eef-bbd7-9d112779a4b2", Controller:(*bool)(0xc0039fe912), BlockOwnerDeletion:(*bool)(0xc0039fe913)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:13.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4281" for this suite.

• [SLOW TEST:5.262 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":6,"skipped":79,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:13.023: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6568
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1444
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:26.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6778" for this suite.
STEP: Destroying namespace "nsdeletetest-6568" for this suite.
Feb 25 09:29:26.547: INFO: Namespace nsdeletetest-6568 was already deleted
STEP: Destroying namespace "nsdeletetest-1444" for this suite.

• [SLOW TEST:13.532 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":7,"skipped":96,"failed":0}
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:26.555: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:26.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1813" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":8,"skipped":96,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:26.783: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Feb 25 09:29:26.953: INFO: Waiting up to 5m0s for pod "var-expansion-d701dd21-9dff-4c13-8531-8549ed9b9492" in namespace "var-expansion-8173" to be "Succeeded or Failed"
Feb 25 09:29:26.960: INFO: Pod "var-expansion-d701dd21-9dff-4c13-8531-8549ed9b9492": Phase="Pending", Reason="", readiness=false. Elapsed: 7.246161ms
Feb 25 09:29:28.970: INFO: Pod "var-expansion-d701dd21-9dff-4c13-8531-8549ed9b9492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016870518s
STEP: Saw pod success
Feb 25 09:29:28.970: INFO: Pod "var-expansion-d701dd21-9dff-4c13-8531-8549ed9b9492" satisfied condition "Succeeded or Failed"
Feb 25 09:29:28.974: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod var-expansion-d701dd21-9dff-4c13-8531-8549ed9b9492 container dapi-container: <nil>
STEP: delete the pod
Feb 25 09:29:29.004: INFO: Waiting for pod var-expansion-d701dd21-9dff-4c13-8531-8549ed9b9492 to disappear
Feb 25 09:29:29.008: INFO: Pod var-expansion-d701dd21-9dff-4c13-8531-8549ed9b9492 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:29.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8173" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":9,"skipped":130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Feb 25 09:29:29.192: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:29:31.202: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Feb 25 09:29:31.226: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:29:33.237: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:29:35.234: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:29:37.235: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 25 09:29:37.239: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.239: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.324: INFO: Exec stderr: ""
Feb 25 09:29:37.324: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.324: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.402: INFO: Exec stderr: ""
Feb 25 09:29:37.402: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.402: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.471: INFO: Exec stderr: ""
Feb 25 09:29:37.471: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.471: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.543: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 25 09:29:37.543: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.543: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.618: INFO: Exec stderr: ""
Feb 25 09:29:37.618: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.618: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.683: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 25 09:29:37.683: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.770: INFO: Exec stderr: ""
Feb 25 09:29:37.770: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.770: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.835: INFO: Exec stderr: ""
Feb 25 09:29:37.835: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.910: INFO: Exec stderr: ""
Feb 25 09:29:37.910: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6241 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:29:37.910: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:29:37.982: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:37.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6241" for this suite.

• [SLOW TEST:8.977 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:38.001: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2041
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Feb 25 09:29:38.169: INFO: Waiting up to 5m0s for pod "var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a" in namespace "var-expansion-2041" to be "Succeeded or Failed"
Feb 25 09:29:38.187: INFO: Pod "var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.357268ms
Feb 25 09:29:40.195: INFO: Pod "var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025591331s
Feb 25 09:29:42.203: INFO: Pod "var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033749232s
STEP: Saw pod success
Feb 25 09:29:42.203: INFO: Pod "var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a" satisfied condition "Succeeded or Failed"
Feb 25 09:29:42.207: INFO: Trying to get logs from node ip-10-1-2-27.eu-central-1.compute.internal pod var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a container dapi-container: <nil>
STEP: delete the pod
Feb 25 09:29:42.236: INFO: Waiting for pod var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a to disappear
Feb 25 09:29:42.243: INFO: Pod var-expansion-c60e7e6f-ffa7-4cbe-aeb5-0c4f2875411a no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:42.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2041" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":240,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:42.261: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 25 09:29:42.423: INFO: Waiting up to 5m0s for pod "pod-76592223-f3a7-4682-a4ce-818cc1310eaa" in namespace "emptydir-2450" to be "Succeeded or Failed"
Feb 25 09:29:42.430: INFO: Pod "pod-76592223-f3a7-4682-a4ce-818cc1310eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.251271ms
Feb 25 09:29:44.439: INFO: Pod "pod-76592223-f3a7-4682-a4ce-818cc1310eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015839562s
Feb 25 09:29:46.446: INFO: Pod "pod-76592223-f3a7-4682-a4ce-818cc1310eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023500449s
Feb 25 09:29:48.456: INFO: Pod "pod-76592223-f3a7-4682-a4ce-818cc1310eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032915464s
STEP: Saw pod success
Feb 25 09:29:48.456: INFO: Pod "pod-76592223-f3a7-4682-a4ce-818cc1310eaa" satisfied condition "Succeeded or Failed"
Feb 25 09:29:48.460: INFO: Trying to get logs from node ip-10-1-2-27.eu-central-1.compute.internal pod pod-76592223-f3a7-4682-a4ce-818cc1310eaa container test-container: <nil>
STEP: delete the pod
Feb 25 09:29:48.494: INFO: Waiting for pod pod-76592223-f3a7-4682-a4ce-818cc1310eaa to disappear
Feb 25 09:29:48.498: INFO: Pod pod-76592223-f3a7-4682-a4ce-818cc1310eaa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:29:48.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2450" for this suite.

• [SLOW TEST:6.251 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":243,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:29:48.513: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 25 09:29:48.679: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 25 09:30:48.738: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Feb 25 09:30:48.773: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 25 09:30:48.785: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 25 09:30:48.806: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 25 09:30:48.816: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 25 09:30:48.839: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 25 09:30:48.850: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:31:04.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3448" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.487 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":13,"skipped":264,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:31:05.000: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 25 09:31:05.176: INFO: The status of Pod pod-update-036d1a2b-ff46-4a39-acff-b8f91fe40af2 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:31:07.185: INFO: The status of Pod pod-update-036d1a2b-ff46-4a39-acff-b8f91fe40af2 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:31:09.186: INFO: The status of Pod pod-update-036d1a2b-ff46-4a39-acff-b8f91fe40af2 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 25 09:31:09.714: INFO: Successfully updated pod "pod-update-036d1a2b-ff46-4a39-acff-b8f91fe40af2"
STEP: verifying the updated pod is in kubernetes
Feb 25 09:31:09.723: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:31:09.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3495" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":271,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:31:09.737: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:31:11.928: INFO: Deleting pod "var-expansion-69d0c528-2e03-434e-8f9d-0feadb6367a0" in namespace "var-expansion-6834"
Feb 25 09:31:11.939: INFO: Wait up to 5m0s for pod "var-expansion-69d0c528-2e03-434e-8f9d-0feadb6367a0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:31:13.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6834" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":15,"skipped":280,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:31:13.974: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3310, will wait for the garbage collector to delete the pods
Feb 25 09:31:16.206: INFO: Deleting Job.batch foo took: 10.653125ms
Feb 25 09:31:16.306: INFO: Terminating Job.batch foo pods took: 100.674708ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:31:47.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3310" for this suite.

• [SLOW TEST:33.254 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":16,"skipped":296,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:31:47.229: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 25 09:31:47.398: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 25 09:32:47.460: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:32:47.465: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-4087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:32:47.632: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Feb 25 09:32:47.639: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:32:47.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4087" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:32:47.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4696" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.545 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":17,"skipped":309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:32:47.774: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:32:53.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4038" for this suite.

• [SLOW TEST:5.361 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":18,"skipped":365,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:32:53.135: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8642
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-02d71b56-b425-490e-a1c0-f7cfccf3c689
STEP: Creating configMap with name cm-test-opt-upd-6a691f8e-ad26-4c87-bccf-7ac249c305d7
STEP: Creating the pod
Feb 25 09:32:53.337: INFO: The status of Pod pod-projected-configmaps-1c30b79b-30b7-4afd-9505-845481969f9e is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:32:55.345: INFO: The status of Pod pod-projected-configmaps-1c30b79b-30b7-4afd-9505-845481969f9e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-02d71b56-b425-490e-a1c0-f7cfccf3c689
STEP: Updating configmap cm-test-opt-upd-6a691f8e-ad26-4c87-bccf-7ac249c305d7
STEP: Creating configMap with name cm-test-opt-create-0ad379c4-a898-4ad5-9b51-01ec1e6e61c2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:32:59.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8642" for this suite.

• [SLOW TEST:6.342 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":19,"skipped":366,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:32:59.477: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-87cd1db5-c8f5-43a4-a314-0723f2124b18
STEP: Creating a pod to test consume secrets
Feb 25 09:32:59.650: INFO: Waiting up to 5m0s for pod "pod-secrets-8838e7e0-57c2-418d-85b2-6f38cb0d120e" in namespace "secrets-7433" to be "Succeeded or Failed"
Feb 25 09:32:59.654: INFO: Pod "pod-secrets-8838e7e0-57c2-418d-85b2-6f38cb0d120e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389772ms
Feb 25 09:33:01.663: INFO: Pod "pod-secrets-8838e7e0-57c2-418d-85b2-6f38cb0d120e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013318368s
STEP: Saw pod success
Feb 25 09:33:01.663: INFO: Pod "pod-secrets-8838e7e0-57c2-418d-85b2-6f38cb0d120e" satisfied condition "Succeeded or Failed"
Feb 25 09:33:01.667: INFO: Trying to get logs from node ip-10-1-2-231.eu-central-1.compute.internal pod pod-secrets-8838e7e0-57c2-418d-85b2-6f38cb0d120e container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 09:33:01.700: INFO: Waiting for pod pod-secrets-8838e7e0-57c2-418d-85b2-6f38cb0d120e to disappear
Feb 25 09:33:01.704: INFO: Pod pod-secrets-8838e7e0-57c2-418d-85b2-6f38cb0d120e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:33:01.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7433" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":20,"skipped":373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:33:01.719: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9880
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 25 09:33:01.870: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:33:09.988: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:33:30.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9880" for this suite.

• [SLOW TEST:28.666 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":21,"skipped":396,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:33:30.386: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7647
STEP: creating service affinity-nodeport in namespace services-7647
STEP: creating replication controller affinity-nodeport in namespace services-7647
I0225 09:33:30.580664      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7647, replica count: 3
I0225 09:33:33.641964      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 09:33:33.661: INFO: Creating new exec pod
Feb 25 09:33:36.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7647 exec execpod-affinityr75m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb 25 09:33:36.982: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 25 09:33:36.982: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:33:36.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7647 exec execpod-affinityr75m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.5.33 80'
Feb 25 09:33:37.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.5.33 80\nConnection to 172.31.5.33 80 port [tcp/http] succeeded!\n"
Feb 25 09:33:37.116: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:33:37.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7647 exec execpod-affinityr75m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.231 31496'
Feb 25 09:33:37.249: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.231 31496\nConnection to 10.1.2.231 31496 port [tcp/*] succeeded!\n"
Feb 25 09:33:37.250: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:33:37.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7647 exec execpod-affinityr75m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.27 31496'
Feb 25 09:33:37.386: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.27 31496\nConnection to 10.1.2.27 31496 port [tcp/*] succeeded!\n"
Feb 25 09:33:37.386: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:33:37.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7647 exec execpod-affinityr75m4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.1.2.231:31496/ ; done'
Feb 25 09:33:37.587: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31496/\n"
Feb 25 09:33:37.587: INFO: stdout: "\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8\naffinity-nodeport-zlhc8"
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Received response from host: affinity-nodeport-zlhc8
Feb 25 09:33:37.587: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7647, will wait for the garbage collector to delete the pods
Feb 25 09:33:37.673: INFO: Deleting ReplicationController affinity-nodeport took: 8.86306ms
Feb 25 09:33:37.773: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.863437ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:33:39.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7647" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.139 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":22,"skipped":417,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:33:39.525: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 25 09:33:39.679: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:33:43.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2226" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":23,"skipped":427,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:33:43.557: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:33:59.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2021" for this suite.

• [SLOW TEST:16.329 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":24,"skipped":438,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:33:59.887: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:34:00.065: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:34:02.074: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:04.075: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:06.075: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:08.073: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:10.073: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:12.074: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:14.076: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:16.074: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:18.073: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:20.076: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = false)
Feb 25 09:34:22.075: INFO: The status of Pod test-webserver-55f93cc7-ba9e-45dd-8f9e-1f4cb46c3278 is Running (Ready = true)
Feb 25 09:34:22.079: INFO: Container started at 2022-02-25 09:34:00 +0000 UTC, pod became ready at 2022-02-25 09:34:20 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:34:22.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7427" for this suite.

• [SLOW TEST:22.211 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":25,"skipped":441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:34:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:34:23.201: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 09:34:25.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378463, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378463, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378463, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378463, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:34:28.239: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:34:28.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8678" for this suite.
STEP: Destroying namespace "webhook-8678-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.301 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":26,"skipped":469,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:34:28.400: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:34:30.590: INFO: Deleting pod "var-expansion-469432a8-1bfe-4c65-a6ae-66720e6451ba" in namespace "var-expansion-4495"
Feb 25 09:34:30.602: INFO: Wait up to 5m0s for pod "var-expansion-469432a8-1bfe-4c65-a6ae-66720e6451ba" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:34:34.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4495" for this suite.

• [SLOW TEST:6.236 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":27,"skipped":471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:34:34.636: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-1777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Feb 25 09:34:34.786: INFO: Major version: 1
STEP: Confirm minor version
Feb 25 09:34:34.786: INFO: cleanMinorVersion: 22
Feb 25 09:34:34.786: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:34:34.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1777" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":28,"skipped":506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:34:34.801: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-08b885b1-a96c-4fca-abbe-b26a7c8b74f5 in namespace container-probe-3110
Feb 25 09:34:36.978: INFO: Started pod liveness-08b885b1-a96c-4fca-abbe-b26a7c8b74f5 in namespace container-probe-3110
STEP: checking the pod's current state and verifying that restartCount is present
Feb 25 09:34:36.983: INFO: Initial restart count of pod liveness-08b885b1-a96c-4fca-abbe-b26a7c8b74f5 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:38:38.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3110" for this suite.

• [SLOW TEST:243.290 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:38:38.091: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9304
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9304
I0225 09:38:38.288116      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9304, replica count: 2
I0225 09:38:41.339725      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 09:38:41.339: INFO: Creating new exec pod
Feb 25 09:38:44.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-9304 exec execpodznqxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 25 09:38:44.530: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 25 09:38:44.530: INFO: stdout: "externalname-service-9p9hh"
Feb 25 09:38:44.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-9304 exec execpodznqxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.60.112 80'
Feb 25 09:38:44.668: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.60.112 80\nConnection to 172.31.60.112 80 port [tcp/http] succeeded!\n"
Feb 25 09:38:44.668: INFO: stdout: ""
Feb 25 09:38:45.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-9304 exec execpodznqxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.60.112 80'
Feb 25 09:38:45.815: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.60.112 80\nConnection to 172.31.60.112 80 port [tcp/http] succeeded!\n"
Feb 25 09:38:45.815: INFO: stdout: ""
Feb 25 09:38:46.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-9304 exec execpodznqxq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.60.112 80'
Feb 25 09:38:46.802: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.60.112 80\nConnection to 172.31.60.112 80 port [tcp/http] succeeded!\n"
Feb 25 09:38:46.802: INFO: stdout: "externalname-service-9p9hh"
Feb 25 09:38:46.802: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:38:46.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9304" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.760 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":30,"skipped":596,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:38:46.850: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:38:58.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6924" for this suite.

• [SLOW TEST:11.259 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":31,"skipped":598,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:38:58.110: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 09:38:58.278: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1194fa41-df33-41e0-b4d5-02de7489b461" in namespace "projected-2319" to be "Succeeded or Failed"
Feb 25 09:38:58.285: INFO: Pod "downwardapi-volume-1194fa41-df33-41e0-b4d5-02de7489b461": Phase="Pending", Reason="", readiness=false. Elapsed: 7.113956ms
Feb 25 09:39:00.295: INFO: Pod "downwardapi-volume-1194fa41-df33-41e0-b4d5-02de7489b461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016832259s
STEP: Saw pod success
Feb 25 09:39:00.295: INFO: Pod "downwardapi-volume-1194fa41-df33-41e0-b4d5-02de7489b461" satisfied condition "Succeeded or Failed"
Feb 25 09:39:00.300: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod downwardapi-volume-1194fa41-df33-41e0-b4d5-02de7489b461 container client-container: <nil>
STEP: delete the pod
Feb 25 09:39:00.335: INFO: Waiting for pod downwardapi-volume-1194fa41-df33-41e0-b4d5-02de7489b461 to disappear
Feb 25 09:39:00.339: INFO: Pod downwardapi-volume-1194fa41-df33-41e0-b4d5-02de7489b461 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:00.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2319" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":610,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:00.353: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-4d138ffd-7fce-4de7-9ee9-6c43a9e00a85
STEP: Creating a pod to test consume configMaps
Feb 25 09:39:00.522: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-262bb94a-10e3-4399-91ea-d3e45f13aa25" in namespace "projected-9446" to be "Succeeded or Failed"
Feb 25 09:39:00.528: INFO: Pod "pod-projected-configmaps-262bb94a-10e3-4399-91ea-d3e45f13aa25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.9427ms
Feb 25 09:39:02.537: INFO: Pod "pod-projected-configmaps-262bb94a-10e3-4399-91ea-d3e45f13aa25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014734881s
STEP: Saw pod success
Feb 25 09:39:02.537: INFO: Pod "pod-projected-configmaps-262bb94a-10e3-4399-91ea-d3e45f13aa25" satisfied condition "Succeeded or Failed"
Feb 25 09:39:02.542: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-projected-configmaps-262bb94a-10e3-4399-91ea-d3e45f13aa25 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 09:39:02.565: INFO: Waiting for pod pod-projected-configmaps-262bb94a-10e3-4399-91ea-d3e45f13aa25 to disappear
Feb 25 09:39:02.569: INFO: Pod pod-projected-configmaps-262bb94a-10e3-4399-91ea-d3e45f13aa25 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:02.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9446" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":621,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:02.584: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 25 09:39:02.757: INFO: Waiting up to 5m0s for pod "downward-api-8fc8b183-4766-4d21-bd28-509a22904535" in namespace "downward-api-6820" to be "Succeeded or Failed"
Feb 25 09:39:02.764: INFO: Pod "downward-api-8fc8b183-4766-4d21-bd28-509a22904535": Phase="Pending", Reason="", readiness=false. Elapsed: 7.330979ms
Feb 25 09:39:04.773: INFO: Pod "downward-api-8fc8b183-4766-4d21-bd28-509a22904535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016225808s
STEP: Saw pod success
Feb 25 09:39:04.773: INFO: Pod "downward-api-8fc8b183-4766-4d21-bd28-509a22904535" satisfied condition "Succeeded or Failed"
Feb 25 09:39:04.777: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod downward-api-8fc8b183-4766-4d21-bd28-509a22904535 container dapi-container: <nil>
STEP: delete the pod
Feb 25 09:39:04.805: INFO: Waiting for pod downward-api-8fc8b183-4766-4d21-bd28-509a22904535 to disappear
Feb 25 09:39:04.809: INFO: Pod downward-api-8fc8b183-4766-4d21-bd28-509a22904535 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:04.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6820" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":632,"failed":0}
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:04.822: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:39:04.986: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3f45ef65-1225-4a3c-9b75-44257f47a546" in namespace "security-context-test-9749" to be "Succeeded or Failed"
Feb 25 09:39:04.992: INFO: Pod "alpine-nnp-false-3f45ef65-1225-4a3c-9b75-44257f47a546": Phase="Pending", Reason="", readiness=false. Elapsed: 5.932748ms
Feb 25 09:39:07.001: INFO: Pod "alpine-nnp-false-3f45ef65-1225-4a3c-9b75-44257f47a546": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014926383s
Feb 25 09:39:09.007: INFO: Pod "alpine-nnp-false-3f45ef65-1225-4a3c-9b75-44257f47a546": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021495462s
Feb 25 09:39:09.007: INFO: Pod "alpine-nnp-false-3f45ef65-1225-4a3c-9b75-44257f47a546" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:09.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9749" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":35,"skipped":634,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:09.037: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:39:09.186: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 25 09:39:09.206: INFO: The status of Pod pod-logs-websocket-cc09a28f-3d14-4b90-9fde-a29225867889 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:39:11.214: INFO: The status of Pod pod-logs-websocket-cc09a28f-3d14-4b90-9fde-a29225867889 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:11.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7279" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":648,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:11.249: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:39:11.638: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 09:39:13.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378751, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378751, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378751, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378751, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:39:16.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:16.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3283" for this suite.
STEP: Destroying namespace "webhook-3283-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.557 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":37,"skipped":657,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:16.807: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-t5bx
STEP: Creating a pod to test atomic-volume-subpath
Feb 25 09:39:16.989: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-t5bx" in namespace "subpath-6447" to be "Succeeded or Failed"
Feb 25 09:39:16.997: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.444744ms
Feb 25 09:39:19.003: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 2.013782252s
Feb 25 09:39:21.014: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 4.024620412s
Feb 25 09:39:23.026: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 6.036660843s
Feb 25 09:39:25.036: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 8.046671443s
Feb 25 09:39:27.046: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 10.056259536s
Feb 25 09:39:29.053: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 12.063357372s
Feb 25 09:39:31.063: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 14.073958721s
Feb 25 09:39:33.074: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 16.084386619s
Feb 25 09:39:35.085: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 18.095103658s
Feb 25 09:39:37.095: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Running", Reason="", readiness=true. Elapsed: 20.105027783s
Feb 25 09:39:39.102: INFO: Pod "pod-subpath-test-configmap-t5bx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.112166424s
STEP: Saw pod success
Feb 25 09:39:39.102: INFO: Pod "pod-subpath-test-configmap-t5bx" satisfied condition "Succeeded or Failed"
Feb 25 09:39:39.106: INFO: Trying to get logs from node ip-10-1-2-231.eu-central-1.compute.internal pod pod-subpath-test-configmap-t5bx container test-container-subpath-configmap-t5bx: <nil>
STEP: delete the pod
Feb 25 09:39:39.139: INFO: Waiting for pod pod-subpath-test-configmap-t5bx to disappear
Feb 25 09:39:39.143: INFO: Pod pod-subpath-test-configmap-t5bx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-t5bx
Feb 25 09:39:39.143: INFO: Deleting pod "pod-subpath-test-configmap-t5bx" in namespace "subpath-6447"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:39.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6447" for this suite.

• [SLOW TEST:22.354 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":38,"skipped":660,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:39.161: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 25 09:39:49.444: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0225 09:39:49.444316      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 25 09:39:49.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-2psjc" in namespace "gc-2320"
Feb 25 09:39:49.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vj84" in namespace "gc-2320"
Feb 25 09:39:49.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-457zj" in namespace "gc-2320"
Feb 25 09:39:49.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7xnk" in namespace "gc-2320"
Feb 25 09:39:49.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-lm7xw" in namespace "gc-2320"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:39:49.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2320" for this suite.

• [SLOW TEST:10.391 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":39,"skipped":667,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:39:49.551: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-8804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Feb 25 09:39:49.708: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 25 09:40:49.762: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:40:49.766: INFO: Starting informer...
STEP: Starting pod...
Feb 25 09:40:49.997: INFO: Pod is running on ip-10-1-2-67.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 25 09:40:50.025: INFO: Pod wasn't evicted. Proceeding
Feb 25 09:40:50.025: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 25 09:42:05.069: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:05.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8804" for this suite.

• [SLOW TEST:135.545 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":40,"skipped":690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:05.098: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4173
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4173
STEP: creating replication controller externalsvc in namespace services-4173
I0225 09:42:05.296155      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4173, replica count: 2
I0225 09:42:08.349019      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 25 09:42:08.394: INFO: Creating new exec pod
Feb 25 09:42:10.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-4173 exec execpodjcqtv -- /bin/sh -x -c nslookup nodeport-service.services-4173.svc.cluster.local'
Feb 25 09:42:10.614: INFO: stderr: "+ nslookup nodeport-service.services-4173.svc.cluster.local\n"
Feb 25 09:42:10.614: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nnodeport-service.services-4173.svc.cluster.local\tcanonical name = externalsvc.services-4173.svc.cluster.local.\nName:\texternalsvc.services-4173.svc.cluster.local\nAddress: 172.31.116.224\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4173, will wait for the garbage collector to delete the pods
Feb 25 09:42:10.682: INFO: Deleting ReplicationController externalsvc took: 12.056822ms
Feb 25 09:42:10.783: INFO: Terminating ReplicationController externalsvc pods took: 100.748624ms
Feb 25 09:42:12.812: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:12.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4173" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.747 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":41,"skipped":722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:12.846: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Feb 25 09:42:13.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 create -f -'
Feb 25 09:42:13.586: INFO: stderr: ""
Feb 25 09:42:13.586: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 25 09:42:13.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 09:42:13.674: INFO: stderr: ""
Feb 25 09:42:13.674: INFO: stdout: "update-demo-nautilus-ftf9m update-demo-nautilus-zz5gd "
Feb 25 09:42:13.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-ftf9m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 09:42:13.722: INFO: stderr: ""
Feb 25 09:42:13.722: INFO: stdout: ""
Feb 25 09:42:13.722: INFO: update-demo-nautilus-ftf9m is created but not running
Feb 25 09:42:18.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 09:42:18.777: INFO: stderr: ""
Feb 25 09:42:18.777: INFO: stdout: "update-demo-nautilus-ftf9m update-demo-nautilus-zz5gd "
Feb 25 09:42:18.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-ftf9m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 09:42:18.839: INFO: stderr: ""
Feb 25 09:42:18.839: INFO: stdout: "true"
Feb 25 09:42:18.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-ftf9m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 25 09:42:18.895: INFO: stderr: ""
Feb 25 09:42:18.895: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 25 09:42:18.895: INFO: validating pod update-demo-nautilus-ftf9m
Feb 25 09:42:18.902: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 25 09:42:18.902: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 25 09:42:18.902: INFO: update-demo-nautilus-ftf9m is verified up and running
Feb 25 09:42:18.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-zz5gd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 09:42:18.963: INFO: stderr: ""
Feb 25 09:42:18.963: INFO: stdout: "true"
Feb 25 09:42:18.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-zz5gd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 25 09:42:19.027: INFO: stderr: ""
Feb 25 09:42:19.027: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 25 09:42:19.027: INFO: validating pod update-demo-nautilus-zz5gd
Feb 25 09:42:19.035: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 25 09:42:19.035: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 25 09:42:19.035: INFO: update-demo-nautilus-zz5gd is verified up and running
STEP: scaling down the replication controller
Feb 25 09:42:19.036: INFO: scanned /root for discovery docs: <nil>
Feb 25 09:42:19.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 25 09:42:20.123: INFO: stderr: ""
Feb 25 09:42:20.123: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 25 09:42:20.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 09:42:20.205: INFO: stderr: ""
Feb 25 09:42:20.205: INFO: stdout: "update-demo-nautilus-ftf9m update-demo-nautilus-zz5gd "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 25 09:42:25.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 09:42:25.263: INFO: stderr: ""
Feb 25 09:42:25.263: INFO: stdout: "update-demo-nautilus-zz5gd "
Feb 25 09:42:25.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-zz5gd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 09:42:25.325: INFO: stderr: ""
Feb 25 09:42:25.325: INFO: stdout: "true"
Feb 25 09:42:25.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-zz5gd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 25 09:42:25.381: INFO: stderr: ""
Feb 25 09:42:25.381: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 25 09:42:25.381: INFO: validating pod update-demo-nautilus-zz5gd
Feb 25 09:42:25.387: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 25 09:42:25.387: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 25 09:42:25.387: INFO: update-demo-nautilus-zz5gd is verified up and running
STEP: scaling up the replication controller
Feb 25 09:42:25.389: INFO: scanned /root for discovery docs: <nil>
Feb 25 09:42:25.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 25 09:42:26.493: INFO: stderr: ""
Feb 25 09:42:26.493: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 25 09:42:26.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 09:42:26.549: INFO: stderr: ""
Feb 25 09:42:26.549: INFO: stdout: "update-demo-nautilus-bfszt update-demo-nautilus-zz5gd "
Feb 25 09:42:26.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-bfszt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 09:42:26.610: INFO: stderr: ""
Feb 25 09:42:26.610: INFO: stdout: ""
Feb 25 09:42:26.610: INFO: update-demo-nautilus-bfszt is created but not running
Feb 25 09:42:31.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 09:42:31.676: INFO: stderr: ""
Feb 25 09:42:31.676: INFO: stdout: "update-demo-nautilus-bfszt update-demo-nautilus-zz5gd "
Feb 25 09:42:31.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-bfszt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 09:42:31.734: INFO: stderr: ""
Feb 25 09:42:31.734: INFO: stdout: "true"
Feb 25 09:42:31.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-bfszt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 25 09:42:31.780: INFO: stderr: ""
Feb 25 09:42:31.780: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 25 09:42:31.780: INFO: validating pod update-demo-nautilus-bfszt
Feb 25 09:42:31.788: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 25 09:42:31.788: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 25 09:42:31.788: INFO: update-demo-nautilus-bfszt is verified up and running
Feb 25 09:42:31.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-zz5gd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 09:42:31.844: INFO: stderr: ""
Feb 25 09:42:31.844: INFO: stdout: "true"
Feb 25 09:42:31.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods update-demo-nautilus-zz5gd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 25 09:42:31.893: INFO: stderr: ""
Feb 25 09:42:31.893: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 25 09:42:31.893: INFO: validating pod update-demo-nautilus-zz5gd
Feb 25 09:42:31.899: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 25 09:42:31.899: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 25 09:42:31.899: INFO: update-demo-nautilus-zz5gd is verified up and running
STEP: using delete to clean up resources
Feb 25 09:42:31.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 delete --grace-period=0 --force -f -'
Feb 25 09:42:31.959: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:42:31.959: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 25 09:42:31.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get rc,svc -l name=update-demo --no-headers'
Feb 25 09:42:32.028: INFO: stderr: "No resources found in kubectl-8702 namespace.\n"
Feb 25 09:42:32.028: INFO: stdout: ""
Feb 25 09:42:32.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8702 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 25 09:42:32.087: INFO: stderr: ""
Feb 25 09:42:32.087: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:32.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8702" for this suite.

• [SLOW TEST:19.255 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":42,"skipped":764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:32.102: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:42:32.638: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 09:42:34.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378952, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378952, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378952, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378952, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:42:37.683: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:42:37.689: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1423-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:45.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9872" for this suite.
STEP: Destroying namespace "webhook-9872-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.769 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":43,"skipped":791,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:45.874: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-6552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 25 09:42:46.066: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 25 09:42:46.073: INFO: starting watch
STEP: patching
STEP: updating
Feb 25 09:42:46.092: INFO: waiting for watch events with expected annotations
Feb 25 09:42:46.092: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:46.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6552" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":44,"skipped":804,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:46.147: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-28ae07bc-a289-40b1-ade1-80f82803deca
STEP: Creating a pod to test consume secrets
Feb 25 09:42:46.334: INFO: Waiting up to 5m0s for pod "pod-secrets-9c8ca9c2-77f5-4efb-af9d-3800ad626f07" in namespace "secrets-2761" to be "Succeeded or Failed"
Feb 25 09:42:46.346: INFO: Pod "pod-secrets-9c8ca9c2-77f5-4efb-af9d-3800ad626f07": Phase="Pending", Reason="", readiness=false. Elapsed: 12.289183ms
Feb 25 09:42:48.354: INFO: Pod "pod-secrets-9c8ca9c2-77f5-4efb-af9d-3800ad626f07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020507381s
STEP: Saw pod success
Feb 25 09:42:48.354: INFO: Pod "pod-secrets-9c8ca9c2-77f5-4efb-af9d-3800ad626f07" satisfied condition "Succeeded or Failed"
Feb 25 09:42:48.359: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-secrets-9c8ca9c2-77f5-4efb-af9d-3800ad626f07 container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 09:42:48.391: INFO: Waiting for pod pod-secrets-9c8ca9c2-77f5-4efb-af9d-3800ad626f07 to disappear
Feb 25 09:42:48.395: INFO: Pod pod-secrets-9c8ca9c2-77f5-4efb-af9d-3800ad626f07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:48.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2761" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":811,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:48.408: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 25 09:42:48.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2113  9aa1def4-c988-4869-8a84-3541d6e31197 13503 0 2022-02-25 09:42:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-02-25 09:42:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 09:42:48.597: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2113  9aa1def4-c988-4869-8a84-3541d6e31197 13504 0 2022-02-25 09:42:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-02-25 09:42:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:48.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2113" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":46,"skipped":812,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:48.611: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:42:49.135: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 09:42:51.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378969, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378969, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378969, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781378969, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:42:54.191: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:42:54.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2677" for this suite.
STEP: Destroying namespace "webhook-2677-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.732 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":47,"skipped":822,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:42:54.344: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 25 09:42:54.527: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:42:56.535: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 25 09:42:56.558: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:42:58.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:00.569: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:02.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:04.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:06.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:08.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:10.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:12.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:14.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:16.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:18.570: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:20.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:22.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:24.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:26.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:28.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:30.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:32.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:34.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:36.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:38.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:40.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:42.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:44.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:46.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:48.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:50.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:52.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:54.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:56.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:43:58.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:00.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:02.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:04.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:06.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:08.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:10.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:12.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:14.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:16.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:18.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:20.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:22.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:24.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:26.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:28.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:30.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:32.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:34.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:36.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:38.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:40.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:42.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:44.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:46.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:48.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:50.565: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:52.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:54.568: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:56.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:44:58.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:45:00.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:45:02.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:45:04.567: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:45:06.566: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:45:08.567: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 25 09:45:08.600: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 25 09:45:08.605: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 25 09:45:10.605: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 25 09:45:10.612: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 25 09:45:12.605: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 25 09:45:12.614: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:45:12.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6364" for this suite.

• [SLOW TEST:138.286 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":829,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:45:12.631: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-530
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 25 09:45:12.802: INFO: Waiting up to 5m0s for pod "pod-adf99356-dc78-43dc-88b8-e0bb8485af01" in namespace "emptydir-530" to be "Succeeded or Failed"
Feb 25 09:45:12.807: INFO: Pod "pod-adf99356-dc78-43dc-88b8-e0bb8485af01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.848155ms
Feb 25 09:45:14.817: INFO: Pod "pod-adf99356-dc78-43dc-88b8-e0bb8485af01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014362069s
STEP: Saw pod success
Feb 25 09:45:14.817: INFO: Pod "pod-adf99356-dc78-43dc-88b8-e0bb8485af01" satisfied condition "Succeeded or Failed"
Feb 25 09:45:14.821: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-adf99356-dc78-43dc-88b8-e0bb8485af01 container test-container: <nil>
STEP: delete the pod
Feb 25 09:45:14.852: INFO: Waiting for pod pod-adf99356-dc78-43dc-88b8-e0bb8485af01 to disappear
Feb 25 09:45:14.857: INFO: Pod pod-adf99356-dc78-43dc-88b8-e0bb8485af01 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:45:14.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-530" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":848,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:45:14.870: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:45:15.263: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:45:18.296: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:45:18.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2969" for this suite.
STEP: Destroying namespace "webhook-2969-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":50,"skipped":857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:45:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Feb 25 09:45:18.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 create -f -'
Feb 25 09:45:19.692: INFO: stderr: ""
Feb 25 09:45:19.692: INFO: stdout: "pod/pause created\n"
Feb 25 09:45:19.692: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 25 09:45:19.692: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9764" to be "running and ready"
Feb 25 09:45:19.700: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05497ms
Feb 25 09:45:21.709: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016844432s
Feb 25 09:45:21.709: INFO: Pod "pause" satisfied condition "running and ready"
Feb 25 09:45:21.709: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 25 09:45:21.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 label pods pause testing-label=testing-label-value'
Feb 25 09:45:21.782: INFO: stderr: ""
Feb 25 09:45:21.782: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 25 09:45:21.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 get pod pause -L testing-label'
Feb 25 09:45:21.834: INFO: stderr: ""
Feb 25 09:45:21.834: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 25 09:45:21.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 label pods pause testing-label-'
Feb 25 09:45:21.897: INFO: stderr: ""
Feb 25 09:45:21.897: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 25 09:45:21.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 get pod pause -L testing-label'
Feb 25 09:45:21.949: INFO: stderr: ""
Feb 25 09:45:21.949: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Feb 25 09:45:21.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 delete --grace-period=0 --force -f -'
Feb 25 09:45:22.015: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:45:22.015: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 25 09:45:22.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 get rc,svc -l name=pause --no-headers'
Feb 25 09:45:22.075: INFO: stderr: "No resources found in kubectl-9764 namespace.\n"
Feb 25 09:45:22.075: INFO: stdout: ""
Feb 25 09:45:22.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9764 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 25 09:45:22.131: INFO: stderr: ""
Feb 25 09:45:22.132: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:45:22.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9764" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":51,"skipped":896,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:45:22.145: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:45:22.298: INFO: Creating deployment "webserver-deployment"
Feb 25 09:45:22.305: INFO: Waiting for observed generation 1
Feb 25 09:45:24.320: INFO: Waiting for all required pods to come up
Feb 25 09:45:24.325: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 25 09:45:30.342: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 25 09:45:30.350: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 25 09:45:30.366: INFO: Updating deployment webserver-deployment
Feb 25 09:45:30.366: INFO: Waiting for observed generation 2
Feb 25 09:45:32.379: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 25 09:45:32.383: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 25 09:45:32.387: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 25 09:45:32.399: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 25 09:45:32.399: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 25 09:45:32.403: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 25 09:45:32.411: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 25 09:45:32.411: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 25 09:45:32.424: INFO: Updating deployment webserver-deployment
Feb 25 09:45:32.424: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 25 09:45:32.435: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 25 09:45:32.442: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 09:45:32.469: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6570  d6a6b211-6686-4121-a881-97ac3d1a29e5 14592 3 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00282ca58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-02-25 09:45:30 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-02-25 09:45:32 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 25 09:45:32.502: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-6570  7038ed59-aba9-4cd5-a409-34ee010a1ca7 14583 3 2022-02-25 09:45:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d6a6b211-6686-4121-a881-97ac3d1a29e5 0xc0006ab6e7 0xc0006ab6e8}] []  [{kube-controller-manager Update apps/v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6a6b211-6686-4121-a881-97ac3d1a29e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006ab7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 25 09:45:32.509: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 25 09:45:32.510: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-6570  f67939e8-2b33-4f8d-8820-3772ed881ab6 14581 3 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d6a6b211-6686-4121-a881-97ac3d1a29e5 0xc0006ab847 0xc0006ab848}] []  [{kube-controller-manager Update apps/v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6a6b211-6686-4121-a881-97ac3d1a29e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 09:45:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006ab998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 25 09:45:32.542: INFO: Pod "webserver-deployment-795d758f88-2jfps" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2jfps webserver-deployment-795d758f88- deployment-6570  cca0d959-e905-412d-b815-49263d644b76 14547 0 2022-02-25 09:45:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc00282ce77 0xc00282ce78}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbthw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbthw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:,StartTime:2022-02-25 09:45:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.543: INFO: Pod "webserver-deployment-795d758f88-572gb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-572gb webserver-deployment-795d758f88- deployment-6570  4037be86-52fd-431a-b75f-42c2ee8aa51b 14540 0 2022-02-25 09:45:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc00282d250 0xc00282d251}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkhr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkhr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.67,PodIP:,StartTime:2022-02-25 09:45:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.544: INFO: Pod "webserver-deployment-795d758f88-9f2c6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9f2c6 webserver-deployment-795d758f88- deployment-6570  6605abf3-e9f2-4910-b788-90e4df641cf2 14598 0 2022-02-25 09:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc00282d520 0xc00282d521}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cbtf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cbtf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-231.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.231,PodIP:,StartTime:2022-02-25 09:45:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.544: INFO: Pod "webserver-deployment-795d758f88-g8r7d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-g8r7d webserver-deployment-795d758f88- deployment-6570  8e2897f5-92af-4657-a48b-757a88345113 14566 0 2022-02-25 09:45:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc00282d890 0xc00282d891}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fn2jm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fn2jm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:,StartTime:2022-02-25 09:45:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.545: INFO: Pod "webserver-deployment-795d758f88-k96px" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k96px webserver-deployment-795d758f88- deployment-6570  50847cd4-e245-4b6c-af02-c58137b8bb90 14562 0 2022-02-25 09:45:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc00282dc20 0xc00282dc21}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llqtk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llqtk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.67,PodIP:,StartTime:2022-02-25 09:45:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.545: INFO: Pod "webserver-deployment-795d758f88-p4wzr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-p4wzr webserver-deployment-795d758f88- deployment-6570  fd1bf693-aaf5-40c8-b352-8ce102d2ba18 14541 0 2022-02-25 09:45:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc00282df00 0xc00282df01}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gkp7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gkp7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-231.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.231,PodIP:,StartTime:2022-02-25 09:45:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.546: INFO: Pod "webserver-deployment-795d758f88-wfjnq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wfjnq webserver-deployment-795d758f88- deployment-6570  463ff502-0d74-408a-845b-3eb9ef03f801 14600 0 2022-02-25 09:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc002c9a640 0xc002c9a641}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bvp5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvp5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.546: INFO: Pod "webserver-deployment-795d758f88-wzm7t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wzm7t webserver-deployment-795d758f88- deployment-6570  c8129224-044d-4e27-93e4-298b5810286c 14599 0 2022-02-25 09:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7038ed59-aba9-4cd5-a409-34ee010a1ca7 0xc002c9ab97 0xc002c9ab98}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7038ed59-aba9-4cd5-a409-34ee010a1ca7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zdp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zdp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.598: INFO: Pod "webserver-deployment-847dcfb7fb-2wsrj" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2wsrj webserver-deployment-847dcfb7fb- deployment-6570  4e393ba9-8f3a-4d21-a0d8-78e8028df90f 14487 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c9b137 0xc002c9b138}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.25.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dj69x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dj69x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-231.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.231,PodIP:10.10.25.104,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://66d3d7380213b54919613424e5f5f9e9dfb4bd1642982f4a91b24f2d367401a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.25.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.631: INFO: Pod "webserver-deployment-847dcfb7fb-5bdbf" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5bdbf webserver-deployment-847dcfb7fb- deployment-6570  b4547481-cd7b-4937-bacd-43256dd2f873 14462 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c9b740 0xc002c9b741}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.41.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vgwlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vgwlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.67,PodIP:10.10.41.45,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://3d4e3c1a716370c54c45d8556059e8ad80a04d550ec34f731bb00e130cbbc64d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.41.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.632: INFO: Pod "webserver-deployment-847dcfb7fb-8v2s2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8v2s2 webserver-deployment-847dcfb7fb- deployment-6570  efae1dfd-9eb8-493e-8148-98d08c064ab1 14601 0 2022-02-25 09:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c9b910 0xc002c9b911}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7474b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7474b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.632: INFO: Pod "webserver-deployment-847dcfb7fb-dzl64" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-dzl64 webserver-deployment-847dcfb7fb- deployment-6570  511a3fcc-9df1-4184-ac52-d72762d7ecff 14474 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c9ba57 0xc002c9ba58}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.47.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fxkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fxkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.67,PodIP:10.10.47.213,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://161aeb569b0eaf4fabcf573511896fe31c2e9ecb39a971cfc0b7af651629a911,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.47.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.633: INFO: Pod "webserver-deployment-847dcfb7fb-gn7j6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gn7j6 webserver-deployment-847dcfb7fb- deployment-6570  b303019a-1916-485f-b4fd-a032577802f9 14490 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c9bc30 0xc002c9bc31}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.37.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8psbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8psbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-231.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.231,PodIP:10.10.37.195,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://b94e9ea9eebb3e32fbee94056efe632c9595f5e13677d3f9b9df4d59ab216f12,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.37.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.635: INFO: Pod "webserver-deployment-847dcfb7fb-hfpqq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hfpqq webserver-deployment-847dcfb7fb- deployment-6570  eef25ebe-9d3c-4499-a5a4-6867dad525f0 14495 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c9be00 0xc002c9be01}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.4.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kmlf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kmlf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-231.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.231,PodIP:10.10.4.129,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://cc8e4ae1a8bfcc0da282d6d53f3143c1b1fe295aea984cf9d83395393c593b87,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.4.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.636: INFO: Pod "webserver-deployment-847dcfb7fb-jtzwb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jtzwb webserver-deployment-847dcfb7fb- deployment-6570  add5c1fd-7dc9-4267-961b-edce26207312 14499 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c9bfd0 0xc002c9bfd1}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.5.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6c2hp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6c2hp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:10.10.5.136,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://416e6b7fe3b2abb06545918963f6427b7f1302bab793223619686be009861254,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.5.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.636: INFO: Pod "webserver-deployment-847dcfb7fb-ppcx4" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ppcx4 webserver-deployment-847dcfb7fb- deployment-6570  68bd32ce-3bc1-4cef-b11d-2cd60fae3060 14597 0 2022-02-25 09:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c101a0 0xc002c101a1}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djvf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djvf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.639: INFO: Pod "webserver-deployment-847dcfb7fb-s48wz" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-s48wz webserver-deployment-847dcfb7fb- deployment-6570  5b6bfbad-7d16-4f1b-b723-854d0e7e11de 14596 0 2022-02-25 09:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c102f0 0xc002c102f1}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wszmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wszmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.640: INFO: Pod "webserver-deployment-847dcfb7fb-sh6lf" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-sh6lf webserver-deployment-847dcfb7fb- deployment-6570  904b4172-7c34-494c-8e23-59c32b17c7b4 14587 0 2022-02-25 09:45:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c10440 0xc002c10441}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qj5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qj5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.642: INFO: Pod "webserver-deployment-847dcfb7fb-vm6fr" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vm6fr webserver-deployment-847dcfb7fb- deployment-6570  11d55917-5872-458a-bf06-8fe243bd02c6 14505 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c10590 0xc002c10591}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.10.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phsl9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phsl9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:10.10.10.190,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://8a5d9e32375b9461333512b2b33db88d4fb6e59eecc2d579387342d09a8a3401,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.10.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:45:32.643: INFO: Pod "webserver-deployment-847dcfb7fb-zbttf" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-zbttf webserver-deployment-847dcfb7fb- deployment-6570  25e5ce5b-3bb7-4bf0-b2f6-340aa2887d88 14502 0 2022-02-25 09:45:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f67939e8-2b33-4f8d-8820-3772ed881ab6 0xc002c10760 0xc002c10761}] []  [{kube-controller-manager Update v1 2022-02-25 09:45:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67939e8-2b33-4f8d-8820-3772ed881ab6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:45:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.6.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7f9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7f9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:45:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:10.10.6.14,StartTime:2022-02-25 09:45:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:45:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://67973ae0c3a0f9b3feebf666ee344191da9a77e03ea5ac29df96c6259340fa8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.6.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:45:32.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6570" for this suite.

• [SLOW TEST:10.592 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":52,"skipped":906,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:45:32.738: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9631
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-6530fb05-926c-4e38-acc5-fdbeb1815dac
STEP: Creating a pod to test consume secrets
Feb 25 09:45:32.915: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd" in namespace "projected-9631" to be "Succeeded or Failed"
Feb 25 09:45:32.921: INFO: Pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.768294ms
Feb 25 09:45:34.929: INFO: Pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014341163s
Feb 25 09:45:36.938: INFO: Pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023502165s
Feb 25 09:45:38.947: INFO: Pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032282555s
Feb 25 09:45:40.954: INFO: Pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038932504s
Feb 25 09:45:42.964: INFO: Pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.049015746s
STEP: Saw pod success
Feb 25 09:45:42.964: INFO: Pod "pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd" satisfied condition "Succeeded or Failed"
Feb 25 09:45:42.968: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 25 09:45:42.996: INFO: Waiting for pod pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd to disappear
Feb 25 09:45:43.001: INFO: Pod pod-projected-secrets-873eef56-e37a-4bfc-a331-c088f05de6fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:45:43.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9631" for this suite.

• [SLOW TEST:10.276 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":922,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:45:43.014: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8470
Feb 25 09:45:43.186: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:45:45.196: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:45:47.194: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 25 09:45:47.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 25 09:45:47.340: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 25 09:45:47.340: INFO: stdout: "iptables"
Feb 25 09:45:47.340: INFO: proxyMode: iptables
Feb 25 09:45:47.355: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 25 09:45:47.360: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8470
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8470
I0225 09:45:47.388202      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8470, replica count: 3
I0225 09:45:50.445590      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 09:45:50.460: INFO: Creating new exec pod
Feb 25 09:45:53.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb 25 09:45:53.627: INFO: stderr: "+ + ncecho hostName\n -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 25 09:45:53.627: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:45:53.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.117.232 80'
Feb 25 09:45:53.760: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.117.232 80\nConnection to 172.31.117.232 80 port [tcp/http] succeeded!\n"
Feb 25 09:45:53.761: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:45:53.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.27 31379'
Feb 25 09:45:53.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.27 31379\nConnection to 10.1.2.27 31379 port [tcp/*] succeeded!\n"
Feb 25 09:45:53.906: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:45:53.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.67 31379'
Feb 25 09:45:54.031: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.67 31379\nConnection to 10.1.2.67 31379 port [tcp/*] succeeded!\n"
Feb 25 09:45:54.031: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:45:54.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.1.2.231:31379/ ; done'
Feb 25 09:45:54.225: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n"
Feb 25 09:45:54.225: INFO: stdout: "\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s\naffinity-nodeport-timeout-ttm5s"
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Received response from host: affinity-nodeport-timeout-ttm5s
Feb 25 09:45:54.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.1.2.231:31379/'
Feb 25 09:45:54.350: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n"
Feb 25 09:45:54.350: INFO: stdout: "affinity-nodeport-timeout-ttm5s"
Feb 25 09:46:14.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.1.2.231:31379/'
Feb 25 09:46:14.518: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n"
Feb 25 09:46:14.518: INFO: stdout: "affinity-nodeport-timeout-ttm5s"
Feb 25 09:46:34.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8470 exec execpod-affinityzh8s4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.1.2.231:31379/'
Feb 25 09:46:34.682: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.1.2.231:31379/\n"
Feb 25 09:46:34.682: INFO: stdout: "affinity-nodeport-timeout-4cbgx"
Feb 25 09:46:34.682: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8470, will wait for the garbage collector to delete the pods
Feb 25 09:46:34.788: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 8.494408ms
Feb 25 09:46:34.888: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.409455ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:46:36.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8470" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:53.621 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":54,"skipped":926,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:46:36.635: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3717
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3717
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 25 09:46:36.785: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 25 09:46:36.836: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:46:38.844: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:40.843: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:42.845: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:44.847: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:46.845: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:48.846: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:50.843: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:52.845: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:54.846: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 09:46:56.844: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 25 09:46:56.857: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 25 09:46:56.865: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 25 09:46:58.899: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 25 09:46:58.899: INFO: Breadth first check of 10.10.4.129 on host 10.1.2.231...
Feb 25 09:46:58.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.46.110:9080/dial?request=hostname&protocol=http&host=10.10.4.129&port=8083&tries=1'] Namespace:pod-network-test-3717 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:46:58.903: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:46:59.051: INFO: Waiting for responses: map[]
Feb 25 09:46:59.051: INFO: reached 10.10.4.129 after 0/1 tries
Feb 25 09:46:59.051: INFO: Breadth first check of 10.10.6.14 on host 10.1.2.27...
Feb 25 09:46:59.056: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.46.110:9080/dial?request=hostname&protocol=http&host=10.10.6.14&port=8083&tries=1'] Namespace:pod-network-test-3717 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:46:59.056: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:46:59.142: INFO: Waiting for responses: map[]
Feb 25 09:46:59.142: INFO: reached 10.10.6.14 after 0/1 tries
Feb 25 09:46:59.142: INFO: Breadth first check of 10.10.40.2 on host 10.1.2.67...
Feb 25 09:46:59.147: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.46.110:9080/dial?request=hostname&protocol=http&host=10.10.40.2&port=8083&tries=1'] Namespace:pod-network-test-3717 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 09:46:59.147: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 09:46:59.226: INFO: Waiting for responses: map[]
Feb 25 09:46:59.226: INFO: reached 10.10.40.2 after 0/1 tries
Feb 25 09:46:59.226: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:46:59.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3717" for this suite.

• [SLOW TEST:22.608 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":936,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:46:59.243: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5579
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:46:59.415: INFO: created pod
Feb 25 09:46:59.415: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5579" to be "Succeeded or Failed"
Feb 25 09:46:59.424: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.250424ms
Feb 25 09:47:01.433: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017922177s
Feb 25 09:47:03.443: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 4.028146023s
Feb 25 09:47:05.452: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 6.037159889s
Feb 25 09:47:07.462: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.046683262s
STEP: Saw pod success
Feb 25 09:47:07.462: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 25 09:47:37.462: INFO: polling logs
Feb 25 09:47:37.477: INFO: Pod logs: 
2022/02/25 09:47:00 OK: Got token
2022/02/25 09:47:00 validating with in-cluster discovery
2022/02/25 09:47:00 OK: got issuer https://api.fx5p6.k8s.gaia.eu-central-1.aws.gigantic.io
2022/02/25 09:47:00 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.fx5p6.k8s.gaia.eu-central-1.aws.gigantic.io", Subject:"system:serviceaccount:svcaccounts-5579:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1645783019, NotBefore:1645782419, IssuedAt:1645782419, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5579", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e585b163-439b-4ae2-9776-a7abb78a1ccf"}}}
2022/02/25 09:47:05 OK: Constructed OIDC provider for issuer https://api.fx5p6.k8s.gaia.eu-central-1.aws.gigantic.io
2022/02/25 09:47:05 OK: Validated signature on JWT
2022/02/25 09:47:05 OK: Got valid claims from token!
2022/02/25 09:47:05 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.fx5p6.k8s.gaia.eu-central-1.aws.gigantic.io", Subject:"system:serviceaccount:svcaccounts-5579:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1645783019, NotBefore:1645782419, IssuedAt:1645782419, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5579", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e585b163-439b-4ae2-9776-a7abb78a1ccf"}}}

Feb 25 09:47:37.477: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:37.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5579" for this suite.

• [SLOW TEST:38.267 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":56,"skipped":952,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:37.511: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-274/configmap-test-a1f2fb69-3d4b-4006-9a8f-28a9c9275d2e
STEP: Creating a pod to test consume configMaps
Feb 25 09:47:37.678: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a" in namespace "configmap-274" to be "Succeeded or Failed"
Feb 25 09:47:37.685: INFO: Pod "pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.532263ms
Feb 25 09:47:39.694: INFO: Pod "pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015310425s
Feb 25 09:47:41.702: INFO: Pod "pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023825379s
STEP: Saw pod success
Feb 25 09:47:41.702: INFO: Pod "pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a" satisfied condition "Succeeded or Failed"
Feb 25 09:47:41.707: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a container env-test: <nil>
STEP: delete the pod
Feb 25 09:47:41.738: INFO: Waiting for pod pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a to disappear
Feb 25 09:47:41.742: INFO: Pod pod-configmaps-e9fad3d8-9aaa-45c5-b633-a2714708eb7a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:41.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-274" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":965,"failed":0}
SSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:41.755: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7174
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-7174/secret-test-9425de0f-8ee4-432d-baeb-b3f9c67ab0ef
STEP: Creating a pod to test consume secrets
Feb 25 09:47:41.929: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4a9ddb7-b7c9-499d-96d1-af5f45c0266f" in namespace "secrets-7174" to be "Succeeded or Failed"
Feb 25 09:47:41.935: INFO: Pod "pod-configmaps-f4a9ddb7-b7c9-499d-96d1-af5f45c0266f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.587881ms
Feb 25 09:47:43.944: INFO: Pod "pod-configmaps-f4a9ddb7-b7c9-499d-96d1-af5f45c0266f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014344208s
STEP: Saw pod success
Feb 25 09:47:43.944: INFO: Pod "pod-configmaps-f4a9ddb7-b7c9-499d-96d1-af5f45c0266f" satisfied condition "Succeeded or Failed"
Feb 25 09:47:43.948: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-configmaps-f4a9ddb7-b7c9-499d-96d1-af5f45c0266f container env-test: <nil>
STEP: delete the pod
Feb 25 09:47:43.971: INFO: Waiting for pod pod-configmaps-f4a9ddb7-b7c9-499d-96d1-af5f45c0266f to disappear
Feb 25 09:47:43.976: INFO: Pod pod-configmaps-f4a9ddb7-b7c9-499d-96d1-af5f45c0266f no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:43.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7174" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:43.991: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-144
STEP: creating service affinity-clusterip-transition in namespace services-144
STEP: creating replication controller affinity-clusterip-transition in namespace services-144
I0225 09:47:44.166973      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-144, replica count: 3
I0225 09:47:47.223548      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 09:47:47.236: INFO: Creating new exec pod
Feb 25 09:47:50.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-144 exec execpod-affinitynrh75 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb 25 09:47:50.422: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 25 09:47:50.422: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:47:50.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-144 exec execpod-affinitynrh75 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.43.207 80'
Feb 25 09:47:50.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.43.207 80\nConnection to 172.31.43.207 80 port [tcp/http] succeeded!\n"
Feb 25 09:47:50.553: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 09:47:50.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-144 exec execpod-affinitynrh75 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.43.207:80/ ; done'
Feb 25 09:47:50.774: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n"
Feb 25 09:47:50.774: INFO: stdout: "\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-7h5jc\naffinity-clusterip-transition-jsbfx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-jsbfx\naffinity-clusterip-transition-jsbfx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-7h5jc\naffinity-clusterip-transition-7h5jc\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-7h5jc\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-7h5jc\naffinity-clusterip-transition-7h5jc"
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-7h5jc
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-jsbfx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-jsbfx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-jsbfx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-7h5jc
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-7h5jc
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-7h5jc
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-7h5jc
Feb 25 09:47:50.774: INFO: Received response from host: affinity-clusterip-transition-7h5jc
Feb 25 09:47:50.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-144 exec execpod-affinitynrh75 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.43.207:80/ ; done'
Feb 25 09:47:50.993: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.43.207:80/\n"
Feb 25 09:47:50.993: INFO: stdout: "\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx\naffinity-clusterip-transition-kzmdx"
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Received response from host: affinity-clusterip-transition-kzmdx
Feb 25 09:47:50.993: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-144, will wait for the garbage collector to delete the pods
Feb 25 09:47:51.078: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.68401ms
Feb 25 09:47:51.179: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.740669ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:53.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-144" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.337 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":59,"skipped":1044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:53.329: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 25 09:47:53.993: INFO: starting watch
STEP: patching
STEP: updating
Feb 25 09:47:54.013: INFO: waiting for watch events with expected annotations
Feb 25 09:47:54.013: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:54.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-922" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":60,"skipped":1087,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:54.112: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 25 09:47:56.302: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:56.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5226" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1089,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:56.348: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 25 09:47:56.502: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 25 09:47:56.513: INFO: Waiting for terminating namespaces to be deleted...
Feb 25 09:47:56.517: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-231.eu-central-1.compute.internal before test
Feb 25 09:47:56.527: INFO: aws-node-s6sb8 from kube-system started at 2022-02-25 09:05:49 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 09:47:56.528: INFO: calico-node-bdc87 from kube-system started at 2022-02-25 09:05:49 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 09:47:56.528: INFO: cert-exporter-daemonset-4nlmn from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:47:56.528: INFO: coredns-6dc8c559ff-q8x59 from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container coredns ready: true, restart count 0
Feb 25 09:47:56.528: INFO: ebs-csi-node-vnqtd from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 09:47:56.528: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 09:47:56.528: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 09:47:56.528: INFO: kiam-agent-w9stq from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container kiam-agent ready: true, restart count 3
Feb 25 09:47:56.528: INFO: kiam-watchdog-rhftg from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 09:47:56.528: INFO: kube-proxy-882gm from kube-system started at 2022-02-25 09:06:05 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 09:47:56.528: INFO: kube-state-metrics-v2-3-0-6bd4cb8c9c-zlddp from kube-system started at 2022-02-25 09:11:16 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 25 09:47:56.528: INFO: net-exporter-rn29g from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 09:47:56.528: INFO: node-exporter-v1-3-1-qrh8m from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 09:47:56.528: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-qpjvm from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 09:47:56.528: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 09:47:56.528: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 09:47:56.528: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-27.eu-central-1.compute.internal before test
Feb 25 09:47:56.539: INFO: aws-node-xhhxn from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 09:47:56.539: INFO: calico-node-bql82 from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 09:47:56.539: INFO: cert-exporter-daemonset-gzc9c from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:47:56.539: INFO: cert-exporter-deployment-b69c5dd78-rt48g from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:47:56.539: INFO: cert-manager-webhook-6c68d58df7-v8gjv from kube-system started at 2022-02-25 09:14:12 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container webhook ready: true, restart count 0
Feb 25 09:47:56.539: INFO: coredns-6dc8c559ff-w8szs from kube-system started at 2022-02-25 09:11:15 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container coredns ready: true, restart count 0
Feb 25 09:47:56.539: INFO: ebs-csi-node-9n94t from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 09:47:56.539: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 09:47:56.539: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 09:47:56.539: INFO: external-dns-d56669c5f-pkknw from kube-system started at 2022-02-25 09:40:50 +0000 UTC (2 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container external-dns ready: true, restart count 0
Feb 25 09:47:56.539: INFO: 	Container external-dns-check-iam ready: true, restart count 0
Feb 25 09:47:56.539: INFO: kiam-agent-6nk9v from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container kiam-agent ready: true, restart count 2
Feb 25 09:47:56.539: INFO: kiam-watchdog-szdf9 from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 09:47:56.539: INFO: kube-proxy-wcpg5 from kube-system started at 2022-02-25 09:06:05 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 09:47:56.539: INFO: net-exporter-sf92w from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 09:47:56.539: INFO: node-exporter-v1-3-1-2zzkq from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 09:47:56.539: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-vhpzn from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:47:56.539: INFO: vertical-pod-autoscaler-recommender-7ffc6b994d-pkqc2 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:47:56.539: INFO: vertical-pod-autoscaler-updater-5c6675d987-wwf47 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:47:56.539: INFO: sonobuoy from sonobuoy started at 2022-02-25 09:27:07 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 25 09:47:56.539: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-ql7w7 from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 09:47:56.539: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 09:47:56.539: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 09:47:56.539: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-67.eu-central-1.compute.internal before test
Feb 25 09:47:56.549: INFO: aws-node-jg76c from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 09:47:56.549: INFO: calico-node-kttwk from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 09:47:56.549: INFO: cert-exporter-daemonset-9rbmr from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:47:56.549: INFO: cert-manager-cainjector-ff5c85897-tpzhd from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container cainjector ready: true, restart count 0
Feb 25 09:47:56.549: INFO: ebs-csi-node-t9n4m from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 09:47:56.549: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 09:47:56.549: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 09:47:56.549: INFO: kiam-agent-8nj67 from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container kiam-agent ready: true, restart count 3
Feb 25 09:47:56.549: INFO: kiam-watchdog-xc6qf from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 09:47:56.549: INFO: kube-proxy-scj7q from kube-system started at 2022-02-25 09:06:06 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 09:47:56.549: INFO: metrics-server-6dff979d99-nqcsg from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container metrics-server ready: true, restart count 0
Feb 25 09:47:56.549: INFO: net-exporter-x9rvx from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 09:47:56.549: INFO: node-exporter-v1-3-1-p97bn from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 09:47:56.549: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-m926h from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:47:56.549: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2tbqx from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 09:47:56.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 09:47:56.549: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16d6fe7156ab5288], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:57.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3263" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":62,"skipped":1092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:57.613: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4278
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-ed9fafd0-8f7f-4305-91a2-417a46fa0bf6
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:47:59.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4278" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1169,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:47:59.840: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 25 09:48:00.003: INFO: Waiting up to 5m0s for pod "downward-api-6de740d0-c7ec-42df-84df-e2c82c4df715" in namespace "downward-api-462" to be "Succeeded or Failed"
Feb 25 09:48:00.008: INFO: Pod "downward-api-6de740d0-c7ec-42df-84df-e2c82c4df715": Phase="Pending", Reason="", readiness=false. Elapsed: 4.879156ms
Feb 25 09:48:02.016: INFO: Pod "downward-api-6de740d0-c7ec-42df-84df-e2c82c4df715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012641086s
STEP: Saw pod success
Feb 25 09:48:02.016: INFO: Pod "downward-api-6de740d0-c7ec-42df-84df-e2c82c4df715" satisfied condition "Succeeded or Failed"
Feb 25 09:48:02.021: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod downward-api-6de740d0-c7ec-42df-84df-e2c82c4df715 container dapi-container: <nil>
STEP: delete the pod
Feb 25 09:48:02.048: INFO: Waiting for pod downward-api-6de740d0-c7ec-42df-84df-e2c82c4df715 to disappear
Feb 25 09:48:02.053: INFO: Pod downward-api-6de740d0-c7ec-42df-84df-e2c82c4df715 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:02.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-462" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:02.069: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8890
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Feb 25 09:48:02.240: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:27.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8890" for this suite.

• [SLOW TEST:25.155 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":65,"skipped":1201,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:27.224: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-9670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 25 09:48:27.413: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 25 09:48:27.420: INFO: starting watch
STEP: patching
STEP: updating
Feb 25 09:48:27.439: INFO: waiting for watch events with expected annotations
Feb 25 09:48:27.439: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:27.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9670" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":66,"skipped":1211,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:27.519: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Feb 25 09:48:27.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 25 09:48:27.739: INFO: stderr: ""
Feb 25 09:48:27.739: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Feb 25 09:48:27.739: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 25 09:48:27.739: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7134" to be "running and ready, or succeeded"
Feb 25 09:48:27.745: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.11019ms
Feb 25 09:48:29.753: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.013622177s
Feb 25 09:48:29.753: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 25 09:48:29.753: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 25 09:48:29.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 logs logs-generator logs-generator'
Feb 25 09:48:29.824: INFO: stderr: ""
Feb 25 09:48:29.825: INFO: stdout: "I0225 09:48:28.382504       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/sd4g 429\nI0225 09:48:28.582617       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/dvf 231\nI0225 09:48:28.783056       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/pf8 470\nI0225 09:48:28.983373       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/t7j6 242\nI0225 09:48:29.183478       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/dzs 340\nI0225 09:48:29.382782       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/ng76 565\nI0225 09:48:29.583088       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/w5t 261\nI0225 09:48:29.783406       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/hljc 386\n"
STEP: limiting log lines
Feb 25 09:48:29.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 logs logs-generator logs-generator --tail=1'
Feb 25 09:48:29.881: INFO: stderr: ""
Feb 25 09:48:29.881: INFO: stdout: "I0225 09:48:29.783406       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/hljc 386\n"
Feb 25 09:48:29.881: INFO: got output "I0225 09:48:29.783406       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/hljc 386\n"
STEP: limiting log bytes
Feb 25 09:48:29.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 logs logs-generator logs-generator --limit-bytes=1'
Feb 25 09:48:29.955: INFO: stderr: ""
Feb 25 09:48:29.955: INFO: stdout: "I"
Feb 25 09:48:29.955: INFO: got output "I"
STEP: exposing timestamps
Feb 25 09:48:29.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 25 09:48:30.022: INFO: stderr: ""
Feb 25 09:48:30.022: INFO: stdout: "2022-02-25T09:48:29.982730929Z I0225 09:48:29.982645       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zvp 286\n"
Feb 25 09:48:30.022: INFO: got output "2022-02-25T09:48:29.982730929Z I0225 09:48:29.982645       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zvp 286\n"
STEP: restricting to a time range
Feb 25 09:48:32.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 logs logs-generator logs-generator --since=1s'
Feb 25 09:48:32.613: INFO: stderr: ""
Feb 25 09:48:32.613: INFO: stdout: "I0225 09:48:31.783261       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/2kw 520\nI0225 09:48:31.982503       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/xpk2 266\nI0225 09:48:32.182807       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/4wlj 213\nI0225 09:48:32.383114       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/zvr 457\nI0225 09:48:32.583345       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/bcb 249\n"
Feb 25 09:48:32.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 logs logs-generator logs-generator --since=24h'
Feb 25 09:48:32.687: INFO: stderr: ""
Feb 25 09:48:32.687: INFO: stdout: "I0225 09:48:28.382504       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/sd4g 429\nI0225 09:48:28.582617       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/dvf 231\nI0225 09:48:28.783056       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/pf8 470\nI0225 09:48:28.983373       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/t7j6 242\nI0225 09:48:29.183478       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/dzs 340\nI0225 09:48:29.382782       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/ng76 565\nI0225 09:48:29.583088       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/w5t 261\nI0225 09:48:29.783406       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/hljc 386\nI0225 09:48:29.982645       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/zvp 286\nI0225 09:48:30.182951       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/4gq 205\nI0225 09:48:30.383264       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/85n 289\nI0225 09:48:30.582510       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/nxk7 330\nI0225 09:48:30.782813       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/6flw 418\nI0225 09:48:30.983135       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/q2vt 405\nI0225 09:48:31.183409       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/mns 360\nI0225 09:48:31.382655       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/8m4 260\nI0225 09:48:31.582958       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/nj8p 299\nI0225 09:48:31.783261       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/2kw 520\nI0225 09:48:31.982503       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/xpk2 266\nI0225 09:48:32.182807       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/4wlj 213\nI0225 09:48:32.383114       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/zvr 457\nI0225 09:48:32.583345       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/bcb 249\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Feb 25 09:48:32.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-7134 delete pod logs-generator'
Feb 25 09:48:33.344: INFO: stderr: ""
Feb 25 09:48:33.344: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:33.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7134" for this suite.

• [SLOW TEST:5.842 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":67,"skipped":1228,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:33.361: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 09:48:33.525: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ce5f888-d533-4d94-a12d-04ca9cb4d3e2" in namespace "downward-api-9980" to be "Succeeded or Failed"
Feb 25 09:48:33.533: INFO: Pod "downwardapi-volume-4ce5f888-d533-4d94-a12d-04ca9cb4d3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.55542ms
Feb 25 09:48:35.543: INFO: Pod "downwardapi-volume-4ce5f888-d533-4d94-a12d-04ca9cb4d3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017956153s
STEP: Saw pod success
Feb 25 09:48:35.543: INFO: Pod "downwardapi-volume-4ce5f888-d533-4d94-a12d-04ca9cb4d3e2" satisfied condition "Succeeded or Failed"
Feb 25 09:48:35.548: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod downwardapi-volume-4ce5f888-d533-4d94-a12d-04ca9cb4d3e2 container client-container: <nil>
STEP: delete the pod
Feb 25 09:48:35.572: INFO: Waiting for pod downwardapi-volume-4ce5f888-d533-4d94-a12d-04ca9cb4d3e2 to disappear
Feb 25 09:48:35.577: INFO: Pod downwardapi-volume-4ce5f888-d533-4d94-a12d-04ca9cb4d3e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:35.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9980" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1241,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:35.591: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:39.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6532" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1253,"failed":0}
SSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:39.790: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-8170
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Feb 25 09:48:39.979: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Feb 25 09:48:40.009: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:40.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8170" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":70,"skipped":1257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:40.064: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4338
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Feb 25 09:48:40.234: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 25 09:48:45.252: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:45.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4338" for this suite.

• [SLOW TEST:5.245 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":71,"skipped":1311,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:45.312: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Feb 25 09:48:45.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-5167 cluster-info'
Feb 25 09:48:45.526: INFO: stderr: ""
Feb 25 09:48:45.526: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:45.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5167" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":72,"skipped":1339,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:45.541: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9941
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Feb 25 09:48:45.704: INFO: Waiting up to 5m0s for pod "test-pod-59ebf3f9-bbe3-494e-8299-dbf33c87ae9c" in namespace "svcaccounts-9941" to be "Succeeded or Failed"
Feb 25 09:48:45.711: INFO: Pod "test-pod-59ebf3f9-bbe3-494e-8299-dbf33c87ae9c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.758579ms
Feb 25 09:48:47.720: INFO: Pod "test-pod-59ebf3f9-bbe3-494e-8299-dbf33c87ae9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015853506s
STEP: Saw pod success
Feb 25 09:48:47.720: INFO: Pod "test-pod-59ebf3f9-bbe3-494e-8299-dbf33c87ae9c" satisfied condition "Succeeded or Failed"
Feb 25 09:48:47.725: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod test-pod-59ebf3f9-bbe3-494e-8299-dbf33c87ae9c container agnhost-container: <nil>
STEP: delete the pod
Feb 25 09:48:47.749: INFO: Waiting for pod test-pod-59ebf3f9-bbe3-494e-8299-dbf33c87ae9c to disappear
Feb 25 09:48:47.754: INFO: Pod test-pod-59ebf3f9-bbe3-494e-8299-dbf33c87ae9c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:48:47.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9941" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":73,"skipped":1358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:48:47.770: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4230
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:48:47.945: INFO: Found 0 stateful pods, waiting for 1
Feb 25 09:48:57.955: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Feb 25 09:48:57.988: INFO: Found 1 stateful pods, waiting for 2
Feb 25 09:49:07.999: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 09:49:07.999: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 09:49:08.034: INFO: Deleting all statefulset in ns statefulset-4230
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:08.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4230" for this suite.

• [SLOW TEST:20.300 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":74,"skipped":1382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:08.071: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Feb 25 09:49:08.251: INFO: Waiting up to 5m0s for pod "client-containers-42af9c6b-2108-4e03-8d8d-38cb7db24379" in namespace "containers-3043" to be "Succeeded or Failed"
Feb 25 09:49:08.258: INFO: Pod "client-containers-42af9c6b-2108-4e03-8d8d-38cb7db24379": Phase="Pending", Reason="", readiness=false. Elapsed: 7.523734ms
Feb 25 09:49:10.268: INFO: Pod "client-containers-42af9c6b-2108-4e03-8d8d-38cb7db24379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01714901s
STEP: Saw pod success
Feb 25 09:49:10.268: INFO: Pod "client-containers-42af9c6b-2108-4e03-8d8d-38cb7db24379" satisfied condition "Succeeded or Failed"
Feb 25 09:49:10.273: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod client-containers-42af9c6b-2108-4e03-8d8d-38cb7db24379 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 09:49:10.302: INFO: Waiting for pod client-containers-42af9c6b-2108-4e03-8d8d-38cb7db24379 to disappear
Feb 25 09:49:10.307: INFO: Pod client-containers-42af9c6b-2108-4e03-8d8d-38cb7db24379 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:10.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3043" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1410,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:10.322: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:49:10.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 09:49:12.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379350, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379350, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379350, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379350, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:49:15.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:15.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2114" for this suite.
STEP: Destroying namespace "webhook-2114-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.714 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":76,"skipped":1420,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:16.039: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-05ea26db-2558-4dc8-838d-c0ead1653e87
STEP: Creating a pod to test consume secrets
Feb 25 09:49:16.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2fe6a09f-3f3b-4127-aeba-1a4b3578b527" in namespace "projected-9872" to be "Succeeded or Failed"
Feb 25 09:49:16.222: INFO: Pod "pod-projected-secrets-2fe6a09f-3f3b-4127-aeba-1a4b3578b527": Phase="Pending", Reason="", readiness=false. Elapsed: 5.223993ms
Feb 25 09:49:18.232: INFO: Pod "pod-projected-secrets-2fe6a09f-3f3b-4127-aeba-1a4b3578b527": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015187415s
STEP: Saw pod success
Feb 25 09:49:18.232: INFO: Pod "pod-projected-secrets-2fe6a09f-3f3b-4127-aeba-1a4b3578b527" satisfied condition "Succeeded or Failed"
Feb 25 09:49:18.237: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-projected-secrets-2fe6a09f-3f3b-4127-aeba-1a4b3578b527 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 25 09:49:18.266: INFO: Waiting for pod pod-projected-secrets-2fe6a09f-3f3b-4127-aeba-1a4b3578b527 to disappear
Feb 25 09:49:18.271: INFO: Pod pod-projected-secrets-2fe6a09f-3f3b-4127-aeba-1a4b3578b527 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:18.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9872" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":77,"skipped":1432,"failed":0}

------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:18.285: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Feb 25 09:49:18.438: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 25 09:49:18.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 create -f -'
Feb 25 09:49:18.999: INFO: stderr: ""
Feb 25 09:49:18.999: INFO: stdout: "service/agnhost-replica created\n"
Feb 25 09:49:18.999: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 25 09:49:18.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 create -f -'
Feb 25 09:49:19.166: INFO: stderr: ""
Feb 25 09:49:19.166: INFO: stdout: "service/agnhost-primary created\n"
Feb 25 09:49:19.166: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 25 09:49:19.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 create -f -'
Feb 25 09:49:19.770: INFO: stderr: ""
Feb 25 09:49:19.770: INFO: stdout: "service/frontend created\n"
Feb 25 09:49:19.770: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 25 09:49:19.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 create -f -'
Feb 25 09:49:19.915: INFO: stderr: ""
Feb 25 09:49:19.915: INFO: stdout: "deployment.apps/frontend created\n"
Feb 25 09:49:19.915: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 25 09:49:19.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 create -f -'
Feb 25 09:49:20.118: INFO: stderr: ""
Feb 25 09:49:20.118: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 25 09:49:20.118: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 25 09:49:20.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 create -f -'
Feb 25 09:49:20.845: INFO: stderr: ""
Feb 25 09:49:20.845: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Feb 25 09:49:20.845: INFO: Waiting for all frontend pods to be Running.
Feb 25 09:49:25.896: INFO: Waiting for frontend to serve content.
Feb 25 09:49:25.910: INFO: Trying to add a new entry to the guestbook.
Feb 25 09:49:25.921: INFO: Verifying that added entry can be retrieved.
Feb 25 09:49:25.933: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Feb 25 09:49:30.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 delete --grace-period=0 --force -f -'
Feb 25 09:49:31.060: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:49:31.060: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Feb 25 09:49:31.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 delete --grace-period=0 --force -f -'
Feb 25 09:49:31.145: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:49:31.145: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 25 09:49:31.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 delete --grace-period=0 --force -f -'
Feb 25 09:49:31.227: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:49:31.227: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 25 09:49:31.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 delete --grace-period=0 --force -f -'
Feb 25 09:49:31.281: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:49:31.281: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 25 09:49:31.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 delete --grace-period=0 --force -f -'
Feb 25 09:49:31.345: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:49:31.345: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 25 09:49:31.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4282 delete --grace-period=0 --force -f -'
Feb 25 09:49:31.406: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 09:49:31.406: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:31.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4282" for this suite.

• [SLOW TEST:13.148 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":78,"skipped":1432,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:31.434: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 09:49:31.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67b57864-3ec2-45c2-8aa9-de11b70d8c61" in namespace "projected-2026" to be "Succeeded or Failed"
Feb 25 09:49:31.613: INFO: Pod "downwardapi-volume-67b57864-3ec2-45c2-8aa9-de11b70d8c61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.793476ms
Feb 25 09:49:33.623: INFO: Pod "downwardapi-volume-67b57864-3ec2-45c2-8aa9-de11b70d8c61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017209242s
STEP: Saw pod success
Feb 25 09:49:33.623: INFO: Pod "downwardapi-volume-67b57864-3ec2-45c2-8aa9-de11b70d8c61" satisfied condition "Succeeded or Failed"
Feb 25 09:49:33.628: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod downwardapi-volume-67b57864-3ec2-45c2-8aa9-de11b70d8c61 container client-container: <nil>
STEP: delete the pod
Feb 25 09:49:33.662: INFO: Waiting for pod downwardapi-volume-67b57864-3ec2-45c2-8aa9-de11b70d8c61 to disappear
Feb 25 09:49:33.670: INFO: Pod downwardapi-volume-67b57864-3ec2-45c2-8aa9-de11b70d8c61 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:33.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2026" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1436,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:33.685: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Feb 25 09:49:33.859: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:49:35.870: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:36.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1620" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":80,"skipped":1437,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:36.907: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6407
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:49:37.059: INFO: Creating pod...
Feb 25 09:49:37.079: INFO: Pod Quantity: 1 Status: Pending
Feb 25 09:49:38.087: INFO: Pod Quantity: 1 Status: Pending
Feb 25 09:49:39.086: INFO: Pod Status: Running
Feb 25 09:49:39.086: INFO: Creating service...
Feb 25 09:49:39.103: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/pods/agnhost/proxy/some/path/with/DELETE
Feb 25 09:49:39.111: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 25 09:49:39.111: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/pods/agnhost/proxy/some/path/with/GET
Feb 25 09:49:39.117: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 25 09:49:39.117: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/pods/agnhost/proxy/some/path/with/HEAD
Feb 25 09:49:39.122: INFO: http.Client request:HEAD | StatusCode:200
Feb 25 09:49:39.122: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 25 09:49:39.128: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 25 09:49:39.128: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/pods/agnhost/proxy/some/path/with/PATCH
Feb 25 09:49:39.133: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 25 09:49:39.133: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/pods/agnhost/proxy/some/path/with/POST
Feb 25 09:49:39.138: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 25 09:49:39.138: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/pods/agnhost/proxy/some/path/with/PUT
Feb 25 09:49:39.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 25 09:49:39.143: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/services/test-service/proxy/some/path/with/DELETE
Feb 25 09:49:39.151: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 25 09:49:39.151: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/services/test-service/proxy/some/path/with/GET
Feb 25 09:49:39.159: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 25 09:49:39.159: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/services/test-service/proxy/some/path/with/HEAD
Feb 25 09:49:39.167: INFO: http.Client request:HEAD | StatusCode:200
Feb 25 09:49:39.167: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/services/test-service/proxy/some/path/with/OPTIONS
Feb 25 09:49:39.175: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 25 09:49:39.176: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/services/test-service/proxy/some/path/with/PATCH
Feb 25 09:49:39.185: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 25 09:49:39.185: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/services/test-service/proxy/some/path/with/POST
Feb 25 09:49:39.193: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 25 09:49:39.193: INFO: Starting http.Client for https://172.31.0.1:443/api/v1/namespaces/proxy-6407/services/test-service/proxy/some/path/with/PUT
Feb 25 09:49:39.201: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:49:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6407" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":81,"skipped":1448,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:49:39.217: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-pw9p
STEP: Creating a pod to test atomic-volume-subpath
Feb 25 09:49:39.396: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pw9p" in namespace "subpath-3485" to be "Succeeded or Failed"
Feb 25 09:49:39.404: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012004ms
Feb 25 09:49:41.414: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.018404176s
Feb 25 09:49:43.425: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 4.029325866s
Feb 25 09:49:45.437: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 6.041195697s
Feb 25 09:49:47.445: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 8.049441758s
Feb 25 09:49:49.453: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 10.057385182s
Feb 25 09:49:51.467: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 12.070792962s
Feb 25 09:49:53.477: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 14.081516089s
Feb 25 09:49:55.488: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 16.092390768s
Feb 25 09:49:57.496: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 18.099962268s
Feb 25 09:49:59.506: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Running", Reason="", readiness=true. Elapsed: 20.110301428s
Feb 25 09:50:01.515: INFO: Pod "pod-subpath-test-downwardapi-pw9p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.11946383s
STEP: Saw pod success
Feb 25 09:50:01.515: INFO: Pod "pod-subpath-test-downwardapi-pw9p" satisfied condition "Succeeded or Failed"
Feb 25 09:50:01.520: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-pw9p container test-container-subpath-downwardapi-pw9p: <nil>
STEP: delete the pod
Feb 25 09:50:01.549: INFO: Waiting for pod pod-subpath-test-downwardapi-pw9p to disappear
Feb 25 09:50:01.554: INFO: Pod pod-subpath-test-downwardapi-pw9p no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-pw9p
Feb 25 09:50:01.554: INFO: Deleting pod "pod-subpath-test-downwardapi-pw9p" in namespace "subpath-3485"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:50:01.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3485" for this suite.

• [SLOW TEST:22.356 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":82,"skipped":1448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:50:01.574: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b in namespace container-probe-2199
Feb 25 09:50:03.785: INFO: Started pod liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b in namespace container-probe-2199
STEP: checking the pod's current state and verifying that restartCount is present
Feb 25 09:50:03.790: INFO: Initial restart count of pod liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b is 0
Feb 25 09:50:23.891: INFO: Restart count of pod container-probe-2199/liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b is now 1 (20.100595287s elapsed)
Feb 25 09:50:43.987: INFO: Restart count of pod container-probe-2199/liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b is now 2 (40.196842989s elapsed)
Feb 25 09:51:04.092: INFO: Restart count of pod container-probe-2199/liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b is now 3 (1m0.301544665s elapsed)
Feb 25 09:51:24.202: INFO: Restart count of pod container-probe-2199/liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b is now 4 (1m20.411819962s elapsed)
Feb 25 09:52:36.551: INFO: Restart count of pod container-probe-2199/liveness-a8f29079-efe7-44a4-b8f1-17eef9820e3b is now 5 (2m32.760593708s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:52:36.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2199" for this suite.

• [SLOW TEST:155.018 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1470,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:52:36.592: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:52:37.115: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 09:52:39.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379557, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379557, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379557, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379557, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:52:42.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:52:42.170: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:52:50.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1687" for this suite.
STEP: Destroying namespace "webhook-1687-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.802 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":84,"skipped":1470,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:52:50.394: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2040
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-f403a4c7-c669-44a4-94a8-553922609874
STEP: Creating the pod
Feb 25 09:52:50.686: INFO: The status of Pod pod-configmaps-9d2e4f3e-6ee6-46ec-8222-d4e40a289916 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:52:52.696: INFO: The status of Pod pod-configmaps-9d2e4f3e-6ee6-46ec-8222-d4e40a289916 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-f403a4c7-c669-44a4-94a8-553922609874
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:52:54.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2040" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1486,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:52:54.769: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-710
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Feb 25 09:52:55.463: INFO: created pod pod-service-account-defaultsa
Feb 25 09:52:55.463: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 25 09:52:55.478: INFO: created pod pod-service-account-mountsa
Feb 25 09:52:55.479: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 25 09:52:55.489: INFO: created pod pod-service-account-nomountsa
Feb 25 09:52:55.490: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 25 09:52:55.501: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 25 09:52:55.504: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 25 09:52:55.528: INFO: created pod pod-service-account-mountsa-mountspec
Feb 25 09:52:55.528: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 25 09:52:55.551: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 25 09:52:55.551: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 25 09:52:55.569: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 25 09:52:55.570: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 25 09:52:55.586: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 25 09:52:55.586: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 25 09:52:55.600: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 25 09:52:55.600: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:52:55.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-710" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":86,"skipped":1491,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:52:55.621: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7550
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 25 09:53:01.858: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:53:01.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7550" for this suite.

• [SLOW TEST:6.271 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:53:01.892: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3618
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-3618
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3618
Feb 25 09:53:02.068: INFO: Found 0 stateful pods, waiting for 1
Feb 25 09:53:12.082: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 25 09:53:12.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-3618 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 09:53:12.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 09:53:12.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 09:53:12.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 09:53:12.241: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 25 09:53:22.255: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 09:53:22.255: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 09:53:22.276: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 25 09:53:22.276: INFO: ss-0  ip-10-1-2-67.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:02 +0000 UTC  }]
Feb 25 09:53:22.276: INFO: 
Feb 25 09:53:22.276: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 25 09:53:23.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995251783s
Feb 25 09:53:24.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987032883s
Feb 25 09:53:25.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978989117s
Feb 25 09:53:26.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970737024s
Feb 25 09:53:27.317: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962087664s
Feb 25 09:53:28.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951156643s
Feb 25 09:53:29.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.943089316s
Feb 25 09:53:30.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.93509497s
Feb 25 09:53:31.351: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.347373ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3618
Feb 25 09:53:32.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-3618 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 09:53:32.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 25 09:53:32.507: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 09:53:32.507: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 09:53:32.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-3618 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 09:53:32.656: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 25 09:53:32.656: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 09:53:32.656: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 09:53:32.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 09:53:32.807: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 25 09:53:32.807: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 09:53:32.807: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 09:53:32.813: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 25 09:53:42.826: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 09:53:42.826: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 09:53:42.826: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 25 09:53:42.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-3618 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 09:53:42.987: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 09:53:42.987: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 09:53:42.987: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 09:53:42.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-3618 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 09:53:43.149: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 09:53:43.149: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 09:53:43.149: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 09:53:43.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-3618 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 09:53:43.297: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 09:53:43.297: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 09:53:43.297: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 09:53:43.297: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 09:53:43.303: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 25 09:53:53.322: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 09:53:53.322: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 09:53:53.322: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 09:53:53.338: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 25 09:53:53.338: INFO: ss-0  ip-10-1-2-67.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:02 +0000 UTC  }]
Feb 25 09:53:53.339: INFO: ss-1  ip-10-1-2-27.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:22 +0000 UTC  }]
Feb 25 09:53:53.339: INFO: ss-2  ip-10-1-2-231.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 09:53:22 +0000 UTC  }]
Feb 25 09:53:53.339: INFO: 
Feb 25 09:53:53.339: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 25 09:53:54.346: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.994649566s
Feb 25 09:53:55.353: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987644966s
Feb 25 09:53:56.361: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979648923s
Feb 25 09:53:57.370: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.971651304s
Feb 25 09:53:58.377: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.963639696s
Feb 25 09:53:59.384: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.956264846s
Feb 25 09:54:00.391: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.949647851s
Feb 25 09:54:01.400: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.941865167s
Feb 25 09:54:02.409: INFO: Verifying statefulset ss doesn't scale past 0 for another 934.113009ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3618
Feb 25 09:54:03.416: INFO: Scaling statefulset ss to 0
Feb 25 09:54:03.432: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 09:54:03.436: INFO: Deleting all statefulset in ns statefulset-3618
Feb 25 09:54:03.441: INFO: Scaling statefulset ss to 0
Feb 25 09:54:03.457: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 09:54:03.461: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:03.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3618" for this suite.

• [SLOW TEST:61.603 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":88,"skipped":1551,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:03.496: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8757
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:54:03.656: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 25 09:54:13.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-8757 --namespace=crd-publish-openapi-8757 create -f -'
Feb 25 09:54:14.081: INFO: stderr: ""
Feb 25 09:54:14.082: INFO: stdout: "e2e-test-crd-publish-openapi-5166-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 25 09:54:14.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-8757 --namespace=crd-publish-openapi-8757 delete e2e-test-crd-publish-openapi-5166-crds test-cr'
Feb 25 09:54:14.167: INFO: stderr: ""
Feb 25 09:54:14.167: INFO: stdout: "e2e-test-crd-publish-openapi-5166-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 25 09:54:14.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-8757 --namespace=crd-publish-openapi-8757 apply -f -'
Feb 25 09:54:14.755: INFO: stderr: ""
Feb 25 09:54:14.755: INFO: stdout: "e2e-test-crd-publish-openapi-5166-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 25 09:54:14.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-8757 --namespace=crd-publish-openapi-8757 delete e2e-test-crd-publish-openapi-5166-crds test-cr'
Feb 25 09:54:14.807: INFO: stderr: ""
Feb 25 09:54:14.807: INFO: stdout: "e2e-test-crd-publish-openapi-5166-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 25 09:54:14.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-8757 explain e2e-test-crd-publish-openapi-5166-crds'
Feb 25 09:54:14.949: INFO: stderr: ""
Feb 25 09:54:14.949: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5166-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:19.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8757" for this suite.

• [SLOW TEST:16.005 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":89,"skipped":1558,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:19.501: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Feb 25 09:54:19.657: INFO: Creating simple deployment test-deployment-lqrxq
Feb 25 09:54:19.672: INFO: new replicaset for deployment "test-deployment-lqrxq" is yet to be created
STEP: Getting /status
Feb 25 09:54:21.702: INFO: Deployment test-deployment-lqrxq has Conditions: [{Available True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lqrxq-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Feb 25 09:54:21.715: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379661, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379661, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379661, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379659, loc:(*time.Location)(0xa0aae00)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-lqrxq-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Feb 25 09:54:21.718: INFO: Observed &Deployment event: ADDED
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lqrxq-794dd694d8"}
Feb 25 09:54:21.718: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lqrxq-794dd694d8"}
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 25 09:54:21.718: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lqrxq-794dd694d8" is progressing.}
Feb 25 09:54:21.718: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lqrxq-794dd694d8" has successfully progressed.}
Feb 25 09:54:21.718: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 25 09:54:21.718: INFO: Observed Deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lqrxq-794dd694d8" has successfully progressed.}
Feb 25 09:54:21.718: INFO: Found Deployment test-deployment-lqrxq in namespace deployment-283 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 25 09:54:21.718: INFO: Deployment test-deployment-lqrxq has an updated status
STEP: patching the Statefulset Status
Feb 25 09:54:21.718: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 25 09:54:21.727: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Feb 25 09:54:21.729: INFO: Observed &Deployment event: ADDED
Feb 25 09:54:21.729: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lqrxq-794dd694d8"}
Feb 25 09:54:21.730: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.730: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-lqrxq-794dd694d8"}
Feb 25 09:54:21.730: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 25 09:54:21.730: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.730: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 25 09:54:21.730: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:19 +0000 UTC 2022-02-25 09:54:19 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-lqrxq-794dd694d8" is progressing.}
Feb 25 09:54:21.730: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.730: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 25 09:54:21.730: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lqrxq-794dd694d8" has successfully progressed.}
Feb 25 09:54:21.731: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.731: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 25 09:54:21.731: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-25 09:54:21 +0000 UTC 2022-02-25 09:54:19 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-lqrxq-794dd694d8" has successfully progressed.}
Feb 25 09:54:21.731: INFO: Observed deployment test-deployment-lqrxq in namespace deployment-283 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 25 09:54:21.731: INFO: Observed &Deployment event: MODIFIED
Feb 25 09:54:21.731: INFO: Found deployment test-deployment-lqrxq in namespace deployment-283 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 25 09:54:21.731: INFO: Deployment test-deployment-lqrxq has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 09:54:21.736: INFO: Deployment "test-deployment-lqrxq":
&Deployment{ObjectMeta:{test-deployment-lqrxq  deployment-283  1c118038-1150-44d4-a613-62a21957a081 18726 1 2022-02-25 09:54:19 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-02-25 09:54:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-02-25 09:54:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-02-25 09:54:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048318b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-lqrxq-794dd694d8",LastUpdateTime:2022-02-25 09:54:21 +0000 UTC,LastTransitionTime:2022-02-25 09:54:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 25 09:54:21.740: INFO: New ReplicaSet "test-deployment-lqrxq-794dd694d8" of Deployment "test-deployment-lqrxq":
&ReplicaSet{ObjectMeta:{test-deployment-lqrxq-794dd694d8  deployment-283  8eb8871a-29cc-4191-8005-a814aeaed841 18716 1 2022-02-25 09:54:19 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-lqrxq 1c118038-1150-44d4-a613-62a21957a081 0xc004831df0 0xc004831df1}] []  [{kube-controller-manager Update apps/v1 2022-02-25 09:54:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c118038-1150-44d4-a613-62a21957a081\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 09:54:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004831ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 25 09:54:21.747: INFO: Pod "test-deployment-lqrxq-794dd694d8-jlcvx" is available:
&Pod{ObjectMeta:{test-deployment-lqrxq-794dd694d8-jlcvx test-deployment-lqrxq-794dd694d8- deployment-283  e2a1cda3-7fe8-43e1-86fb-e22abae7d84d 18715 0 2022-02-25 09:54:19 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-lqrxq-794dd694d8 8eb8871a-29cc-4191-8005-a814aeaed841 0xc0045db1d0 0xc0045db1d1}] []  [{kube-controller-manager Update v1 2022-02-25 09:54:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8eb8871a-29cc-4191-8005-a814aeaed841\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:54:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.61.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvpzc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvpzc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:54:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:54:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.67,PodIP:10.10.61.105,StartTime:2022-02-25 09:54:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:54:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://03c8d289426bd9c03fdbc803a398d9e3bbee37b9e40ed992db16e9401a43c4a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.61.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:21.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-283" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":90,"skipped":1573,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:21.761: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 25 09:54:21.925: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 25 09:54:21.931: INFO: starting watch
STEP: patching
STEP: updating
Feb 25 09:54:21.954: INFO: waiting for watch events with expected annotations
Feb 25 09:54:21.954: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:22.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-294" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":91,"skipped":1581,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:22.024: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Feb 25 09:54:22.184: INFO: Waiting up to 5m0s for pod "var-expansion-478f99ae-53c9-4452-b515-a3877de5302d" in namespace "var-expansion-3345" to be "Succeeded or Failed"
Feb 25 09:54:22.191: INFO: Pod "var-expansion-478f99ae-53c9-4452-b515-a3877de5302d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.142839ms
Feb 25 09:54:24.206: INFO: Pod "var-expansion-478f99ae-53c9-4452-b515-a3877de5302d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021886391s
STEP: Saw pod success
Feb 25 09:54:24.206: INFO: Pod "var-expansion-478f99ae-53c9-4452-b515-a3877de5302d" satisfied condition "Succeeded or Failed"
Feb 25 09:54:24.211: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod var-expansion-478f99ae-53c9-4452-b515-a3877de5302d container dapi-container: <nil>
STEP: delete the pod
Feb 25 09:54:24.238: INFO: Waiting for pod var-expansion-478f99ae-53c9-4452-b515-a3877de5302d to disappear
Feb 25 09:54:24.243: INFO: Pod var-expansion-478f99ae-53c9-4452-b515-a3877de5302d no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:24.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3345" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1596,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:24.257: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:54:24.433: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 25 09:54:24.446: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:24.446: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:24.446: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:24.450: INFO: Number of nodes with available pods: 0
Feb 25 09:54:24.450: INFO: Node ip-10-1-2-231.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:25.460: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:25.460: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:25.460: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:25.466: INFO: Number of nodes with available pods: 2
Feb 25 09:54:25.466: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:26.458: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:26.458: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:26.458: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:26.463: INFO: Number of nodes with available pods: 3
Feb 25 09:54:26.463: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 25 09:54:26.505: INFO: Wrong image for pod: daemon-set-2hjkp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:26.506: INFO: Wrong image for pod: daemon-set-g57g2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:26.506: INFO: Wrong image for pod: daemon-set-kshx8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:26.514: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:26.514: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:26.514: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:27.523: INFO: Wrong image for pod: daemon-set-2hjkp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:27.523: INFO: Wrong image for pod: daemon-set-kshx8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:27.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:27.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:27.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:28.522: INFO: Wrong image for pod: daemon-set-2hjkp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:28.522: INFO: Pod daemon-set-6hsjb is not available
Feb 25 09:54:28.522: INFO: Wrong image for pod: daemon-set-kshx8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:28.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:28.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:28.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:29.521: INFO: Wrong image for pod: daemon-set-2hjkp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:29.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:29.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:29.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:30.523: INFO: Wrong image for pod: daemon-set-2hjkp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:30.523: INFO: Pod daemon-set-br6lf is not available
Feb 25 09:54:30.529: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:30.529: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:30.529: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:31.522: INFO: Wrong image for pod: daemon-set-2hjkp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Feb 25 09:54:31.522: INFO: Pod daemon-set-br6lf is not available
Feb 25 09:54:31.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:31.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:31.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:32.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:32.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:32.528: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:33.524: INFO: Pod daemon-set-5h52p is not available
Feb 25 09:54:33.529: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:33.529: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:33.529: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 25 09:54:33.535: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:33.535: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:33.535: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:33.541: INFO: Number of nodes with available pods: 2
Feb 25 09:54:33.541: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:34.551: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:34.551: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:34.551: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:34.556: INFO: Number of nodes with available pods: 3
Feb 25 09:54:34.556: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1477, will wait for the garbage collector to delete the pods
Feb 25 09:54:34.642: INFO: Deleting DaemonSet.extensions daemon-set took: 9.127192ms
Feb 25 09:54:34.742: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.11125ms
Feb 25 09:54:36.851: INFO: Number of nodes with available pods: 0
Feb 25 09:54:36.851: INFO: Number of running nodes: 0, number of available pods: 0
Feb 25 09:54:36.855: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18971"},"items":null}

Feb 25 09:54:36.860: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18971"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:36.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1477" for this suite.

• [SLOW TEST:12.637 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":93,"skipped":1598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:36.894: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-3a025263-dd66-4aec-8f49-d4605fe9fdc4
STEP: Creating a pod to test consume configMaps
Feb 25 09:54:37.070: INFO: Waiting up to 5m0s for pod "pod-configmaps-061bead9-14fa-4c39-83fc-265a959401a8" in namespace "configmap-2286" to be "Succeeded or Failed"
Feb 25 09:54:37.077: INFO: Pod "pod-configmaps-061bead9-14fa-4c39-83fc-265a959401a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.659796ms
Feb 25 09:54:39.086: INFO: Pod "pod-configmaps-061bead9-14fa-4c39-83fc-265a959401a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015345195s
STEP: Saw pod success
Feb 25 09:54:39.086: INFO: Pod "pod-configmaps-061bead9-14fa-4c39-83fc-265a959401a8" satisfied condition "Succeeded or Failed"
Feb 25 09:54:39.090: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-configmaps-061bead9-14fa-4c39-83fc-265a959401a8 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 09:54:39.120: INFO: Waiting for pod pod-configmaps-061bead9-14fa-4c39-83fc-265a959401a8 to disappear
Feb 25 09:54:39.125: INFO: Pod pod-configmaps-061bead9-14fa-4c39-83fc-265a959401a8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:39.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2286" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1620,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:39.140: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 25 09:54:39.330: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:39.330: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:39.330: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:39.334: INFO: Number of nodes with available pods: 0
Feb 25 09:54:39.334: INFO: Node ip-10-1-2-231.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:40.343: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:40.343: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:40.343: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:40.348: INFO: Number of nodes with available pods: 0
Feb 25 09:54:40.348: INFO: Node ip-10-1-2-231.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:41.344: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:41.344: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:41.344: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:41.349: INFO: Number of nodes with available pods: 3
Feb 25 09:54:41.349: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 25 09:54:41.376: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:41.376: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:41.376: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:41.380: INFO: Number of nodes with available pods: 2
Feb 25 09:54:41.380: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:42.388: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:42.388: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:42.388: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:42.393: INFO: Number of nodes with available pods: 2
Feb 25 09:54:42.393: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:43.389: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:43.389: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:43.389: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:43.394: INFO: Number of nodes with available pods: 2
Feb 25 09:54:43.394: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:44.389: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:44.389: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:44.390: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:44.395: INFO: Number of nodes with available pods: 2
Feb 25 09:54:44.395: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 09:54:45.389: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:45.390: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:45.390: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 09:54:45.395: INFO: Number of nodes with available pods: 3
Feb 25 09:54:45.395: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7061, will wait for the garbage collector to delete the pods
Feb 25 09:54:45.465: INFO: Deleting DaemonSet.extensions daemon-set took: 9.829013ms
Feb 25 09:54:45.566: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.976979ms
Feb 25 09:54:48.071: INFO: Number of nodes with available pods: 0
Feb 25 09:54:48.071: INFO: Number of running nodes: 0, number of available pods: 0
Feb 25 09:54:48.076: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19172"},"items":null}

Feb 25 09:54:48.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19172"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:48.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7061" for this suite.

• [SLOW TEST:8.973 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":95,"skipped":1625,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:48.114: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-168/configmap-test-be9d706a-41a2-4c82-97ae-0b2874806db5
STEP: Creating a pod to test consume configMaps
Feb 25 09:54:48.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-db9fcf00-af9f-4e73-88eb-5762a2609ac1" in namespace "configmap-168" to be "Succeeded or Failed"
Feb 25 09:54:48.289: INFO: Pod "pod-configmaps-db9fcf00-af9f-4e73-88eb-5762a2609ac1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.854487ms
Feb 25 09:54:50.299: INFO: Pod "pod-configmaps-db9fcf00-af9f-4e73-88eb-5762a2609ac1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014689052s
STEP: Saw pod success
Feb 25 09:54:50.299: INFO: Pod "pod-configmaps-db9fcf00-af9f-4e73-88eb-5762a2609ac1" satisfied condition "Succeeded or Failed"
Feb 25 09:54:50.303: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-configmaps-db9fcf00-af9f-4e73-88eb-5762a2609ac1 container env-test: <nil>
STEP: delete the pod
Feb 25 09:54:50.331: INFO: Waiting for pod pod-configmaps-db9fcf00-af9f-4e73-88eb-5762a2609ac1 to disappear
Feb 25 09:54:50.336: INFO: Pod pod-configmaps-db9fcf00-af9f-4e73-88eb-5762a2609ac1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:50.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-168" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1630,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:50.349: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-5ceb9dbc-e911-4694-9927-aa0e1d0a47b7
STEP: Creating a pod to test consume configMaps
Feb 25 09:54:50.524: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d90faad-85a0-41de-8459-b5ec0f0bba33" in namespace "projected-471" to be "Succeeded or Failed"
Feb 25 09:54:50.531: INFO: Pod "pod-projected-configmaps-4d90faad-85a0-41de-8459-b5ec0f0bba33": Phase="Pending", Reason="", readiness=false. Elapsed: 7.399074ms
Feb 25 09:54:52.541: INFO: Pod "pod-projected-configmaps-4d90faad-85a0-41de-8459-b5ec0f0bba33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017615977s
STEP: Saw pod success
Feb 25 09:54:52.541: INFO: Pod "pod-projected-configmaps-4d90faad-85a0-41de-8459-b5ec0f0bba33" satisfied condition "Succeeded or Failed"
Feb 25 09:54:52.546: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-projected-configmaps-4d90faad-85a0-41de-8459-b5ec0f0bba33 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 09:54:52.572: INFO: Waiting for pod pod-projected-configmaps-4d90faad-85a0-41de-8459-b5ec0f0bba33 to disappear
Feb 25 09:54:52.577: INFO: Pod pod-projected-configmaps-4d90faad-85a0-41de-8459-b5ec0f0bba33 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:54:52.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-471" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:54:52.591: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1610
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1610 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1610;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1610 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1610;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1610.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1610.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1610.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1610.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1610.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1610.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1610.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1610.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1610.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.59.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.59.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.59.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.59.53_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1610 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1610;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1610 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1610;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1610.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1610.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1610.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1610.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1610.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1610.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1610.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1610.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1610.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1610.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.59.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.59.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.59.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.59.53_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 09:55:02.820: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.826: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.832: INFO: Unable to read wheezy_udp@dns-test-service.dns-1610 from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.837: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1610 from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.843: INFO: Unable to read wheezy_udp@dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.848: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.853: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.858: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.863: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.869: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.874: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.879: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.886: INFO: Unable to read 172.31.59.53_udp@PTR from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.891: INFO: Unable to read 172.31.59.53_tcp@PTR from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.896: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.902: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.907: INFO: Unable to read jessie_udp@dns-test-service.dns-1610 from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.912: INFO: Unable to read jessie_tcp@dns-test-service.dns-1610 from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.918: INFO: Unable to read jessie_udp@dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.923: INFO: Unable to read jessie_tcp@dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.928: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.934: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.939: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.945: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-1610.svc from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.951: INFO: Unable to read jessie_udp@PodARecord from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.956: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.961: INFO: Unable to read 172.31.59.53_udp@PTR from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.966: INFO: Unable to read 172.31.59.53_tcp@PTR from pod dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c: the server could not find the requested resource (get pods dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c)
Feb 25 09:55:02.966: INFO: Lookups using dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1610 wheezy_tcp@dns-test-service.dns-1610 wheezy_udp@dns-test-service.dns-1610.svc wheezy_tcp@dns-test-service.dns-1610.svc wheezy_udp@_http._tcp.dns-test-service.dns-1610.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1610.svc wheezy_udp@_http._tcp.test-service-2.dns-1610.svc wheezy_tcp@_http._tcp.test-service-2.dns-1610.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 172.31.59.53_udp@PTR 172.31.59.53_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1610 jessie_tcp@dns-test-service.dns-1610 jessie_udp@dns-test-service.dns-1610.svc jessie_tcp@dns-test-service.dns-1610.svc jessie_udp@_http._tcp.dns-test-service.dns-1610.svc jessie_tcp@_http._tcp.dns-test-service.dns-1610.svc jessie_udp@_http._tcp.test-service-2.dns-1610.svc jessie_tcp@_http._tcp.test-service-2.dns-1610.svc jessie_udp@PodARecord jessie_tcp@PodARecord 172.31.59.53_udp@PTR 172.31.59.53_tcp@PTR]

Feb 25 09:55:08.127: INFO: DNS probes using dns-1610/dns-test-48cc704a-238b-425e-9a92-3c58daf2db2c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:55:08.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1610" for this suite.

• [SLOW TEST:15.642 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":98,"skipped":1745,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:55:08.235: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9846
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Feb 25 09:55:08.417: INFO: Found 0 stateful pods, waiting for 3
Feb 25 09:55:18.423: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 09:55:18.423: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 09:55:18.423: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 09:55:18.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 09:55:18.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 09:55:18.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 09:55:18.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Feb 25 09:55:28.618: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 25 09:55:38.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 09:55:38.781: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 25 09:55:38.781: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 09:55:38.781: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 09:55:58.813: INFO: Waiting for StatefulSet statefulset-9846/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 25 09:56:08.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 09:56:08.974: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 09:56:08.974: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 09:56:08.974: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 09:56:19.021: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 25 09:56:29.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-9846 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 09:56:29.196: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 25 09:56:29.196: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 09:56:29.196: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 09:56:39.228: INFO: Deleting all statefulset in ns statefulset-9846
Feb 25 09:56:39.232: INFO: Scaling statefulset ss2 to 0
Feb 25 09:56:49.257: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 09:56:49.262: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:56:49.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9846" for this suite.

• [SLOW TEST:101.061 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":99,"skipped":1753,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:56:49.296: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-adb22e99-9c9c-4b9b-8791-8d92e4971e8f
STEP: Creating a pod to test consume secrets
Feb 25 09:56:49.469: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4c6f349-890d-477a-a8aa-41744ff7f190" in namespace "projected-8707" to be "Succeeded or Failed"
Feb 25 09:56:49.475: INFO: Pod "pod-projected-secrets-f4c6f349-890d-477a-a8aa-41744ff7f190": Phase="Pending", Reason="", readiness=false. Elapsed: 5.93374ms
Feb 25 09:56:51.485: INFO: Pod "pod-projected-secrets-f4c6f349-890d-477a-a8aa-41744ff7f190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015326021s
STEP: Saw pod success
Feb 25 09:56:51.485: INFO: Pod "pod-projected-secrets-f4c6f349-890d-477a-a8aa-41744ff7f190" satisfied condition "Succeeded or Failed"
Feb 25 09:56:51.489: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-projected-secrets-f4c6f349-890d-477a-a8aa-41744ff7f190 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 25 09:56:51.522: INFO: Waiting for pod pod-projected-secrets-f4c6f349-890d-477a-a8aa-41744ff7f190 to disappear
Feb 25 09:56:51.527: INFO: Pod pod-projected-secrets-f4c6f349-890d-477a-a8aa-41744ff7f190 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:56:51.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8707" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:56:51.542: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:56:51.698: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 25 09:56:51.719: INFO: The status of Pod pod-exec-websocket-7bfc8532-e327-4526-a42a-b5e4c87c41d5 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:56:53.727: INFO: The status of Pod pod-exec-websocket-7bfc8532-e327-4526-a42a-b5e4c87c41d5 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:56:53.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9192" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1911,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:56:53.813: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 09:56:53.980: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98d8676c-56d4-4e40-95eb-7afac503122e" in namespace "projected-6552" to be "Succeeded or Failed"
Feb 25 09:56:53.989: INFO: Pod "downwardapi-volume-98d8676c-56d4-4e40-95eb-7afac503122e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.048921ms
Feb 25 09:56:56.000: INFO: Pod "downwardapi-volume-98d8676c-56d4-4e40-95eb-7afac503122e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020149756s
STEP: Saw pod success
Feb 25 09:56:56.000: INFO: Pod "downwardapi-volume-98d8676c-56d4-4e40-95eb-7afac503122e" satisfied condition "Succeeded or Failed"
Feb 25 09:56:56.005: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod downwardapi-volume-98d8676c-56d4-4e40-95eb-7afac503122e container client-container: <nil>
STEP: delete the pod
Feb 25 09:56:56.033: INFO: Waiting for pod downwardapi-volume-98d8676c-56d4-4e40-95eb-7afac503122e to disappear
Feb 25 09:56:56.038: INFO: Pod downwardapi-volume-98d8676c-56d4-4e40-95eb-7afac503122e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:56:56.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6552" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":1911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:56:56.056: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:00.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2616" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":103,"skipped":1934,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:00.312: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6066
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-f2d90421-ec16-4d39-83a3-e04b27266384
STEP: Creating configMap with name cm-test-opt-upd-43caff9c-93ac-467e-9e40-aafbc525c4ec
STEP: Creating the pod
Feb 25 09:57:00.509: INFO: The status of Pod pod-configmaps-25fe0b1e-c341-408d-8dba-d1782bc0023d is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:57:02.518: INFO: The status of Pod pod-configmaps-25fe0b1e-c341-408d-8dba-d1782bc0023d is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-f2d90421-ec16-4d39-83a3-e04b27266384
STEP: Updating configmap cm-test-opt-upd-43caff9c-93ac-467e-9e40-aafbc525c4ec
STEP: Creating configMap with name cm-test-opt-create-924fe8c3-a46c-4206-bcfd-7a590450ce52
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:06.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6066" for this suite.

• [SLOW TEST:6.339 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1950,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:06.652: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-1c7b3572-92c9-400b-ab5c-5b6ecca8c386
STEP: Creating a pod to test consume configMaps
Feb 25 09:57:06.825: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5efe9c3-3a9d-4aaf-8fbc-3a4f137bd0de" in namespace "configmap-4000" to be "Succeeded or Failed"
Feb 25 09:57:06.832: INFO: Pod "pod-configmaps-e5efe9c3-3a9d-4aaf-8fbc-3a4f137bd0de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.892095ms
Feb 25 09:57:08.841: INFO: Pod "pod-configmaps-e5efe9c3-3a9d-4aaf-8fbc-3a4f137bd0de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016510445s
STEP: Saw pod success
Feb 25 09:57:08.841: INFO: Pod "pod-configmaps-e5efe9c3-3a9d-4aaf-8fbc-3a4f137bd0de" satisfied condition "Succeeded or Failed"
Feb 25 09:57:08.846: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-configmaps-e5efe9c3-3a9d-4aaf-8fbc-3a4f137bd0de container agnhost-container: <nil>
STEP: delete the pod
Feb 25 09:57:08.873: INFO: Waiting for pod pod-configmaps-e5efe9c3-3a9d-4aaf-8fbc-3a4f137bd0de to disappear
Feb 25 09:57:08.877: INFO: Pod pod-configmaps-e5efe9c3-3a9d-4aaf-8fbc-3a4f137bd0de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:08.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4000" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":1966,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:08.891: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:57:09.067: INFO: The status of Pod busybox-host-aliases0d914254-6b12-4a85-a58a-b8046b4a3ef4 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 09:57:11.077: INFO: The status of Pod busybox-host-aliases0d914254-6b12-4a85-a58a-b8046b4a3ef4 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:11.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-788" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":1968,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:11.107: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Feb 25 09:57:11.277: INFO: Found Service test-service-mdgkg in namespace services-598 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 25 09:57:11.277: INFO: Service test-service-mdgkg created
STEP: Getting /status
Feb 25 09:57:11.282: INFO: Service test-service-mdgkg has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Feb 25 09:57:11.293: INFO: observed Service test-service-mdgkg in namespace services-598 with annotations: map[] & LoadBalancer: {[]}
Feb 25 09:57:11.293: INFO: Found Service test-service-mdgkg in namespace services-598 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 25 09:57:11.293: INFO: Service test-service-mdgkg has service status patched
STEP: updating the ServiceStatus
Feb 25 09:57:11.309: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Feb 25 09:57:11.311: INFO: Observed Service test-service-mdgkg in namespace services-598 with annotations: map[] & Conditions: {[]}
Feb 25 09:57:11.311: INFO: Observed event: &Service{ObjectMeta:{test-service-mdgkg  services-598  6e8eb15d-f891-4d66-a51d-e04fe6c2f600 20472 0 2022-02-25 09:57:11 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-02-25 09:57:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-02-25 09:57:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.31.155.188,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.31.155.188],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 25 09:57:11.311: INFO: Found Service test-service-mdgkg in namespace services-598 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 25 09:57:11.311: INFO: Service test-service-mdgkg has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Feb 25 09:57:11.332: INFO: observed Service test-service-mdgkg in namespace services-598 with labels: map[test-service-static:true]
Feb 25 09:57:11.332: INFO: observed Service test-service-mdgkg in namespace services-598 with labels: map[test-service-static:true]
Feb 25 09:57:11.332: INFO: observed Service test-service-mdgkg in namespace services-598 with labels: map[test-service-static:true]
Feb 25 09:57:11.332: INFO: Found Service test-service-mdgkg in namespace services-598 with labels: map[test-service:patched test-service-static:true]
Feb 25 09:57:11.332: INFO: Service test-service-mdgkg patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Feb 25 09:57:11.353: INFO: Observed event: ADDED
Feb 25 09:57:11.353: INFO: Observed event: MODIFIED
Feb 25 09:57:11.353: INFO: Observed event: MODIFIED
Feb 25 09:57:11.353: INFO: Observed event: MODIFIED
Feb 25 09:57:11.354: INFO: Found Service test-service-mdgkg in namespace services-598 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 25 09:57:11.354: INFO: Service test-service-mdgkg deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:11.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-598" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":107,"skipped":1975,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:11.370: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 09:57:11.935: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 09:57:13.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379831, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379831, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379831, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781379831, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 09:57:16.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:17.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4281" for this suite.
STEP: Destroying namespace "webhook-4281-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.936 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":108,"skipped":1992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:17.307: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6290
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6290
STEP: creating replication controller externalsvc in namespace services-6290
I0225 09:57:17.523974      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6290, replica count: 2
I0225 09:57:20.575586      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 25 09:57:20.612: INFO: Creating new exec pod
Feb 25 09:57:22.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-6290 exec execpod8dgzz -- /bin/sh -x -c nslookup clusterip-service.services-6290.svc.cluster.local'
Feb 25 09:57:22.809: INFO: stderr: "+ nslookup clusterip-service.services-6290.svc.cluster.local\n"
Feb 25 09:57:22.809: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nclusterip-service.services-6290.svc.cluster.local\tcanonical name = externalsvc.services-6290.svc.cluster.local.\nName:\texternalsvc.services-6290.svc.cluster.local\nAddress: 172.31.201.50\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6290, will wait for the garbage collector to delete the pods
Feb 25 09:57:22.876: INFO: Deleting ReplicationController externalsvc took: 10.322137ms
Feb 25 09:57:22.976: INFO: Terminating ReplicationController externalsvc pods took: 100.240993ms
Feb 25 09:57:25.106: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:25.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6290" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.835 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":109,"skipped":2033,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:25.142: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-3648
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3648
STEP: Deleting pre-stop pod
Feb 25 09:57:34.374: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:34.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3648" for this suite.

• [SLOW TEST:9.281 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":110,"skipped":2051,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:34.423: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 25 09:57:34.594: INFO: Waiting up to 5m0s for pod "pod-74ffc85e-2223-446f-8c06-2f4b8e5d793d" in namespace "emptydir-3061" to be "Succeeded or Failed"
Feb 25 09:57:34.601: INFO: Pod "pod-74ffc85e-2223-446f-8c06-2f4b8e5d793d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.332191ms
Feb 25 09:57:36.610: INFO: Pod "pod-74ffc85e-2223-446f-8c06-2f4b8e5d793d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015740837s
STEP: Saw pod success
Feb 25 09:57:36.610: INFO: Pod "pod-74ffc85e-2223-446f-8c06-2f4b8e5d793d" satisfied condition "Succeeded or Failed"
Feb 25 09:57:36.615: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-74ffc85e-2223-446f-8c06-2f4b8e5d793d container test-container: <nil>
STEP: delete the pod
Feb 25 09:57:36.641: INFO: Waiting for pod pod-74ffc85e-2223-446f-8c06-2f4b8e5d793d to disappear
Feb 25 09:57:36.646: INFO: Pod pod-74ffc85e-2223-446f-8c06-2f4b8e5d793d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:36.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3061" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2064,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:36.665: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6669
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:57:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 25 09:57:46.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6669 --namespace=crd-publish-openapi-6669 create -f -'
Feb 25 09:57:47.279: INFO: stderr: ""
Feb 25 09:57:47.279: INFO: stdout: "e2e-test-crd-publish-openapi-6385-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 25 09:57:47.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6669 --namespace=crd-publish-openapi-6669 delete e2e-test-crd-publish-openapi-6385-crds test-cr'
Feb 25 09:57:47.339: INFO: stderr: ""
Feb 25 09:57:47.339: INFO: stdout: "e2e-test-crd-publish-openapi-6385-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 25 09:57:47.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6669 --namespace=crd-publish-openapi-6669 apply -f -'
Feb 25 09:57:47.501: INFO: stderr: ""
Feb 25 09:57:47.501: INFO: stdout: "e2e-test-crd-publish-openapi-6385-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 25 09:57:47.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6669 --namespace=crd-publish-openapi-6669 delete e2e-test-crd-publish-openapi-6385-crds test-cr'
Feb 25 09:57:47.561: INFO: stderr: ""
Feb 25 09:57:47.561: INFO: stdout: "e2e-test-crd-publish-openapi-6385-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 25 09:57:47.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6669 explain e2e-test-crd-publish-openapi-6385-crds'
Feb 25 09:57:48.177: INFO: stderr: ""
Feb 25 09:57:48.177: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6385-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:52.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6669" for this suite.

• [SLOW TEST:16.205 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":112,"skipped":2074,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:52.870: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:55.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8881" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":113,"skipped":2081,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:55.514: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-95dcd514-af0f-4f7f-823e-a3beb59a1c6c
STEP: Creating a pod to test consume secrets
Feb 25 09:57:55.683: INFO: Waiting up to 5m0s for pod "pod-secrets-942ad8cb-0b50-49a4-96ab-08b000a095a5" in namespace "secrets-8331" to be "Succeeded or Failed"
Feb 25 09:57:55.689: INFO: Pod "pod-secrets-942ad8cb-0b50-49a4-96ab-08b000a095a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0308ms
Feb 25 09:57:57.699: INFO: Pod "pod-secrets-942ad8cb-0b50-49a4-96ab-08b000a095a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016299338s
STEP: Saw pod success
Feb 25 09:57:57.699: INFO: Pod "pod-secrets-942ad8cb-0b50-49a4-96ab-08b000a095a5" satisfied condition "Succeeded or Failed"
Feb 25 09:57:57.704: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod pod-secrets-942ad8cb-0b50-49a4-96ab-08b000a095a5 container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 09:57:57.730: INFO: Waiting for pod pod-secrets-942ad8cb-0b50-49a4-96ab-08b000a095a5 to disappear
Feb 25 09:57:57.734: INFO: Pod pod-secrets-942ad8cb-0b50-49a4-96ab-08b000a095a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:57:57.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8331" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:57:57.752: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:57:57.951: INFO: Creating simple deployment test-new-deployment
Feb 25 09:57:57.979: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 09:58:00.050: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1692  f27c80e1-f18b-4095-91ba-4b80db784ee3 21110 3 2022-02-25 09:57:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-02-25 09:57:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 09:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d229a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-25 09:57:59 +0000 UTC,LastTransitionTime:2022-02-25 09:57:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-02-25 09:57:59 +0000 UTC,LastTransitionTime:2022-02-25 09:57:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 25 09:58:00.068: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-1692  cb86cf94-b1ce-4a73-92a7-97449940664b 21113 3 2022-02-25 09:57:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f27c80e1-f18b-4095-91ba-4b80db784ee3 0xc004d23307 0xc004d23308}] []  [{kube-controller-manager Update apps/v1 2022-02-25 09:57:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f27c80e1-f18b-4095-91ba-4b80db784ee3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 09:57:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d233d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 25 09:58:00.084: INFO: Pod "test-new-deployment-847dcfb7fb-7jqt6" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-7jqt6 test-new-deployment-847dcfb7fb- deployment-1692  5c0f384a-7a28-409f-a8f0-4dd8150e2a89 21101 0 2022-02-25 09:57:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb cb86cf94-b1ce-4a73-92a7-97449940664b 0xc004c86187 0xc004c86188}] []  [{kube-controller-manager Update v1 2022-02-25 09:57:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb86cf94-b1ce-4a73-92a7-97449940664b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:57:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.24.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9c8q8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9c8q8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-67.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:57:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:57:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:57:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.67,PodIP:10.10.24.156,StartTime:2022-02-25 09:57:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 09:57:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://2418d9eafd49c02f3146c05873636f5fe5d1c13b48d5d678c75e2c2b28f0f684,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.24.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 09:58:00.084: INFO: Pod "test-new-deployment-847dcfb7fb-kk2pk" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-kk2pk test-new-deployment-847dcfb7fb- deployment-1692  d3e6740a-4cb0-4acb-9cb2-19bd7fd49fe4 21120 0 2022-02-25 09:58:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb cb86cf94-b1ce-4a73-92a7-97449940664b 0xc004c864c0 0xc004c864c1}] []  [{kube-controller-manager Update v1 2022-02-25 09:58:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb86cf94-b1ce-4a73-92a7-97449940664b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 09:58:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24pjz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24pjz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:58:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:58:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:58:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 09:58:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:,StartTime:2022-02-25 09:58:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:58:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1692" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":115,"skipped":2210,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:58:00.119: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1711
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:58:17.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1711" for this suite.

• [SLOW TEST:17.262 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":116,"skipped":2227,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:58:17.381: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 09:58:17.552: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24186288-29b5-478b-a89b-e20e4682b1d1" in namespace "downward-api-7216" to be "Succeeded or Failed"
Feb 25 09:58:17.565: INFO: Pod "downwardapi-volume-24186288-29b5-478b-a89b-e20e4682b1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.215141ms
Feb 25 09:58:19.571: INFO: Pod "downwardapi-volume-24186288-29b5-478b-a89b-e20e4682b1d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01903264s
STEP: Saw pod success
Feb 25 09:58:19.571: INFO: Pod "downwardapi-volume-24186288-29b5-478b-a89b-e20e4682b1d1" satisfied condition "Succeeded or Failed"
Feb 25 09:58:19.578: INFO: Trying to get logs from node ip-10-1-2-67.eu-central-1.compute.internal pod downwardapi-volume-24186288-29b5-478b-a89b-e20e4682b1d1 container client-container: <nil>
STEP: delete the pod
Feb 25 09:58:19.608: INFO: Waiting for pod downwardapi-volume-24186288-29b5-478b-a89b-e20e4682b1d1 to disappear
Feb 25 09:58:19.613: INFO: Pod downwardapi-volume-24186288-29b5-478b-a89b-e20e4682b1d1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:58:19.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7216" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":2239,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:58:19.627: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Feb 25 09:58:19.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4692 api-versions'
Feb 25 09:58:19.834: INFO: stderr: ""
Feb 25 09:58:19.834: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napplication.giantswarm.io/v1alpha1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.k8s.amazonaws.com/v1alpha1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:58:19.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4692" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":118,"skipped":2247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:58:19.849: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 25 09:58:20.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8109 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 25 09:58:20.082: INFO: stderr: ""
Feb 25 09:58:20.082: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Feb 25 09:58:20.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8109 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Feb 25 09:58:20.730: INFO: stderr: ""
Feb 25 09:58:20.730: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 25 09:58:20.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8109 delete pods e2e-test-httpd-pod'
Feb 25 09:58:21.879: INFO: stderr: ""
Feb 25 09:58:21.879: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:58:21.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8109" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":119,"skipped":2271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:58:21.900: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 09:58:22.054: INFO: Creating ReplicaSet my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385
Feb 25 09:58:22.067: INFO: Pod name my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385: Found 0 pods out of 1
Feb 25 09:58:27.080: INFO: Pod name my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385: Found 1 pods out of 1
Feb 25 09:58:27.080: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385" is running
Feb 25 09:58:27.085: INFO: Pod "my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385-lv8q2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 09:58:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 09:58:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 09:58:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 09:58:22 +0000 UTC Reason: Message:}])
Feb 25 09:58:27.085: INFO: Trying to dial the pod
Feb 25 09:58:32.108: INFO: Controller my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385: Got expected result from replica 1 [my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385-lv8q2]: "my-hostname-basic-fcddadec-b865-4ff2-a427-fd9eefcca385-lv8q2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:58:32.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1816" for this suite.

• [SLOW TEST:10.227 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":120,"skipped":2317,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:58:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-fc3ec1ab-8018-431c-b451-08036f9b3beb in namespace container-probe-6534
Feb 25 09:58:34.312: INFO: Started pod liveness-fc3ec1ab-8018-431c-b451-08036f9b3beb in namespace container-probe-6534
STEP: checking the pod's current state and verifying that restartCount is present
Feb 25 09:58:34.322: INFO: Initial restart count of pod liveness-fc3ec1ab-8018-431c-b451-08036f9b3beb is 0
Feb 25 09:58:54.429: INFO: Restart count of pod container-probe-6534/liveness-fc3ec1ab-8018-431c-b451-08036f9b3beb is now 1 (20.106293847s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 09:58:54.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6534" for this suite.

• [SLOW TEST:22.333 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2327,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 09:58:54.460: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 25 09:58:54.617: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 25 09:58:54.628: INFO: Waiting for terminating namespaces to be deleted...
Feb 25 09:58:54.633: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-231.eu-central-1.compute.internal before test
Feb 25 09:58:54.643: INFO: aws-node-s6sb8 from kube-system started at 2022-02-25 09:05:49 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 09:58:54.643: INFO: calico-node-bdc87 from kube-system started at 2022-02-25 09:05:49 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 09:58:54.643: INFO: cert-exporter-daemonset-4nlmn from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:58:54.643: INFO: coredns-6dc8c559ff-q8x59 from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container coredns ready: true, restart count 0
Feb 25 09:58:54.643: INFO: ebs-csi-node-vnqtd from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 09:58:54.643: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 09:58:54.643: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 09:58:54.643: INFO: kiam-agent-w9stq from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container kiam-agent ready: true, restart count 3
Feb 25 09:58:54.643: INFO: kiam-watchdog-rhftg from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 09:58:54.643: INFO: kube-proxy-882gm from kube-system started at 2022-02-25 09:06:05 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 09:58:54.643: INFO: kube-state-metrics-v2-3-0-6bd4cb8c9c-zlddp from kube-system started at 2022-02-25 09:11:16 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 25 09:58:54.643: INFO: net-exporter-rn29g from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 09:58:54.643: INFO: node-exporter-v1-3-1-qrh8m from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 09:58:54.643: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-qpjvm from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 09:58:54.643: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 09:58:54.643: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 09:58:54.644: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-27.eu-central-1.compute.internal before test
Feb 25 09:58:54.654: INFO: aws-node-xhhxn from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 09:58:54.654: INFO: calico-node-bql82 from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 09:58:54.654: INFO: cert-exporter-daemonset-gzc9c from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:58:54.654: INFO: cert-exporter-deployment-b69c5dd78-rt48g from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:58:54.654: INFO: cert-manager-webhook-6c68d58df7-v8gjv from kube-system started at 2022-02-25 09:14:12 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container webhook ready: true, restart count 0
Feb 25 09:58:54.654: INFO: coredns-6dc8c559ff-w8szs from kube-system started at 2022-02-25 09:11:15 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container coredns ready: true, restart count 0
Feb 25 09:58:54.654: INFO: ebs-csi-node-9n94t from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 09:58:54.654: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 09:58:54.654: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 09:58:54.654: INFO: external-dns-d56669c5f-pkknw from kube-system started at 2022-02-25 09:40:50 +0000 UTC (2 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container external-dns ready: true, restart count 0
Feb 25 09:58:54.654: INFO: 	Container external-dns-check-iam ready: true, restart count 0
Feb 25 09:58:54.654: INFO: kiam-agent-6nk9v from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container kiam-agent ready: true, restart count 2
Feb 25 09:58:54.654: INFO: kiam-watchdog-szdf9 from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 09:58:54.654: INFO: kube-proxy-wcpg5 from kube-system started at 2022-02-25 09:06:05 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 09:58:54.654: INFO: net-exporter-sf92w from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 09:58:54.654: INFO: node-exporter-v1-3-1-2zzkq from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 09:58:54.654: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-vhpzn from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:58:54.654: INFO: vertical-pod-autoscaler-recommender-7ffc6b994d-pkqc2 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:58:54.654: INFO: vertical-pod-autoscaler-updater-5c6675d987-wwf47 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:58:54.654: INFO: sonobuoy from sonobuoy started at 2022-02-25 09:27:07 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 25 09:58:54.654: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-ql7w7 from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 09:58:54.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 09:58:54.654: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 09:58:54.654: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-67.eu-central-1.compute.internal before test
Feb 25 09:58:54.666: INFO: aws-node-jg76c from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 09:58:54.666: INFO: calico-node-kttwk from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 09:58:54.666: INFO: cert-exporter-daemonset-9rbmr from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 09:58:54.666: INFO: cert-manager-cainjector-ff5c85897-tpzhd from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container cainjector ready: true, restart count 0
Feb 25 09:58:54.666: INFO: ebs-csi-node-t9n4m from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 09:58:54.666: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 09:58:54.666: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 09:58:54.666: INFO: kiam-agent-8nj67 from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container kiam-agent ready: true, restart count 3
Feb 25 09:58:54.666: INFO: kiam-watchdog-xc6qf from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 09:58:54.666: INFO: kube-proxy-scj7q from kube-system started at 2022-02-25 09:06:06 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 09:58:54.666: INFO: metrics-server-6dff979d99-nqcsg from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container metrics-server ready: true, restart count 0
Feb 25 09:58:54.666: INFO: net-exporter-x9rvx from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 09:58:54.666: INFO: node-exporter-v1-3-1-p97bn from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 09:58:54.666: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-m926h from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 09:58:54.666: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2tbqx from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 09:58:54.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 09:58:54.666: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bf860b59-3532-40fb-b16e-42326247f2a3 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.1.2.67 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-bf860b59-3532-40fb-b16e-42326247f2a3 off the node ip-10-1-2-67.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bf860b59-3532-40fb-b16e-42326247f2a3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:03:58.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-501" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.380 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":122,"skipped":2334,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:03:58.841: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8877
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 25 10:03:58.994: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 25 10:03:59.074: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:01.081: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:03.083: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:05.084: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:07.083: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:04:09.084: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:04:11.082: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:04:13.084: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:04:15.086: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:04:17.085: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:04:19.083: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 25 10:04:19.092: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:21.100: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:23.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:25.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:27.102: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:29.102: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:31.098: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:33.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:35.104: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:37.102: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:39.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:41.099: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:43.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:45.104: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:47.102: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:49.101: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:51.100: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:53.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:55.104: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:57.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:04:59.102: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:05:01.099: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:05:03.101: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:05:05.104: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:05:07.103: INFO: The status of Pod netserver-1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:05:09.101: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:05:11.102: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:05:13.102: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:05:15.104: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:05:17.102: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:05:19.101: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 25 10:05:19.111: INFO: The status of Pod netserver-2 is Running (Ready = true)
Feb 25 10:05:19.119: INFO: The status of Pod netserver-3 is Running (Ready = true)
Feb 25 10:05:19.128: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Feb 25 10:05:21.198: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Feb 25 10:05:21.198: INFO: Going to poll 10.10.3.13 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Feb 25 10:05:21.202: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.3.13:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:05:21.202: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:05:21.289: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 25 10:05:21.289: INFO: Going to poll 10.10.43.196 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Feb 25 10:05:21.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.43.196:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:05:21.295: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:05:21.380: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 25 10:05:21.380: INFO: Going to poll 10.10.37.195 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Feb 25 10:05:21.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.37.195:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:05:21.385: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:05:21.462: INFO: Found all 1 expected endpoints: [netserver-2]
Feb 25 10:05:21.462: INFO: Going to poll 10.10.10.190 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Feb 25 10:05:21.468: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.10.190:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:05:21.468: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:05:21.550: INFO: Found all 1 expected endpoints: [netserver-3]
Feb 25 10:05:21.550: INFO: Going to poll 10.10.24.156 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Feb 25 10:05:21.555: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.24.156:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8877 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:05:21.555: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:05:21.640: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:05:21.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8877" for this suite.

• [SLOW TEST:82.818 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:05:21.660: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:05:21.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2227" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":124,"skipped":2377,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:05:21.860: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-2308
STEP: creating replication controller nodeport-test in namespace services-2308
I0225 10:05:22.045896      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2308, replica count: 2
I0225 10:05:25.097587      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 10:05:25.097: INFO: Creating new exec pod
Feb 25 10:05:28.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-2308 exec execpod8sp4l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 25 10:05:28.280: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 25 10:05:28.280: INFO: stdout: "nodeport-test-72hv8"
Feb 25 10:05:28.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-2308 exec execpod8sp4l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.140.185 80'
Feb 25 10:05:28.411: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.140.185 80\nConnection to 172.31.140.185 80 port [tcp/http] succeeded!\n"
Feb 25 10:05:28.411: INFO: stdout: ""
Feb 25 10:05:29.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-2308 exec execpod8sp4l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.140.185 80'
Feb 25 10:05:29.550: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.140.185 80\nConnection to 172.31.140.185 80 port [tcp/http] succeeded!\n"
Feb 25 10:05:29.550: INFO: stdout: ""
Feb 25 10:05:30.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-2308 exec execpod8sp4l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.140.185 80'
Feb 25 10:05:30.538: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.140.185 80\nConnection to 172.31.140.185 80 port [tcp/http] succeeded!\n"
Feb 25 10:05:30.538: INFO: stdout: ""
Feb 25 10:05:31.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-2308 exec execpod8sp4l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.140.185 80'
Feb 25 10:05:31.558: INFO: stderr: "+ nc -v -t -w 2 172.31.140.185 80\n+ echo hostName\nConnection to 172.31.140.185 80 port [tcp/http] succeeded!\n"
Feb 25 10:05:31.558: INFO: stdout: "nodeport-test-htvsk"
Feb 25 10:05:31.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-2308 exec execpod8sp4l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.27 30148'
Feb 25 10:05:31.685: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.27 30148\nConnection to 10.1.2.27 30148 port [tcp/*] succeeded!\n"
Feb 25 10:05:31.685: INFO: stdout: "nodeport-test-72hv8"
Feb 25 10:05:31.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-2308 exec execpod8sp4l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.231 30148'
Feb 25 10:05:31.825: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.231 30148\nConnection to 10.1.2.231 30148 port [tcp/*] succeeded!\n"
Feb 25 10:05:31.825: INFO: stdout: "nodeport-test-htvsk"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:05:31.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2308" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.983 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":125,"skipped":2384,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:05:31.844: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 25 10:05:32.047: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:32.047: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:32.047: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:32.051: INFO: Number of nodes with available pods: 0
Feb 25 10:05:32.051: INFO: Node ip-10-1-2-150.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:05:33.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:33.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:33.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:33.066: INFO: Number of nodes with available pods: 1
Feb 25 10:05:33.066: INFO: Node ip-10-1-2-150.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:05:34.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:34.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:34.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:34.065: INFO: Number of nodes with available pods: 3
Feb 25 10:05:34.065: INFO: Node ip-10-1-2-150.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:05:35.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:35.062: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:35.062: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:35.067: INFO: Number of nodes with available pods: 3
Feb 25 10:05:35.067: INFO: Node ip-10-1-2-150.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:05:36.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:36.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:36.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:36.066: INFO: Number of nodes with available pods: 3
Feb 25 10:05:36.066: INFO: Node ip-10-1-2-150.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:05:37.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:37.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:37.061: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:37.066: INFO: Number of nodes with available pods: 4
Feb 25 10:05:37.066: INFO: Node ip-10-1-2-150.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:05:38.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:38.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:38.060: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:05:38.065: INFO: Number of nodes with available pods: 5
Feb 25 10:05:38.065: INFO: Number of running nodes: 5, number of available pods: 5
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Feb 25 10:05:38.118: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24019"},"items":null}

Feb 25 10:05:38.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24021"},"items":[{"metadata":{"name":"daemon-set-2s4hs","generateName":"daemon-set-","namespace":"daemonsets-9062","uid":"dc8b99a2-1f7b-4639-a4d9-d9273f72d38e","resourceVersion":"24013","creationTimestamp":"2022-02-25T10:05:32Z","deletionTimestamp":"2022-02-25T10:06:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ad1482f4-daa1-4171-bf61-fa6932685286","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad1482f4-daa1-4171-bf61-fa6932685286\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.4.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4z662","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4z662","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-1-2-231.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-1-2-231.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"}],"hostIP":"10.1.2.231","podIP":"10.10.4.129","podIPs":[{"ip":"10.10.4.129"}],"startTime":"2022-02-25T10:05:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-25T10:05:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://138b5a986fa84f2e4449d5fd01ca51da7b54a0d04a5c88460f6ef085455f0fac","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6t2lf","generateName":"daemon-set-","namespace":"daemonsets-9062","uid":"72b43fc5-0854-4960-b545-7c6e053ce4d5","resourceVersion":"24016","creationTimestamp":"2022-02-25T10:05:32Z","deletionTimestamp":"2022-02-25T10:06:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ad1482f4-daa1-4171-bf61-fa6932685286","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad1482f4-daa1-4171-bf61-fa6932685286\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.25.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bg67g","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bg67g","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-1-2-179.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-1-2-179.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:36Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:36Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"}],"hostIP":"10.1.2.179","podIP":"10.10.25.239","podIPs":[{"ip":"10.10.25.239"}],"startTime":"2022-02-25T10:05:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-25T10:05:36Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://f25581d74912da310b58aecf701051d8fd67a51b61b5673d4d3be00a132ff4fb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-926wk","generateName":"daemon-set-","namespace":"daemonsets-9062","uid":"140982b4-43ec-4a55-b805-3caf5fd1444d","resourceVersion":"24015","creationTimestamp":"2022-02-25T10:05:32Z","deletionTimestamp":"2022-02-25T10:06:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ad1482f4-daa1-4171-bf61-fa6932685286","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad1482f4-daa1-4171-bf61-fa6932685286\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.35.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-94m9c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-94m9c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-1-2-150.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-1-2-150.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:37Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:37Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"}],"hostIP":"10.1.2.150","podIP":"10.10.35.68","podIPs":[{"ip":"10.10.35.68"}],"startTime":"2022-02-25T10:05:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-25T10:05:36Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://8381ac315f86d339c6fe9ba7f75a1a9e362cfd7f4493f349794c5b27dc9c4a4c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pmk58","generateName":"daemon-set-","namespace":"daemonsets-9062","uid":"41d27c5a-53c6-4638-bf09-7d4bee80054c","resourceVersion":"24017","creationTimestamp":"2022-02-25T10:05:32Z","deletionTimestamp":"2022-02-25T10:06:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ad1482f4-daa1-4171-bf61-fa6932685286","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad1482f4-daa1-4171-bf61-fa6932685286\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.6.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nnxfx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nnxfx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-1-2-27.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-1-2-27.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"}],"hostIP":"10.1.2.27","podIP":"10.10.6.14","podIPs":[{"ip":"10.10.6.14"}],"startTime":"2022-02-25T10:05:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-25T10:05:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://232b2b74a7a34966625999bd794260aa975593d5a09dcb77d88b216ed5b817e8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-s4flg","generateName":"daemon-set-","namespace":"daemonsets-9062","uid":"f0712753-0b65-4ef9-aee6-33b1d69b643c","resourceVersion":"24014","creationTimestamp":"2022-02-25T10:05:32Z","deletionTimestamp":"2022-02-25T10:06:08Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ad1482f4-daa1-4171-bf61-fa6932685286","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad1482f4-daa1-4171-bf61-fa6932685286\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-02-25T10:05:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.46.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gmlzf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gmlzf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-1-2-67.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-1-2-67.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-25T10:05:32Z"}],"hostIP":"10.1.2.67","podIP":"10.10.46.110","podIPs":[{"ip":"10.10.46.110"}],"startTime":"2022-02-25T10:05:32Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-25T10:05:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"docker://0a452cae17f908f0a80c3d0b2270ff3be0795737e3339809cbc7a542233429b1","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:05:38.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9062" for this suite.

• [SLOW TEST:6.327 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":126,"skipped":2403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:05:38.171: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:10:38.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9961" for this suite.

• [SLOW TEST:300.207 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":127,"skipped":2426,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:10:38.379: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 25 10:10:38.860: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 25 10:10:40.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781380638, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781380638, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781380638, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781380638, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 10:10:43.904: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:10:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:10:52.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2991" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.870 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":128,"skipped":2434,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:10:52.249: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:10:52.431: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6bad25c-4a1d-49e1-b8ff-cb9f95acf26b" in namespace "downward-api-374" to be "Succeeded or Failed"
Feb 25 10:10:52.439: INFO: Pod "downwardapi-volume-c6bad25c-4a1d-49e1-b8ff-cb9f95acf26b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.75988ms
Feb 25 10:10:54.448: INFO: Pod "downwardapi-volume-c6bad25c-4a1d-49e1-b8ff-cb9f95acf26b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016921455s
STEP: Saw pod success
Feb 25 10:10:54.449: INFO: Pod "downwardapi-volume-c6bad25c-4a1d-49e1-b8ff-cb9f95acf26b" satisfied condition "Succeeded or Failed"
Feb 25 10:10:54.454: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-c6bad25c-4a1d-49e1-b8ff-cb9f95acf26b container client-container: <nil>
STEP: delete the pod
Feb 25 10:10:54.484: INFO: Waiting for pod downwardapi-volume-c6bad25c-4a1d-49e1-b8ff-cb9f95acf26b to disappear
Feb 25 10:10:54.488: INFO: Pod downwardapi-volume-c6bad25c-4a1d-49e1-b8ff-cb9f95acf26b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:10:54.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-374" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:10:54.504: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 25 10:10:54.679: INFO: The status of Pod labelsupdatecb0f8022-c0e0-4f75-bc7f-4c4d83b484e6 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:10:56.688: INFO: The status of Pod labelsupdatecb0f8022-c0e0-4f75-bc7f-4c4d83b484e6 is Running (Ready = true)
Feb 25 10:10:57.221: INFO: Successfully updated pod "labelsupdatecb0f8022-c0e0-4f75-bc7f-4c4d83b484e6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:11:01.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7313" for this suite.

• [SLOW TEST:6.778 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2472,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:11:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-1404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:11:01.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1404" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":131,"skipped":2487,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:11:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:11:01.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7786" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":132,"skipped":2498,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:11:01.728: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-9362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Feb 25 10:11:03.922: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:11:10.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9362" for this suite.

• [SLOW TEST:8.409 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":133,"skipped":2515,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:11:10.137: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-829a1f9e-cca9-410b-8c3e-088fd341470e
STEP: Creating a pod to test consume configMaps
Feb 25 10:11:10.317: INFO: Waiting up to 5m0s for pod "pod-configmaps-a394a911-66e5-4c07-9b8d-90f6dd31f580" in namespace "configmap-4645" to be "Succeeded or Failed"
Feb 25 10:11:10.325: INFO: Pod "pod-configmaps-a394a911-66e5-4c07-9b8d-90f6dd31f580": Phase="Pending", Reason="", readiness=false. Elapsed: 7.867704ms
Feb 25 10:11:12.334: INFO: Pod "pod-configmaps-a394a911-66e5-4c07-9b8d-90f6dd31f580": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016817004s
STEP: Saw pod success
Feb 25 10:11:12.334: INFO: Pod "pod-configmaps-a394a911-66e5-4c07-9b8d-90f6dd31f580" satisfied condition "Succeeded or Failed"
Feb 25 10:11:12.339: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-configmaps-a394a911-66e5-4c07-9b8d-90f6dd31f580 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:11:12.372: INFO: Waiting for pod pod-configmaps-a394a911-66e5-4c07-9b8d-90f6dd31f580 to disappear
Feb 25 10:11:12.377: INFO: Pod pod-configmaps-a394a911-66e5-4c07-9b8d-90f6dd31f580 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:11:12.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4645" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:11:12.405: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:11:20.623: INFO: Unable to read wheezy_udp@PodARecord from pod dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475: the server could not find the requested resource (get pods dns-test-be696352-5ebf-4490-a58a-eb941320e475)
Feb 25 10:11:20.628: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475: the server could not find the requested resource (get pods dns-test-be696352-5ebf-4490-a58a-eb941320e475)
Feb 25 10:11:20.633: INFO: Unable to read jessie_hosts@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local from pod dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475: the server could not find the requested resource (get pods dns-test-be696352-5ebf-4490-a58a-eb941320e475)
Feb 25 10:11:20.638: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475: the server could not find the requested resource (get pods dns-test-be696352-5ebf-4490-a58a-eb941320e475)
Feb 25 10:11:20.644: INFO: Unable to read jessie_udp@PodARecord from pod dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475: the server could not find the requested resource (get pods dns-test-be696352-5ebf-4490-a58a-eb941320e475)
Feb 25 10:11:20.649: INFO: Unable to read jessie_tcp@PodARecord from pod dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475: the server could not find the requested resource (get pods dns-test-be696352-5ebf-4490-a58a-eb941320e475)
Feb 25 10:11:20.649: INFO: Lookups using dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-2.dns-test-service-2.dns-408.svc.cluster.local jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 25 10:11:25.694: INFO: DNS probes using dns-408/dns-test-be696352-5ebf-4490-a58a-eb941320e475 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:11:25.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-408" for this suite.

• [SLOW TEST:13.359 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":135,"skipped":2638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:11:25.769: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9561
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:11:25.920: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:12:27.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9561" for this suite.

• [SLOW TEST:61.615 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":136,"skipped":2665,"failed":0}
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:12:27.384: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3384
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 25 10:12:29.579: INFO: &Pod{ObjectMeta:{send-events-76ebd83a-d752-47fb-a045-9581658b6a9a  events-3384  954a2049-1062-4ac2-b5c7-9d002acfb8b5 26080 0 2022-02-25 10:12:27 +0000 UTC <nil> <nil> map[name:foo time:541175978] map[kubernetes.io/psp:aws-cni] [] []  [{e2e.test Update v1 2022-02-25 10:12:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:12:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.19.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4d2hp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4d2hp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-179.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:12:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:12:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:12:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:12:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.179,PodIP:10.10.19.151,StartTime:2022-02-25 10:12:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:12:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://0c7e4c6afa7f7713150a482719e24741ffdc023725ebce6c841a9e516ec6238e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.19.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 25 10:12:31.595: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 25 10:12:33.603: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:12:33.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3384" for this suite.

• [SLOW TEST:6.259 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":137,"skipped":2665,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:12:33.642: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Feb 25 10:12:33.819: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:12:33.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1009" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":138,"skipped":2667,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:12:33.868: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:14:02.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7233" for this suite.

• [SLOW TEST:88.197 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":139,"skipped":2683,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:14:02.065: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:14:02.307: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 25 10:14:02.342: INFO: Number of nodes with available pods: 0
Feb 25 10:14:02.342: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 25 10:14:02.370: INFO: Number of nodes with available pods: 0
Feb 25 10:14:02.370: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:14:03.377: INFO: Number of nodes with available pods: 0
Feb 25 10:14:03.377: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:14:04.377: INFO: Number of nodes with available pods: 1
Feb 25 10:14:04.377: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 25 10:14:04.404: INFO: Number of nodes with available pods: 1
Feb 25 10:14:04.404: INFO: Number of running nodes: 0, number of available pods: 1
Feb 25 10:14:05.413: INFO: Number of nodes with available pods: 0
Feb 25 10:14:05.413: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 25 10:14:05.427: INFO: Number of nodes with available pods: 0
Feb 25 10:14:05.427: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:14:06.435: INFO: Number of nodes with available pods: 0
Feb 25 10:14:06.435: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:14:07.433: INFO: Number of nodes with available pods: 0
Feb 25 10:14:07.433: INFO: Node ip-10-1-2-27.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:14:08.434: INFO: Number of nodes with available pods: 1
Feb 25 10:14:08.434: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5058, will wait for the garbage collector to delete the pods
Feb 25 10:14:08.508: INFO: Deleting DaemonSet.extensions daemon-set took: 10.028904ms
Feb 25 10:14:08.609: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.073947ms
Feb 25 10:14:10.719: INFO: Number of nodes with available pods: 0
Feb 25 10:14:10.719: INFO: Number of running nodes: 0, number of available pods: 0
Feb 25 10:14:10.723: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26617"},"items":null}

Feb 25 10:14:10.728: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26617"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:14:10.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5058" for this suite.

• [SLOW TEST:8.719 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":140,"skipped":2694,"failed":0}
S
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:14:10.784: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-9615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Feb 25 10:14:10.968: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:14:12.976: INFO: The status of Pod pod1 is Running (Ready = false)
Feb 25 10:14:14.978: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.1.2.67 on the node which pod1 resides and expect scheduled
Feb 25 10:14:14.996: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:14:17.007: INFO: The status of Pod pod2 is Running (Ready = false)
Feb 25 10:14:19.006: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.1.2.67 but use UDP protocol on the node which pod2 resides
Feb 25 10:14:19.023: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:14:21.033: INFO: The status of Pod pod3 is Running (Ready = false)
Feb 25 10:14:23.033: INFO: The status of Pod pod3 is Running (Ready = true)
Feb 25 10:14:23.048: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:14:25.058: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Feb 25 10:14:25.063: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.1.2.67 http://127.0.0.1:54323/hostname] Namespace:hostport-9615 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:14:25.063: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.1.2.67, port: 54323
Feb 25 10:14:25.154: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.1.2.67:54323/hostname] Namespace:hostport-9615 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:14:25.154: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.1.2.67, port: 54323 UDP
Feb 25 10:14:25.234: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.1.2.67 54323] Namespace:hostport-9615 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:14:25.234: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:14:30.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9615" for this suite.

• [SLOW TEST:19.553 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":141,"skipped":2695,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:14:30.338: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3042
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:14:30.494: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:14:38.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3042" for this suite.

• [SLOW TEST:8.314 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":142,"skipped":2712,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:14:38.652: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-18
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-18
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Feb 25 10:14:38.825: INFO: Found 0 stateful pods, waiting for 3
Feb 25 10:14:48.839: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 10:14:48.839: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 10:14:48.839: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Feb 25 10:14:48.878: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 25 10:14:58.930: INFO: Updating stateful set ss2
Feb 25 10:14:58.940: INFO: Waiting for Pod statefulset-18/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Feb 25 10:15:09.007: INFO: Found 1 stateful pods, waiting for 3
Feb 25 10:15:19.021: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 10:15:19.021: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 10:15:19.021: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 25 10:15:19.054: INFO: Updating stateful set ss2
Feb 25 10:15:19.070: INFO: Waiting for Pod statefulset-18/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Feb 25 10:15:29.115: INFO: Updating stateful set ss2
Feb 25 10:15:29.124: INFO: Waiting for StatefulSet statefulset-18/ss2 to complete update
Feb 25 10:15:29.124: INFO: Waiting for Pod statefulset-18/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 10:15:39.143: INFO: Deleting all statefulset in ns statefulset-18
Feb 25 10:15:39.147: INFO: Scaling statefulset ss2 to 0
Feb 25 10:15:49.179: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 10:15:49.183: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:15:49.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-18" for this suite.

• [SLOW TEST:70.567 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":143,"skipped":2715,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:15:49.220: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Feb 25 10:15:49.398: INFO: observed Pod pod-test in namespace pods-4306 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 25 10:15:49.402: INFO: observed Pod pod-test in namespace pods-4306 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:49 +0000 UTC  }]
Feb 25 10:15:49.423: INFO: observed Pod pod-test in namespace pods-4306 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:49 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:49 +0000 UTC  }]
Feb 25 10:15:50.274: INFO: Found Pod pod-test in namespace pods-4306 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-25 10:15:49 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Feb 25 10:15:50.288: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Feb 25 10:15:50.323: INFO: observed event type ADDED
Feb 25 10:15:50.323: INFO: observed event type MODIFIED
Feb 25 10:15:50.323: INFO: observed event type MODIFIED
Feb 25 10:15:50.324: INFO: observed event type MODIFIED
Feb 25 10:15:50.324: INFO: observed event type MODIFIED
Feb 25 10:15:50.324: INFO: observed event type MODIFIED
Feb 25 10:15:50.324: INFO: observed event type MODIFIED
Feb 25 10:15:52.289: INFO: observed event type MODIFIED
Feb 25 10:15:53.303: INFO: observed event type MODIFIED
Feb 25 10:15:53.315: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:15:53.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4306" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":144,"skipped":2723,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:15:53.339: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 25 10:15:53.509: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 25 10:16:53.577: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Feb 25 10:16:53.614: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 25 10:16:53.626: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 25 10:16:53.651: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 25 10:16:53.663: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 25 10:16:53.690: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 25 10:16:53.702: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:17:07.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9862" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.573 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":145,"skipped":2742,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:17:07.913: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:17:08.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1396" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:17:08.107: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1536
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 25 10:17:08.258: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 25 10:17:08.270: INFO: Waiting for terminating namespaces to be deleted...
Feb 25 10:17:08.275: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-179.eu-central-1.compute.internal before test
Feb 25 10:17:08.286: INFO: aws-node-g7wjt from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 10:17:08.286: INFO: calico-node-565zj from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 10:17:08.286: INFO: cert-exporter-daemonset-t425f from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:17:08.286: INFO: coredns-6dc8c559ff-pbzck from kube-system started at 2022-02-25 10:14:54 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container coredns ready: true, restart count 0
Feb 25 10:17:08.286: INFO: ebs-csi-node-482mx from kube-system started at 2022-02-25 10:01:56 +0000 UTC (3 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 10:17:08.286: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 10:17:08.286: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 10:17:08.286: INFO: kiam-agent-mdxsl from kube-system started at 2022-02-25 10:01:56 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container kiam-agent ready: true, restart count 0
Feb 25 10:17:08.286: INFO: kiam-watchdog-4mgp6 from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 10:17:08.286: INFO: kube-proxy-qmkg2 from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 10:17:08.286: INFO: net-exporter-qfm5c from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 10:17:08.286: INFO: node-exporter-v1-3-1-ft9bj from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 10:17:08.286: INFO: bin-falseda7b2bee-a0ef-46b5-8d27-0e49eb398e87 from kubelet-test-1396 started at 2022-02-25 10:17:08 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container bin-falseda7b2bee-a0ef-46b5-8d27-0e49eb398e87 ready: false, restart count 0
Feb 25 10:17:08.286: INFO: pod0-1-sched-preemption-medium-priority from sched-preemption-9862 started at 2022-02-25 10:17:00 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container pod0-1-sched-preemption-medium-priority ready: true, restart count 0
Feb 25 10:17:08.286: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2dkfg from sonobuoy started at 2022-02-25 10:01:38 +0000 UTC (2 container statuses recorded)
Feb 25 10:17:08.286: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Feb 25 10:17:08.286: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 10:17:08.286: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-27.eu-central-1.compute.internal before test
Feb 25 10:17:08.297: INFO: aws-node-xhhxn from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.297: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 10:17:08.297: INFO: calico-node-bql82 from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.297: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 10:17:08.297: INFO: cert-exporter-daemonset-gzc9c from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.297: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:17:08.297: INFO: cert-exporter-deployment-b69c5dd78-rt48g from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.297: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:17:08.297: INFO: cert-manager-webhook-6c68d58df7-v8gjv from kube-system started at 2022-02-25 09:14:12 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container webhook ready: true, restart count 0
Feb 25 10:17:08.298: INFO: coredns-6dc8c559ff-w8szs from kube-system started at 2022-02-25 09:11:15 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container coredns ready: true, restart count 0
Feb 25 10:17:08.298: INFO: ebs-csi-node-9n94t from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 10:17:08.298: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 10:17:08.298: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 10:17:08.298: INFO: external-dns-d56669c5f-pkknw from kube-system started at 2022-02-25 09:40:50 +0000 UTC (2 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container external-dns ready: true, restart count 0
Feb 25 10:17:08.298: INFO: 	Container external-dns-check-iam ready: true, restart count 0
Feb 25 10:17:08.298: INFO: kiam-agent-6nk9v from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container kiam-agent ready: true, restart count 2
Feb 25 10:17:08.298: INFO: kiam-watchdog-szdf9 from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 10:17:08.298: INFO: kube-proxy-wcpg5 from kube-system started at 2022-02-25 09:06:05 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 10:17:08.298: INFO: net-exporter-sf92w from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 10:17:08.298: INFO: node-exporter-v1-3-1-2zzkq from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 10:17:08.298: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-vhpzn from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:17:08.298: INFO: vertical-pod-autoscaler-recommender-7ffc6b994d-pkqc2 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:17:08.298: INFO: vertical-pod-autoscaler-updater-5c6675d987-wwf47 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:17:08.298: INFO: pod1-0-sched-preemption-medium-priority from sched-preemption-9862 started at 2022-02-25 10:16:55 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container pod1-0-sched-preemption-medium-priority ready: true, restart count 0
Feb 25 10:17:08.298: INFO: pod1-1-sched-preemption-medium-priority from sched-preemption-9862 started at 2022-02-25 10:16:55 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container pod1-1-sched-preemption-medium-priority ready: true, restart count 0
Feb 25 10:17:08.298: INFO: sonobuoy from sonobuoy started at 2022-02-25 09:27:07 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 25 10:17:08.298: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-ql7w7 from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 10:17:08.298: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 10:17:08.298: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 10:17:08.298: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-67.eu-central-1.compute.internal before test
Feb 25 10:17:08.315: INFO: aws-node-jg76c from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.315: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 10:17:08.315: INFO: calico-node-kttwk from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.315: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 10:17:08.315: INFO: cert-exporter-daemonset-9rbmr from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.315: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:17:08.316: INFO: cert-manager-cainjector-ff5c85897-tpzhd from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container cainjector ready: true, restart count 0
Feb 25 10:17:08.316: INFO: ebs-csi-node-t9n4m from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 10:17:08.316: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 10:17:08.316: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 10:17:08.316: INFO: kiam-agent-8nj67 from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container kiam-agent ready: true, restart count 3
Feb 25 10:17:08.316: INFO: kiam-watchdog-xc6qf from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 10:17:08.316: INFO: kube-proxy-scj7q from kube-system started at 2022-02-25 09:06:06 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 10:17:08.316: INFO: kube-state-metrics-v2-3-0-6bd4cb8c9c-7jp46 from kube-system started at 2022-02-25 10:14:54 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 25 10:17:08.316: INFO: metrics-server-6dff979d99-nqcsg from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container metrics-server ready: true, restart count 0
Feb 25 10:17:08.316: INFO: net-exporter-x9rvx from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 10:17:08.316: INFO: node-exporter-v1-3-1-p97bn from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 10:17:08.316: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-m926h from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:17:08.316: INFO: pod2-0-sched-preemption-medium-priority from sched-preemption-9862 started at 2022-02-25 10:16:55 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.316: INFO: 	Container pod2-0-sched-preemption-medium-priority ready: true, restart count 0
Feb 25 10:17:08.316: INFO: pod2-1-sched-preemption-medium-priority from sched-preemption-9862 started at 2022-02-25 10:16:55 +0000 UTC (1 container statuses recorded)
Feb 25 10:17:08.317: INFO: 	Container pod2-1-sched-preemption-medium-priority ready: true, restart count 0
Feb 25 10:17:08.317: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2tbqx from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 10:17:08.317: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 10:17:08.317: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node ip-10-1-2-179.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-10-1-2-27.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod aws-node-g7wjt requesting resource cpu=30m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod aws-node-jg76c requesting resource cpu=30m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod aws-node-xhhxn requesting resource cpu=30m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod calico-node-565zj requesting resource cpu=250m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod calico-node-bql82 requesting resource cpu=250m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod calico-node-kttwk requesting resource cpu=250m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod cert-exporter-daemonset-9rbmr requesting resource cpu=50m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod cert-exporter-daemonset-gzc9c requesting resource cpu=50m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod cert-exporter-daemonset-t425f requesting resource cpu=50m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod cert-exporter-deployment-b69c5dd78-rt48g requesting resource cpu=50m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod cert-manager-cainjector-ff5c85897-tpzhd requesting resource cpu=10m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod cert-manager-webhook-6c68d58df7-v8gjv requesting resource cpu=20m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod coredns-6dc8c559ff-pbzck requesting resource cpu=250m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod coredns-6dc8c559ff-w8szs requesting resource cpu=250m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod ebs-csi-node-482mx requesting resource cpu=0m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod ebs-csi-node-9n94t requesting resource cpu=0m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod ebs-csi-node-t9n4m requesting resource cpu=0m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod external-dns-d56669c5f-pkknw requesting resource cpu=100m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kiam-agent-6nk9v requesting resource cpu=50m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kiam-agent-8nj67 requesting resource cpu=50m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kiam-agent-mdxsl requesting resource cpu=50m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kiam-watchdog-4mgp6 requesting resource cpu=200m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kiam-watchdog-szdf9 requesting resource cpu=200m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kiam-watchdog-xc6qf requesting resource cpu=200m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kube-proxy-qmkg2 requesting resource cpu=75m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kube-proxy-scj7q requesting resource cpu=75m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kube-proxy-wcpg5 requesting resource cpu=75m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod kube-state-metrics-v2-3-0-6bd4cb8c9c-7jp46 requesting resource cpu=500m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod metrics-server-6dff979d99-nqcsg requesting resource cpu=0m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod net-exporter-qfm5c requesting resource cpu=50m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod net-exporter-sf92w requesting resource cpu=50m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod net-exporter-x9rvx requesting resource cpu=50m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod node-exporter-v1-3-1-2zzkq requesting resource cpu=75m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod node-exporter-v1-3-1-ft9bj requesting resource cpu=75m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod node-exporter-v1-3-1-p97bn requesting resource cpu=75m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod vertical-pod-autoscaler-admission-controller-d8b6c54b4-m926h requesting resource cpu=50m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod vertical-pod-autoscaler-admission-controller-d8b6c54b4-vhpzn requesting resource cpu=50m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod vertical-pod-autoscaler-recommender-7ffc6b994d-pkqc2 requesting resource cpu=50m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod vertical-pod-autoscaler-updater-5c6675d987-wwf47 requesting resource cpu=50m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod bin-falseda7b2bee-a0ef-46b5-8d27-0e49eb398e87 requesting resource cpu=0m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod pod0-1-sched-preemption-medium-priority requesting resource cpu=0m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod pod1-0-sched-preemption-medium-priority requesting resource cpu=0m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod pod1-1-sched-preemption-medium-priority requesting resource cpu=0m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod pod2-0-sched-preemption-medium-priority requesting resource cpu=0m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod pod2-1-sched-preemption-medium-priority requesting resource cpu=0m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2dkfg requesting resource cpu=0m on Node ip-10-1-2-179.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2tbqx requesting resource cpu=0m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.410: INFO: Pod sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-ql7w7 requesting resource cpu=0m on Node ip-10-1-2-27.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Feb 25 10:17:08.410: INFO: Creating a pod which consumes cpu=1505m on Node ip-10-1-2-27.eu-central-1.compute.internal
Feb 25 10:17:08.424: INFO: Creating a pod which consumes cpu=1512m on Node ip-10-1-2-67.eu-central-1.compute.internal
Feb 25 10:17:08.434: INFO: Creating a pod which consumes cpu=1729m on Node ip-10-1-2-179.eu-central-1.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31c5264d-92d8-4743-9bac-0ceb34b57ad2.16d70009387ac9ee], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1536/filler-pod-31c5264d-92d8-4743-9bac-0ceb34b57ad2 to ip-10-1-2-27.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31c5264d-92d8-4743-9bac-0ceb34b57ad2.16d70009583f72bd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31c5264d-92d8-4743-9bac-0ceb34b57ad2.16d70009592f289f], Reason = [Created], Message = [Created container filler-pod-31c5264d-92d8-4743-9bac-0ceb34b57ad2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-31c5264d-92d8-4743-9bac-0ceb34b57ad2.16d700095cf2d653], Reason = [Started], Message = [Started container filler-pod-31c5264d-92d8-4743-9bac-0ceb34b57ad2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-37511994-57c3-4c46-a5b5-a4f1d913210d.16d70009391b22fc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1536/filler-pod-37511994-57c3-4c46-a5b5-a4f1d913210d to ip-10-1-2-67.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-37511994-57c3-4c46-a5b5-a4f1d913210d.16d700095a96d1ef], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-37511994-57c3-4c46-a5b5-a4f1d913210d.16d700095bd8d37d], Reason = [Created], Message = [Created container filler-pod-37511994-57c3-4c46-a5b5-a4f1d913210d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-37511994-57c3-4c46-a5b5-a4f1d913210d.16d700095f8f159a], Reason = [Started], Message = [Started container filler-pod-37511994-57c3-4c46-a5b5-a4f1d913210d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b72f959d-16d6-4784-944d-8a44dd985b17.16d7000939ae9bff], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1536/filler-pod-b72f959d-16d6-4784-944d-8a44dd985b17 to ip-10-1-2-179.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b72f959d-16d6-4784-944d-8a44dd985b17.16d700095cb5a688], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b72f959d-16d6-4784-944d-8a44dd985b17.16d700095dee1bd6], Reason = [Created], Message = [Created container filler-pod-b72f959d-16d6-4784-944d-8a44dd985b17]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b72f959d-16d6-4784-944d-8a44dd985b17.16d7000967d2aa04], Reason = [Started], Message = [Started container filler-pod-b72f959d-16d6-4784-944d-8a44dd985b17]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16d70009b3b26f5e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node ip-10-1-2-179.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-1-2-27.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-1-2-67.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:17:11.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1536" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":147,"skipped":2789,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:17:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:17:11.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7123" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":148,"skipped":2796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:17:11.771: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Feb 25 10:17:11.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-980 create -f -'
Feb 25 10:17:12.979: INFO: stderr: ""
Feb 25 10:17:12.979: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 25 10:17:13.988: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:17:13.988: INFO: Found 0 / 1
Feb 25 10:17:14.985: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:17:14.985: INFO: Found 1 / 1
Feb 25 10:17:14.985: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 25 10:17:14.990: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:17:14.990: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 25 10:17:14.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-980 patch pod agnhost-primary-sq29j -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 25 10:17:15.062: INFO: stderr: ""
Feb 25 10:17:15.062: INFO: stdout: "pod/agnhost-primary-sq29j patched\n"
STEP: checking annotations
Feb 25 10:17:15.068: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:17:15.068: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:17:15.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-980" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":149,"skipped":2840,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:17:15.082: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 25 10:17:15.248: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 25 10:17:20.261: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:17:21.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3544" for this suite.

• [SLOW TEST:6.224 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":150,"skipped":2849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:17:21.306: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Feb 25 10:17:21.474: INFO: Waiting up to 5m0s for pod "client-containers-36344d29-70fb-46fa-8043-93114d122945" in namespace "containers-2694" to be "Succeeded or Failed"
Feb 25 10:17:21.483: INFO: Pod "client-containers-36344d29-70fb-46fa-8043-93114d122945": Phase="Pending", Reason="", readiness=false. Elapsed: 9.1668ms
Feb 25 10:17:23.492: INFO: Pod "client-containers-36344d29-70fb-46fa-8043-93114d122945": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018612534s
STEP: Saw pod success
Feb 25 10:17:23.492: INFO: Pod "client-containers-36344d29-70fb-46fa-8043-93114d122945" satisfied condition "Succeeded or Failed"
Feb 25 10:17:23.497: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod client-containers-36344d29-70fb-46fa-8043-93114d122945 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:17:23.529: INFO: Waiting for pod client-containers-36344d29-70fb-46fa-8043-93114d122945 to disappear
Feb 25 10:17:23.534: INFO: Pod client-containers-36344d29-70fb-46fa-8043-93114d122945 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:17:23.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2694" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2893,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:17:23.549: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Feb 25 10:17:25.741: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5364 PodName:var-expansion-d23eef1d-1657-4ec9-941c-5e9accda3aa0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:17:25.741: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: test for file in mounted path
Feb 25 10:17:25.830: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5364 PodName:var-expansion-d23eef1d-1657-4ec9-941c-5e9accda3aa0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:17:25.830: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: updating the annotation value
Feb 25 10:17:26.423: INFO: Successfully updated pod "var-expansion-d23eef1d-1657-4ec9-941c-5e9accda3aa0"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Feb 25 10:17:26.428: INFO: Deleting pod "var-expansion-d23eef1d-1657-4ec9-941c-5e9accda3aa0" in namespace "var-expansion-5364"
Feb 25 10:17:26.440: INFO: Wait up to 5m0s for pod "var-expansion-d23eef1d-1657-4ec9-941c-5e9accda3aa0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:18:00.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5364" for this suite.

• [SLOW TEST:36.930 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":152,"skipped":2910,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:18:00.480: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 25 10:18:00.652: INFO: Waiting up to 5m0s for pod "pod-8322c845-068c-4b12-a365-5d559dd6c36f" in namespace "emptydir-2087" to be "Succeeded or Failed"
Feb 25 10:18:00.658: INFO: Pod "pod-8322c845-068c-4b12-a365-5d559dd6c36f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.78976ms
Feb 25 10:18:02.668: INFO: Pod "pod-8322c845-068c-4b12-a365-5d559dd6c36f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016376113s
STEP: Saw pod success
Feb 25 10:18:02.668: INFO: Pod "pod-8322c845-068c-4b12-a365-5d559dd6c36f" satisfied condition "Succeeded or Failed"
Feb 25 10:18:02.674: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-8322c845-068c-4b12-a365-5d559dd6c36f container test-container: <nil>
STEP: delete the pod
Feb 25 10:18:02.704: INFO: Waiting for pod pod-8322c845-068c-4b12-a365-5d559dd6c36f to disappear
Feb 25 10:18:02.708: INFO: Pod pod-8322c845-068c-4b12-a365-5d559dd6c36f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:18:02.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2087" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:18:02.723: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 25 10:18:03.351: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 25 10:18:05.369: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381083, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381083, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381083, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381083, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 10:18:08.394: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:18:08.401: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:18:16.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1039" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:13.878 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":154,"skipped":2956,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:18:16.602: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-1bd2c538-fc69-4c8c-b96a-0a127083c419 in namespace container-probe-5448
Feb 25 10:18:18.792: INFO: Started pod busybox-1bd2c538-fc69-4c8c-b96a-0a127083c419 in namespace container-probe-5448
STEP: checking the pod's current state and verifying that restartCount is present
Feb 25 10:18:18.797: INFO: Initial restart count of pod busybox-1bd2c538-fc69-4c8c-b96a-0a127083c419 is 0
Feb 25 10:19:09.043: INFO: Restart count of pod container-probe-5448/busybox-1bd2c538-fc69-4c8c-b96a-0a127083c419 is now 1 (50.246259691s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:09.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5448" for this suite.

• [SLOW TEST:52.475 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:09.077: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 10:19:09.565: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 10:19:12.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 25 10:19:14.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=webhook-7622 attach --namespace=webhook-7622 to-be-attached-pod -i -c=container1'
Feb 25 10:19:14.727: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:14.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7622" for this suite.
STEP: Destroying namespace "webhook-7622-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.739 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":156,"skipped":3034,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:14.818: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:19:14.979: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 25 10:19:14.991: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 25 10:19:20.005: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 25 10:19:20.005: INFO: Creating deployment "test-rolling-update-deployment"
Feb 25 10:19:20.015: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 25 10:19:20.026: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 25 10:19:22.040: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 25 10:19:22.045: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 10:19:22.058: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6373  179ca6b0-fb97-4c90-8f86-3b1bfe95e9f4 29320 1 2022-02-25 10:19:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-02-25 10:19:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:19:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046d9388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-25 10:19:20 +0000 UTC,LastTransitionTime:2022-02-25 10:19:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-02-25 10:19:21 +0000 UTC,LastTransitionTime:2022-02-25 10:19:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 25 10:19:22.063: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-6373  905d2a38-5b6c-4332-8503-259b5eaa0a13 29310 1 2022-02-25 10:19:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 179ca6b0-fb97-4c90-8f86-3b1bfe95e9f4 0xc0046d98f7 0xc0046d98f8}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:19:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"179ca6b0-fb97-4c90-8f86-3b1bfe95e9f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:19:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046d9b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:19:22.063: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 25 10:19:22.063: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6373  e9aa04a8-c490-4b1a-96ac-f2434e316b8f 29319 2 2022-02-25 10:19:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 179ca6b0-fb97-4c90-8f86-3b1bfe95e9f4 0xc0046d9757 0xc0046d9758}] []  [{e2e.test Update apps/v1 2022-02-25 10:19:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:19:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"179ca6b0-fb97-4c90-8f86-3b1bfe95e9f4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:19:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046d9818 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:19:22.067: INFO: Pod "test-rolling-update-deployment-585b757574-dnzbc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-dnzbc test-rolling-update-deployment-585b757574- deployment-6373  274c5c01-07f5-4a4d-a828-133653894930 29309 0 2022-02-25 10:19:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 905d2a38-5b6c-4332-8503-259b5eaa0a13 0xc00465c5f7 0xc00465c5f8}] []  [{kube-controller-manager Update v1 2022-02-25 10:19:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"905d2a38-5b6c-4332-8503-259b5eaa0a13\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:19:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.19.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vhv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vhv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-179.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:19:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:19:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.179,PodIP:10.10.19.184,StartTime:2022-02-25 10:19:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:19:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://ce968855a66ce48bbf3c66a5c9f3e51f692f45fe92bdff3b1c834b0d13a2f195,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.19.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:22.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6373" for this suite.

• [SLOW TEST:7.264 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":157,"skipped":3053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:22.083: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1329
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-1329
Feb 25 10:19:22.257: INFO: Found 0 stateful pods, waiting for 1
Feb 25 10:19:32.274: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 10:19:32.318: INFO: Deleting all statefulset in ns statefulset-1329
Feb 25 10:19:32.327: INFO: Scaling statefulset ss to 0
Feb 25 10:19:42.366: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 10:19:42.371: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:42.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1329" for this suite.

• [SLOW TEST:20.324 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":158,"skipped":3093,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:42.407: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Feb 25 10:19:42.580: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:19:44.588: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 25 10:19:45.617: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:46.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5478" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":159,"skipped":3108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:46.661: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-5808badf-0d6e-4418-86cc-51d38ef192d8
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:46.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4561" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":160,"skipped":3137,"failed":0}
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:46.828: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-4928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:46.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4928" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":161,"skipped":3143,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:47.002: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:47.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1432" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":162,"skipped":3193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:47.216: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5326
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-d82c1827-1ee4-4851-a604-b7bb206443c2
STEP: Creating secret with name s-test-opt-upd-4d97eb92-35c2-4798-859a-9f2ecf922f1e
STEP: Creating the pod
Feb 25 10:19:47.428: INFO: The status of Pod pod-secrets-f40b3cd0-f6ed-4555-8663-4859ae2ed301 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:19:49.436: INFO: The status of Pod pod-secrets-f40b3cd0-f6ed-4555-8663-4859ae2ed301 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-d82c1827-1ee4-4851-a604-b7bb206443c2
STEP: Updating secret s-test-opt-upd-4d97eb92-35c2-4798-859a-9f2ecf922f1e
STEP: Creating secret with name s-test-opt-create-83302d7a-26f3-4c59-a0d2-ed0c1355c8f6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:19:53.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5326" for this suite.

• [SLOW TEST:6.357 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":3250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:19:53.574: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:20:21.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3438" for this suite.

• [SLOW TEST:28.242 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":164,"skipped":3335,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:20:21.816: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3168
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Feb 25 10:20:21.968: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3168 proxy --unix-socket=/tmp/kubectl-proxy-unix431468551/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:20:22.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3168" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":165,"skipped":3369,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:20:22.017: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4010
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-1bbf9a14-02b7-4af9-acb3-fd07791d1a21-4236
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:20:22.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4010" for this suite.
STEP: Destroying namespace "nspatchtest-1bbf9a14-02b7-4af9-acb3-fd07791d1a21-4236" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":166,"skipped":3371,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:20:22.345: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6208
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:20:22.493: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 25 10:20:30.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 create -f -'
Feb 25 10:20:31.438: INFO: stderr: ""
Feb 25 10:20:31.438: INFO: stdout: "e2e-test-crd-publish-openapi-9933-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 25 10:20:31.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 delete e2e-test-crd-publish-openapi-9933-crds test-foo'
Feb 25 10:20:31.526: INFO: stderr: ""
Feb 25 10:20:31.526: INFO: stdout: "e2e-test-crd-publish-openapi-9933-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 25 10:20:31.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 apply -f -'
Feb 25 10:20:31.680: INFO: stderr: ""
Feb 25 10:20:31.680: INFO: stdout: "e2e-test-crd-publish-openapi-9933-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 25 10:20:31.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 delete e2e-test-crd-publish-openapi-9933-crds test-foo'
Feb 25 10:20:31.764: INFO: stderr: ""
Feb 25 10:20:31.764: INFO: stdout: "e2e-test-crd-publish-openapi-9933-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 25 10:20:31.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 create -f -'
Feb 25 10:20:32.288: INFO: rc: 1
Feb 25 10:20:32.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 apply -f -'
Feb 25 10:20:32.429: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 25 10:20:32.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 create -f -'
Feb 25 10:20:32.567: INFO: rc: 1
Feb 25 10:20:32.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 --namespace=crd-publish-openapi-6208 apply -f -'
Feb 25 10:20:32.698: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 25 10:20:32.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 explain e2e-test-crd-publish-openapi-9933-crds'
Feb 25 10:20:32.831: INFO: stderr: ""
Feb 25 10:20:32.831: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9933-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 25 10:20:32.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 explain e2e-test-crd-publish-openapi-9933-crds.metadata'
Feb 25 10:20:33.370: INFO: stderr: ""
Feb 25 10:20:33.370: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9933-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 25 10:20:33.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 explain e2e-test-crd-publish-openapi-9933-crds.spec'
Feb 25 10:20:33.506: INFO: stderr: ""
Feb 25 10:20:33.506: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9933-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 25 10:20:33.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 explain e2e-test-crd-publish-openapi-9933-crds.spec.bars'
Feb 25 10:20:33.655: INFO: stderr: ""
Feb 25 10:20:33.655: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9933-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 25 10:20:33.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-6208 explain e2e-test-crd-publish-openapi-9933-crds.spec.bars2'
Feb 25 10:20:33.800: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:20:36.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6208" for this suite.

• [SLOW TEST:14.580 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":167,"skipped":3376,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:20:36.926: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-qwv8
STEP: Creating a pod to test atomic-volume-subpath
Feb 25 10:20:37.101: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qwv8" in namespace "subpath-8954" to be "Succeeded or Failed"
Feb 25 10:20:37.107: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644843ms
Feb 25 10:20:39.116: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015388393s
Feb 25 10:20:41.125: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 4.024035116s
Feb 25 10:20:43.134: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 6.033291808s
Feb 25 10:20:45.142: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 8.041647267s
Feb 25 10:20:47.153: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 10.052351069s
Feb 25 10:20:49.161: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 12.060630229s
Feb 25 10:20:51.171: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 14.070322373s
Feb 25 10:20:53.181: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 16.080867065s
Feb 25 10:20:55.191: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 18.090100193s
Feb 25 10:20:57.200: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Running", Reason="", readiness=true. Elapsed: 20.09905275s
Feb 25 10:20:59.208: INFO: Pod "pod-subpath-test-secret-qwv8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.107148097s
STEP: Saw pod success
Feb 25 10:20:59.208: INFO: Pod "pod-subpath-test-secret-qwv8" satisfied condition "Succeeded or Failed"
Feb 25 10:20:59.212: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-subpath-test-secret-qwv8 container test-container-subpath-secret-qwv8: <nil>
STEP: delete the pod
Feb 25 10:20:59.239: INFO: Waiting for pod pod-subpath-test-secret-qwv8 to disappear
Feb 25 10:20:59.243: INFO: Pod pod-subpath-test-secret-qwv8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-qwv8
Feb 25 10:20:59.243: INFO: Deleting pod "pod-subpath-test-secret-qwv8" in namespace "subpath-8954"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:20:59.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8954" for this suite.

• [SLOW TEST:22.339 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":168,"skipped":3386,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:20:59.265: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 25 10:20:59.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9564 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 25 10:20:59.500: INFO: stderr: ""
Feb 25 10:20:59.500: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 25 10:21:04.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9564 get pod e2e-test-httpd-pod -o json'
Feb 25 10:21:04.620: INFO: stderr: ""
Feb 25 10:21:04.620: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"aws-cni\"\n        },\n        \"creationTimestamp\": \"2022-02-25T10:20:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9564\",\n        \"resourceVersion\": \"30074\",\n        \"uid\": \"b87764b8-c1d2-4c51-813f-6c09fe2c6574\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-j7kbm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-1-2-179.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-j7kbm\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-25T10:20:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-25T10:21:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-25T10:21:01Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-25T10:20:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://37f5304268e1c5d83855dc68efef344946fb0d79de6c5e975be9e19169ba9dfb\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-02-25T10:21:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.1.2.179\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.25.239\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.25.239\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-02-25T10:20:59Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 25 10:21:04.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9564 replace -f -'
Feb 25 10:21:05.291: INFO: stderr: ""
Feb 25 10:21:05.291: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Feb 25 10:21:05.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9564 delete pods e2e-test-httpd-pod'
Feb 25 10:21:07.125: INFO: stderr: ""
Feb 25 10:21:07.125: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:21:07.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9564" for this suite.

• [SLOW TEST:7.879 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":169,"skipped":3397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:21:07.145: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 10:21:07.703: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 10:21:09.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381267, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381267, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381267, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381267, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 10:21:12.752: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:21:12.760: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7488-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:21:20.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1023" for this suite.
STEP: Destroying namespace "webhook-1023-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.898 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":170,"skipped":3428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:21:21.046: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 25 10:21:21.241: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:21:23.250: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 25 10:21:23.274: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:21:25.284: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 25 10:21:25.307: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 25 10:21:25.312: INFO: Pod pod-with-poststart-http-hook still exists
Feb 25 10:21:27.312: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 25 10:21:27.321: INFO: Pod pod-with-poststart-http-hook still exists
Feb 25 10:21:29.312: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 25 10:21:29.320: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:21:29.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9814" for this suite.

• [SLOW TEST:8.293 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:21:29.339: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Feb 25 10:21:31.531: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:21:33.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-714" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":172,"skipped":3498,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:21:33.604: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:21:33.766: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5debd1a8-a787-432e-8e25-e452ee06ff6b" in namespace "security-context-test-6670" to be "Succeeded or Failed"
Feb 25 10:21:33.774: INFO: Pod "busybox-user-65534-5debd1a8-a787-432e-8e25-e452ee06ff6b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.421928ms
Feb 25 10:21:35.780: INFO: Pod "busybox-user-65534-5debd1a8-a787-432e-8e25-e452ee06ff6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013892808s
Feb 25 10:21:35.780: INFO: Pod "busybox-user-65534-5debd1a8-a787-432e-8e25-e452ee06ff6b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:21:35.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6670" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3511,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:21:35.796: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-efe3a77d-3650-4969-89bf-7ac02fcfc342
STEP: Creating a pod to test consume configMaps
Feb 25 10:21:35.968: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f44974bf-f421-4d69-97bd-3cd75ce4990b" in namespace "projected-2792" to be "Succeeded or Failed"
Feb 25 10:21:35.973: INFO: Pod "pod-projected-configmaps-f44974bf-f421-4d69-97bd-3cd75ce4990b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.43972ms
Feb 25 10:21:37.982: INFO: Pod "pod-projected-configmaps-f44974bf-f421-4d69-97bd-3cd75ce4990b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014498706s
STEP: Saw pod success
Feb 25 10:21:37.982: INFO: Pod "pod-projected-configmaps-f44974bf-f421-4d69-97bd-3cd75ce4990b" satisfied condition "Succeeded or Failed"
Feb 25 10:21:37.987: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-configmaps-f44974bf-f421-4d69-97bd-3cd75ce4990b container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:21:38.018: INFO: Waiting for pod pod-projected-configmaps-f44974bf-f421-4d69-97bd-3cd75ce4990b to disappear
Feb 25 10:21:38.023: INFO: Pod pod-projected-configmaps-f44974bf-f421-4d69-97bd-3cd75ce4990b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:21:38.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2792" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":3538,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:21:38.038: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-8c797364-1f0d-472e-89a3-7fed66ed2dea in namespace container-probe-2336
Feb 25 10:21:40.218: INFO: Started pod test-webserver-8c797364-1f0d-472e-89a3-7fed66ed2dea in namespace container-probe-2336
STEP: checking the pod's current state and verifying that restartCount is present
Feb 25 10:21:40.223: INFO: Initial restart count of pod test-webserver-8c797364-1f0d-472e-89a3-7fed66ed2dea is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:25:41.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2336" for this suite.

• [SLOW TEST:243.307 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3544,"failed":0}
S
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:25:41.345: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-7954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Feb 25 10:25:41.520: INFO: Waiting up to 5m0s for pod "security-context-b27f7b52-f513-4284-8dbf-0bf67b5346a3" in namespace "security-context-7954" to be "Succeeded or Failed"
Feb 25 10:25:41.528: INFO: Pod "security-context-b27f7b52-f513-4284-8dbf-0bf67b5346a3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.924158ms
Feb 25 10:25:43.537: INFO: Pod "security-context-b27f7b52-f513-4284-8dbf-0bf67b5346a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016402271s
STEP: Saw pod success
Feb 25 10:25:43.537: INFO: Pod "security-context-b27f7b52-f513-4284-8dbf-0bf67b5346a3" satisfied condition "Succeeded or Failed"
Feb 25 10:25:43.542: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod security-context-b27f7b52-f513-4284-8dbf-0bf67b5346a3 container test-container: <nil>
STEP: delete the pod
Feb 25 10:25:43.574: INFO: Waiting for pod security-context-b27f7b52-f513-4284-8dbf-0bf67b5346a3 to disappear
Feb 25 10:25:43.578: INFO: Pod security-context-b27f7b52-f513-4284-8dbf-0bf67b5346a3 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:25:43.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7954" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":176,"skipped":3545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:25:43.594: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-1e001498-d5d0-4855-97b4-ec90911282ca
STEP: Creating a pod to test consume secrets
Feb 25 10:25:43.770: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-86dbc00c-4678-4117-947d-84ef861becf5" in namespace "projected-1085" to be "Succeeded or Failed"
Feb 25 10:25:43.780: INFO: Pod "pod-projected-secrets-86dbc00c-4678-4117-947d-84ef861becf5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.005445ms
Feb 25 10:25:45.789: INFO: Pod "pod-projected-secrets-86dbc00c-4678-4117-947d-84ef861becf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017931288s
STEP: Saw pod success
Feb 25 10:25:45.789: INFO: Pod "pod-projected-secrets-86dbc00c-4678-4117-947d-84ef861becf5" satisfied condition "Succeeded or Failed"
Feb 25 10:25:45.793: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-secrets-86dbc00c-4678-4117-947d-84ef861becf5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 25 10:25:45.823: INFO: Waiting for pod pod-projected-secrets-86dbc00c-4678-4117-947d-84ef861becf5 to disappear
Feb 25 10:25:45.827: INFO: Pod pod-projected-secrets-86dbc00c-4678-4117-947d-84ef861becf5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:25:45.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1085" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3581,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:25:45.842: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:25:46.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-097efcb9-41f5-4681-90e6-5d1c04ccc393" in namespace "downward-api-6791" to be "Succeeded or Failed"
Feb 25 10:25:46.014: INFO: Pod "downwardapi-volume-097efcb9-41f5-4681-90e6-5d1c04ccc393": Phase="Pending", Reason="", readiness=false. Elapsed: 6.604873ms
Feb 25 10:25:48.022: INFO: Pod "downwardapi-volume-097efcb9-41f5-4681-90e6-5d1c04ccc393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014505349s
STEP: Saw pod success
Feb 25 10:25:48.022: INFO: Pod "downwardapi-volume-097efcb9-41f5-4681-90e6-5d1c04ccc393" satisfied condition "Succeeded or Failed"
Feb 25 10:25:48.027: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-097efcb9-41f5-4681-90e6-5d1c04ccc393 container client-container: <nil>
STEP: delete the pod
Feb 25 10:25:48.056: INFO: Waiting for pod downwardapi-volume-097efcb9-41f5-4681-90e6-5d1c04ccc393 to disappear
Feb 25 10:25:48.061: INFO: Pod downwardapi-volume-097efcb9-41f5-4681-90e6-5d1c04ccc393 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:25:48.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6791" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":3585,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:25:48.075: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9328
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9328
STEP: Waiting until pod test-pod will start running in namespace statefulset-9328
STEP: Creating statefulset with conflicting port in namespace statefulset-9328
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9328
Feb 25 10:25:50.296: INFO: Observed stateful pod in namespace: statefulset-9328, name: ss-0, uid: 2e2ebe12-a558-40cf-92a3-06848221c254, status phase: Pending. Waiting for statefulset controller to delete.
Feb 25 10:25:50.316: INFO: Observed stateful pod in namespace: statefulset-9328, name: ss-0, uid: 2e2ebe12-a558-40cf-92a3-06848221c254, status phase: Failed. Waiting for statefulset controller to delete.
Feb 25 10:25:50.324: INFO: Observed stateful pod in namespace: statefulset-9328, name: ss-0, uid: 2e2ebe12-a558-40cf-92a3-06848221c254, status phase: Failed. Waiting for statefulset controller to delete.
Feb 25 10:25:50.327: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9328
STEP: Removing pod with conflicting port in namespace statefulset-9328
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9328 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 10:25:52.370: INFO: Deleting all statefulset in ns statefulset-9328
Feb 25 10:25:52.375: INFO: Scaling statefulset ss to 0
Feb 25 10:26:02.408: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 10:26:02.412: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:26:02.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9328" for this suite.

• [SLOW TEST:14.375 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":179,"skipped":3590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:26:02.452: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3705
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-3705
Feb 25 10:26:02.623: INFO: Found 0 stateful pods, waiting for 1
Feb 25 10:26:12.641: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Feb 25 10:26:12.666: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Feb 25 10:26:12.678: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Feb 25 10:26:12.680: INFO: Observed &StatefulSet event: ADDED
Feb 25 10:26:12.680: INFO: Found Statefulset ss in namespace statefulset-3705 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 25 10:26:12.680: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Feb 25 10:26:12.680: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 25 10:26:12.691: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Feb 25 10:26:12.693: INFO: Observed &StatefulSet event: ADDED
Feb 25 10:26:12.693: INFO: Observed Statefulset ss in namespace statefulset-3705 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 25 10:26:12.693: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 10:26:12.693: INFO: Deleting all statefulset in ns statefulset-3705
Feb 25 10:26:12.698: INFO: Scaling statefulset ss to 0
Feb 25 10:26:22.727: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 10:26:22.731: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:26:22.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3705" for this suite.

• [SLOW TEST:20.312 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":180,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:26:22.764: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-340
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-340
I0225 10:26:22.961221      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-340, replica count: 2
Feb 25 10:26:26.015: INFO: Creating new exec pod
I0225 10:26:26.015050      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 10:26:29.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-340 exec execpodxw6v5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 25 10:26:29.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 25 10:26:29.194: INFO: stdout: "externalname-service-492g4"
Feb 25 10:26:29.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-340 exec execpodxw6v5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.4.136 80'
Feb 25 10:26:29.332: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.4.136 80\nConnection to 172.31.4.136 80 port [tcp/http] succeeded!\n"
Feb 25 10:26:29.332: INFO: stdout: ""
Feb 25 10:26:30.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-340 exec execpodxw6v5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.4.136 80'
Feb 25 10:26:30.463: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.4.136 80\nConnection to 172.31.4.136 80 port [tcp/http] succeeded!\n"
Feb 25 10:26:30.463: INFO: stdout: "externalname-service-492g4"
Feb 25 10:26:30.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-340 exec execpodxw6v5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.179 32615'
Feb 25 10:26:30.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.179 32615\nConnection to 10.1.2.179 32615 port [tcp/*] succeeded!\n"
Feb 25 10:26:30.608: INFO: stdout: "externalname-service-492g4"
Feb 25 10:26:30.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-340 exec execpodxw6v5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.67 32615'
Feb 25 10:26:30.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.67 32615\nConnection to 10.1.2.67 32615 port [tcp/*] succeeded!\n"
Feb 25 10:26:30.746: INFO: stdout: "externalname-service-nzpzq"
Feb 25 10:26:30.746: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:26:30.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-340" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.031 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":181,"skipped":3662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:26:30.797: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:26:37.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4672" for this suite.

• [SLOW TEST:7.195 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":182,"skipped":3727,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:26:37.992: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:27:03.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7486" for this suite.

• [SLOW TEST:25.538 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3731,"failed":0}
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:27:03.531: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-6558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:27:05.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6558" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":184,"skipped":3732,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:27:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-732
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:27:05.893: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 25 10:27:14.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-732 --namespace=crd-publish-openapi-732 create -f -'
Feb 25 10:27:14.816: INFO: stderr: ""
Feb 25 10:27:14.816: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 25 10:27:14.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-732 --namespace=crd-publish-openapi-732 delete e2e-test-crd-publish-openapi-7121-crds test-cr'
Feb 25 10:27:14.876: INFO: stderr: ""
Feb 25 10:27:14.876: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 25 10:27:14.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-732 --namespace=crd-publish-openapi-732 apply -f -'
Feb 25 10:27:15.466: INFO: stderr: ""
Feb 25 10:27:15.466: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 25 10:27:15.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-732 --namespace=crd-publish-openapi-732 delete e2e-test-crd-publish-openapi-7121-crds test-cr'
Feb 25 10:27:15.527: INFO: stderr: ""
Feb 25 10:27:15.527: INFO: stdout: "e2e-test-crd-publish-openapi-7121-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 25 10:27:15.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=crd-publish-openapi-732 explain e2e-test-crd-publish-openapi-7121-crds'
Feb 25 10:27:15.671: INFO: stderr: ""
Feb 25 10:27:15.671: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7121-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:27:20.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-732" for this suite.

• [SLOW TEST:14.503 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":185,"skipped":3766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:27:20.245: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:27:20.409: INFO: The status of Pod server-envvars-6039ad32-5677-4901-8d92-3d781480bdfd is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:27:22.416: INFO: The status of Pod server-envvars-6039ad32-5677-4901-8d92-3d781480bdfd is Running (Ready = true)
Feb 25 10:27:22.454: INFO: Waiting up to 5m0s for pod "client-envvars-2b71315e-e41b-4d15-9e12-13a36d869633" in namespace "pods-5885" to be "Succeeded or Failed"
Feb 25 10:27:22.457: INFO: Pod "client-envvars-2b71315e-e41b-4d15-9e12-13a36d869633": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116725ms
Feb 25 10:27:24.464: INFO: Pod "client-envvars-2b71315e-e41b-4d15-9e12-13a36d869633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010001377s
STEP: Saw pod success
Feb 25 10:27:24.464: INFO: Pod "client-envvars-2b71315e-e41b-4d15-9e12-13a36d869633" satisfied condition "Succeeded or Failed"
Feb 25 10:27:24.467: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod client-envvars-2b71315e-e41b-4d15-9e12-13a36d869633 container env3cont: <nil>
STEP: delete the pod
Feb 25 10:27:24.491: INFO: Waiting for pod client-envvars-2b71315e-e41b-4d15-9e12-13a36d869633 to disappear
Feb 25 10:27:24.494: INFO: Pod client-envvars-2b71315e-e41b-4d15-9e12-13a36d869633 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:27:24.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5885" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3806,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:27:24.503: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 25 10:27:24.659: INFO: The status of Pod labelsupdate20beb987-3188-47d7-a9af-339025538d95 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:27:26.666: INFO: The status of Pod labelsupdate20beb987-3188-47d7-a9af-339025538d95 is Running (Ready = true)
Feb 25 10:27:27.187: INFO: Successfully updated pod "labelsupdate20beb987-3188-47d7-a9af-339025538d95"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:27:31.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8251" for this suite.

• [SLOW TEST:6.723 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3807,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:27:31.226: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:27:31.383: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5868ce71-8af0-4449-91b2-b96cda9b6fba" in namespace "projected-2263" to be "Succeeded or Failed"
Feb 25 10:27:31.388: INFO: Pod "downwardapi-volume-5868ce71-8af0-4449-91b2-b96cda9b6fba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.552659ms
Feb 25 10:27:33.394: INFO: Pod "downwardapi-volume-5868ce71-8af0-4449-91b2-b96cda9b6fba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010076592s
STEP: Saw pod success
Feb 25 10:27:33.394: INFO: Pod "downwardapi-volume-5868ce71-8af0-4449-91b2-b96cda9b6fba" satisfied condition "Succeeded or Failed"
Feb 25 10:27:33.396: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-5868ce71-8af0-4449-91b2-b96cda9b6fba container client-container: <nil>
STEP: delete the pod
Feb 25 10:27:33.419: INFO: Waiting for pod downwardapi-volume-5868ce71-8af0-4449-91b2-b96cda9b6fba to disappear
Feb 25 10:27:33.422: INFO: Pod downwardapi-volume-5868ce71-8af0-4449-91b2-b96cda9b6fba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:27:33.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2263" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3813,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:27:33.431: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:27:41.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6614" for this suite.

• [SLOW TEST:8.163 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":189,"skipped":3818,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:27:41.594: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 25 10:27:41.772: INFO: PodSpec: initContainers in spec.initContainers
Feb 25 10:28:27.460: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-609bc93c-6c62-4f2a-a801-ad0c2c9d8169", GenerateName:"", Namespace:"init-container-5757", SelfLink:"", UID:"b513b457-0065-4bef-a306-3d1919497c43", ResourceVersion:"32688", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63781381661, loc:(*time.Location)(0xa0aae00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"772962463"}, Annotations:map[string]string{"kubernetes.io/psp":"aws-cni"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00456ff80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00456ff98), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0016a2000), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016a2018), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-6zthd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003790760), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6zthd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6zthd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6zthd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0038d89a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-1-2-179.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003754e70), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038d8a20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038d8a40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0038d8a48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0038d8a4c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000cc9400), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381661, loc:(*time.Location)(0xa0aae00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381661, loc:(*time.Location)(0xa0aae00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381661, loc:(*time.Location)(0xa0aae00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781381661, loc:(*time.Location)(0xa0aae00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.1.2.179", PodIP:"10.10.22.6", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.22.6"}}, StartTime:(*v1.Time)(0xc0016a2048), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003754f50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003754fc0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"docker://1391eeab8e8d9b3ba1da1b92c5cea264e2024cfce9c2336ea905395abae906b8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003790800), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0037907e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc0038d8acf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:28:27.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5757" for this suite.

• [SLOW TEST:45.880 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":190,"skipped":3819,"failed":0}
SS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:28:27.474: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:28:27.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-205" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":191,"skipped":3821,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:28:27.667: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-8229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8229 to expose endpoints map[]
Feb 25 10:28:27.818: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 25 10:28:28.828: INFO: successfully validated that service multi-endpoint-test in namespace services-8229 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8229
Feb 25 10:28:28.842: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:28:30.849: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8229 to expose endpoints map[pod1:[100]]
Feb 25 10:28:30.859: INFO: successfully validated that service multi-endpoint-test in namespace services-8229 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8229
Feb 25 10:28:30.871: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:28:32.879: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8229 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 25 10:28:32.895: INFO: successfully validated that service multi-endpoint-test in namespace services-8229 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Feb 25 10:28:32.895: INFO: Creating new exec pod
Feb 25 10:28:35.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8229 exec execpodskjw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb 25 10:28:36.048: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 25 10:28:36.048: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:28:36.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8229 exec execpodskjw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.13.76 80'
Feb 25 10:28:36.175: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.13.76 80\nConnection to 172.31.13.76 80 port [tcp/http] succeeded!\n"
Feb 25 10:28:36.175: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:28:36.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8229 exec execpodskjw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb 25 10:28:36.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 25 10:28:36.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:28:36.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8229 exec execpodskjw9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.13.76 81'
Feb 25 10:28:36.456: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.13.76 81\nConnection to 172.31.13.76 81 port [tcp/*] succeeded!\n"
Feb 25 10:28:36.456: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8229 to expose endpoints map[pod2:[101]]
Feb 25 10:28:36.489: INFO: successfully validated that service multi-endpoint-test in namespace services-8229 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8229 to expose endpoints map[]
Feb 25 10:28:37.532: INFO: successfully validated that service multi-endpoint-test in namespace services-8229 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:28:37.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8229" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.892 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":192,"skipped":3828,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:28:37.559: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2777.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2777.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2777.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2777.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2777.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2777.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:28:39.738: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02: the server could not find the requested resource (get pods dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02)
Feb 25 10:28:39.740: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02: the server could not find the requested resource (get pods dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02)
Feb 25 10:28:39.743: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-2777.svc.cluster.local from pod dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02: the server could not find the requested resource (get pods dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02)
Feb 25 10:28:39.746: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02: the server could not find the requested resource (get pods dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02)
Feb 25 10:28:39.749: INFO: Unable to read jessie_udp@PodARecord from pod dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02: the server could not find the requested resource (get pods dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02)
Feb 25 10:28:39.752: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02: the server could not find the requested resource (get pods dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02)
Feb 25 10:28:39.752: INFO: Lookups using dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1.dns-test-service.dns-2777.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 25 10:28:44.780: INFO: DNS probes using dns-2777/dns-test-c23d11ae-0535-4805-846c-d5de5e72cc02 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:28:44.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2777" for this suite.

• [SLOW TEST:7.248 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":193,"skipped":3842,"failed":0}
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:28:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Feb 25 10:28:44.961: INFO: Waiting up to 5m0s for pod "client-containers-04cd4c72-9055-45bd-8e4e-b20953eaf147" in namespace "containers-4143" to be "Succeeded or Failed"
Feb 25 10:28:44.965: INFO: Pod "client-containers-04cd4c72-9055-45bd-8e4e-b20953eaf147": Phase="Pending", Reason="", readiness=false. Elapsed: 3.422643ms
Feb 25 10:28:46.969: INFO: Pod "client-containers-04cd4c72-9055-45bd-8e4e-b20953eaf147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007317892s
STEP: Saw pod success
Feb 25 10:28:46.969: INFO: Pod "client-containers-04cd4c72-9055-45bd-8e4e-b20953eaf147" satisfied condition "Succeeded or Failed"
Feb 25 10:28:46.972: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod client-containers-04cd4c72-9055-45bd-8e4e-b20953eaf147 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:28:46.991: INFO: Waiting for pod client-containers-04cd4c72-9055-45bd-8e4e-b20953eaf147 to disappear
Feb 25 10:28:46.994: INFO: Pod client-containers-04cd4c72-9055-45bd-8e4e-b20953eaf147 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:28:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4143" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:28:47.006: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:28:47.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8414 version'
Feb 25 10:28:47.194: INFO: stderr: ""
Feb 25 10:28:47.194: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.6\", GitCommit:\"f59f5c2fda36e4036b49ec027e556a15456108f0\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:33:06Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.6\", GitCommit:\"f59f5c2fda36e4036b49ec027e556a15456108f0\", GitTreeState:\"clean\", BuildDate:\"2022-01-19T17:26:47Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:28:47.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8414" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":195,"skipped":3878,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:28:47.205: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9435
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 25 10:28:47.348: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 25 10:28:47.399: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:28:49.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:28:51.405: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:28:53.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:28:55.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:28:57.403: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 25 10:28:57.409: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:28:59.416: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:29:01.415: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:29:03.416: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:29:05.416: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 25 10:29:07.416: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 25 10:29:07.424: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 25 10:29:09.451: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 25 10:29:09.451: INFO: Breadth first check of 10.10.11.47 on host 10.1.2.179...
Feb 25 10:29:09.454: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.26.176:9080/dial?request=hostname&protocol=udp&host=10.10.11.47&port=8081&tries=1'] Namespace:pod-network-test-9435 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:29:09.454: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:29:09.530: INFO: Waiting for responses: map[]
Feb 25 10:29:09.530: INFO: reached 10.10.11.47 after 0/1 tries
Feb 25 10:29:09.530: INFO: Breadth first check of 10.10.23.77 on host 10.1.2.27...
Feb 25 10:29:09.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.26.176:9080/dial?request=hostname&protocol=udp&host=10.10.23.77&port=8081&tries=1'] Namespace:pod-network-test-9435 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:29:09.533: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:29:09.616: INFO: Waiting for responses: map[]
Feb 25 10:29:09.616: INFO: reached 10.10.23.77 after 0/1 tries
Feb 25 10:29:09.616: INFO: Breadth first check of 10.10.19.65 on host 10.1.2.67...
Feb 25 10:29:09.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.26.176:9080/dial?request=hostname&protocol=udp&host=10.10.19.65&port=8081&tries=1'] Namespace:pod-network-test-9435 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:29:09.620: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:29:09.702: INFO: Waiting for responses: map[]
Feb 25 10:29:09.702: INFO: reached 10.10.19.65 after 0/1 tries
Feb 25 10:29:09.702: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:29:09.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9435" for this suite.

• [SLOW TEST:22.506 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3948,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:29:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8084
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Feb 25 10:29:09.857: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:29:36.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8084" for this suite.

• [SLOW TEST:26.590 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":197,"skipped":3959,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:29:36.303: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:29:36.456: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 25 10:29:38.499: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:29:39.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5880" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":198,"skipped":3968,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:29:39.518: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1293.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1293.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:29:41.707: INFO: File wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local from pod  dns-1293/dns-test-cc183866-63b3-4560-a05f-56c538517e50 contains '' instead of 'foo.example.com.'
Feb 25 10:29:41.710: INFO: File jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local from pod  dns-1293/dns-test-cc183866-63b3-4560-a05f-56c538517e50 contains '' instead of 'foo.example.com.'
Feb 25 10:29:41.710: INFO: Lookups using dns-1293/dns-test-cc183866-63b3-4560-a05f-56c538517e50 failed for: [wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local]

Feb 25 10:29:46.718: INFO: DNS probes using dns-test-cc183866-63b3-4560-a05f-56c538517e50 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1293.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1293.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:29:48.782: INFO: File wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local from pod  dns-1293/dns-test-69c4590a-e011-435a-8f2c-0d6e21053be2 contains '' instead of 'bar.example.com.'
Feb 25 10:29:48.785: INFO: File jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local from pod  dns-1293/dns-test-69c4590a-e011-435a-8f2c-0d6e21053be2 contains '' instead of 'bar.example.com.'
Feb 25 10:29:48.785: INFO: Lookups using dns-1293/dns-test-69c4590a-e011-435a-8f2c-0d6e21053be2 failed for: [wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local]

Feb 25 10:29:53.793: INFO: DNS probes using dns-test-69c4590a-e011-435a-8f2c-0d6e21053be2 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1293.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1293.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:29:55.871: INFO: File wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local from pod  dns-1293/dns-test-e226c6ba-0b51-4918-8906-2b94df1745cc contains '' instead of '172.31.46.103'
Feb 25 10:29:55.874: INFO: File jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local from pod  dns-1293/dns-test-e226c6ba-0b51-4918-8906-2b94df1745cc contains '' instead of '172.31.46.103'
Feb 25 10:29:55.874: INFO: Lookups using dns-1293/dns-test-e226c6ba-0b51-4918-8906-2b94df1745cc failed for: [wheezy_udp@dns-test-service-3.dns-1293.svc.cluster.local jessie_udp@dns-test-service-3.dns-1293.svc.cluster.local]

Feb 25 10:30:00.885: INFO: DNS probes using dns-test-e226c6ba-0b51-4918-8906-2b94df1745cc succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:00.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1293" for this suite.

• [SLOW TEST:21.414 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":199,"skipped":3976,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:00.933: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:30:01.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b705bb23-f4bc-463d-a372-595bf5ba2bfe" in namespace "projected-6902" to be "Succeeded or Failed"
Feb 25 10:30:01.100: INFO: Pod "downwardapi-volume-b705bb23-f4bc-463d-a372-595bf5ba2bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.932248ms
Feb 25 10:30:03.105: INFO: Pod "downwardapi-volume-b705bb23-f4bc-463d-a372-595bf5ba2bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012408494s
STEP: Saw pod success
Feb 25 10:30:03.105: INFO: Pod "downwardapi-volume-b705bb23-f4bc-463d-a372-595bf5ba2bfe" satisfied condition "Succeeded or Failed"
Feb 25 10:30:03.108: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-b705bb23-f4bc-463d-a372-595bf5ba2bfe container client-container: <nil>
STEP: delete the pod
Feb 25 10:30:03.130: INFO: Waiting for pod downwardapi-volume-b705bb23-f4bc-463d-a372-595bf5ba2bfe to disappear
Feb 25 10:30:03.134: INFO: Pod downwardapi-volume-b705bb23-f4bc-463d-a372-595bf5ba2bfe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:03.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6902" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3977,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:03.145: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-1724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:09.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1724" for this suite.

• [SLOW TEST:6.215 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":201,"skipped":3978,"failed":0}
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:09.360: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 25 10:30:09.536: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:09.536: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:09.536: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:09.540: INFO: Number of nodes with available pods: 0
Feb 25 10:30:09.540: INFO: Node ip-10-1-2-179.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:30:10.547: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:10.547: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:10.547: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:10.550: INFO: Number of nodes with available pods: 1
Feb 25 10:30:10.550: INFO: Node ip-10-1-2-179.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:30:11.546: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:11.546: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:11.546: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:30:11.549: INFO: Number of nodes with available pods: 3
Feb 25 10:30:11.549: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Feb 25 10:30:11.555: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Feb 25 10:30:11.564: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Feb 25 10:30:11.566: INFO: Observed &DaemonSet event: ADDED
Feb 25 10:30:11.566: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.566: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.566: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.567: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.567: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.567: INFO: Found daemon set daemon-set in namespace daemonsets-4883 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 25 10:30:11.567: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Feb 25 10:30:11.575: INFO: Observed &DaemonSet event: ADDED
Feb 25 10:30:11.575: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.575: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.575: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.576: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.576: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.576: INFO: Observed daemon set daemon-set in namespace daemonsets-4883 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 25 10:30:11.576: INFO: Observed &DaemonSet event: MODIFIED
Feb 25 10:30:11.576: INFO: Found daemon set daemon-set in namespace daemonsets-4883 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 25 10:30:11.576: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4883, will wait for the garbage collector to delete the pods
Feb 25 10:30:11.646: INFO: Deleting DaemonSet.extensions daemon-set took: 5.940417ms
Feb 25 10:30:11.747: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.248681ms
Feb 25 10:30:14.055: INFO: Number of nodes with available pods: 0
Feb 25 10:30:14.055: INFO: Number of running nodes: 0, number of available pods: 0
Feb 25 10:30:14.057: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33668"},"items":null}

Feb 25 10:30:14.060: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33668"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:14.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4883" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":202,"skipped":3978,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:14.088: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7032
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:27.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7032" for this suite.

• [SLOW TEST:13.233 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":203,"skipped":3978,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:27.321: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 25 10:30:27.463: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 25 10:30:27.470: INFO: Waiting for terminating namespaces to be deleted...
Feb 25 10:30:27.473: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-179.eu-central-1.compute.internal before test
Feb 25 10:30:27.482: INFO: aws-node-g7wjt from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 10:30:27.482: INFO: calico-node-565zj from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 10:30:27.482: INFO: cert-exporter-daemonset-t425f from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:30:27.482: INFO: coredns-6dc8c559ff-pbzck from kube-system started at 2022-02-25 10:14:54 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container coredns ready: true, restart count 0
Feb 25 10:30:27.482: INFO: ebs-csi-node-482mx from kube-system started at 2022-02-25 10:01:56 +0000 UTC (3 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 10:30:27.482: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 10:30:27.482: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 10:30:27.482: INFO: kiam-agent-mdxsl from kube-system started at 2022-02-25 10:01:56 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container kiam-agent ready: true, restart count 0
Feb 25 10:30:27.482: INFO: kiam-watchdog-4mgp6 from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 10:30:27.482: INFO: kube-proxy-qmkg2 from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 10:30:27.482: INFO: net-exporter-qfm5c from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 10:30:27.482: INFO: node-exporter-v1-3-1-ft9bj from kube-system started at 2022-02-25 10:01:38 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 10:30:27.482: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2dkfg from sonobuoy started at 2022-02-25 10:01:38 +0000 UTC (2 container statuses recorded)
Feb 25 10:30:27.482: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Feb 25 10:30:27.482: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 10:30:27.482: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-27.eu-central-1.compute.internal before test
Feb 25 10:30:27.491: INFO: aws-node-xhhxn from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 10:30:27.491: INFO: calico-node-bql82 from kube-system started at 2022-02-25 09:05:35 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 10:30:27.491: INFO: cert-exporter-daemonset-gzc9c from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:30:27.491: INFO: cert-exporter-deployment-b69c5dd78-rt48g from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:30:27.491: INFO: cert-manager-webhook-6c68d58df7-v8gjv from kube-system started at 2022-02-25 09:14:12 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container webhook ready: true, restart count 0
Feb 25 10:30:27.491: INFO: coredns-6dc8c559ff-w8szs from kube-system started at 2022-02-25 09:11:15 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container coredns ready: true, restart count 0
Feb 25 10:30:27.491: INFO: ebs-csi-node-9n94t from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 10:30:27.491: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 10:30:27.491: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 10:30:27.491: INFO: external-dns-d56669c5f-pkknw from kube-system started at 2022-02-25 09:40:50 +0000 UTC (2 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container external-dns ready: true, restart count 0
Feb 25 10:30:27.491: INFO: 	Container external-dns-check-iam ready: true, restart count 0
Feb 25 10:30:27.491: INFO: kiam-agent-6nk9v from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container kiam-agent ready: true, restart count 2
Feb 25 10:30:27.491: INFO: kiam-watchdog-szdf9 from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 10:30:27.491: INFO: kube-proxy-wcpg5 from kube-system started at 2022-02-25 09:06:05 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 10:30:27.491: INFO: net-exporter-sf92w from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 10:30:27.491: INFO: node-exporter-v1-3-1-2zzkq from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 10:30:27.491: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-vhpzn from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:30:27.491: INFO: vertical-pod-autoscaler-recommender-7ffc6b994d-pkqc2 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:30:27.491: INFO: vertical-pod-autoscaler-updater-5c6675d987-wwf47 from kube-system started at 2022-02-25 09:13:57 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:30:27.491: INFO: sonobuoy from sonobuoy started at 2022-02-25 09:27:07 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 25 10:30:27.491: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-ql7w7 from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 10:30:27.491: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 10:30:27.491: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 25 10:30:27.491: INFO: 
Logging pods the apiserver thinks is on node ip-10-1-2-67.eu-central-1.compute.internal before test
Feb 25 10:30:27.500: INFO: aws-node-jg76c from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.500: INFO: 	Container aws-node ready: true, restart count 0
Feb 25 10:30:27.500: INFO: calico-node-kttwk from kube-system started at 2022-02-25 09:05:52 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.500: INFO: 	Container calico-node ready: true, restart count 0
Feb 25 10:30:27.500: INFO: cert-exporter-daemonset-9rbmr from kube-system started at 2022-02-25 09:13:19 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.500: INFO: 	Container cert-exporter ready: true, restart count 0
Feb 25 10:30:27.500: INFO: cert-manager-cainjector-ff5c85897-tpzhd from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.500: INFO: 	Container cainjector ready: true, restart count 0
Feb 25 10:30:27.500: INFO: ebs-csi-node-t9n4m from kube-system started at 2022-02-25 09:11:15 +0000 UTC (3 container statuses recorded)
Feb 25 10:30:27.500: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 25 10:30:27.500: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 25 10:30:27.500: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 25 10:30:27.500: INFO: kiam-agent-8nj67 from kube-system started at 2022-02-25 09:13:20 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.500: INFO: 	Container kiam-agent ready: true, restart count 3
Feb 25 10:30:27.500: INFO: kiam-watchdog-xc6qf from kube-system started at 2022-02-25 09:13:18 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.500: INFO: 	Container kiam-watchdog ready: true, restart count 0
Feb 25 10:30:27.500: INFO: kube-proxy-scj7q from kube-system started at 2022-02-25 09:06:06 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.501: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 25 10:30:27.501: INFO: kube-state-metrics-v2-3-0-6bd4cb8c9c-7jp46 from kube-system started at 2022-02-25 10:14:54 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.501: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 25 10:30:27.501: INFO: metrics-server-6dff979d99-nqcsg from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.501: INFO: 	Container metrics-server ready: true, restart count 0
Feb 25 10:30:27.501: INFO: net-exporter-x9rvx from kube-system started at 2022-02-25 09:11:17 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.501: INFO: 	Container net-exporter ready: true, restart count 0
Feb 25 10:30:27.501: INFO: node-exporter-v1-3-1-p97bn from kube-system started at 2022-02-25 09:13:23 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.501: INFO: 	Container node-exporter ready: true, restart count 0
Feb 25 10:30:27.501: INFO: vertical-pod-autoscaler-admission-controller-d8b6c54b4-m926h from kube-system started at 2022-02-25 09:40:50 +0000 UTC (1 container statuses recorded)
Feb 25 10:30:27.501: INFO: 	Container vertical-pod-autoscaler-app ready: true, restart count 0
Feb 25 10:30:27.501: INFO: sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2tbqx from sonobuoy started at 2022-02-25 09:27:13 +0000 UTC (2 container statuses recorded)
Feb 25 10:30:27.501: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 25 10:30:27.501: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2099374a-6307-4193-a4cd-65a2983e8161 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2099374a-6307-4193-a4cd-65a2983e8161 off the node ip-10-1-2-179.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2099374a-6307-4193-a4cd-65a2983e8161
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:31.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1186" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":204,"skipped":3983,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:31.599: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 25 10:30:31.761: INFO: Waiting up to 5m0s for pod "pod-443b8407-249c-4c6d-b746-409d45b6fed0" in namespace "emptydir-3754" to be "Succeeded or Failed"
Feb 25 10:30:31.768: INFO: Pod "pod-443b8407-249c-4c6d-b746-409d45b6fed0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794001ms
Feb 25 10:30:33.774: INFO: Pod "pod-443b8407-249c-4c6d-b746-409d45b6fed0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013310117s
STEP: Saw pod success
Feb 25 10:30:33.774: INFO: Pod "pod-443b8407-249c-4c6d-b746-409d45b6fed0" satisfied condition "Succeeded or Failed"
Feb 25 10:30:33.777: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-443b8407-249c-4c6d-b746-409d45b6fed0 container test-container: <nil>
STEP: delete the pod
Feb 25 10:30:33.798: INFO: Waiting for pod pod-443b8407-249c-4c6d-b746-409d45b6fed0 to disappear
Feb 25 10:30:33.801: INFO: Pod pod-443b8407-249c-4c6d-b746-409d45b6fed0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:33.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3754" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":205,"skipped":3984,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:33.812: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 25 10:30:33.954: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:37.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4893" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":206,"skipped":4005,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:37.595: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-ffb2be9c-df12-4f60-bc6d-5c163b4357c2
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:37.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4697" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":207,"skipped":4015,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:37.752: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:30:37.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3501 create -f -'
Feb 25 10:30:38.515: INFO: stderr: ""
Feb 25 10:30:38.515: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 25 10:30:38.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3501 create -f -'
Feb 25 10:30:39.188: INFO: stderr: ""
Feb 25 10:30:39.188: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 25 10:30:40.194: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:30:40.194: INFO: Found 1 / 1
Feb 25 10:30:40.194: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 25 10:30:40.198: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:30:40.198: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 25 10:30:40.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3501 describe pod agnhost-primary-zxksd'
Feb 25 10:30:40.273: INFO: stderr: ""
Feb 25 10:30:40.273: INFO: stdout: "Name:         agnhost-primary-zxksd\nNamespace:    kubectl-3501\nPriority:     0\nNode:         ip-10-1-2-179.eu-central-1.compute.internal/10.1.2.179\nStart Time:   Fri, 25 Feb 2022 10:30:38 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.10.43.196\nIPs:\n  IP:           10.10.43.196\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://000a38bab05800c467c6d8ae4e5c27e73013dfc3c7568fa99c376f771dc6ce6d\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 25 Feb 2022 10:30:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m8tgl (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-m8tgl:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3501/agnhost-primary-zxksd to ip-10-1-2-179.eu-central-1.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Feb 25 10:30:40.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3501 describe rc agnhost-primary'
Feb 25 10:30:40.360: INFO: stderr: ""
Feb 25 10:30:40.360: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3501\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-zxksd\n"
Feb 25 10:30:40.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3501 describe service agnhost-primary'
Feb 25 10:30:40.438: INFO: stderr: ""
Feb 25 10:30:40.438: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3501\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.31.182.37\nIPs:               172.31.182.37\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.43.196:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 25 10:30:40.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3501 describe node ip-10-1-2-179.eu-central-1.compute.internal'
Feb 25 10:30:40.536: INFO: stderr: ""
Feb 25 10:30:40.536: INFO: stdout: "Name:               ip-10-1-2-179.eu-central-1.compute.internal\nRoles:              worker\nLabels:             aws-operator.giantswarm.io/version=10.17.0\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1a\n                    giantswarm.io/machine-deployment=e4z4w\n                    giantswarm.io/provider=aws\n                    ip=10.1.2.179\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-1-2-179.eu-central-1.compute.internal\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=worker\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=m5.xlarge\n                    node.kubernetes.io/worker=\n                    role=worker\n                    topology.ebs.csi.aws.com/zone=eu-central-1a\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1a\n                    vpc.amazonaws.com/eniConfig=eu-central-1a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0da4c480d96bfc102\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.1.2.179/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 25 Feb 2022 10:01:37 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-1-2-179.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 25 Feb 2022 10:30:32 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 25 Feb 2022 10:27:02 +0000   Fri, 25 Feb 2022 10:01:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 25 Feb 2022 10:27:02 +0000   Fri, 25 Feb 2022 10:01:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 25 Feb 2022 10:27:02 +0000   Fri, 25 Feb 2022 10:01:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 25 Feb 2022 10:27:02 +0000   Fri, 25 Feb 2022 10:02:07 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.1.2.179\n  Hostname:     ip-10-1-2-179.eu-central-1.compute.internal\n  InternalDNS:  ip-10-1-2-179.eu-central-1.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         4\n  ephemeral-storage:           102350Mi\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16090540Ki\n  pods:                        47\n  scheduling.k8s.io/foo:       5\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         3500m\n  ephemeral-storage:           101326Mi\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      14706092Ki\n  pods:                        47\n  scheduling.k8s.io/foo:       5\nSystem Info:\n  Machine ID:                 ec2be5950643e32cf06786b7fdf3a4a7\n  System UUID:                ec2be595-0643-e32c-f067-86b7fdf3a4a7\n  Boot ID:                    a455b7ff-fc7f-48e6-b0f9-1d80a7eff89c\n  Kernel Version:             5.10.96-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 3033.2.2 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.12\n  Kubelet Version:            v1.22.6\n  Kube-Proxy Version:         v1.22.6\nProviderID:                   aws:///eu-central-1a/i-0da4c480d96bfc102\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 aws-node-g7wjt                                             30m (0%)      0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 calico-node-565zj                                          250m (7%)     0 (0%)      150Mi (1%)       0 (0%)         29m\n  kube-system                 cert-exporter-daemonset-t425f                              50m (1%)      150m (4%)   50Mi (0%)        50Mi (0%)      29m\n  kube-system                 coredns-6dc8c559ff-pbzck                                   250m (7%)     0 (0%)      192Mi (1%)       192Mi (1%)     15m\n  kube-system                 ebs-csi-node-482mx                                         10m (0%)      10m (0%)    50Mi (0%)        50Mi (0%)      28m\n  kube-system                 kiam-agent-mdxsl                                           50m (1%)      0 (0%)      50Mi (0%)        50Mi (0%)      28m\n  kube-system                 kiam-watchdog-4mgp6                                        200m (5%)     200m (5%)   200Mi (1%)       200Mi (1%)     29m\n  kube-system                 kube-proxy-qmkg2                                           75m (2%)      0 (0%)      80Mi (0%)        0 (0%)         29m\n  kube-system                 net-exporter-qfm5c                                         50m (1%)      0 (0%)      75Mi (0%)        150Mi (1%)     29m\n  kube-system                 node-exporter-v1-3-1-ft9bj                                 75m (2%)      0 (0%)      50Mi (0%)        75Mi (0%)      29m\n  kubectl-3501                agnhost-primary-zxksd                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b2a7d08dbd194ce4-2dkfg    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         1040m (29%)  360m (10%)\n  memory                      897Mi (6%)   767Mi (5%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  hugepages-1Gi               0 (0%)       0 (0%)\n  hugepages-2Mi               0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\n  scheduling.k8s.io/foo       0            0\nEvents:\n  Type    Reason                   Age                From        Message\n  ----    ------                   ----               ----        -------\n  Normal  Starting                 28m                kube-proxy  \n  Normal  Starting                 29m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  29m (x2 over 29m)  kubelet     Node ip-10-1-2-179.eu-central-1.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    29m (x2 over 29m)  kubelet     Node ip-10-1-2-179.eu-central-1.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     29m (x2 over 29m)  kubelet     Node ip-10-1-2-179.eu-central-1.compute.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  29m                kubelet     Updated Node Allocatable limit across pods\n  Normal  NodeReady                28m                kubelet     Node ip-10-1-2-179.eu-central-1.compute.internal status is now: NodeReady\n"
Feb 25 10:30:40.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-3501 describe namespace kubectl-3501'
Feb 25 10:30:40.609: INFO: stderr: ""
Feb 25 10:30:40.609: INFO: stdout: "Name:         kubectl-3501\nLabels:       e2e-framework=kubectl\n              e2e-run=6d994d1c-d9a8-48f3-9b8e-d365eea525f1\n              kubernetes.io/metadata.name=kubectl-3501\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:40.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3501" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":208,"skipped":4015,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:40.622: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8759
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Feb 25 10:30:40.772: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:40.772: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:40.786: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:40.786: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:40.799: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:40.799: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:40.835: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:40.835: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 25 10:30:41.657: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 25 10:30:41.657: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 25 10:30:41.946: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Feb 25 10:30:41.959: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 0
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:41.961: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:41.972: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:41.972: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:41.989: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:41.989: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:42.023: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:42.023: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:42.038: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:42.038: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:43.765: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:43.765: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:43.792: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
STEP: listing Deployments
Feb 25 10:30:43.802: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Feb 25 10:30:43.812: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Feb 25 10:30:43.822: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:43.826: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:43.844: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:43.870: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:43.884: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:43.891: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:44.736: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:44.801: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:44.827: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:44.851: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 25 10:30:45.777: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Feb 25 10:30:45.832: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:45.832: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:45.833: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:45.833: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:45.833: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:45.833: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 1
Feb 25 10:30:45.834: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:45.834: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 3
Feb 25 10:30:45.834: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:45.834: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 2
Feb 25 10:30:45.834: INFO: observed Deployment test-deployment in namespace deployment-8759 with ReadyReplicas 3
STEP: deleting the Deployment
Feb 25 10:30:45.844: INFO: observed event type MODIFIED
Feb 25 10:30:45.844: INFO: observed event type MODIFIED
Feb 25 10:30:45.844: INFO: observed event type MODIFIED
Feb 25 10:30:45.844: INFO: observed event type MODIFIED
Feb 25 10:30:45.844: INFO: observed event type MODIFIED
Feb 25 10:30:45.844: INFO: observed event type MODIFIED
Feb 25 10:30:45.844: INFO: observed event type MODIFIED
Feb 25 10:30:45.845: INFO: observed event type MODIFIED
Feb 25 10:30:45.845: INFO: observed event type MODIFIED
Feb 25 10:30:45.845: INFO: observed event type MODIFIED
Feb 25 10:30:45.845: INFO: observed event type MODIFIED
Feb 25 10:30:45.845: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 10:30:45.848: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 25 10:30:45.851: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-8759  f06be06c-941e-4057-9214-6eee5ad1f9d5 34145 4 2022-02-25 10:30:41 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 28f9fe9d-7a3e-44e1-861c-82112548caa6 0xc003b38f17 0xc003b38f18}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:30:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28f9fe9d-7a3e-44e1-861c-82112548caa6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:30:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b38fc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 25 10:30:45.859: INFO: pod: "test-deployment-56c98d85f9-hml5w":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-hml5w test-deployment-56c98d85f9- deployment-8759  7c147243-d8f0-4144-8386-ff196d6172d1 34109 0 2022-02-25 10:30:41 +0000 UTC 2022-02-25 10:30:45 +0000 UTC 0xc003b39588 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 f06be06c-941e-4057-9214-6eee5ad1f9d5 0xc003b395b7 0xc003b395b8}] []  [{kube-controller-manager Update v1 2022-02-25 10:30:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f06be06c-941e-4057-9214-6eee5ad1f9d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:30:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.50.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xm6hw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xm6hw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-179.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.179,PodIP:10.10.50.4,StartTime:2022-02-25 10:30:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:30:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:docker://8e3ba120497dc6361919e37873d6958ed277420c0b29d5603c18a99d977f7397,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.50.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 25 10:30:45.860: INFO: pod: "test-deployment-56c98d85f9-r6pfl":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-r6pfl test-deployment-56c98d85f9- deployment-8759  d8a1e825-93d2-40a2-8f6b-573d1e0843b5 34141 0 2022-02-25 10:30:43 +0000 UTC 2022-02-25 10:30:46 +0000 UTC 0xc003b397d0 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 f06be06c-941e-4057-9214-6eee5ad1f9d5 0xc003b39827 0xc003b39828}] []  [{kube-controller-manager Update v1 2022-02-25 10:30:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f06be06c-941e-4057-9214-6eee5ad1f9d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:30:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.59.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxlm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxlm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:10.10.59.61,StartTime:2022-02-25 10:30:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:30:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:docker://41b07193a8b5295604b080878ee6d28085384283a3237e2b3cac9230b9b8fbd5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.59.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 25 10:30:45.860: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-8759  ded2798e-c8fd-448f-a1af-6edccbf36fcc 34062 3 2022-02-25 10:30:40 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 28f9fe9d-7a3e-44e1-861c-82112548caa6 0xc003b39037 0xc003b39038}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:30:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28f9fe9d-7a3e-44e1-861c-82112548caa6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:30:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b390e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 25 10:30:45.865: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-8759  fcaec8ec-1d23-4915-8ddf-72dc917e314f 34136 2 2022-02-25 10:30:43 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 28f9fe9d-7a3e-44e1-861c-82112548caa6 0xc003b39157 0xc003b39158}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:30:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28f9fe9d-7a3e-44e1-861c-82112548caa6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:30:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003b392a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb 25 10:30:45.870: INFO: pod: "test-deployment-d4dfddfbf-qlpxb":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-qlpxb test-deployment-d4dfddfbf- deployment-8759  7cef7b15-c71b-49bc-8aee-b688ecdbcfc4 34104 0 2022-02-25 10:30:43 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf fcaec8ec-1d23-4915-8ddf-72dc917e314f 0xc00624ce47 0xc00624ce48}] []  [{kube-controller-manager Update v1 2022-02-25 10:30:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcaec8ec-1d23-4915-8ddf-72dc917e314f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:30:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.22.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rlx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rlx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-179.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.179,PodIP:10.10.22.6,StartTime:2022-02-25 10:30:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:30:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://e5a40b7e5c3776fc56db06b00d504de176e9d7018303aac573847b96050d8916,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.22.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 25 10:30:45.870: INFO: pod: "test-deployment-d4dfddfbf-z4qfn":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-z4qfn test-deployment-d4dfddfbf- deployment-8759  44f93ed8-184a-473c-9904-81e6db06c0d7 34135 0 2022-02-25 10:30:44 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf fcaec8ec-1d23-4915-8ddf-72dc917e314f 0xc00624d0b7 0xc00624d0b8}] []  [{kube-controller-manager Update v1 2022-02-25 10:30:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcaec8ec-1d23-4915-8ddf-72dc917e314f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:30:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.46.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bq8gq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bq8gq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-27.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:30:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.27,PodIP:10.10.46.127,StartTime:2022-02-25 10:30:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:30:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://0c2e991a02548db4d5d566661faf7f915f17bce4e78b94125b37254c2e660514,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.46.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:45.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8759" for this suite.

• [SLOW TEST:5.266 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":209,"skipped":4016,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:45.888: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7272
STEP: creating service affinity-nodeport-transition in namespace services-7272
STEP: creating replication controller affinity-nodeport-transition in namespace services-7272
I0225 10:30:46.055382      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7272, replica count: 3
I0225 10:30:49.107021      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 10:30:49.120: INFO: Creating new exec pod
Feb 25 10:30:52.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7272 exec execpod-affinity945ds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb 25 10:30:52.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 25 10:30:52.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:30:52.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7272 exec execpod-affinity945ds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.204.247 80'
Feb 25 10:30:52.465: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.204.247 80\nConnection to 172.31.204.247 80 port [tcp/http] succeeded!\n"
Feb 25 10:30:52.465: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:30:52.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7272 exec execpod-affinity945ds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.67 31213'
Feb 25 10:30:52.594: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.67 31213\nConnection to 10.1.2.67 31213 port [tcp/*] succeeded!\n"
Feb 25 10:30:52.594: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:30:52.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7272 exec execpod-affinity945ds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.1.2.179 31213'
Feb 25 10:30:52.728: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.1.2.179 31213\nConnection to 10.1.2.179 31213 port [tcp/*] succeeded!\n"
Feb 25 10:30:52.728: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:30:52.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7272 exec execpod-affinity945ds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.1.2.179:31213/ ; done'
Feb 25 10:30:52.945: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n"
Feb 25 10:30:52.945: INFO: stdout: "\naffinity-nodeport-transition-5g22n\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-8lkh7\naffinity-nodeport-transition-8lkh7\naffinity-nodeport-transition-8lkh7\naffinity-nodeport-transition-5g22n\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-8lkh7\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-8lkh7\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-5g22n\naffinity-nodeport-transition-5g22n\naffinity-nodeport-transition-5g22n\naffinity-nodeport-transition-5g22n"
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-5g22n
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-8lkh7
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-8lkh7
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-8lkh7
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-5g22n
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-8lkh7
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-8lkh7
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-5g22n
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-5g22n
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-5g22n
Feb 25 10:30:52.945: INFO: Received response from host: affinity-nodeport-transition-5g22n
Feb 25 10:30:52.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-7272 exec execpod-affinity945ds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.1.2.179:31213/ ; done'
Feb 25 10:30:53.144: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.1.2.179:31213/\n"
Feb 25 10:30:53.144: INFO: stdout: "\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g\naffinity-nodeport-transition-7nk2g"
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Received response from host: affinity-nodeport-transition-7nk2g
Feb 25 10:30:53.144: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7272, will wait for the garbage collector to delete the pods
Feb 25 10:30:53.220: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.725457ms
Feb 25 10:30:53.321: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.018671ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:55.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7272" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.374 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":210,"skipped":4019,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:55.262: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-4519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:30:55.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-4519" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":211,"skipped":4029,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:55.422: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-9332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:30:55.565: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-2097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-9332
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:31:01.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-2097" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:31:01.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9332" for this suite.

• [SLOW TEST:6.369 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":212,"skipped":4097,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:31:01.791: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Feb 25 10:31:01.942: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 25 10:31:06.947: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Feb 25 10:31:06.950: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:31:06.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6447" for this suite.

• [SLOW TEST:5.193 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":213,"skipped":4109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:31:07.003: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-49b1f7e7-88cd-43a1-bbca-dd2a61190e82
STEP: Creating a pod to test consume configMaps
Feb 25 10:31:07.171: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5be06ef-d05b-4f62-90e3-4cd155a1d2e6" in namespace "projected-5583" to be "Succeeded or Failed"
Feb 25 10:31:07.178: INFO: Pod "pod-projected-configmaps-a5be06ef-d05b-4f62-90e3-4cd155a1d2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.710347ms
Feb 25 10:31:09.184: INFO: Pod "pod-projected-configmaps-a5be06ef-d05b-4f62-90e3-4cd155a1d2e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013264682s
STEP: Saw pod success
Feb 25 10:31:09.184: INFO: Pod "pod-projected-configmaps-a5be06ef-d05b-4f62-90e3-4cd155a1d2e6" satisfied condition "Succeeded or Failed"
Feb 25 10:31:09.187: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-configmaps-a5be06ef-d05b-4f62-90e3-4cd155a1d2e6 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:31:09.208: INFO: Waiting for pod pod-projected-configmaps-a5be06ef-d05b-4f62-90e3-4cd155a1d2e6 to disappear
Feb 25 10:31:09.211: INFO: Pod pod-projected-configmaps-a5be06ef-d05b-4f62-90e3-4cd155a1d2e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:31:09.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5583" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4141,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:31:09.221: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-e2040114-1af2-4cfa-a98d-90a640e9d985
STEP: Creating secret with name secret-projected-all-test-volume-358ba4b2-b74f-4d69-b390-dfc0182cd30f
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 25 10:31:09.379: INFO: Waiting up to 5m0s for pod "projected-volume-b993cde4-dfe6-45bc-a623-a81d64c717da" in namespace "projected-8234" to be "Succeeded or Failed"
Feb 25 10:31:09.382: INFO: Pod "projected-volume-b993cde4-dfe6-45bc-a623-a81d64c717da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.067453ms
Feb 25 10:31:11.388: INFO: Pod "projected-volume-b993cde4-dfe6-45bc-a623-a81d64c717da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009415609s
STEP: Saw pod success
Feb 25 10:31:11.388: INFO: Pod "projected-volume-b993cde4-dfe6-45bc-a623-a81d64c717da" satisfied condition "Succeeded or Failed"
Feb 25 10:31:11.391: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod projected-volume-b993cde4-dfe6-45bc-a623-a81d64c717da container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 25 10:31:11.411: INFO: Waiting for pod projected-volume-b993cde4-dfe6-45bc-a623-a81d64c717da to disappear
Feb 25 10:31:11.414: INFO: Pod projected-volume-b993cde4-dfe6-45bc-a623-a81d64c717da no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:31:11.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8234" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":215,"skipped":4144,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:31:11.425: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-8347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 25 10:31:11.592: INFO: starting watch
STEP: patching
STEP: updating
Feb 25 10:31:11.603: INFO: waiting for watch events with expected annotations
Feb 25 10:31:11.603: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:31:11.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8347" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":216,"skipped":4146,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:31:11.637: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-68213141-7b0e-4ca9-b508-093360c48f06
STEP: Creating a pod to test consume configMaps
Feb 25 10:31:11.790: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-074cd48c-e440-4547-81e6-9503aff820a6" in namespace "projected-5442" to be "Succeeded or Failed"
Feb 25 10:31:11.804: INFO: Pod "pod-projected-configmaps-074cd48c-e440-4547-81e6-9503aff820a6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.988089ms
Feb 25 10:31:13.812: INFO: Pod "pod-projected-configmaps-074cd48c-e440-4547-81e6-9503aff820a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021778924s
STEP: Saw pod success
Feb 25 10:31:13.812: INFO: Pod "pod-projected-configmaps-074cd48c-e440-4547-81e6-9503aff820a6" satisfied condition "Succeeded or Failed"
Feb 25 10:31:13.815: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-configmaps-074cd48c-e440-4547-81e6-9503aff820a6 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:31:13.833: INFO: Waiting for pod pod-projected-configmaps-074cd48c-e440-4547-81e6-9503aff820a6 to disappear
Feb 25 10:31:13.836: INFO: Pod pod-projected-configmaps-074cd48c-e440-4547-81e6-9503aff820a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:31:13.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5442" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":4162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:31:13.846: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-6834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Feb 25 10:31:13.990: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 25 10:32:14.037: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:32:14.040: INFO: Starting informer...
STEP: Starting pods...
Feb 25 10:32:14.258: INFO: Pod1 is running on ip-10-1-2-179.eu-central-1.compute.internal. Tainting Node
Feb 25 10:32:16.485: INFO: Pod2 is running on ip-10-1-2-179.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 25 10:32:22.091: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 25 10:32:42.284: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:32:42.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6834" for this suite.

• [SLOW TEST:88.468 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":218,"skipped":4200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:32:42.315: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-5542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Feb 25 10:32:42.478: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Feb 25 10:32:44.494: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Feb 25 10:32:46.509: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:32:48.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5542" for this suite.

• [SLOW TEST:6.213 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":219,"skipped":4253,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:32:48.528: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5072
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-370304d8-7175-42ac-ba4d-5675320f2992
STEP: Creating a pod to test consume secrets
Feb 25 10:32:48.689: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58ce26c9-4cb2-454f-96ef-60ec1f647e84" in namespace "projected-5072" to be "Succeeded or Failed"
Feb 25 10:32:48.693: INFO: Pod "pod-projected-secrets-58ce26c9-4cb2-454f-96ef-60ec1f647e84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751986ms
Feb 25 10:32:50.700: INFO: Pod "pod-projected-secrets-58ce26c9-4cb2-454f-96ef-60ec1f647e84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011459367s
STEP: Saw pod success
Feb 25 10:32:50.700: INFO: Pod "pod-projected-secrets-58ce26c9-4cb2-454f-96ef-60ec1f647e84" satisfied condition "Succeeded or Failed"
Feb 25 10:32:50.703: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-secrets-58ce26c9-4cb2-454f-96ef-60ec1f647e84 container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 10:32:50.724: INFO: Waiting for pod pod-projected-secrets-58ce26c9-4cb2-454f-96ef-60ec1f647e84 to disappear
Feb 25 10:32:50.727: INFO: Pod pod-projected-secrets-58ce26c9-4cb2-454f-96ef-60ec1f647e84 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:32:50.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5072" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":220,"skipped":4259,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:32:50.737: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:32:50.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9332997-64ec-496d-af39-eb84a4e2ede5" in namespace "projected-1529" to be "Succeeded or Failed"
Feb 25 10:32:50.904: INFO: Pod "downwardapi-volume-a9332997-64ec-496d-af39-eb84a4e2ede5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383028ms
Feb 25 10:32:52.911: INFO: Pod "downwardapi-volume-a9332997-64ec-496d-af39-eb84a4e2ede5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009914859s
STEP: Saw pod success
Feb 25 10:32:52.911: INFO: Pod "downwardapi-volume-a9332997-64ec-496d-af39-eb84a4e2ede5" satisfied condition "Succeeded or Failed"
Feb 25 10:32:52.913: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-a9332997-64ec-496d-af39-eb84a4e2ede5 container client-container: <nil>
STEP: delete the pod
Feb 25 10:32:52.933: INFO: Waiting for pod downwardapi-volume-a9332997-64ec-496d-af39-eb84a4e2ede5 to disappear
Feb 25 10:32:52.936: INFO: Pod downwardapi-volume-a9332997-64ec-496d-af39-eb84a4e2ede5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:32:52.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1529" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:32:52.948: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 10:32:53.714: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 10:32:56.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:32:56.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4734" for this suite.
STEP: Destroying namespace "webhook-4734-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":222,"skipped":4284,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:32:56.858: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6979
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:32:57.026: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-863c2a7d-9704-4c9a-b4c8-210822b51140" in namespace "security-context-test-6979" to be "Succeeded or Failed"
Feb 25 10:32:57.033: INFO: Pod "busybox-readonly-false-863c2a7d-9704-4c9a-b4c8-210822b51140": Phase="Pending", Reason="", readiness=false. Elapsed: 5.434372ms
Feb 25 10:32:59.039: INFO: Pod "busybox-readonly-false-863c2a7d-9704-4c9a-b4c8-210822b51140": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012292921s
Feb 25 10:32:59.040: INFO: Pod "busybox-readonly-false-863c2a7d-9704-4c9a-b4c8-210822b51140" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:32:59.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6979" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":4296,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:32:59.072: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 25 10:32:59.227: INFO: Waiting up to 5m0s for pod "pod-48e5aabe-8f4c-41ab-aba6-c50fc38e3e62" in namespace "emptydir-1519" to be "Succeeded or Failed"
Feb 25 10:32:59.234: INFO: Pod "pod-48e5aabe-8f4c-41ab-aba6-c50fc38e3e62": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644989ms
Feb 25 10:33:01.241: INFO: Pod "pod-48e5aabe-8f4c-41ab-aba6-c50fc38e3e62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013028987s
STEP: Saw pod success
Feb 25 10:33:01.241: INFO: Pod "pod-48e5aabe-8f4c-41ab-aba6-c50fc38e3e62" satisfied condition "Succeeded or Failed"
Feb 25 10:33:01.244: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-48e5aabe-8f4c-41ab-aba6-c50fc38e3e62 container test-container: <nil>
STEP: delete the pod
Feb 25 10:33:01.263: INFO: Waiting for pod pod-48e5aabe-8f4c-41ab-aba6-c50fc38e3e62 to disappear
Feb 25 10:33:01.266: INFO: Pod pod-48e5aabe-8f4c-41ab-aba6-c50fc38e3e62 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:33:01.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1519" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":4305,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:33:01.275: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:33:12.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2637" for this suite.

• [SLOW TEST:11.366 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":225,"skipped":4324,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:33:12.641: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2778
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2778
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2778
Feb 25 10:33:12.802: INFO: Found 0 stateful pods, waiting for 1
Feb 25 10:33:22.808: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 25 10:33:22.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 10:33:22.957: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 10:33:22.957: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 10:33:22.957: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 10:33:22.960: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 25 10:33:32.966: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 10:33:32.966: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 10:33:32.980: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999753s
Feb 25 10:33:33.987: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995233717s
Feb 25 10:33:34.992: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989236446s
Feb 25 10:33:35.997: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984292274s
Feb 25 10:33:37.004: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978171002s
Feb 25 10:33:38.009: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971776835s
Feb 25 10:33:39.015: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96623671s
Feb 25 10:33:40.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960234987s
Feb 25 10:33:41.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.954239882s
Feb 25 10:33:42.032: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.627557ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2778
Feb 25 10:33:43.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 10:33:43.200: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 25 10:33:43.200: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 10:33:43.200: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 10:33:43.204: INFO: Found 1 stateful pods, waiting for 3
Feb 25 10:33:53.214: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 10:33:53.214: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 25 10:33:53.214: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 25 10:33:53.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 10:33:53.377: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 10:33:53.377: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 10:33:53.377: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 10:33:53.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 10:33:53.510: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 10:33:53.510: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 10:33:53.510: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 10:33:53.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 25 10:33:53.634: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 25 10:33:53.634: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 25 10:33:53.634: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 25 10:33:53.634: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 10:33:53.638: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 25 10:34:03.651: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 10:34:03.651: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 10:34:03.651: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 25 10:34:03.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999824s
Feb 25 10:34:04.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996149122s
Feb 25 10:34:05.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990431472s
Feb 25 10:34:06.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985560126s
Feb 25 10:34:07.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980387772s
Feb 25 10:34:08.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974971823s
Feb 25 10:34:09.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969490586s
Feb 25 10:34:10.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96500017s
Feb 25 10:34:11.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958153549s
Feb 25 10:34:12.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.459814ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2778
Feb 25 10:34:13.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 10:34:13.940: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 25 10:34:13.940: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 10:34:13.940: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 10:34:13.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 10:34:14.086: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 25 10:34:14.086: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 10:34:14.086: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 10:34:14.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=statefulset-2778 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 25 10:34:14.228: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 25 10:34:14.228: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 25 10:34:14.228: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 25 10:34:14.228: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Feb 25 10:34:24.252: INFO: Deleting all statefulset in ns statefulset-2778
Feb 25 10:34:24.254: INFO: Scaling statefulset ss to 0
Feb 25 10:34:24.264: INFO: Waiting for statefulset status.replicas updated to 0
Feb 25 10:34:24.266: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:24.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2778" for this suite.

• [SLOW TEST:71.644 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":226,"skipped":4330,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:24.286: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:24.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1580" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":227,"skipped":4337,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 25 10:34:24.589: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6392  0ce68253-9757-484a-a9bb-5a3d3d6652a2 35964 0 2022-02-25 10:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-25 10:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:34:24.589: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6392  0ce68253-9757-484a-a9bb-5a3d3d6652a2 35965 0 2022-02-25 10:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-25 10:34:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 25 10:34:24.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6392  0ce68253-9757-484a-a9bb-5a3d3d6652a2 35966 0 2022-02-25 10:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-25 10:34:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:34:24.601: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6392  0ce68253-9757-484a-a9bb-5a3d3d6652a2 35967 0 2022-02-25 10:34:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-25 10:34:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:24.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6392" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":228,"skipped":4339,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:24.610: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3176
STEP: creating service affinity-clusterip in namespace services-3176
STEP: creating replication controller affinity-clusterip in namespace services-3176
I0225 10:34:24.769884      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-3176, replica count: 3
I0225 10:34:27.831118      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 10:34:27.837: INFO: Creating new exec pod
Feb 25 10:34:30.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-3176 exec execpod-affinitytcqm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb 25 10:34:30.996: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 25 10:34:30.996: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:34:30.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-3176 exec execpod-affinitytcqm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.115.62 80'
Feb 25 10:34:31.133: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.115.62 80\nConnection to 172.31.115.62 80 port [tcp/http] succeeded!\n"
Feb 25 10:34:31.133: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:34:31.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-3176 exec execpod-affinitytcqm8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.115.62:80/ ; done'
Feb 25 10:34:31.318: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.115.62:80/\n"
Feb 25 10:34:31.318: INFO: stdout: "\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt\naffinity-clusterip-z7ztt"
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Received response from host: affinity-clusterip-z7ztt
Feb 25 10:34:31.318: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3176, will wait for the garbage collector to delete the pods
Feb 25 10:34:31.393: INFO: Deleting ReplicationController affinity-clusterip took: 6.194128ms
Feb 25 10:34:31.494: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.994642ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:33.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3176" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.623 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":229,"skipped":4342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:33.234: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:33.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6465" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":230,"skipped":4405,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:33.446: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9041
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-044ed584-6057-4605-a4d8-340485ec690a
STEP: Creating a pod to test consume configMaps
Feb 25 10:34:33.604: INFO: Waiting up to 5m0s for pod "pod-configmaps-15f209a3-bb87-4f3e-9b96-17fa4a9a4382" in namespace "configmap-9041" to be "Succeeded or Failed"
Feb 25 10:34:33.607: INFO: Pod "pod-configmaps-15f209a3-bb87-4f3e-9b96-17fa4a9a4382": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054473ms
Feb 25 10:34:35.612: INFO: Pod "pod-configmaps-15f209a3-bb87-4f3e-9b96-17fa4a9a4382": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007496387s
STEP: Saw pod success
Feb 25 10:34:35.612: INFO: Pod "pod-configmaps-15f209a3-bb87-4f3e-9b96-17fa4a9a4382" satisfied condition "Succeeded or Failed"
Feb 25 10:34:35.614: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-configmaps-15f209a3-bb87-4f3e-9b96-17fa4a9a4382 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:34:35.633: INFO: Waiting for pod pod-configmaps-15f209a3-bb87-4f3e-9b96-17fa4a9a4382 to disappear
Feb 25 10:34:35.640: INFO: Pod pod-configmaps-15f209a3-bb87-4f3e-9b96-17fa4a9a4382 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:35.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9041" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":231,"skipped":4412,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:35.650: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:34:35.805: INFO: Waiting up to 5m0s for pod "downwardapi-volume-434b8cbc-d92c-4d37-a30e-f71e61957bc0" in namespace "downward-api-3490" to be "Succeeded or Failed"
Feb 25 10:34:35.807: INFO: Pod "downwardapi-volume-434b8cbc-d92c-4d37-a30e-f71e61957bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.783676ms
Feb 25 10:34:37.812: INFO: Pod "downwardapi-volume-434b8cbc-d92c-4d37-a30e-f71e61957bc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007354552s
STEP: Saw pod success
Feb 25 10:34:37.812: INFO: Pod "downwardapi-volume-434b8cbc-d92c-4d37-a30e-f71e61957bc0" satisfied condition "Succeeded or Failed"
Feb 25 10:34:37.815: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-434b8cbc-d92c-4d37-a30e-f71e61957bc0 container client-container: <nil>
STEP: delete the pod
Feb 25 10:34:37.833: INFO: Waiting for pod downwardapi-volume-434b8cbc-d92c-4d37-a30e-f71e61957bc0 to disappear
Feb 25 10:34:37.837: INFO: Pod downwardapi-volume-434b8cbc-d92c-4d37-a30e-f71e61957bc0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:37.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3490" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4423,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:37.847: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 25 10:34:37.998: INFO: Waiting up to 5m0s for pod "pod-7af0f186-ce8c-4820-ab5d-0160900579a6" in namespace "emptydir-3219" to be "Succeeded or Failed"
Feb 25 10:34:38.001: INFO: Pod "pod-7af0f186-ce8c-4820-ab5d-0160900579a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.104436ms
Feb 25 10:34:40.008: INFO: Pod "pod-7af0f186-ce8c-4820-ab5d-0160900579a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01031421s
STEP: Saw pod success
Feb 25 10:34:40.008: INFO: Pod "pod-7af0f186-ce8c-4820-ab5d-0160900579a6" satisfied condition "Succeeded or Failed"
Feb 25 10:34:40.011: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-7af0f186-ce8c-4820-ab5d-0160900579a6 container test-container: <nil>
STEP: delete the pod
Feb 25 10:34:40.033: INFO: Waiting for pod pod-7af0f186-ce8c-4820-ab5d-0160900579a6 to disappear
Feb 25 10:34:40.036: INFO: Pod pod-7af0f186-ce8c-4820-ab5d-0160900579a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:40.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3219" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":233,"skipped":4433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:40.046: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9348
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 10:34:40.526: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 25 10:34:42.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382080, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382080, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382080, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382080, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 10:34:45.554: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:45.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9348" for this suite.
STEP: Destroying namespace "webhook-9348-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.716 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":234,"skipped":4466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:45.765: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 25 10:34:45.926: INFO: Waiting up to 5m0s for pod "downward-api-331217b7-6d99-4cd2-ae23-6aef821d2cca" in namespace "downward-api-8628" to be "Succeeded or Failed"
Feb 25 10:34:45.930: INFO: Pod "downward-api-331217b7-6d99-4cd2-ae23-6aef821d2cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.748909ms
Feb 25 10:34:47.935: INFO: Pod "downward-api-331217b7-6d99-4cd2-ae23-6aef821d2cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008528603s
STEP: Saw pod success
Feb 25 10:34:47.935: INFO: Pod "downward-api-331217b7-6d99-4cd2-ae23-6aef821d2cca" satisfied condition "Succeeded or Failed"
Feb 25 10:34:47.937: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downward-api-331217b7-6d99-4cd2-ae23-6aef821d2cca container dapi-container: <nil>
STEP: delete the pod
Feb 25 10:34:47.958: INFO: Waiting for pod downward-api-331217b7-6d99-4cd2-ae23-6aef821d2cca to disappear
Feb 25 10:34:47.960: INFO: Pod downward-api-331217b7-6d99-4cd2-ae23-6aef821d2cca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:47.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8628" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4506,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:47.969: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-3679f1cb-e6d2-4f07-ac9d-2de916105402
STEP: Creating a pod to test consume secrets
Feb 25 10:34:48.123: INFO: Waiting up to 5m0s for pod "pod-secrets-81886abd-46c0-4107-875b-ca83126e8429" in namespace "secrets-9975" to be "Succeeded or Failed"
Feb 25 10:34:48.126: INFO: Pod "pod-secrets-81886abd-46c0-4107-875b-ca83126e8429": Phase="Pending", Reason="", readiness=false. Elapsed: 3.187427ms
Feb 25 10:34:50.135: INFO: Pod "pod-secrets-81886abd-46c0-4107-875b-ca83126e8429": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01212075s
STEP: Saw pod success
Feb 25 10:34:50.135: INFO: Pod "pod-secrets-81886abd-46c0-4107-875b-ca83126e8429" satisfied condition "Succeeded or Failed"
Feb 25 10:34:50.138: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-secrets-81886abd-46c0-4107-875b-ca83126e8429 container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 10:34:50.160: INFO: Waiting for pod pod-secrets-81886abd-46c0-4107-875b-ca83126e8429 to disappear
Feb 25 10:34:50.164: INFO: Pod pod-secrets-81886abd-46c0-4107-875b-ca83126e8429 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:34:50.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9975" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4514,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:34:50.180: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:34:50.335: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 25 10:34:55.343: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 25 10:34:55.344: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 25 10:34:57.347: INFO: Creating deployment "test-rollover-deployment"
Feb 25 10:34:57.355: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 25 10:34:59.365: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 25 10:34:59.372: INFO: Ensure that both replica sets have 1 created replica
Feb 25 10:34:59.378: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 25 10:34:59.388: INFO: Updating deployment test-rollover-deployment
Feb 25 10:34:59.388: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 25 10:35:01.398: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 25 10:35:01.403: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 25 10:35:01.409: INFO: all replica sets need to contain the pod-template-hash label
Feb 25 10:35:01.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382100, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:03.420: INFO: all replica sets need to contain the pod-template-hash label
Feb 25 10:35:03.420: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382100, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:05.419: INFO: all replica sets need to contain the pod-template-hash label
Feb 25 10:35:05.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382100, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:07.416: INFO: all replica sets need to contain the pod-template-hash label
Feb 25 10:35:07.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382100, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:09.418: INFO: all replica sets need to contain the pod-template-hash label
Feb 25 10:35:09.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382100, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382097, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:11.420: INFO: 
Feb 25 10:35:11.420: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 10:35:11.428: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8859  5695a81a-edd7-4f27-ab0d-5f63f551fbf5 36663 2 2022-02-25 10:34:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-25 10:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:35:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030d93b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-25 10:34:57 +0000 UTC,LastTransitionTime:2022-02-25 10:34:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-02-25 10:35:10 +0000 UTC,LastTransitionTime:2022-02-25 10:34:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 25 10:35:11.431: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-8859  e726b699-9344-4071-b91e-4f239838916f 36653 2 2022-02-25 10:34:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5695a81a-edd7-4f27-ab0d-5f63f551fbf5 0xc0030d99a0 0xc0030d99a1}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5695a81a-edd7-4f27-ab0d-5f63f551fbf5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:35:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030d9a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:35:11.431: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 25 10:35:11.431: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8859  6e2ce831-e691-49ca-b564-7106f3baf1f6 36662 2 2022-02-25 10:34:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5695a81a-edd7-4f27-ab0d-5f63f551fbf5 0xc0030d9757 0xc0030d9758}] []  [{e2e.test Update apps/v1 2022-02-25 10:34:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:35:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5695a81a-edd7-4f27-ab0d-5f63f551fbf5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:35:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0030d9818 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:35:11.431: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8859  fb29e7e6-a725-45c2-9207-9c997475c647 36598 2 2022-02-25 10:34:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5695a81a-edd7-4f27-ab0d-5f63f551fbf5 0xc0030d9887 0xc0030d9888}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:34:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5695a81a-edd7-4f27-ab0d-5f63f551fbf5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:34:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030d9938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:35:11.434: INFO: Pod "test-rollover-deployment-98c5f4599-jpcss" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-jpcss test-rollover-deployment-98c5f4599- deployment-8859  4bbe8831-8d16-4039-a42c-925f3129df8c 36612 0 2022-02-25 10:34:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 e726b699-9344-4071-b91e-4f239838916f 0xc0030d9f40 0xc0030d9f41}] []  [{kube-controller-manager Update v1 2022-02-25 10:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e726b699-9344-4071-b91e-4f239838916f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:35:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.37.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbngb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbngb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-179.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:34:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:35:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:35:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:34:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.179,PodIP:10.10.37.72,StartTime:2022-02-25 10:34:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:35:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://bfa19acd7ed547d6d7f86626e003e99a44971b2d51388d9e3e7eab1e79a0ef2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.37.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:35:11.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8859" for this suite.

• [SLOW TEST:21.263 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":237,"skipped":4518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:35:11.446: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Feb 25 10:35:11.591: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Feb 25 10:35:12.168: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 25 10:35:14.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:16.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:18.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:20.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781382112, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 25 10:35:22.647: INFO: Waited 417.602391ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Feb 25 10:35:22.845: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:35:23.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8848" for this suite.

• [SLOW TEST:12.298 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":238,"skipped":4549,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:35:23.746: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:35:23.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7447" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":239,"skipped":4551,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:35:23.928: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Feb 25 10:35:24.075: INFO: created test-podtemplate-1
Feb 25 10:35:24.083: INFO: created test-podtemplate-2
Feb 25 10:35:24.088: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Feb 25 10:35:24.091: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Feb 25 10:35:24.109: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:35:24.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6851" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":240,"skipped":4590,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:35:24.123: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 25 10:35:26.788: INFO: Successfully updated pod "adopt-release--1-4cpx9"
STEP: Checking that the Job readopts the Pod
Feb 25 10:35:26.790: INFO: Waiting up to 15m0s for pod "adopt-release--1-4cpx9" in namespace "job-8226" to be "adopted"
Feb 25 10:35:26.794: INFO: Pod "adopt-release--1-4cpx9": Phase="Running", Reason="", readiness=true. Elapsed: 3.913616ms
Feb 25 10:35:28.798: INFO: Pod "adopt-release--1-4cpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008539992s
Feb 25 10:35:28.799: INFO: Pod "adopt-release--1-4cpx9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 25 10:35:29.311: INFO: Successfully updated pod "adopt-release--1-4cpx9"
STEP: Checking that the Job releases the Pod
Feb 25 10:35:29.312: INFO: Waiting up to 15m0s for pod "adopt-release--1-4cpx9" in namespace "job-8226" to be "released"
Feb 25 10:35:29.315: INFO: Pod "adopt-release--1-4cpx9": Phase="Running", Reason="", readiness=true. Elapsed: 3.428566ms
Feb 25 10:35:31.323: INFO: Pod "adopt-release--1-4cpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011598056s
Feb 25 10:35:31.323: INFO: Pod "adopt-release--1-4cpx9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:35:31.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8226" for this suite.

• [SLOW TEST:7.212 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":241,"skipped":4606,"failed":0}
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:35:31.335: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:35:31.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4954" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":242,"skipped":4606,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:35:31.523: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-376
Feb 25 10:35:31.675: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:35:33.682: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 25 10:35:33.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-376 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 25 10:35:33.838: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 25 10:35:33.838: INFO: stdout: "iptables"
Feb 25 10:35:33.838: INFO: proxyMode: iptables
Feb 25 10:35:33.855: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 25 10:35:33.858: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-376
STEP: creating replication controller affinity-clusterip-timeout in namespace services-376
I0225 10:35:33.879609      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-376, replica count: 3
I0225 10:35:36.929809      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 10:35:36.937: INFO: Creating new exec pod
Feb 25 10:35:39.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-376 exec execpod-affinitytbjcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb 25 10:35:40.099: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 25 10:35:40.099: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:35:40.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-376 exec execpod-affinitytbjcd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.172.217 80'
Feb 25 10:35:40.245: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.172.217 80\nConnection to 172.31.172.217 80 port [tcp/http] succeeded!\n"
Feb 25 10:35:40.245: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:35:40.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-376 exec execpod-affinitytbjcd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.172.217:80/ ; done'
Feb 25 10:35:40.448: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n"
Feb 25 10:35:40.448: INFO: stdout: "\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k\naffinity-clusterip-timeout-blp4k"
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.448: INFO: Received response from host: affinity-clusterip-timeout-blp4k
Feb 25 10:35:40.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-376 exec execpod-affinitytbjcd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.172.217:80/'
Feb 25 10:35:40.578: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n"
Feb 25 10:35:40.578: INFO: stdout: "affinity-clusterip-timeout-blp4k"
Feb 25 10:36:00.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-376 exec execpod-affinitytbjcd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.172.217:80/'
Feb 25 10:36:00.720: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.172.217:80/\n"
Feb 25 10:36:00.720: INFO: stdout: "affinity-clusterip-timeout-krmjt"
Feb 25 10:36:00.720: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-376, will wait for the garbage collector to delete the pods
Feb 25 10:36:00.798: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.738017ms
Feb 25 10:36:00.899: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.000159ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:36:02.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-376" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:31.407 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":243,"skipped":4608,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:36:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 10:36:03.643: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 10:36:06.666: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:36:06.671: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8280-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:36:14.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5073" for this suite.
STEP: Destroying namespace "webhook-5073-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.905 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":244,"skipped":4615,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:36:14.835: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5472
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 25 10:36:14.996: INFO: Waiting up to 5m0s for pod "pod-19b03510-49d9-4be4-aa57-ee8369a9fa42" in namespace "emptydir-5472" to be "Succeeded or Failed"
Feb 25 10:36:15.002: INFO: Pod "pod-19b03510-49d9-4be4-aa57-ee8369a9fa42": Phase="Pending", Reason="", readiness=false. Elapsed: 5.607628ms
Feb 25 10:36:17.008: INFO: Pod "pod-19b03510-49d9-4be4-aa57-ee8369a9fa42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011786192s
STEP: Saw pod success
Feb 25 10:36:17.008: INFO: Pod "pod-19b03510-49d9-4be4-aa57-ee8369a9fa42" satisfied condition "Succeeded or Failed"
Feb 25 10:36:17.011: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-19b03510-49d9-4be4-aa57-ee8369a9fa42 container test-container: <nil>
STEP: delete the pod
Feb 25 10:36:17.031: INFO: Waiting for pod pod-19b03510-49d9-4be4-aa57-ee8369a9fa42 to disappear
Feb 25 10:36:17.034: INFO: Pod pod-19b03510-49d9-4be4-aa57-ee8369a9fa42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:36:17.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5472" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:36:17.046: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1383
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 25 10:36:23.249: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0225 10:36:23.249573      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:36:23.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1383" for this suite.

• [SLOW TEST:6.214 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":246,"skipped":4673,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:36:23.260: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:36:23.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6725" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":247,"skipped":4681,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:36:23.412: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Feb 25 10:38:24.093: INFO: Successfully updated pod "var-expansion-b9b97d46-bc78-4f67-828a-20c7e36045a0"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Feb 25 10:38:26.102: INFO: Deleting pod "var-expansion-b9b97d46-bc78-4f67-828a-20c7e36045a0" in namespace "var-expansion-2648"
Feb 25 10:38:26.112: INFO: Wait up to 5m0s for pod "var-expansion-b9b97d46-bc78-4f67-828a-20c7e36045a0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:38:58.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2648" for this suite.

• [SLOW TEST:154.719 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":248,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:38:58.132: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Feb 25 10:38:58.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 create -f -'
Feb 25 10:38:59.335: INFO: stderr: ""
Feb 25 10:38:59.335: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 25 10:38:59.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 10:38:59.419: INFO: stderr: ""
Feb 25 10:38:59.419: INFO: stdout: "update-demo-nautilus-fcpdb update-demo-nautilus-tx4dc "
Feb 25 10:38:59.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods update-demo-nautilus-fcpdb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 10:38:59.470: INFO: stderr: ""
Feb 25 10:38:59.470: INFO: stdout: ""
Feb 25 10:38:59.470: INFO: update-demo-nautilus-fcpdb is created but not running
Feb 25 10:39:04.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 25 10:39:04.532: INFO: stderr: ""
Feb 25 10:39:04.532: INFO: stdout: "update-demo-nautilus-fcpdb update-demo-nautilus-tx4dc "
Feb 25 10:39:04.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods update-demo-nautilus-fcpdb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 10:39:04.590: INFO: stderr: ""
Feb 25 10:39:04.590: INFO: stdout: "true"
Feb 25 10:39:04.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods update-demo-nautilus-fcpdb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 25 10:39:04.647: INFO: stderr: ""
Feb 25 10:39:04.647: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 25 10:39:04.647: INFO: validating pod update-demo-nautilus-fcpdb
Feb 25 10:39:04.652: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 25 10:39:04.652: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 25 10:39:04.652: INFO: update-demo-nautilus-fcpdb is verified up and running
Feb 25 10:39:04.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods update-demo-nautilus-tx4dc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 25 10:39:04.701: INFO: stderr: ""
Feb 25 10:39:04.701: INFO: stdout: "true"
Feb 25 10:39:04.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods update-demo-nautilus-tx4dc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 25 10:39:04.753: INFO: stderr: ""
Feb 25 10:39:04.753: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Feb 25 10:39:04.753: INFO: validating pod update-demo-nautilus-tx4dc
Feb 25 10:39:04.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 25 10:39:04.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 25 10:39:04.757: INFO: update-demo-nautilus-tx4dc is verified up and running
STEP: using delete to clean up resources
Feb 25 10:39:04.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 delete --grace-period=0 --force -f -'
Feb 25 10:39:04.824: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 25 10:39:04.824: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 25 10:39:04.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get rc,svc -l name=update-demo --no-headers'
Feb 25 10:39:04.890: INFO: stderr: "No resources found in kubectl-157 namespace.\n"
Feb 25 10:39:04.890: INFO: stdout: ""
Feb 25 10:39:04.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-157 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 25 10:39:04.947: INFO: stderr: ""
Feb 25 10:39:04.947: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:39:04.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-157" for this suite.

• [SLOW TEST:6.825 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":249,"skipped":4738,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:39:04.957: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 25 10:39:07.623: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8858 pod-service-account-619df400-0305-49ac-a750-27f445442fde -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 25 10:39:07.760: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8858 pod-service-account-619df400-0305-49ac-a750-27f445442fde -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 25 10:39:07.884: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8858 pod-service-account-619df400-0305-49ac-a750-27f445442fde -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:39:08.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8858" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":250,"skipped":4747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:39:08.103: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 25 10:39:08.261: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7979  6e85c0c0-482e-49fc-af57-d6a0af8f9afc 38361 0 2022-02-25 10:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-25 10:39:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:39:08.261: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7979  6e85c0c0-482e-49fc-af57-d6a0af8f9afc 38362 0 2022-02-25 10:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-25 10:39:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:39:08.261: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7979  6e85c0c0-482e-49fc-af57-d6a0af8f9afc 38363 0 2022-02-25 10:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-25 10:39:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 25 10:39:18.291: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7979  6e85c0c0-482e-49fc-af57-d6a0af8f9afc 38430 0 2022-02-25 10:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-25 10:39:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:39:18.291: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7979  6e85c0c0-482e-49fc-af57-d6a0af8f9afc 38431 0 2022-02-25 10:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-25 10:39:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:39:18.292: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7979  6e85c0c0-482e-49fc-af57-d6a0af8f9afc 38432 0 2022-02-25 10:39:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-25 10:39:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:39:18.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7979" for this suite.

• [SLOW TEST:10.198 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":251,"skipped":4802,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:39:18.302: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-7747
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:39:18.448: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Creating first CR 
Feb 25 10:39:26.004: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-25T10:39:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-25T10:39:26Z]] name:name1 resourceVersion:38482 uid:b4e9414b-32ef-4452-b4c3-20276cf0f9f3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 25 10:39:36.014: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-25T10:39:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-25T10:39:36Z]] name:name2 resourceVersion:38516 uid:cd3b772b-1880-4c8e-a934-00b00d35af4d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 25 10:39:46.031: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-25T10:39:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-25T10:39:46Z]] name:name1 resourceVersion:38549 uid:b4e9414b-32ef-4452-b4c3-20276cf0f9f3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 25 10:39:56.047: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-25T10:39:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-25T10:39:56Z]] name:name2 resourceVersion:38584 uid:cd3b772b-1880-4c8e-a934-00b00d35af4d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 25 10:40:06.066: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-25T10:39:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-25T10:39:46Z]] name:name1 resourceVersion:38622 uid:b4e9414b-32ef-4452-b4c3-20276cf0f9f3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 25 10:40:16.080: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-25T10:39:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-25T10:39:56Z]] name:name2 resourceVersion:38655 uid:cd3b772b-1880-4c8e-a934-00b00d35af4d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:40:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7747" for this suite.

• [SLOW TEST:68.312 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":252,"skipped":4815,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:40:26.619: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:40:26.820: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc870966-2655-4550-8f53-3785d5c5d335" in namespace "projected-8613" to be "Succeeded or Failed"
Feb 25 10:40:26.823: INFO: Pod "downwardapi-volume-cc870966-2655-4550-8f53-3785d5c5d335": Phase="Pending", Reason="", readiness=false. Elapsed: 3.169572ms
Feb 25 10:40:28.829: INFO: Pod "downwardapi-volume-cc870966-2655-4550-8f53-3785d5c5d335": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009558125s
STEP: Saw pod success
Feb 25 10:40:28.829: INFO: Pod "downwardapi-volume-cc870966-2655-4550-8f53-3785d5c5d335" satisfied condition "Succeeded or Failed"
Feb 25 10:40:28.832: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-cc870966-2655-4550-8f53-3785d5c5d335 container client-container: <nil>
STEP: delete the pod
Feb 25 10:40:28.857: INFO: Waiting for pod downwardapi-volume-cc870966-2655-4550-8f53-3785d5c5d335 to disappear
Feb 25 10:40:28.860: INFO: Pod downwardapi-volume-cc870966-2655-4550-8f53-3785d5c5d335 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:40:28.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8613" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":253,"skipped":4832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:40:28.871: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-8ebb34ab-a761-4ece-b3f7-e16e7b3ab0ed
STEP: Creating a pod to test consume secrets
Feb 25 10:40:29.028: INFO: Waiting up to 5m0s for pod "pod-secrets-51f688df-75d9-46cd-91ba-1ec5b9d188c4" in namespace "secrets-7843" to be "Succeeded or Failed"
Feb 25 10:40:29.033: INFO: Pod "pod-secrets-51f688df-75d9-46cd-91ba-1ec5b9d188c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587447ms
Feb 25 10:40:31.037: INFO: Pod "pod-secrets-51f688df-75d9-46cd-91ba-1ec5b9d188c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008752015s
STEP: Saw pod success
Feb 25 10:40:31.037: INFO: Pod "pod-secrets-51f688df-75d9-46cd-91ba-1ec5b9d188c4" satisfied condition "Succeeded or Failed"
Feb 25 10:40:31.040: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-secrets-51f688df-75d9-46cd-91ba-1ec5b9d188c4 container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 10:40:31.061: INFO: Waiting for pod pod-secrets-51f688df-75d9-46cd-91ba-1ec5b9d188c4 to disappear
Feb 25 10:40:31.064: INFO: Pod pod-secrets-51f688df-75d9-46cd-91ba-1ec5b9d188c4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:40:31.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7843" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4857,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:40:31.074: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9158.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9158.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9158.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9158.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9158.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9158.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9158.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:40:33.270: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.273: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.276: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.280: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.283: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.286: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.289: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.292: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.295: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.298: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9158.svc.cluster.local from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.301: INFO: Unable to read jessie_udp@PodARecord from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.304: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37: the server could not find the requested resource (get pods dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37)
Feb 25 10:40:33.304: INFO: Lookups using dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9158.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9158.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9158.svc.cluster.local jessie_udp@dns-test-service-2.dns-9158.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9158.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 25 10:40:38.362: INFO: DNS probes using dns-9158/dns-test-5f64f0d5-3a6d-4bd6-867f-447a70f3ac37 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:40:38.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9158" for this suite.

• [SLOW TEST:7.339 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":255,"skipped":4860,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:40:38.415: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4913
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-56e57093-b42e-4f9c-b160-9d9d86f64163
STEP: Creating the pod
Feb 25 10:40:38.582: INFO: The status of Pod pod-projected-configmaps-e22556e2-a1a8-4846-b7d9-818a4237505b is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:40:40.590: INFO: The status of Pod pod-projected-configmaps-e22556e2-a1a8-4846-b7d9-818a4237505b is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-56e57093-b42e-4f9c-b160-9d9d86f64163
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:40:44.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4913" for this suite.

• [SLOW TEST:6.229 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:40:44.645: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-799
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 25 10:40:44.787: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 25 10:40:44.842: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:40:46.847: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:40:48.848: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:40:50.850: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:40:52.849: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:40:54.849: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:40:56.850: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:40:58.847: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:41:00.853: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:41:02.849: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 25 10:41:04.851: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 25 10:41:04.857: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 25 10:41:04.866: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb 25 10:41:06.903: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 25 10:41:06.903: INFO: Going to poll 10.10.19.151 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 25 10:41:06.906: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.19.151 8081 | grep -v '^\s*$'] Namespace:pod-network-test-799 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:41:06.906: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:41:07.983: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 25 10:41:07.983: INFO: Going to poll 10.10.23.77 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 25 10:41:07.987: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.23.77 8081 | grep -v '^\s*$'] Namespace:pod-network-test-799 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:41:07.987: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:41:09.077: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 25 10:41:09.077: INFO: Going to poll 10.10.19.65 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 25 10:41:09.081: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.19.65 8081 | grep -v '^\s*$'] Namespace:pod-network-test-799 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:41:09.081: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:41:10.151: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:10.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-799" for this suite.

• [SLOW TEST:25.520 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:10.166: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4395
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-022920e8-084d-4c6a-81d4-94896340c4af
STEP: Creating a pod to test consume secrets
Feb 25 10:41:10.334: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3a8fd463-a6d1-47a6-b75a-72426ad169a1" in namespace "projected-4395" to be "Succeeded or Failed"
Feb 25 10:41:10.337: INFO: Pod "pod-projected-secrets-3a8fd463-a6d1-47a6-b75a-72426ad169a1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575121ms
Feb 25 10:41:12.344: INFO: Pod "pod-projected-secrets-3a8fd463-a6d1-47a6-b75a-72426ad169a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010414436s
STEP: Saw pod success
Feb 25 10:41:12.344: INFO: Pod "pod-projected-secrets-3a8fd463-a6d1-47a6-b75a-72426ad169a1" satisfied condition "Succeeded or Failed"
Feb 25 10:41:12.347: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-secrets-3a8fd463-a6d1-47a6-b75a-72426ad169a1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 25 10:41:12.367: INFO: Waiting for pod pod-projected-secrets-3a8fd463-a6d1-47a6-b75a-72426ad169a1 to disappear
Feb 25 10:41:12.373: INFO: Pod pod-projected-secrets-3a8fd463-a6d1-47a6-b75a-72426ad169a1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:12.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4395" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4945,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:12.382: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-w5q7
STEP: Creating a pod to test atomic-volume-subpath
Feb 25 10:41:12.545: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-w5q7" in namespace "subpath-5834" to be "Succeeded or Failed"
Feb 25 10:41:12.548: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.344871ms
Feb 25 10:41:14.559: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 2.014877102s
Feb 25 10:41:16.563: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 4.018910936s
Feb 25 10:41:18.570: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 6.025181423s
Feb 25 10:41:20.576: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 8.031690967s
Feb 25 10:41:22.582: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 10.03711908s
Feb 25 10:41:24.589: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 12.044477377s
Feb 25 10:41:26.597: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 14.052911407s
Feb 25 10:41:28.603: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 16.058554296s
Feb 25 10:41:30.610: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 18.065293276s
Feb 25 10:41:32.617: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Running", Reason="", readiness=true. Elapsed: 20.072302641s
Feb 25 10:41:34.624: INFO: Pod "pod-subpath-test-projected-w5q7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.078978395s
STEP: Saw pod success
Feb 25 10:41:34.624: INFO: Pod "pod-subpath-test-projected-w5q7" satisfied condition "Succeeded or Failed"
Feb 25 10:41:34.626: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-subpath-test-projected-w5q7 container test-container-subpath-projected-w5q7: <nil>
STEP: delete the pod
Feb 25 10:41:34.645: INFO: Waiting for pod pod-subpath-test-projected-w5q7 to disappear
Feb 25 10:41:34.647: INFO: Pod pod-subpath-test-projected-w5q7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-w5q7
Feb 25 10:41:34.647: INFO: Deleting pod "pod-subpath-test-projected-w5q7" in namespace "subpath-5834"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:34.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5834" for this suite.

• [SLOW TEST:22.287 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":259,"skipped":4947,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:34.669: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Feb 25 10:41:34.823: INFO: namespace kubectl-4420
Feb 25 10:41:34.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4420 create -f -'
Feb 25 10:41:35.747: INFO: stderr: ""
Feb 25 10:41:35.747: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 25 10:41:36.752: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:41:36.752: INFO: Found 0 / 1
Feb 25 10:41:37.752: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:41:37.752: INFO: Found 1 / 1
Feb 25 10:41:37.752: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 25 10:41:37.755: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 25 10:41:37.755: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 25 10:41:37.755: INFO: wait on agnhost-primary startup in kubectl-4420 
Feb 25 10:41:37.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4420 logs agnhost-primary-fm6kv agnhost-primary'
Feb 25 10:41:37.812: INFO: stderr: ""
Feb 25 10:41:37.812: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 25 10:41:37.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4420 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 25 10:41:37.894: INFO: stderr: ""
Feb 25 10:41:37.894: INFO: stdout: "service/rm2 exposed\n"
Feb 25 10:41:37.899: INFO: Service rm2 in namespace kubectl-4420 found.
STEP: exposing service
Feb 25 10:41:39.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-4420 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 25 10:41:40.003: INFO: stderr: ""
Feb 25 10:41:40.004: INFO: stdout: "service/rm3 exposed\n"
Feb 25 10:41:40.012: INFO: Service rm3 in namespace kubectl-4420 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:42.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4420" for this suite.

• [SLOW TEST:7.365 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":260,"skipped":4952,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:42.035: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6431
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1923
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:48.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5557" for this suite.
STEP: Destroying namespace "nsdeletetest-6431" for this suite.
Feb 25 10:41:48.515: INFO: Namespace nsdeletetest-6431 was already deleted
STEP: Destroying namespace "nsdeletetest-1923" for this suite.

• [SLOW TEST:6.488 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":261,"skipped":4956,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:48.522: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:41:48.679: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 25 10:41:53.691: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 25 10:41:53.691: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 10:41:53.716: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8442  d41d7e55-f729-4b52-9dce-1da1bde25c35 39439 1 2022-02-25 10:41:53 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-02-25 10:41:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ba6b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 25 10:41:53.719: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb 25 10:41:53.719: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 25 10:41:53.719: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8442  a90a5613-b17a-460b-a834-824f0d7ac825 39440 1 2022-02-25 10:41:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d41d7e55-f729-4b52-9dce-1da1bde25c35 0xc004ba6ea7 0xc004ba6ea8}] []  [{e2e.test Update apps/v1 2022-02-25 10:41:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:41:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-02-25 10:41:53 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d41d7e55-f729-4b52-9dce-1da1bde25c35\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004ba6f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:41:53.724: INFO: Pod "test-cleanup-controller-wc2f9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-wc2f9 test-cleanup-controller- deployment-8442  fd1f2f02-6bcc-4d1f-b710-e3ed63c54454 39416 0 2022-02-25 10:41:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller a90a5613-b17a-460b-a834-824f0d7ac825 0xc0061a47d7 0xc0061a47d8}] []  [{kube-controller-manager Update v1 2022-02-25 10:41:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a90a5613-b17a-460b-a834-824f0d7ac825\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:41:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.19.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2nt7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2nt7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-179.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:41:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:41:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:41:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.179,PodIP:10.10.19.184,StartTime:2022-02-25 10:41:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-25 10:41:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://ee5bdf4245d2eb7a706b2b99d34fda94c8bd8b3740fa7cf56e60ae61eb70e02c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.19.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:53.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8442" for this suite.

• [SLOW TEST:5.223 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":262,"skipped":4959,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:53.745: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:41:53.918: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8672bfb0-7b7f-4c38-b01b-4647dc2f589f" in namespace "security-context-test-7576" to be "Succeeded or Failed"
Feb 25 10:41:53.922: INFO: Pod "busybox-privileged-false-8672bfb0-7b7f-4c38-b01b-4647dc2f589f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.720793ms
Feb 25 10:41:55.930: INFO: Pod "busybox-privileged-false-8672bfb0-7b7f-4c38-b01b-4647dc2f589f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012004577s
Feb 25 10:41:55.930: INFO: Pod "busybox-privileged-false-8672bfb0-7b7f-4c38-b01b-4647dc2f589f" satisfied condition "Succeeded or Failed"
Feb 25 10:41:55.937: INFO: Got logs for pod "busybox-privileged-false-8672bfb0-7b7f-4c38-b01b-4647dc2f589f": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:55.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7576" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":4961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:55.949: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1538
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:56.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1538" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":264,"skipped":4992,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:56.162: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-52539507-b82f-4fbb-9f34-dd540da6d6ba
STEP: Creating a pod to test consume configMaps
Feb 25 10:41:56.322: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3173c401-6dd4-422c-8e2b-99952994e58b" in namespace "projected-9101" to be "Succeeded or Failed"
Feb 25 10:41:56.325: INFO: Pod "pod-projected-configmaps-3173c401-6dd4-422c-8e2b-99952994e58b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.094749ms
Feb 25 10:41:58.330: INFO: Pod "pod-projected-configmaps-3173c401-6dd4-422c-8e2b-99952994e58b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008157815s
STEP: Saw pod success
Feb 25 10:41:58.330: INFO: Pod "pod-projected-configmaps-3173c401-6dd4-422c-8e2b-99952994e58b" satisfied condition "Succeeded or Failed"
Feb 25 10:41:58.332: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-configmaps-3173c401-6dd4-422c-8e2b-99952994e58b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 25 10:41:58.350: INFO: Waiting for pod pod-projected-configmaps-3173c401-6dd4-422c-8e2b-99952994e58b to disappear
Feb 25 10:41:58.353: INFO: Pod pod-projected-configmaps-3173c401-6dd4-422c-8e2b-99952994e58b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:41:58.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9101" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":4993,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:41:58.363: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8985
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 25 10:41:58.514: INFO: Waiting up to 5m0s for pod "pod-465c4cbd-e3f1-4629-a3a5-72e1c2e1b49a" in namespace "emptydir-8985" to be "Succeeded or Failed"
Feb 25 10:41:58.521: INFO: Pod "pod-465c4cbd-e3f1-4629-a3a5-72e1c2e1b49a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064089ms
Feb 25 10:42:00.528: INFO: Pod "pod-465c4cbd-e3f1-4629-a3a5-72e1c2e1b49a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012446528s
STEP: Saw pod success
Feb 25 10:42:00.528: INFO: Pod "pod-465c4cbd-e3f1-4629-a3a5-72e1c2e1b49a" satisfied condition "Succeeded or Failed"
Feb 25 10:42:00.531: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-465c4cbd-e3f1-4629-a3a5-72e1c2e1b49a container test-container: <nil>
STEP: delete the pod
Feb 25 10:42:00.548: INFO: Waiting for pod pod-465c4cbd-e3f1-4629-a3a5-72e1c2e1b49a to disappear
Feb 25 10:42:00.551: INFO: Pod pod-465c4cbd-e3f1-4629-a3a5-72e1c2e1b49a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:42:00.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8985" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":266,"skipped":5023,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:42:00.562: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Feb 25 10:42:00.718: INFO: The status of Pod pod-hostip-b121de2e-7a72-40c1-b074-69dec5136478 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:42:02.725: INFO: The status of Pod pod-hostip-b121de2e-7a72-40c1-b074-69dec5136478 is Running (Ready = true)
Feb 25 10:42:02.731: INFO: Pod pod-hostip-b121de2e-7a72-40c1-b074-69dec5136478 has hostIP: 10.1.2.179
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:42:02.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4965" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":5041,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:42:02.740: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-26b48bce-9284-499e-9b9d-242cf19cec54 in namespace container-probe-4454
Feb 25 10:42:04.903: INFO: Started pod busybox-26b48bce-9284-499e-9b9d-242cf19cec54 in namespace container-probe-4454
STEP: checking the pod's current state and verifying that restartCount is present
Feb 25 10:42:04.906: INFO: Initial restart count of pod busybox-26b48bce-9284-499e-9b9d-242cf19cec54 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:46:05.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4454" for this suite.

• [SLOW TEST:243.004 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":5044,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:46:05.746: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 25 10:46:07.928: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:46:07.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8859" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":5054,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:46:07.953: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1619
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:46:08.111: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 25 10:46:13.115: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Feb 25 10:46:13.127: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Feb 25 10:46:13.135: INFO: observed ReplicaSet test-rs in namespace replicaset-1619 with ReadyReplicas 1, AvailableReplicas 1
Feb 25 10:46:13.166: INFO: observed ReplicaSet test-rs in namespace replicaset-1619 with ReadyReplicas 1, AvailableReplicas 1
Feb 25 10:46:13.198: INFO: observed ReplicaSet test-rs in namespace replicaset-1619 with ReadyReplicas 1, AvailableReplicas 1
Feb 25 10:46:13.204: INFO: observed ReplicaSet test-rs in namespace replicaset-1619 with ReadyReplicas 1, AvailableReplicas 1
Feb 25 10:46:14.273: INFO: observed ReplicaSet test-rs in namespace replicaset-1619 with ReadyReplicas 2, AvailableReplicas 2
Feb 25 10:46:14.547: INFO: observed Replicaset test-rs in namespace replicaset-1619 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:46:14.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1619" for this suite.

• [SLOW TEST:6.606 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":270,"skipped":5060,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:46:14.560: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-9875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:46:14.982: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 25 10:46:14.982: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 25 10:46:14.982: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 25 10:46:14.982: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 25 10:46:14.982: INFO: Checking APIGroup: apps
Feb 25 10:46:14.983: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 25 10:46:14.983: INFO: Versions found [{apps/v1 v1}]
Feb 25 10:46:14.983: INFO: apps/v1 matches apps/v1
Feb 25 10:46:14.983: INFO: Checking APIGroup: events.k8s.io
Feb 25 10:46:14.984: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 25 10:46:14.984: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Feb 25 10:46:14.984: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 25 10:46:14.984: INFO: Checking APIGroup: authentication.k8s.io
Feb 25 10:46:14.984: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 25 10:46:14.984: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 25 10:46:14.984: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 25 10:46:14.984: INFO: Checking APIGroup: authorization.k8s.io
Feb 25 10:46:14.985: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 25 10:46:14.985: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 25 10:46:14.985: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 25 10:46:14.985: INFO: Checking APIGroup: autoscaling
Feb 25 10:46:14.986: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Feb 25 10:46:14.986: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Feb 25 10:46:14.986: INFO: autoscaling/v1 matches autoscaling/v1
Feb 25 10:46:14.986: INFO: Checking APIGroup: batch
Feb 25 10:46:14.986: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 25 10:46:14.986: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Feb 25 10:46:14.986: INFO: batch/v1 matches batch/v1
Feb 25 10:46:14.986: INFO: Checking APIGroup: certificates.k8s.io
Feb 25 10:46:14.987: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 25 10:46:14.987: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 25 10:46:14.987: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 25 10:46:14.987: INFO: Checking APIGroup: networking.k8s.io
Feb 25 10:46:14.987: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 25 10:46:14.987: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 25 10:46:14.987: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 25 10:46:14.987: INFO: Checking APIGroup: policy
Feb 25 10:46:14.988: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 25 10:46:14.988: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Feb 25 10:46:14.988: INFO: policy/v1 matches policy/v1
Feb 25 10:46:14.988: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 25 10:46:14.989: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 25 10:46:14.989: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1alpha1 v1alpha1}]
Feb 25 10:46:14.989: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 25 10:46:14.989: INFO: Checking APIGroup: storage.k8s.io
Feb 25 10:46:14.989: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 25 10:46:14.989: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1} {storage.k8s.io/v1alpha1 v1alpha1}]
Feb 25 10:46:14.989: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 25 10:46:14.989: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 25 10:46:14.990: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 25 10:46:14.990: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 25 10:46:14.990: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 25 10:46:14.990: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 25 10:46:14.990: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 25 10:46:14.991: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 25 10:46:14.991: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 25 10:46:14.991: INFO: Checking APIGroup: scheduling.k8s.io
Feb 25 10:46:14.991: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 25 10:46:14.991: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
Feb 25 10:46:14.991: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 25 10:46:14.991: INFO: Checking APIGroup: coordination.k8s.io
Feb 25 10:46:14.992: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 25 10:46:14.992: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 25 10:46:14.992: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 25 10:46:14.992: INFO: Checking APIGroup: node.k8s.io
Feb 25 10:46:14.992: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 25 10:46:14.992: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1} {node.k8s.io/v1alpha1 v1alpha1}]
Feb 25 10:46:14.992: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 25 10:46:14.992: INFO: Checking APIGroup: discovery.k8s.io
Feb 25 10:46:14.993: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 25 10:46:14.993: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Feb 25 10:46:14.993: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 25 10:46:14.993: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 25 10:46:14.994: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Feb 25 10:46:14.994: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 25 10:46:14.994: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Feb 25 10:46:14.994: INFO: Checking APIGroup: internal.apiserver.k8s.io
Feb 25 10:46:14.994: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Feb 25 10:46:14.994: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Feb 25 10:46:14.994: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Feb 25 10:46:14.994: INFO: Checking APIGroup: acme.cert-manager.io
Feb 25 10:46:14.995: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Feb 25 10:46:14.995: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Feb 25 10:46:14.995: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Feb 25 10:46:14.995: INFO: Checking APIGroup: autoscaling.k8s.io
Feb 25 10:46:14.996: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Feb 25 10:46:14.996: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Feb 25 10:46:14.996: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Feb 25 10:46:14.996: INFO: Checking APIGroup: cert-manager.io
Feb 25 10:46:14.996: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Feb 25 10:46:14.996: INFO: Versions found [{cert-manager.io/v1 v1}]
Feb 25 10:46:14.996: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Feb 25 10:46:14.996: INFO: Checking APIGroup: crd.projectcalico.org
Feb 25 10:46:14.997: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 25 10:46:14.997: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 25 10:46:14.997: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 25 10:46:14.997: INFO: Checking APIGroup: application.giantswarm.io
Feb 25 10:46:14.998: INFO: PreferredVersion.GroupVersion: application.giantswarm.io/v1alpha1
Feb 25 10:46:14.998: INFO: Versions found [{application.giantswarm.io/v1alpha1 v1alpha1}]
Feb 25 10:46:14.998: INFO: application.giantswarm.io/v1alpha1 matches application.giantswarm.io/v1alpha1
Feb 25 10:46:14.998: INFO: Checking APIGroup: crd.k8s.amazonaws.com
Feb 25 10:46:14.998: INFO: PreferredVersion.GroupVersion: crd.k8s.amazonaws.com/v1alpha1
Feb 25 10:46:14.998: INFO: Versions found [{crd.k8s.amazonaws.com/v1alpha1 v1alpha1}]
Feb 25 10:46:14.998: INFO: crd.k8s.amazonaws.com/v1alpha1 matches crd.k8s.amazonaws.com/v1alpha1
Feb 25 10:46:14.998: INFO: Checking APIGroup: metrics.k8s.io
Feb 25 10:46:14.999: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 25 10:46:14.999: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 25 10:46:14.999: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:46:14.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9875" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":271,"skipped":5068,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:46:15.010: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Feb 25 10:46:17.205: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:46:19.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-440" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":272,"skipped":5077,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:46:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-2736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:48:01.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2736" for this suite.

• [SLOW TEST:102.182 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":273,"skipped":5078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:48:01.407: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:48:01.565: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd14b107-82a9-4667-b370-aecc4953d21f" in namespace "projected-5792" to be "Succeeded or Failed"
Feb 25 10:48:01.569: INFO: Pod "downwardapi-volume-dd14b107-82a9-4667-b370-aecc4953d21f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117017ms
Feb 25 10:48:03.577: INFO: Pod "downwardapi-volume-dd14b107-82a9-4667-b370-aecc4953d21f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01135715s
STEP: Saw pod success
Feb 25 10:48:03.577: INFO: Pod "downwardapi-volume-dd14b107-82a9-4667-b370-aecc4953d21f" satisfied condition "Succeeded or Failed"
Feb 25 10:48:03.580: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-dd14b107-82a9-4667-b370-aecc4953d21f container client-container: <nil>
STEP: delete the pod
Feb 25 10:48:03.608: INFO: Waiting for pod downwardapi-volume-dd14b107-82a9-4667-b370-aecc4953d21f to disappear
Feb 25 10:48:03.610: INFO: Pod downwardapi-volume-dd14b107-82a9-4667-b370-aecc4953d21f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:48:03.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5792" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":274,"skipped":5128,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:48:03.622: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-8944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Feb 25 10:48:23.884: INFO: EndpointSlice for Service endpointslice-8944/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:48:33.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8944" for this suite.

• [SLOW TEST:30.284 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":275,"skipped":5141,"failed":0}
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:48:33.907: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:48:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7789" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":276,"skipped":5145,"failed":0}
SSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:48:34.082: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-2677
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Feb 25 10:48:34.231: INFO: Waiting up to 5m0s for pod "security-context-b566fdda-de8b-4524-b396-e463a0eebe5b" in namespace "security-context-2677" to be "Succeeded or Failed"
Feb 25 10:48:34.234: INFO: Pod "security-context-b566fdda-de8b-4524-b396-e463a0eebe5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.575349ms
Feb 25 10:48:36.241: INFO: Pod "security-context-b566fdda-de8b-4524-b396-e463a0eebe5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009355033s
STEP: Saw pod success
Feb 25 10:48:36.241: INFO: Pod "security-context-b566fdda-de8b-4524-b396-e463a0eebe5b" satisfied condition "Succeeded or Failed"
Feb 25 10:48:36.244: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod security-context-b566fdda-de8b-4524-b396-e463a0eebe5b container test-container: <nil>
STEP: delete the pod
Feb 25 10:48:36.263: INFO: Waiting for pod security-context-b566fdda-de8b-4524-b396-e463a0eebe5b to disappear
Feb 25 10:48:36.265: INFO: Pod security-context-b566fdda-de8b-4524-b396-e463a0eebe5b no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:48:36.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2677" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":277,"skipped":5149,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:48:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:48:36.415: INFO: Creating deployment "test-recreate-deployment"
Feb 25 10:48:36.420: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 25 10:48:36.426: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Feb 25 10:48:38.433: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 25 10:48:38.436: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 25 10:48:38.445: INFO: Updating deployment test-recreate-deployment
Feb 25 10:48:38.445: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 25 10:48:38.535: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5792  33571697-d3fa-4ad8-999e-993ab89f778c 41465 2 2022-02-25 10:48:36 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-25 10:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004be4748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-02-25 10:48:38 +0000 UTC,LastTransitionTime:2022-02-25 10:48:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-02-25 10:48:38 +0000 UTC,LastTransitionTime:2022-02-25 10:48:36 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 25 10:48:38.538: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-5792  4f8916b3-c5aa-497b-9d28-13f2fff8496e 41464 1 2022-02-25 10:48:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 33571697-d3fa-4ad8-999e-993ab89f778c 0xc004be4c00 0xc004be4c01}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33571697-d3fa-4ad8-999e-993ab89f778c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:48:38 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004be4c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:48:38.538: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 25 10:48:38.538: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-5792  b1d8e099-de21-4401-970c-e5929dbd5f91 41454 2 2022-02-25 10:48:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 33571697-d3fa-4ad8-999e-993ab89f778c 0xc004be4ad7 0xc004be4ad8}] []  [{kube-controller-manager Update apps/v1 2022-02-25 10:48:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33571697-d3fa-4ad8-999e-993ab89f778c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-25 10:48:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004be4b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 25 10:48:38.541: INFO: Pod "test-recreate-deployment-85d47dcb4-xp26z" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-xp26z test-recreate-deployment-85d47dcb4- deployment-5792  5f2ccc70-6f9e-4893-9400-03c1cbe9674c 41466 0 2022-02-25 10:48:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 4f8916b3-c5aa-497b-9d28-13f2fff8496e 0xc004be5130 0xc004be5131}] []  [{kube-controller-manager Update v1 2022-02-25 10:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f8916b3-c5aa-497b-9d28-13f2fff8496e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-02-25 10:48:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6mjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6mjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-1-2-179.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:48:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:48:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:48:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-25 10:48:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.1.2.179,PodIP:,StartTime:2022-02-25 10:48:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:48:38.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5792" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":278,"skipped":5162,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:48:38.551: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 25 10:48:38.719: INFO: Waiting up to 5m0s for pod "pod-7195cd37-bbd2-4635-8ce4-705783a80efe" in namespace "emptydir-3633" to be "Succeeded or Failed"
Feb 25 10:48:38.724: INFO: Pod "pod-7195cd37-bbd2-4635-8ce4-705783a80efe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.843139ms
Feb 25 10:48:40.730: INFO: Pod "pod-7195cd37-bbd2-4635-8ce4-705783a80efe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010955621s
STEP: Saw pod success
Feb 25 10:48:40.730: INFO: Pod "pod-7195cd37-bbd2-4635-8ce4-705783a80efe" satisfied condition "Succeeded or Failed"
Feb 25 10:48:40.733: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-7195cd37-bbd2-4635-8ce4-705783a80efe container test-container: <nil>
STEP: delete the pod
Feb 25 10:48:40.753: INFO: Waiting for pod pod-7195cd37-bbd2-4635-8ce4-705783a80efe to disappear
Feb 25 10:48:40.755: INFO: Pod pod-7195cd37-bbd2-4635-8ce4-705783a80efe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:48:40.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3633" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":5167,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:48:40.765: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2330
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 25 10:48:40.908: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:48:49.176: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:49:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2330" for this suite.

• [SLOW TEST:27.771 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":280,"skipped":5172,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:49:08.537: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:49:08.723: INFO: Create a RollingUpdate DaemonSet
Feb 25 10:49:08.731: INFO: Check that daemon pods launch on every node of the cluster
Feb 25 10:49:08.738: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:08.738: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:08.738: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:08.743: INFO: Number of nodes with available pods: 0
Feb 25 10:49:08.743: INFO: Node ip-10-1-2-179.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:49:09.751: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:09.751: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:09.751: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:09.756: INFO: Number of nodes with available pods: 2
Feb 25 10:49:09.756: INFO: Node ip-10-1-2-67.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:49:10.751: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:10.751: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:10.751: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:10.756: INFO: Number of nodes with available pods: 3
Feb 25 10:49:10.756: INFO: Number of running nodes: 3, number of available pods: 3
Feb 25 10:49:10.756: INFO: Update the DaemonSet to trigger a rollout
Feb 25 10:49:10.768: INFO: Updating DaemonSet daemon-set
Feb 25 10:49:13.795: INFO: Roll back the DaemonSet before rollout is complete
Feb 25 10:49:13.815: INFO: Updating DaemonSet daemon-set
Feb 25 10:49:13.815: INFO: Make sure DaemonSet rollback is complete
Feb 25 10:49:13.820: INFO: Wrong image for pod: daemon-set-dknfj. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Feb 25 10:49:13.821: INFO: Pod daemon-set-dknfj is not available
Feb 25 10:49:13.826: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:13.826: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:13.827: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:14.861: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:14.861: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:14.861: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:15.839: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:15.839: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:15.839: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:16.840: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:16.841: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:16.841: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:17.835: INFO: Pod daemon-set-8hmtw is not available
Feb 25 10:49:17.841: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:17.841: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:49:17.841: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3616, will wait for the garbage collector to delete the pods
Feb 25 10:49:17.916: INFO: Deleting DaemonSet.extensions daemon-set took: 10.857214ms
Feb 25 10:49:18.017: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.849132ms
Feb 25 10:49:19.526: INFO: Number of nodes with available pods: 0
Feb 25 10:49:19.526: INFO: Number of running nodes: 0, number of available pods: 0
Feb 25 10:49:19.531: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41868"},"items":null}

Feb 25 10:49:19.536: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41868"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:49:19.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3616" for this suite.

• [SLOW TEST:11.040 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":281,"skipped":5178,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:49:19.578: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-12ed5f59-1dfe-401d-8385-59ca4e8f3f5c
STEP: Creating a pod to test consume configMaps
Feb 25 10:49:19.754: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f373ced-5ed8-4b0b-8997-2f75fec292c4" in namespace "configmap-3901" to be "Succeeded or Failed"
Feb 25 10:49:19.762: INFO: Pod "pod-configmaps-2f373ced-5ed8-4b0b-8997-2f75fec292c4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.674829ms
Feb 25 10:49:21.770: INFO: Pod "pod-configmaps-2f373ced-5ed8-4b0b-8997-2f75fec292c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015591574s
STEP: Saw pod success
Feb 25 10:49:21.770: INFO: Pod "pod-configmaps-2f373ced-5ed8-4b0b-8997-2f75fec292c4" satisfied condition "Succeeded or Failed"
Feb 25 10:49:21.775: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-configmaps-2f373ced-5ed8-4b0b-8997-2f75fec292c4 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 10:49:21.804: INFO: Waiting for pod pod-configmaps-2f373ced-5ed8-4b0b-8997-2f75fec292c4 to disappear
Feb 25 10:49:21.808: INFO: Pod pod-configmaps-2f373ced-5ed8-4b0b-8997-2f75fec292c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:49:21.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3901" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":5179,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:49:21.825: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Feb 25 10:49:21.990: INFO: created test-pod-1
Feb 25 10:49:21.999: INFO: created test-pod-2
Feb 25 10:49:22.008: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Feb 25 10:49:22.054: INFO: Pod quantity 3 is different from expected quantity 0
Feb 25 10:49:23.062: INFO: Pod quantity 3 is different from expected quantity 0
Feb 25 10:49:24.062: INFO: Pod quantity 3 is different from expected quantity 0
Feb 25 10:49:25.062: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:49:26.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8436" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":283,"skipped":5195,"failed":0}

------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:49:26.075: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 25 10:49:26.531: INFO: Pod name wrapped-volume-race-a7bb160d-cf62-4fdc-8486-bf4963df5d12: Found 0 pods out of 5
Feb 25 10:49:31.548: INFO: Pod name wrapped-volume-race-a7bb160d-cf62-4fdc-8486-bf4963df5d12: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a7bb160d-cf62-4fdc-8486-bf4963df5d12 in namespace emptydir-wrapper-1101, will wait for the garbage collector to delete the pods
Feb 25 10:49:41.663: INFO: Deleting ReplicationController wrapped-volume-race-a7bb160d-cf62-4fdc-8486-bf4963df5d12 took: 11.960939ms
Feb 25 10:49:41.764: INFO: Terminating ReplicationController wrapped-volume-race-a7bb160d-cf62-4fdc-8486-bf4963df5d12 pods took: 100.215565ms
STEP: Creating RC which spawns configmap-volume pods
Feb 25 10:49:45.095: INFO: Pod name wrapped-volume-race-e61c0d4e-7b07-4614-a29c-44d7760aa2ca: Found 0 pods out of 5
Feb 25 10:49:50.113: INFO: Pod name wrapped-volume-race-e61c0d4e-7b07-4614-a29c-44d7760aa2ca: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e61c0d4e-7b07-4614-a29c-44d7760aa2ca in namespace emptydir-wrapper-1101, will wait for the garbage collector to delete the pods
Feb 25 10:50:00.220: INFO: Deleting ReplicationController wrapped-volume-race-e61c0d4e-7b07-4614-a29c-44d7760aa2ca took: 10.393523ms
Feb 25 10:50:00.320: INFO: Terminating ReplicationController wrapped-volume-race-e61c0d4e-7b07-4614-a29c-44d7760aa2ca pods took: 100.464118ms
STEP: Creating RC which spawns configmap-volume pods
Feb 25 10:50:03.852: INFO: Pod name wrapped-volume-race-3aef68d9-7efe-44b1-a309-d3b24e49360b: Found 0 pods out of 5
Feb 25 10:50:08.865: INFO: Pod name wrapped-volume-race-3aef68d9-7efe-44b1-a309-d3b24e49360b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3aef68d9-7efe-44b1-a309-d3b24e49360b in namespace emptydir-wrapper-1101, will wait for the garbage collector to delete the pods
Feb 25 10:50:20.974: INFO: Deleting ReplicationController wrapped-volume-race-3aef68d9-7efe-44b1-a309-d3b24e49360b took: 10.669599ms
Feb 25 10:50:21.075: INFO: Terminating ReplicationController wrapped-volume-race-3aef68d9-7efe-44b1-a309-d3b24e49360b pods took: 100.439927ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:50:25.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1101" for this suite.

• [SLOW TEST:59.112 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":284,"skipped":5195,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:50:25.187: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-n66h
STEP: Creating a pod to test atomic-volume-subpath
Feb 25 10:50:25.368: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-n66h" in namespace "subpath-4561" to be "Succeeded or Failed"
Feb 25 10:50:25.378: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697032ms
Feb 25 10:50:27.387: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 2.019038299s
Feb 25 10:50:29.397: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 4.029151843s
Feb 25 10:50:31.405: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 6.037549581s
Feb 25 10:50:33.415: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 8.047311814s
Feb 25 10:50:35.424: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 10.056650737s
Feb 25 10:50:37.433: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 12.065521718s
Feb 25 10:50:39.442: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 14.074340062s
Feb 25 10:50:41.452: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 16.084339836s
Feb 25 10:50:43.461: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 18.093554284s
Feb 25 10:50:45.474: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Running", Reason="", readiness=true. Elapsed: 20.106755079s
Feb 25 10:50:47.483: INFO: Pod "pod-subpath-test-configmap-n66h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.11517199s
STEP: Saw pod success
Feb 25 10:50:47.483: INFO: Pod "pod-subpath-test-configmap-n66h" satisfied condition "Succeeded or Failed"
Feb 25 10:50:47.487: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-subpath-test-configmap-n66h container test-container-subpath-configmap-n66h: <nil>
STEP: delete the pod
Feb 25 10:50:47.516: INFO: Waiting for pod pod-subpath-test-configmap-n66h to disappear
Feb 25 10:50:47.521: INFO: Pod pod-subpath-test-configmap-n66h no longer exists
STEP: Deleting pod pod-subpath-test-configmap-n66h
Feb 25 10:50:47.521: INFO: Deleting pod "pod-subpath-test-configmap-n66h" in namespace "subpath-4561"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:50:47.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4561" for this suite.

• [SLOW TEST:22.351 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":285,"skipped":5199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:50:47.540: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 25 10:50:47.704: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43125 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:50:47.704: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43125 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 25 10:50:57.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43178 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:50:57.721: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43178 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 25 10:51:07.738: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43216 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:51:07.738: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43216 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 25 10:51:17.753: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43249 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:51:17.753: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1114  f63f1dc6-3166-429c-bf92-49a767618b3f 43249 0 2022-02-25 10:50:47 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-25 10:50:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 25 10:51:27.768: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1114  ba44f906-75f7-40b7-955b-36fa019d644a 43283 0 2022-02-25 10:51:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-25 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:51:27.768: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1114  ba44f906-75f7-40b7-955b-36fa019d644a 43283 0 2022-02-25 10:51:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-25 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 25 10:51:37.782: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1114  ba44f906-75f7-40b7-955b-36fa019d644a 43319 0 2022-02-25 10:51:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-25 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 25 10:51:37.782: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1114  ba44f906-75f7-40b7-955b-36fa019d644a 43319 0 2022-02-25 10:51:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-25 10:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:51:47.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1114" for this suite.

• [SLOW TEST:60.263 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":286,"skipped":5267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:51:47.803: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9301
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:51:47.957: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:51:53.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9301" for this suite.

• [SLOW TEST:5.736 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":287,"skipped":5295,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:51:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Feb 25 10:51:53.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8123 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Feb 25 10:51:53.892: INFO: stderr: ""
Feb 25 10:51:53.892: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Feb 25 10:51:53.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-8123 delete pods e2e-test-httpd-pod'
Feb 25 10:51:55.963: INFO: stderr: ""
Feb 25 10:51:55.963: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:51:55.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8123" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":288,"skipped":5307,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:51:55.980: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Feb 25 10:51:56.143: INFO: Waiting up to 5m0s for pod "var-expansion-bdebf145-f3d1-46d4-a460-1663ac13c9dd" in namespace "var-expansion-1076" to be "Succeeded or Failed"
Feb 25 10:51:56.150: INFO: Pod "var-expansion-bdebf145-f3d1-46d4-a460-1663ac13c9dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.605385ms
Feb 25 10:51:58.159: INFO: Pod "var-expansion-bdebf145-f3d1-46d4-a460-1663ac13c9dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015521869s
STEP: Saw pod success
Feb 25 10:51:58.159: INFO: Pod "var-expansion-bdebf145-f3d1-46d4-a460-1663ac13c9dd" satisfied condition "Succeeded or Failed"
Feb 25 10:51:58.163: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod var-expansion-bdebf145-f3d1-46d4-a460-1663ac13c9dd container dapi-container: <nil>
STEP: delete the pod
Feb 25 10:51:58.194: INFO: Waiting for pod var-expansion-bdebf145-f3d1-46d4-a460-1663ac13c9dd to disappear
Feb 25 10:51:58.199: INFO: Pod var-expansion-bdebf145-f3d1-46d4-a460-1663ac13c9dd no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:51:58.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1076" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5326,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:51:58.215: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-8748
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8748 to expose endpoints map[]
Feb 25 10:51:58.384: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 25 10:51:59.398: INFO: successfully validated that service endpoint-test2 in namespace services-8748 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8748
Feb 25 10:51:59.421: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:52:01.429: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8748 to expose endpoints map[pod1:[80]]
Feb 25 10:52:01.449: INFO: successfully validated that service endpoint-test2 in namespace services-8748 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Feb 25 10:52:01.449: INFO: Creating new exec pod
Feb 25 10:52:04.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8748 exec execpodc5vrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 25 10:52:04.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 25 10:52:04.622: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:52:04.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8748 exec execpodc5vrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.44.175 80'
Feb 25 10:52:04.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.44.175 80\nConnection to 172.31.44.175 80 port [tcp/http] succeeded!\n"
Feb 25 10:52:04.748: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-8748
Feb 25 10:52:04.767: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:52:06.774: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8748 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 25 10:52:06.797: INFO: successfully validated that service endpoint-test2 in namespace services-8748 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Feb 25 10:52:07.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8748 exec execpodc5vrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 25 10:52:07.938: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 25 10:52:07.938: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:52:07.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8748 exec execpodc5vrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.44.175 80'
Feb 25 10:52:08.070: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 172.31.44.175 80\nConnection to 172.31.44.175 80 port [tcp/http] succeeded!\n"
Feb 25 10:52:08.070: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8748
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8748 to expose endpoints map[pod2:[80]]
Feb 25 10:52:08.114: INFO: successfully validated that service endpoint-test2 in namespace services-8748 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Feb 25 10:52:09.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8748 exec execpodc5vrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 25 10:52:09.264: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 25 10:52:09.264: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 25 10:52:09.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=services-8748 exec execpodc5vrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.44.175 80'
Feb 25 10:52:09.402: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 172.31.44.175 80\nConnection to 172.31.44.175 80 port [tcp/http] succeeded!\n"
Feb 25 10:52:09.402: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-8748
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8748 to expose endpoints map[]
Feb 25 10:52:09.437: INFO: successfully validated that service endpoint-test2 in namespace services-8748 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:52:09.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8748" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.263 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":290,"skipped":5339,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:52:09.477: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-990
STEP: Creating secret with name secret-test-34596783-0f0c-49bd-88ac-d399b491df4f
STEP: Creating a pod to test consume secrets
Feb 25 10:52:09.804: INFO: Waiting up to 5m0s for pod "pod-secrets-29b00b9f-8d0c-453a-985e-d2865b19dc3a" in namespace "secrets-6627" to be "Succeeded or Failed"
Feb 25 10:52:09.810: INFO: Pod "pod-secrets-29b00b9f-8d0c-453a-985e-d2865b19dc3a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942856ms
Feb 25 10:52:11.819: INFO: Pod "pod-secrets-29b00b9f-8d0c-453a-985e-d2865b19dc3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01497161s
STEP: Saw pod success
Feb 25 10:52:11.819: INFO: Pod "pod-secrets-29b00b9f-8d0c-453a-985e-d2865b19dc3a" satisfied condition "Succeeded or Failed"
Feb 25 10:52:11.823: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-secrets-29b00b9f-8d0c-453a-985e-d2865b19dc3a container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 10:52:11.853: INFO: Waiting for pod pod-secrets-29b00b9f-8d0c-453a-985e-d2865b19dc3a to disappear
Feb 25 10:52:11.858: INFO: Pod pod-secrets-29b00b9f-8d0c-453a-985e-d2865b19dc3a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:52:11.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6627" for this suite.
STEP: Destroying namespace "secret-namespace-990" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5342,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:52:11.883: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-g7rvn in namespace proxy-1754
I0225 10:52:12.081052      22 runners.go:190] Created replication controller with name: proxy-service-g7rvn, namespace: proxy-1754, replica count: 1
I0225 10:52:13.134235      22 runners.go:190] proxy-service-g7rvn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0225 10:52:14.135168      22 runners.go:190] proxy-service-g7rvn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0225 10:52:15.135665      22 runners.go:190] proxy-service-g7rvn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 10:52:15.146: INFO: setup took 3.096129389s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 25 10:52:15.156: INFO: (0) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.882592ms)
Feb 25 10:52:15.156: INFO: (0) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 9.284798ms)
Feb 25 10:52:15.156: INFO: (0) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.508593ms)
Feb 25 10:52:15.156: INFO: (0) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 9.19445ms)
Feb 25 10:52:15.156: INFO: (0) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 9.358191ms)
Feb 25 10:52:15.156: INFO: (0) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 9.380545ms)
Feb 25 10:52:15.157: INFO: (0) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 9.677935ms)
Feb 25 10:52:15.157: INFO: (0) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 10.029125ms)
Feb 25 10:52:15.159: INFO: (0) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 12.584351ms)
Feb 25 10:52:15.159: INFO: (0) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 12.187687ms)
Feb 25 10:52:15.159: INFO: (0) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 12.390921ms)
Feb 25 10:52:15.160: INFO: (0) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 13.401704ms)
Feb 25 10:52:15.161: INFO: (0) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 13.983421ms)
Feb 25 10:52:15.161: INFO: (0) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 14.245931ms)
Feb 25 10:52:15.161: INFO: (0) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 14.066687ms)
Feb 25 10:52:15.161: INFO: (0) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 14.150536ms)
Feb 25 10:52:15.169: INFO: (1) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 7.787556ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 8.653568ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 8.727796ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.783555ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 9.008404ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.166052ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.894614ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 8.992517ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 8.972991ms)
Feb 25 10:52:15.170: INFO: (1) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 9.094755ms)
Feb 25 10:52:15.173: INFO: (1) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 11.590036ms)
Feb 25 10:52:15.173: INFO: (1) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 11.806176ms)
Feb 25 10:52:15.175: INFO: (1) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 13.798042ms)
Feb 25 10:52:15.175: INFO: (1) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 13.891189ms)
Feb 25 10:52:15.175: INFO: (1) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 13.945094ms)
Feb 25 10:52:15.176: INFO: (1) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 15.05743ms)
Feb 25 10:52:15.182: INFO: (2) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 5.920042ms)
Feb 25 10:52:15.185: INFO: (2) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 8.703659ms)
Feb 25 10:52:15.185: INFO: (2) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.806524ms)
Feb 25 10:52:15.186: INFO: (2) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 9.400739ms)
Feb 25 10:52:15.186: INFO: (2) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 9.572094ms)
Feb 25 10:52:15.186: INFO: (2) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 9.415717ms)
Feb 25 10:52:15.186: INFO: (2) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 9.983495ms)
Feb 25 10:52:15.187: INFO: (2) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.083517ms)
Feb 25 10:52:15.187: INFO: (2) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.274054ms)
Feb 25 10:52:15.187: INFO: (2) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 10.496436ms)
Feb 25 10:52:15.189: INFO: (2) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 13.092354ms)
Feb 25 10:52:15.190: INFO: (2) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 13.018862ms)
Feb 25 10:52:15.190: INFO: (2) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 13.455033ms)
Feb 25 10:52:15.190: INFO: (2) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 13.383039ms)
Feb 25 10:52:15.190: INFO: (2) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 13.185543ms)
Feb 25 10:52:15.191: INFO: (2) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 14.896337ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 9.419963ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 9.342041ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 9.486505ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.847106ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.088747ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 10.283863ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 10.154592ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.193779ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 10.151414ms)
Feb 25 10:52:15.201: INFO: (3) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.192637ms)
Feb 25 10:52:15.202: INFO: (3) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 11.119718ms)
Feb 25 10:52:15.205: INFO: (3) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 13.864325ms)
Feb 25 10:52:15.205: INFO: (3) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 13.876357ms)
Feb 25 10:52:15.205: INFO: (3) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 13.967165ms)
Feb 25 10:52:15.205: INFO: (3) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 13.943011ms)
Feb 25 10:52:15.205: INFO: (3) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 14.083902ms)
Feb 25 10:52:15.212: INFO: (4) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 6.315789ms)
Feb 25 10:52:15.214: INFO: (4) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 8.463157ms)
Feb 25 10:52:15.214: INFO: (4) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.316743ms)
Feb 25 10:52:15.214: INFO: (4) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 8.523786ms)
Feb 25 10:52:15.214: INFO: (4) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.907495ms)
Feb 25 10:52:15.214: INFO: (4) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.755981ms)
Feb 25 10:52:15.214: INFO: (4) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 8.659598ms)
Feb 25 10:52:15.214: INFO: (4) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 8.754857ms)
Feb 25 10:52:15.215: INFO: (4) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.796711ms)
Feb 25 10:52:15.215: INFO: (4) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 8.850486ms)
Feb 25 10:52:15.217: INFO: (4) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 11.870066ms)
Feb 25 10:52:15.219: INFO: (4) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 13.766455ms)
Feb 25 10:52:15.220: INFO: (4) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 14.213359ms)
Feb 25 10:52:15.220: INFO: (4) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 14.121377ms)
Feb 25 10:52:15.220: INFO: (4) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 13.957773ms)
Feb 25 10:52:15.220: INFO: (4) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 14.236601ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 7.352649ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 7.343955ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 8.028244ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 8.438125ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 7.795797ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.031572ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 8.212951ms)
Feb 25 10:52:15.228: INFO: (5) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 8.001616ms)
Feb 25 10:52:15.230: INFO: (5) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 9.853954ms)
Feb 25 10:52:15.230: INFO: (5) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.277181ms)
Feb 25 10:52:15.230: INFO: (5) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 9.583755ms)
Feb 25 10:52:15.232: INFO: (5) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 11.616609ms)
Feb 25 10:52:15.232: INFO: (5) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 12.095172ms)
Feb 25 10:52:15.232: INFO: (5) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 11.994856ms)
Feb 25 10:52:15.232: INFO: (5) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 12.029187ms)
Feb 25 10:52:15.232: INFO: (5) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 12.186656ms)
Feb 25 10:52:15.237: INFO: (6) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 4.993386ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 9.132639ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 9.329757ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.604859ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.625092ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 9.678571ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 9.873779ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 9.898084ms)
Feb 25 10:52:15.242: INFO: (6) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 9.931823ms)
Feb 25 10:52:15.243: INFO: (6) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 10.139535ms)
Feb 25 10:52:15.245: INFO: (6) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 12.623243ms)
Feb 25 10:52:15.248: INFO: (6) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 15.043617ms)
Feb 25 10:52:15.248: INFO: (6) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 15.198537ms)
Feb 25 10:52:15.248: INFO: (6) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 15.262022ms)
Feb 25 10:52:15.248: INFO: (6) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 15.610935ms)
Feb 25 10:52:15.248: INFO: (6) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 15.602736ms)
Feb 25 10:52:15.255: INFO: (7) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 6.802074ms)
Feb 25 10:52:15.257: INFO: (7) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.9846ms)
Feb 25 10:52:15.257: INFO: (7) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.833162ms)
Feb 25 10:52:15.257: INFO: (7) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 8.996129ms)
Feb 25 10:52:15.257: INFO: (7) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.965081ms)
Feb 25 10:52:15.259: INFO: (7) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.917097ms)
Feb 25 10:52:15.260: INFO: (7) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 11.365534ms)
Feb 25 10:52:15.260: INFO: (7) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 11.179427ms)
Feb 25 10:52:15.260: INFO: (7) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 11.357519ms)
Feb 25 10:52:15.260: INFO: (7) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 11.530693ms)
Feb 25 10:52:15.262: INFO: (7) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 13.234442ms)
Feb 25 10:52:15.264: INFO: (7) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 15.758435ms)
Feb 25 10:52:15.264: INFO: (7) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 15.833318ms)
Feb 25 10:52:15.264: INFO: (7) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 15.91819ms)
Feb 25 10:52:15.264: INFO: (7) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 15.723403ms)
Feb 25 10:52:15.264: INFO: (7) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 15.864215ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 10.229732ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.235839ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.452492ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.52541ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 10.449571ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 10.534297ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 10.665184ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.587103ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 10.782544ms)
Feb 25 10:52:15.275: INFO: (8) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 11.00454ms)
Feb 25 10:52:15.277: INFO: (8) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 12.244395ms)
Feb 25 10:52:15.279: INFO: (8) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 14.763085ms)
Feb 25 10:52:15.279: INFO: (8) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 14.967142ms)
Feb 25 10:52:15.279: INFO: (8) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 14.938216ms)
Feb 25 10:52:15.279: INFO: (8) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 14.907498ms)
Feb 25 10:52:15.279: INFO: (8) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 14.60598ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.553019ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.825161ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 8.785839ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 8.986685ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 9.515296ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 9.702143ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 9.296285ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 9.454186ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 9.597067ms)
Feb 25 10:52:15.289: INFO: (9) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.512555ms)
Feb 25 10:52:15.290: INFO: (9) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 10.774646ms)
Feb 25 10:52:15.293: INFO: (9) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 12.606916ms)
Feb 25 10:52:15.293: INFO: (9) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 12.942029ms)
Feb 25 10:52:15.293: INFO: (9) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 12.523872ms)
Feb 25 10:52:15.293: INFO: (9) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 12.978507ms)
Feb 25 10:52:15.293: INFO: (9) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 12.778147ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 7.521186ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 7.256662ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 7.851538ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 7.81763ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 8.11207ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 7.781869ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 8.046119ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.345846ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 8.572444ms)
Feb 25 10:52:15.301: INFO: (10) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 8.297182ms)
Feb 25 10:52:15.302: INFO: (10) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 9.562793ms)
Feb 25 10:52:15.305: INFO: (10) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 11.767878ms)
Feb 25 10:52:15.305: INFO: (10) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 11.741102ms)
Feb 25 10:52:15.305: INFO: (10) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 11.744203ms)
Feb 25 10:52:15.305: INFO: (10) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 11.804419ms)
Feb 25 10:52:15.305: INFO: (10) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 11.97738ms)
Feb 25 10:52:15.310: INFO: (11) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 5.084344ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 7.006305ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 7.099194ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 7.017852ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 7.434205ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 7.374209ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 7.723067ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 8.030354ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 8.247627ms)
Feb 25 10:52:15.313: INFO: (11) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.179023ms)
Feb 25 10:52:15.315: INFO: (11) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 9.939298ms)
Feb 25 10:52:15.317: INFO: (11) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 11.827534ms)
Feb 25 10:52:15.317: INFO: (11) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 11.935597ms)
Feb 25 10:52:15.318: INFO: (11) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 11.687388ms)
Feb 25 10:52:15.318: INFO: (11) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 11.963569ms)
Feb 25 10:52:15.318: INFO: (11) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 11.646764ms)
Feb 25 10:52:15.331: INFO: (12) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 12.928363ms)
Feb 25 10:52:15.331: INFO: (12) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 13.103372ms)
Feb 25 10:52:15.332: INFO: (12) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 14.05711ms)
Feb 25 10:52:15.332: INFO: (12) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 14.208556ms)
Feb 25 10:52:15.337: INFO: (12) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 19.269559ms)
Feb 25 10:52:15.337: INFO: (12) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 19.556035ms)
Feb 25 10:52:15.337: INFO: (12) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 19.640716ms)
Feb 25 10:52:15.337: INFO: (12) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 19.818283ms)
Feb 25 10:52:15.338: INFO: (12) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 20.470947ms)
Feb 25 10:52:15.338: INFO: (12) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 20.506978ms)
Feb 25 10:52:15.338: INFO: (12) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 20.419968ms)
Feb 25 10:52:15.338: INFO: (12) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 20.565763ms)
Feb 25 10:52:15.338: INFO: (12) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 20.603126ms)
Feb 25 10:52:15.340: INFO: (12) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 21.800038ms)
Feb 25 10:52:15.340: INFO: (12) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 21.943563ms)
Feb 25 10:52:15.340: INFO: (12) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 21.964617ms)
Feb 25 10:52:15.350: INFO: (13) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 9.9038ms)
Feb 25 10:52:15.350: INFO: (13) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.189939ms)
Feb 25 10:52:15.351: INFO: (13) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.78503ms)
Feb 25 10:52:15.351: INFO: (13) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 10.386214ms)
Feb 25 10:52:15.351: INFO: (13) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.817706ms)
Feb 25 10:52:15.352: INFO: (13) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 11.867538ms)
Feb 25 10:52:15.352: INFO: (13) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 11.9297ms)
Feb 25 10:52:15.353: INFO: (13) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 13.12701ms)
Feb 25 10:52:15.354: INFO: (13) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 13.825933ms)
Feb 25 10:52:15.355: INFO: (13) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 14.79855ms)
Feb 25 10:52:15.356: INFO: (13) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 15.572601ms)
Feb 25 10:52:15.359: INFO: (13) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 18.159915ms)
Feb 25 10:52:15.359: INFO: (13) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 18.516607ms)
Feb 25 10:52:15.359: INFO: (13) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 18.561248ms)
Feb 25 10:52:15.359: INFO: (13) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 18.406471ms)
Feb 25 10:52:15.359: INFO: (13) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 18.697157ms)
Feb 25 10:52:15.366: INFO: (14) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 6.722008ms)
Feb 25 10:52:15.369: INFO: (14) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 9.992712ms)
Feb 25 10:52:15.369: INFO: (14) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 10.376721ms)
Feb 25 10:52:15.369: INFO: (14) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 10.209817ms)
Feb 25 10:52:15.369: INFO: (14) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 10.322978ms)
Feb 25 10:52:15.369: INFO: (14) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.234608ms)
Feb 25 10:52:15.370: INFO: (14) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.28748ms)
Feb 25 10:52:15.370: INFO: (14) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.980952ms)
Feb 25 10:52:15.370: INFO: (14) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 10.88927ms)
Feb 25 10:52:15.370: INFO: (14) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 11.254258ms)
Feb 25 10:52:15.373: INFO: (14) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 13.984566ms)
Feb 25 10:52:15.373: INFO: (14) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 13.9208ms)
Feb 25 10:52:15.373: INFO: (14) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 14.11188ms)
Feb 25 10:52:15.373: INFO: (14) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 14.110142ms)
Feb 25 10:52:15.375: INFO: (14) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 16.316489ms)
Feb 25 10:52:15.376: INFO: (14) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 16.679319ms)
Feb 25 10:52:15.387: INFO: (15) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 10.553365ms)
Feb 25 10:52:15.387: INFO: (15) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.928629ms)
Feb 25 10:52:15.387: INFO: (15) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 10.768689ms)
Feb 25 10:52:15.387: INFO: (15) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.771587ms)
Feb 25 10:52:15.387: INFO: (15) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 11.325364ms)
Feb 25 10:52:15.387: INFO: (15) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 11.250573ms)
Feb 25 10:52:15.387: INFO: (15) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 11.205998ms)
Feb 25 10:52:15.388: INFO: (15) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 11.912398ms)
Feb 25 10:52:15.388: INFO: (15) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 11.883252ms)
Feb 25 10:52:15.388: INFO: (15) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 12.282484ms)
Feb 25 10:52:15.389: INFO: (15) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 12.929925ms)
Feb 25 10:52:15.390: INFO: (15) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 13.602006ms)
Feb 25 10:52:15.390: INFO: (15) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 13.774832ms)
Feb 25 10:52:15.392: INFO: (15) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 16.408426ms)
Feb 25 10:52:15.392: INFO: (15) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 16.29194ms)
Feb 25 10:52:15.393: INFO: (15) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 16.639153ms)
Feb 25 10:52:15.400: INFO: (16) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 7.065452ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 12.096267ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 12.268802ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 12.105904ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 12.231333ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 12.198919ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 12.241095ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 12.198895ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 12.419556ms)
Feb 25 10:52:15.405: INFO: (16) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 12.316486ms)
Feb 25 10:52:15.408: INFO: (16) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 15.29305ms)
Feb 25 10:52:15.408: INFO: (16) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 15.228667ms)
Feb 25 10:52:15.409: INFO: (16) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 15.995029ms)
Feb 25 10:52:15.409: INFO: (16) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 15.778897ms)
Feb 25 10:52:15.409: INFO: (16) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 15.865343ms)
Feb 25 10:52:15.410: INFO: (16) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 16.947137ms)
Feb 25 10:52:15.417: INFO: (17) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 6.796923ms)
Feb 25 10:52:15.420: INFO: (17) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 9.60001ms)
Feb 25 10:52:15.420: INFO: (17) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.412336ms)
Feb 25 10:52:15.421: INFO: (17) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 10.419309ms)
Feb 25 10:52:15.421: INFO: (17) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 10.897821ms)
Feb 25 10:52:15.421: INFO: (17) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 11.083372ms)
Feb 25 10:52:15.421: INFO: (17) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.868813ms)
Feb 25 10:52:15.421: INFO: (17) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 10.752813ms)
Feb 25 10:52:15.421: INFO: (17) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 10.864134ms)
Feb 25 10:52:15.420: INFO: (17) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 10.181801ms)
Feb 25 10:52:15.424: INFO: (17) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 13.434968ms)
Feb 25 10:52:15.424: INFO: (17) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 13.608623ms)
Feb 25 10:52:15.424: INFO: (17) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 13.748763ms)
Feb 25 10:52:15.424: INFO: (17) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 13.805472ms)
Feb 25 10:52:15.424: INFO: (17) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 14.11106ms)
Feb 25 10:52:15.426: INFO: (17) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 15.433975ms)
Feb 25 10:52:15.435: INFO: (18) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 8.292117ms)
Feb 25 10:52:15.435: INFO: (18) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 8.411465ms)
Feb 25 10:52:15.435: INFO: (18) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 8.571252ms)
Feb 25 10:52:15.435: INFO: (18) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 8.80835ms)
Feb 25 10:52:15.435: INFO: (18) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 8.550639ms)
Feb 25 10:52:15.435: INFO: (18) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 8.965455ms)
Feb 25 10:52:15.437: INFO: (18) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 11.090663ms)
Feb 25 10:52:15.437: INFO: (18) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.846699ms)
Feb 25 10:52:15.437: INFO: (18) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 10.809542ms)
Feb 25 10:52:15.437: INFO: (18) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 11.265271ms)
Feb 25 10:52:15.437: INFO: (18) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 10.981713ms)
Feb 25 10:52:15.440: INFO: (18) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 13.944514ms)
Feb 25 10:52:15.440: INFO: (18) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 14.018412ms)
Feb 25 10:52:15.440: INFO: (18) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 13.738488ms)
Feb 25 10:52:15.440: INFO: (18) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 13.860044ms)
Feb 25 10:52:15.442: INFO: (18) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 15.551298ms)
Feb 25 10:52:15.450: INFO: (19) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:462/proxy/: tls qux (200; 8.057442ms)
Feb 25 10:52:15.453: INFO: (19) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 10.710862ms)
Feb 25 10:52:15.453: INFO: (19) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname1/proxy/: foo (200; 11.257207ms)
Feb 25 10:52:15.453: INFO: (19) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9/proxy/rewriteme">test</a> (200; 11.071442ms)
Feb 25 10:52:15.453: INFO: (19) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">test<... (200; 10.847489ms)
Feb 25 10:52:15.454: INFO: (19) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:162/proxy/: bar (200; 11.126378ms)
Feb 25 10:52:15.454: INFO: (19) /api/v1/namespaces/proxy-1754/pods/proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 11.194987ms)
Feb 25 10:52:15.454: INFO: (19) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:460/proxy/: tls baz (200; 11.294963ms)
Feb 25 10:52:15.454: INFO: (19) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:1080/proxy/rewriteme">... (200; 11.981728ms)
Feb 25 10:52:15.454: INFO: (19) /api/v1/namespaces/proxy-1754/pods/http:proxy-service-g7rvn-zm8n9:160/proxy/: foo (200; 11.430909ms)
Feb 25 10:52:15.454: INFO: (19) /api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/: <a href="/api/v1/namespaces/proxy-1754/pods/https:proxy-service-g7rvn-zm8n9:443/proxy/tlsrewritem... (200; 12.161115ms)
Feb 25 10:52:15.455: INFO: (19) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname1/proxy/: foo (200; 12.775813ms)
Feb 25 10:52:15.456: INFO: (19) /api/v1/namespaces/proxy-1754/services/http:proxy-service-g7rvn:portname2/proxy/: bar (200; 13.657553ms)
Feb 25 10:52:15.456: INFO: (19) /api/v1/namespaces/proxy-1754/services/proxy-service-g7rvn:portname2/proxy/: bar (200; 13.889869ms)
Feb 25 10:52:15.456: INFO: (19) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname2/proxy/: tls qux (200; 13.850674ms)
Feb 25 10:52:15.458: INFO: (19) /api/v1/namespaces/proxy-1754/services/https:proxy-service-g7rvn:tlsportname1/proxy/: tls baz (200; 15.04474ms)
STEP: deleting ReplicationController proxy-service-g7rvn in namespace proxy-1754, will wait for the garbage collector to delete the pods
Feb 25 10:52:15.523: INFO: Deleting ReplicationController proxy-service-g7rvn took: 9.682797ms
Feb 25 10:52:15.624: INFO: Terminating ReplicationController proxy-service-g7rvn pods took: 101.094658ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:52:17.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1754" for this suite.

• [SLOW TEST:5.661 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":292,"skipped":5355,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:52:17.544: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Feb 25 10:52:17.698: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-2244 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:52:17.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2244" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":293,"skipped":5355,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:52:17.763: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 25 10:52:17.927: INFO: Waiting up to 5m0s for pod "pod-f3820f35-c8e6-4ac3-a89f-736e8d52da14" in namespace "emptydir-8029" to be "Succeeded or Failed"
Feb 25 10:52:17.934: INFO: Pod "pod-f3820f35-c8e6-4ac3-a89f-736e8d52da14": Phase="Pending", Reason="", readiness=false. Elapsed: 7.384201ms
Feb 25 10:52:19.943: INFO: Pod "pod-f3820f35-c8e6-4ac3-a89f-736e8d52da14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01616194s
STEP: Saw pod success
Feb 25 10:52:19.943: INFO: Pod "pod-f3820f35-c8e6-4ac3-a89f-736e8d52da14" satisfied condition "Succeeded or Failed"
Feb 25 10:52:19.948: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-f3820f35-c8e6-4ac3-a89f-736e8d52da14 container test-container: <nil>
STEP: delete the pod
Feb 25 10:52:19.972: INFO: Waiting for pod pod-f3820f35-c8e6-4ac3-a89f-736e8d52da14 to disappear
Feb 25 10:52:19.976: INFO: Pod pod-f3820f35-c8e6-4ac3-a89f-736e8d52da14 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:52:19.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8029" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:52:19.993: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:52:20.164: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23d827b5-e0e6-4ba5-9bc4-c5caec6c9dbb" in namespace "downward-api-2274" to be "Succeeded or Failed"
Feb 25 10:52:20.173: INFO: Pod "downwardapi-volume-23d827b5-e0e6-4ba5-9bc4-c5caec6c9dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.552146ms
Feb 25 10:52:22.182: INFO: Pod "downwardapi-volume-23d827b5-e0e6-4ba5-9bc4-c5caec6c9dbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018014335s
STEP: Saw pod success
Feb 25 10:52:22.182: INFO: Pod "downwardapi-volume-23d827b5-e0e6-4ba5-9bc4-c5caec6c9dbb" satisfied condition "Succeeded or Failed"
Feb 25 10:52:22.186: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-23d827b5-e0e6-4ba5-9bc4-c5caec6c9dbb container client-container: <nil>
STEP: delete the pod
Feb 25 10:52:22.214: INFO: Waiting for pod downwardapi-volume-23d827b5-e0e6-4ba5-9bc4-c5caec6c9dbb to disappear
Feb 25 10:52:22.218: INFO: Pod downwardapi-volume-23d827b5-e0e6-4ba5-9bc4-c5caec6c9dbb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:52:22.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2274" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:52:22.232: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:52:22.408: INFO: The status of Pod busybox-readonly-fse9bd11b9-16f5-4542-bfd2-98cb9c32c13a is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:52:24.417: INFO: The status of Pod busybox-readonly-fse9bd11b9-16f5-4542-bfd2-98cb9c32c13a is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:52:24.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3682" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5413,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:52:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 25 10:53:04.690: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0225 10:53:04.690623      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 25 10:53:04.690: INFO: Deleting pod "simpletest.rc-2djqn" in namespace "gc-4140"
Feb 25 10:53:04.710: INFO: Deleting pod "simpletest.rc-2jq9w" in namespace "gc-4140"
Feb 25 10:53:04.728: INFO: Deleting pod "simpletest.rc-9cmfb" in namespace "gc-4140"
Feb 25 10:53:04.751: INFO: Deleting pod "simpletest.rc-btwpp" in namespace "gc-4140"
Feb 25 10:53:04.771: INFO: Deleting pod "simpletest.rc-fttt7" in namespace "gc-4140"
Feb 25 10:53:04.783: INFO: Deleting pod "simpletest.rc-hxkjw" in namespace "gc-4140"
Feb 25 10:53:04.802: INFO: Deleting pod "simpletest.rc-n8vhl" in namespace "gc-4140"
Feb 25 10:53:04.821: INFO: Deleting pod "simpletest.rc-pm5dt" in namespace "gc-4140"
Feb 25 10:53:04.840: INFO: Deleting pod "simpletest.rc-v7glp" in namespace "gc-4140"
Feb 25 10:53:04.856: INFO: Deleting pod "simpletest.rc-x2qwh" in namespace "gc-4140"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:53:04.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4140" for this suite.

• [SLOW TEST:40.440 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":297,"skipped":5418,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:53:04.889: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6995.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6995.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6995.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6995.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6995.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 154.204.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.204.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.204.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.204.154_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6995.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6995.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6995.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6995.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6995.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6995.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 154.204.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.204.154_udp@PTR;check="$$(dig +tcp +noall +answer +search 154.204.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.204.154_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:53:07.120: INFO: Unable to read wheezy_udp@dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.131: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.137: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.142: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.147: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.153: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.158: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.163: INFO: Unable to read 172.31.204.154_udp@PTR from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.168: INFO: Unable to read 172.31.204.154_tcp@PTR from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.173: INFO: Unable to read jessie_udp@dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.178: INFO: Unable to read jessie_tcp@dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.184: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.190: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.195: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.200: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-6995.svc.cluster.local from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.205: INFO: Unable to read jessie_udp@PodARecord from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.210: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.215: INFO: Unable to read 172.31.204.154_udp@PTR from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.221: INFO: Unable to read 172.31.204.154_tcp@PTR from pod dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935: the server could not find the requested resource (get pods dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935)
Feb 25 10:53:07.221: INFO: Lookups using dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935 failed for: [wheezy_udp@dns-test-service.dns-6995.svc.cluster.local wheezy_tcp@dns-test-service.dns-6995.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-6995.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-6995.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 172.31.204.154_udp@PTR 172.31.204.154_tcp@PTR jessie_udp@dns-test-service.dns-6995.svc.cluster.local jessie_tcp@dns-test-service.dns-6995.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6995.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-6995.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-6995.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 172.31.204.154_udp@PTR 172.31.204.154_tcp@PTR]

Feb 25 10:53:12.339: INFO: DNS probes using dns-6995/dns-test-5b2fad59-b3f6-4590-91aa-91af191d0935 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:53:12.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6995" for this suite.

• [SLOW TEST:7.548 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":298,"skipped":5420,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:53:12.438: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-6841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:00.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6841" for this suite.

• [SLOW TEST:348.212 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":299,"skipped":5423,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 25 10:59:00.845: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:00.845: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:00.845: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:00.850: INFO: Number of nodes with available pods: 0
Feb 25 10:59:00.850: INFO: Node ip-10-1-2-179.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:59:01.858: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:01.858: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:01.858: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:01.863: INFO: Number of nodes with available pods: 1
Feb 25 10:59:01.863: INFO: Node ip-10-1-2-179.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:59:02.858: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:02.858: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:02.858: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:02.863: INFO: Number of nodes with available pods: 3
Feb 25 10:59:02.863: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 25 10:59:02.887: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:02.887: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:02.887: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:02.899: INFO: Number of nodes with available pods: 2
Feb 25 10:59:02.899: INFO: Node ip-10-1-2-179.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:59:03.908: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:03.908: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:03.908: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:03.913: INFO: Number of nodes with available pods: 2
Feb 25 10:59:03.913: INFO: Node ip-10-1-2-179.eu-central-1.compute.internal is running more than one daemon pod
Feb 25 10:59:04.907: INFO: DaemonSet pods can't tolerate node ip-10-1-3-126.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:04.907: INFO: DaemonSet pods can't tolerate node ip-10-1-3-174.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:04.907: INFO: DaemonSet pods can't tolerate node ip-10-1-3-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 25 10:59:04.912: INFO: Number of nodes with available pods: 3
Feb 25 10:59:04.912: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2638, will wait for the garbage collector to delete the pods
Feb 25 10:59:04.985: INFO: Deleting DaemonSet.extensions daemon-set took: 9.473822ms
Feb 25 10:59:05.086: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.446971ms
Feb 25 10:59:07.694: INFO: Number of nodes with available pods: 0
Feb 25 10:59:07.694: INFO: Number of running nodes: 0, number of available pods: 0
Feb 25 10:59:07.699: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45663"},"items":null}

Feb 25 10:59:07.703: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45663"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:07.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2638" for this suite.

• [SLOW TEST:7.084 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":300,"skipped":5443,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:07.735: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 25 10:59:07.913: INFO: The status of Pod annotationupdate74cbf082-31dc-488e-b758-553fdaa5ac67 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:59:09.921: INFO: The status of Pod annotationupdate74cbf082-31dc-488e-b758-553fdaa5ac67 is Running (Ready = true)
Feb 25 10:59:10.455: INFO: Successfully updated pod "annotationupdate74cbf082-31dc-488e-b758-553fdaa5ac67"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:14.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6255" for this suite.

• [SLOW TEST:6.777 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5450,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:14.512: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 25 10:59:14.676: INFO: Waiting up to 5m0s for pod "pod-5f0df44b-b474-49a6-9a13-74072f2290cd" in namespace "emptydir-408" to be "Succeeded or Failed"
Feb 25 10:59:14.680: INFO: Pod "pod-5f0df44b-b474-49a6-9a13-74072f2290cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.547437ms
Feb 25 10:59:16.689: INFO: Pod "pod-5f0df44b-b474-49a6-9a13-74072f2290cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013761887s
STEP: Saw pod success
Feb 25 10:59:16.689: INFO: Pod "pod-5f0df44b-b474-49a6-9a13-74072f2290cd" satisfied condition "Succeeded or Failed"
Feb 25 10:59:16.694: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-5f0df44b-b474-49a6-9a13-74072f2290cd container test-container: <nil>
STEP: delete the pod
Feb 25 10:59:16.723: INFO: Waiting for pod pod-5f0df44b-b474-49a6-9a13-74072f2290cd to disappear
Feb 25 10:59:16.727: INFO: Pod pod-5f0df44b-b474-49a6-9a13-74072f2290cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-408" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5455,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 25 10:59:17.975: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0225 10:59:17.975402      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:17.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7937" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":303,"skipped":5464,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:17.990: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:59:18.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e81b97e-da41-43e4-a109-8d8b431efc03" in namespace "downward-api-5892" to be "Succeeded or Failed"
Feb 25 10:59:18.164: INFO: Pod "downwardapi-volume-5e81b97e-da41-43e4-a109-8d8b431efc03": Phase="Pending", Reason="", readiness=false. Elapsed: 7.121067ms
Feb 25 10:59:20.173: INFO: Pod "downwardapi-volume-5e81b97e-da41-43e4-a109-8d8b431efc03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0158474s
STEP: Saw pod success
Feb 25 10:59:20.173: INFO: Pod "downwardapi-volume-5e81b97e-da41-43e4-a109-8d8b431efc03" satisfied condition "Succeeded or Failed"
Feb 25 10:59:20.178: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-5e81b97e-da41-43e4-a109-8d8b431efc03 container client-container: <nil>
STEP: delete the pod
Feb 25 10:59:20.207: INFO: Waiting for pod downwardapi-volume-5e81b97e-da41-43e4-a109-8d8b431efc03 to disappear
Feb 25 10:59:20.212: INFO: Pod downwardapi-volume-5e81b97e-da41-43e4-a109-8d8b431efc03 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:20.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5892" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5476,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:20.227: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2136
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Feb 25 10:59:22.419: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2136 PodName:pod-sharedvolume-083a7c6b-f014-4376-8aff-df6a62de6217 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 10:59:22.419: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 10:59:22.507: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:22.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2136" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":305,"skipped":5490,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:22.523: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 25 10:59:22.699: INFO: The status of Pod annotationupdate439b75a7-6939-4ee4-a61c-decbc250e409 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:59:24.709: INFO: The status of Pod annotationupdate439b75a7-6939-4ee4-a61c-decbc250e409 is Running (Ready = true)
Feb 25 10:59:25.242: INFO: Successfully updated pod "annotationupdate439b75a7-6939-4ee4-a61c-decbc250e409"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:29.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4409" for this suite.

• [SLOW TEST:6.772 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5503,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:29.295: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5
Feb 25 10:59:29.460: INFO: Pod name my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5: Found 0 pods out of 1
Feb 25 10:59:34.474: INFO: Pod name my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5: Found 1 pods out of 1
Feb 25 10:59:34.474: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5" are running
Feb 25 10:59:34.479: INFO: Pod "my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5-jl6v8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 10:59:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 10:59:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 10:59:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-25 10:59:29 +0000 UTC Reason: Message:}])
Feb 25 10:59:34.479: INFO: Trying to dial the pod
Feb 25 10:59:39.499: INFO: Controller my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5: Got expected result from replica 1 [my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5-jl6v8]: "my-hostname-basic-6c688da3-02eb-4110-b9d6-84dde8075aa5-jl6v8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:39.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-789" for this suite.

• [SLOW TEST:10.220 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":307,"skipped":5510,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:39.515: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 10:59:39.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89ce9c9d-8c7f-49da-9826-f5a1bcf02d60" in namespace "downward-api-8829" to be "Succeeded or Failed"
Feb 25 10:59:39.694: INFO: Pod "downwardapi-volume-89ce9c9d-8c7f-49da-9826-f5a1bcf02d60": Phase="Pending", Reason="", readiness=false. Elapsed: 7.015335ms
Feb 25 10:59:41.702: INFO: Pod "downwardapi-volume-89ce9c9d-8c7f-49da-9826-f5a1bcf02d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015838885s
STEP: Saw pod success
Feb 25 10:59:41.702: INFO: Pod "downwardapi-volume-89ce9c9d-8c7f-49da-9826-f5a1bcf02d60" satisfied condition "Succeeded or Failed"
Feb 25 10:59:41.707: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-89ce9c9d-8c7f-49da-9826-f5a1bcf02d60 container client-container: <nil>
STEP: delete the pod
Feb 25 10:59:41.736: INFO: Waiting for pod downwardapi-volume-89ce9c9d-8c7f-49da-9826-f5a1bcf02d60 to disappear
Feb 25 10:59:41.741: INFO: Pod downwardapi-volume-89ce9c9d-8c7f-49da-9826-f5a1bcf02d60 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:41.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8829" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3476.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3476.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 25 10:59:43.948: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.953: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.958: INFO: Unable to read wheezy_udp@PodARecord from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.963: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.968: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.974: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.979: INFO: Unable to read jessie_udp@PodARecord from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.984: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa: the server could not find the requested resource (get pods dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa)
Feb 25 10:59:43.984: INFO: Lookups using dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb 25 10:59:49.027: INFO: DNS probes using dns-3476/dns-test-cf72b58b-2939-4579-a29c-3fe21a6b15aa succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:49.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3476" for this suite.

• [SLOW TEST:7.310 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":309,"skipped":5564,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:49.067: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 25 10:59:49.233: INFO: The status of Pod pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c is Pending, waiting for it to be Running (with Ready = true)
Feb 25 10:59:51.243: INFO: The status of Pod pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 25 10:59:51.770: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c"
Feb 25 10:59:51.770: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c" in namespace "pods-7102" to be "terminated due to deadline exceeded"
Feb 25 10:59:51.775: INFO: Pod "pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c": Phase="Running", Reason="", readiness=true. Elapsed: 4.430189ms
Feb 25 10:59:53.783: INFO: Pod "pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012969979s
Feb 25 10:59:55.792: INFO: Pod "pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.021732174s
Feb 25 10:59:55.792: INFO: Pod "pod-update-activedeadlineseconds-b36514ff-51da-4d36-a93e-c7bef64bb67c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:55.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7102" for this suite.

• [SLOW TEST:6.740 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5570,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:55.807: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 25 10:59:55.974: INFO: Waiting up to 5m0s for pod "pod-aab0b00b-f374-487c-a6e4-a9165b5c01d0" in namespace "emptydir-5099" to be "Succeeded or Failed"
Feb 25 10:59:55.979: INFO: Pod "pod-aab0b00b-f374-487c-a6e4-a9165b5c01d0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.497456ms
Feb 25 10:59:57.989: INFO: Pod "pod-aab0b00b-f374-487c-a6e4-a9165b5c01d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014718572s
STEP: Saw pod success
Feb 25 10:59:57.989: INFO: Pod "pod-aab0b00b-f374-487c-a6e4-a9165b5c01d0" satisfied condition "Succeeded or Failed"
Feb 25 10:59:57.993: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-aab0b00b-f374-487c-a6e4-a9165b5c01d0 container test-container: <nil>
STEP: delete the pod
Feb 25 10:59:58.022: INFO: Waiting for pod pod-aab0b00b-f374-487c-a6e4-a9165b5c01d0 to disappear
Feb 25 10:59:58.027: INFO: Pod pod-aab0b00b-f374-487c-a6e4-a9165b5c01d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 10:59:58.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5099" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5572,"failed":0}
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 10:59:58.043: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 10:59:58.195: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1070
I0225 10:59:58.207000      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1070, replica count: 1
I0225 10:59:59.258071      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0225 11:00:00.258649      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 25 11:00:00.374: INFO: Created: latency-svc-j7tm7
Feb 25 11:00:00.390: INFO: Got endpoints: latency-svc-j7tm7 [30.981109ms]
Feb 25 11:00:00.408: INFO: Created: latency-svc-llhsq
Feb 25 11:00:00.416: INFO: Got endpoints: latency-svc-llhsq [24.956713ms]
Feb 25 11:00:00.424: INFO: Created: latency-svc-pmhj9
Feb 25 11:00:00.428: INFO: Got endpoints: latency-svc-pmhj9 [36.265434ms]
Feb 25 11:00:00.433: INFO: Created: latency-svc-nszxl
Feb 25 11:00:00.441: INFO: Got endpoints: latency-svc-nszxl [48.906649ms]
Feb 25 11:00:00.445: INFO: Created: latency-svc-7zlwl
Feb 25 11:00:00.452: INFO: Got endpoints: latency-svc-7zlwl [60.086931ms]
Feb 25 11:00:00.461: INFO: Created: latency-svc-54vkx
Feb 25 11:00:00.465: INFO: Got endpoints: latency-svc-54vkx [72.682887ms]
Feb 25 11:00:00.470: INFO: Created: latency-svc-jp8z2
Feb 25 11:00:00.479: INFO: Got endpoints: latency-svc-jp8z2 [86.868649ms]
Feb 25 11:00:00.484: INFO: Created: latency-svc-r9bpn
Feb 25 11:00:00.491: INFO: Got endpoints: latency-svc-r9bpn [99.038061ms]
Feb 25 11:00:00.496: INFO: Created: latency-svc-7rhzc
Feb 25 11:00:00.504: INFO: Got endpoints: latency-svc-7rhzc [112.413613ms]
Feb 25 11:00:00.507: INFO: Created: latency-svc-6774f
Feb 25 11:00:00.514: INFO: Got endpoints: latency-svc-6774f [123.027583ms]
Feb 25 11:00:00.518: INFO: Created: latency-svc-4d7nc
Feb 25 11:00:00.526: INFO: Got endpoints: latency-svc-4d7nc [134.115631ms]
Feb 25 11:00:00.531: INFO: Created: latency-svc-n7md4
Feb 25 11:00:00.540: INFO: Got endpoints: latency-svc-n7md4 [147.800924ms]
Feb 25 11:00:00.542: INFO: Created: latency-svc-hppfc
Feb 25 11:00:00.549: INFO: Got endpoints: latency-svc-hppfc [156.823682ms]
Feb 25 11:00:00.558: INFO: Created: latency-svc-q897r
Feb 25 11:00:00.564: INFO: Got endpoints: latency-svc-q897r [172.038153ms]
Feb 25 11:00:00.567: INFO: Created: latency-svc-t8qll
Feb 25 11:00:00.577: INFO: Got endpoints: latency-svc-t8qll [185.087994ms]
Feb 25 11:00:00.584: INFO: Created: latency-svc-wzt8d
Feb 25 11:00:00.591: INFO: Got endpoints: latency-svc-wzt8d [198.851077ms]
Feb 25 11:00:00.596: INFO: Created: latency-svc-nq9km
Feb 25 11:00:00.604: INFO: Got endpoints: latency-svc-nq9km [188.000621ms]
Feb 25 11:00:00.608: INFO: Created: latency-svc-dcxkm
Feb 25 11:00:00.617: INFO: Got endpoints: latency-svc-dcxkm [188.867699ms]
Feb 25 11:00:00.622: INFO: Created: latency-svc-jn4bx
Feb 25 11:00:00.629: INFO: Got endpoints: latency-svc-jn4bx [187.621753ms]
Feb 25 11:00:00.630: INFO: Created: latency-svc-rprfv
Feb 25 11:00:00.641: INFO: Got endpoints: latency-svc-rprfv [188.736199ms]
Feb 25 11:00:00.644: INFO: Created: latency-svc-v92cd
Feb 25 11:00:00.656: INFO: Created: latency-svc-tgqck
Feb 25 11:00:00.660: INFO: Got endpoints: latency-svc-v92cd [194.995784ms]
Feb 25 11:00:00.663: INFO: Got endpoints: latency-svc-tgqck [183.925261ms]
Feb 25 11:00:00.672: INFO: Created: latency-svc-6dtdq
Feb 25 11:00:00.684: INFO: Got endpoints: latency-svc-6dtdq [192.826991ms]
Feb 25 11:00:00.685: INFO: Created: latency-svc-wf92l
Feb 25 11:00:00.691: INFO: Got endpoints: latency-svc-wf92l [186.985883ms]
Feb 25 11:00:00.696: INFO: Created: latency-svc-hjbqq
Feb 25 11:00:00.701: INFO: Got endpoints: latency-svc-hjbqq [187.253582ms]
Feb 25 11:00:00.705: INFO: Created: latency-svc-4rb55
Feb 25 11:00:00.712: INFO: Got endpoints: latency-svc-4rb55 [185.675689ms]
Feb 25 11:00:00.714: INFO: Created: latency-svc-49wr7
Feb 25 11:00:00.722: INFO: Got endpoints: latency-svc-49wr7 [181.044254ms]
Feb 25 11:00:00.726: INFO: Created: latency-svc-scpft
Feb 25 11:00:00.732: INFO: Got endpoints: latency-svc-scpft [182.793728ms]
Feb 25 11:00:00.737: INFO: Created: latency-svc-4fwf4
Feb 25 11:00:00.744: INFO: Got endpoints: latency-svc-4fwf4 [179.996069ms]
Feb 25 11:00:00.750: INFO: Created: latency-svc-n6bg9
Feb 25 11:00:00.756: INFO: Got endpoints: latency-svc-n6bg9 [178.728641ms]
Feb 25 11:00:00.764: INFO: Created: latency-svc-xjdgt
Feb 25 11:00:00.772: INFO: Got endpoints: latency-svc-xjdgt [180.858481ms]
Feb 25 11:00:00.774: INFO: Created: latency-svc-z2rxf
Feb 25 11:00:00.782: INFO: Got endpoints: latency-svc-z2rxf [177.527421ms]
Feb 25 11:00:00.785: INFO: Created: latency-svc-jwfjp
Feb 25 11:00:00.791: INFO: Got endpoints: latency-svc-jwfjp [173.788017ms]
Feb 25 11:00:00.793: INFO: Created: latency-svc-6fkp2
Feb 25 11:00:00.800: INFO: Got endpoints: latency-svc-6fkp2 [171.465769ms]
Feb 25 11:00:00.804: INFO: Created: latency-svc-phrwr
Feb 25 11:00:00.812: INFO: Got endpoints: latency-svc-phrwr [170.892599ms]
Feb 25 11:00:00.817: INFO: Created: latency-svc-c9xfv
Feb 25 11:00:00.828: INFO: Got endpoints: latency-svc-c9xfv [167.896619ms]
Feb 25 11:00:00.831: INFO: Created: latency-svc-6kj4d
Feb 25 11:00:00.838: INFO: Got endpoints: latency-svc-6kj4d [175.082878ms]
Feb 25 11:00:00.840: INFO: Created: latency-svc-j6mck
Feb 25 11:00:00.846: INFO: Got endpoints: latency-svc-j6mck [161.799331ms]
Feb 25 11:00:00.852: INFO: Created: latency-svc-pr2pq
Feb 25 11:00:00.858: INFO: Got endpoints: latency-svc-pr2pq [166.709997ms]
Feb 25 11:00:00.864: INFO: Created: latency-svc-fgvl2
Feb 25 11:00:00.873: INFO: Created: latency-svc-lbm2x
Feb 25 11:00:00.880: INFO: Got endpoints: latency-svc-fgvl2 [178.605181ms]
Feb 25 11:00:00.884: INFO: Created: latency-svc-qjmn8
Feb 25 11:00:00.892: INFO: Created: latency-svc-v47hh
Feb 25 11:00:00.899: INFO: Created: latency-svc-glhqc
Feb 25 11:00:00.907: INFO: Created: latency-svc-ztc9w
Feb 25 11:00:00.915: INFO: Created: latency-svc-9n299
Feb 25 11:00:00.932: INFO: Got endpoints: latency-svc-lbm2x [219.872697ms]
Feb 25 11:00:00.932: INFO: Created: latency-svc-827qd
Feb 25 11:00:00.941: INFO: Created: latency-svc-rrmrd
Feb 25 11:00:00.949: INFO: Created: latency-svc-bdj47
Feb 25 11:00:00.960: INFO: Created: latency-svc-hwxjp
Feb 25 11:00:00.974: INFO: Created: latency-svc-7764p
Feb 25 11:00:00.983: INFO: Got endpoints: latency-svc-qjmn8 [260.749491ms]
Feb 25 11:00:00.986: INFO: Created: latency-svc-nzpps
Feb 25 11:00:00.996: INFO: Created: latency-svc-hrgr4
Feb 25 11:00:01.007: INFO: Created: latency-svc-qtb4b
Feb 25 11:00:01.015: INFO: Created: latency-svc-tdqt4
Feb 25 11:00:01.023: INFO: Created: latency-svc-9ckls
Feb 25 11:00:01.033: INFO: Created: latency-svc-m4t5m
Feb 25 11:00:01.033: INFO: Got endpoints: latency-svc-v47hh [301.324147ms]
Feb 25 11:00:01.050: INFO: Created: latency-svc-bxlk9
Feb 25 11:00:01.083: INFO: Got endpoints: latency-svc-glhqc [338.645121ms]
Feb 25 11:00:01.100: INFO: Created: latency-svc-64n2m
Feb 25 11:00:01.131: INFO: Got endpoints: latency-svc-ztc9w [374.646501ms]
Feb 25 11:00:01.148: INFO: Created: latency-svc-vjml4
Feb 25 11:00:01.181: INFO: Got endpoints: latency-svc-9n299 [408.675688ms]
Feb 25 11:00:01.203: INFO: Created: latency-svc-5g9gs
Feb 25 11:00:01.231: INFO: Got endpoints: latency-svc-827qd [449.019949ms]
Feb 25 11:00:01.248: INFO: Created: latency-svc-rr4xn
Feb 25 11:00:01.286: INFO: Got endpoints: latency-svc-rrmrd [495.354859ms]
Feb 25 11:00:01.302: INFO: Created: latency-svc-2zfql
Feb 25 11:00:01.335: INFO: Got endpoints: latency-svc-bdj47 [534.423904ms]
Feb 25 11:00:01.349: INFO: Created: latency-svc-g8zqv
Feb 25 11:00:01.383: INFO: Got endpoints: latency-svc-hwxjp [570.30135ms]
Feb 25 11:00:01.400: INFO: Created: latency-svc-rccww
Feb 25 11:00:01.435: INFO: Got endpoints: latency-svc-7764p [606.337552ms]
Feb 25 11:00:01.450: INFO: Created: latency-svc-ftgjf
Feb 25 11:00:01.483: INFO: Got endpoints: latency-svc-nzpps [644.945775ms]
Feb 25 11:00:01.500: INFO: Created: latency-svc-kpg6m
Feb 25 11:00:01.532: INFO: Got endpoints: latency-svc-hrgr4 [686.350688ms]
Feb 25 11:00:01.550: INFO: Created: latency-svc-mqm2l
Feb 25 11:00:01.583: INFO: Got endpoints: latency-svc-qtb4b [724.899291ms]
Feb 25 11:00:01.601: INFO: Created: latency-svc-vh4qn
Feb 25 11:00:01.633: INFO: Got endpoints: latency-svc-tdqt4 [752.609344ms]
Feb 25 11:00:01.649: INFO: Created: latency-svc-v9r29
Feb 25 11:00:01.694: INFO: Got endpoints: latency-svc-9ckls [761.478614ms]
Feb 25 11:00:01.708: INFO: Created: latency-svc-67jwf
Feb 25 11:00:01.732: INFO: Got endpoints: latency-svc-m4t5m [749.168711ms]
Feb 25 11:00:01.756: INFO: Created: latency-svc-wldx7
Feb 25 11:00:01.780: INFO: Got endpoints: latency-svc-bxlk9 [746.890699ms]
Feb 25 11:00:01.795: INFO: Created: latency-svc-s7r7t
Feb 25 11:00:01.831: INFO: Got endpoints: latency-svc-64n2m [747.448798ms]
Feb 25 11:00:01.846: INFO: Created: latency-svc-vmwlx
Feb 25 11:00:01.882: INFO: Got endpoints: latency-svc-vjml4 [750.905636ms]
Feb 25 11:00:01.897: INFO: Created: latency-svc-hkbzk
Feb 25 11:00:01.934: INFO: Got endpoints: latency-svc-5g9gs [753.630541ms]
Feb 25 11:00:01.949: INFO: Created: latency-svc-n68cs
Feb 25 11:00:02.006: INFO: Got endpoints: latency-svc-rr4xn [774.393111ms]
Feb 25 11:00:02.021: INFO: Created: latency-svc-2krsm
Feb 25 11:00:02.045: INFO: Got endpoints: latency-svc-2zfql [758.805238ms]
Feb 25 11:00:02.059: INFO: Created: latency-svc-njsq2
Feb 25 11:00:02.085: INFO: Got endpoints: latency-svc-g8zqv [749.965202ms]
Feb 25 11:00:02.099: INFO: Created: latency-svc-hjn7w
Feb 25 11:00:02.137: INFO: Got endpoints: latency-svc-rccww [754.277238ms]
Feb 25 11:00:02.153: INFO: Created: latency-svc-fnrfl
Feb 25 11:00:02.181: INFO: Got endpoints: latency-svc-ftgjf [746.289537ms]
Feb 25 11:00:02.196: INFO: Created: latency-svc-s9vmz
Feb 25 11:00:02.237: INFO: Got endpoints: latency-svc-kpg6m [753.629307ms]
Feb 25 11:00:02.252: INFO: Created: latency-svc-96tp4
Feb 25 11:00:02.284: INFO: Got endpoints: latency-svc-mqm2l [751.451801ms]
Feb 25 11:00:02.298: INFO: Created: latency-svc-xdkkt
Feb 25 11:00:02.333: INFO: Got endpoints: latency-svc-vh4qn [749.931078ms]
Feb 25 11:00:02.347: INFO: Created: latency-svc-rmnqd
Feb 25 11:00:02.392: INFO: Got endpoints: latency-svc-v9r29 [758.746983ms]
Feb 25 11:00:02.408: INFO: Created: latency-svc-fdk4x
Feb 25 11:00:02.438: INFO: Got endpoints: latency-svc-67jwf [744.456656ms]
Feb 25 11:00:02.455: INFO: Created: latency-svc-k4jj2
Feb 25 11:00:02.482: INFO: Got endpoints: latency-svc-wldx7 [750.511168ms]
Feb 25 11:00:02.499: INFO: Created: latency-svc-4d9xd
Feb 25 11:00:02.533: INFO: Got endpoints: latency-svc-s7r7t [752.427293ms]
Feb 25 11:00:02.549: INFO: Created: latency-svc-hm7t9
Feb 25 11:00:02.588: INFO: Got endpoints: latency-svc-vmwlx [756.841892ms]
Feb 25 11:00:02.603: INFO: Created: latency-svc-xjs56
Feb 25 11:00:02.633: INFO: Got endpoints: latency-svc-hkbzk [751.631989ms]
Feb 25 11:00:02.648: INFO: Created: latency-svc-gjbdz
Feb 25 11:00:02.686: INFO: Got endpoints: latency-svc-n68cs [751.201073ms]
Feb 25 11:00:02.702: INFO: Created: latency-svc-hzw6n
Feb 25 11:00:02.736: INFO: Got endpoints: latency-svc-2krsm [730.381722ms]
Feb 25 11:00:02.752: INFO: Created: latency-svc-hcm7m
Feb 25 11:00:02.784: INFO: Got endpoints: latency-svc-njsq2 [739.053181ms]
Feb 25 11:00:02.799: INFO: Created: latency-svc-ml2j6
Feb 25 11:00:02.830: INFO: Got endpoints: latency-svc-hjn7w [744.805042ms]
Feb 25 11:00:02.845: INFO: Created: latency-svc-bt5qx
Feb 25 11:00:02.882: INFO: Got endpoints: latency-svc-fnrfl [745.453777ms]
Feb 25 11:00:02.897: INFO: Created: latency-svc-c2cq8
Feb 25 11:00:02.932: INFO: Got endpoints: latency-svc-s9vmz [751.332886ms]
Feb 25 11:00:02.947: INFO: Created: latency-svc-8qzfp
Feb 25 11:00:02.987: INFO: Got endpoints: latency-svc-96tp4 [750.87388ms]
Feb 25 11:00:03.005: INFO: Created: latency-svc-tfkkv
Feb 25 11:00:03.034: INFO: Got endpoints: latency-svc-xdkkt [749.92941ms]
Feb 25 11:00:03.050: INFO: Created: latency-svc-mrnhq
Feb 25 11:00:03.097: INFO: Got endpoints: latency-svc-rmnqd [763.651805ms]
Feb 25 11:00:03.114: INFO: Created: latency-svc-tbm7q
Feb 25 11:00:03.134: INFO: Got endpoints: latency-svc-fdk4x [742.782252ms]
Feb 25 11:00:03.150: INFO: Created: latency-svc-pl45j
Feb 25 11:00:03.184: INFO: Got endpoints: latency-svc-k4jj2 [745.878925ms]
Feb 25 11:00:03.199: INFO: Created: latency-svc-vz74s
Feb 25 11:00:03.233: INFO: Got endpoints: latency-svc-4d9xd [750.34797ms]
Feb 25 11:00:03.249: INFO: Created: latency-svc-l9qqt
Feb 25 11:00:03.282: INFO: Got endpoints: latency-svc-hm7t9 [748.944781ms]
Feb 25 11:00:03.296: INFO: Created: latency-svc-q4fp6
Feb 25 11:00:03.330: INFO: Got endpoints: latency-svc-xjs56 [742.842564ms]
Feb 25 11:00:03.347: INFO: Created: latency-svc-8ktr4
Feb 25 11:00:03.385: INFO: Got endpoints: latency-svc-gjbdz [751.053238ms]
Feb 25 11:00:03.401: INFO: Created: latency-svc-wsf4q
Feb 25 11:00:03.430: INFO: Got endpoints: latency-svc-hzw6n [743.713847ms]
Feb 25 11:00:03.447: INFO: Created: latency-svc-9t7nw
Feb 25 11:00:03.482: INFO: Got endpoints: latency-svc-hcm7m [745.543318ms]
Feb 25 11:00:03.499: INFO: Created: latency-svc-h26s6
Feb 25 11:00:03.531: INFO: Got endpoints: latency-svc-ml2j6 [747.055793ms]
Feb 25 11:00:03.547: INFO: Created: latency-svc-wbw6n
Feb 25 11:00:03.581: INFO: Got endpoints: latency-svc-bt5qx [751.774374ms]
Feb 25 11:00:03.596: INFO: Created: latency-svc-w2lp9
Feb 25 11:00:03.630: INFO: Got endpoints: latency-svc-c2cq8 [747.215825ms]
Feb 25 11:00:03.649: INFO: Created: latency-svc-wn95n
Feb 25 11:00:03.682: INFO: Got endpoints: latency-svc-8qzfp [748.985606ms]
Feb 25 11:00:03.698: INFO: Created: latency-svc-lktdw
Feb 25 11:00:03.731: INFO: Got endpoints: latency-svc-tfkkv [743.655242ms]
Feb 25 11:00:03.747: INFO: Created: latency-svc-q5rvm
Feb 25 11:00:03.783: INFO: Got endpoints: latency-svc-mrnhq [749.259612ms]
Feb 25 11:00:03.798: INFO: Created: latency-svc-tgxqx
Feb 25 11:00:03.832: INFO: Got endpoints: latency-svc-tbm7q [734.918947ms]
Feb 25 11:00:03.847: INFO: Created: latency-svc-zbzz2
Feb 25 11:00:03.882: INFO: Got endpoints: latency-svc-pl45j [747.809182ms]
Feb 25 11:00:03.897: INFO: Created: latency-svc-5tq9m
Feb 25 11:00:03.931: INFO: Got endpoints: latency-svc-vz74s [747.248444ms]
Feb 25 11:00:03.946: INFO: Created: latency-svc-w6pdc
Feb 25 11:00:03.983: INFO: Got endpoints: latency-svc-l9qqt [748.624654ms]
Feb 25 11:00:03.997: INFO: Created: latency-svc-5jghd
Feb 25 11:00:04.032: INFO: Got endpoints: latency-svc-q4fp6 [749.604268ms]
Feb 25 11:00:04.048: INFO: Created: latency-svc-7g4ss
Feb 25 11:00:04.084: INFO: Got endpoints: latency-svc-8ktr4 [753.004359ms]
Feb 25 11:00:04.099: INFO: Created: latency-svc-rlpb9
Feb 25 11:00:04.134: INFO: Got endpoints: latency-svc-wsf4q [749.335212ms]
Feb 25 11:00:04.149: INFO: Created: latency-svc-76pnd
Feb 25 11:00:04.183: INFO: Got endpoints: latency-svc-9t7nw [752.701546ms]
Feb 25 11:00:04.198: INFO: Created: latency-svc-lzfmp
Feb 25 11:00:04.233: INFO: Got endpoints: latency-svc-h26s6 [748.246108ms]
Feb 25 11:00:04.248: INFO: Created: latency-svc-b2r5l
Feb 25 11:00:04.284: INFO: Got endpoints: latency-svc-wbw6n [752.389053ms]
Feb 25 11:00:04.298: INFO: Created: latency-svc-s2wd9
Feb 25 11:00:04.333: INFO: Got endpoints: latency-svc-w2lp9 [751.697394ms]
Feb 25 11:00:04.350: INFO: Created: latency-svc-lb754
Feb 25 11:00:04.385: INFO: Got endpoints: latency-svc-wn95n [755.407126ms]
Feb 25 11:00:04.406: INFO: Created: latency-svc-c7vc5
Feb 25 11:00:04.434: INFO: Got endpoints: latency-svc-lktdw [752.673737ms]
Feb 25 11:00:04.451: INFO: Created: latency-svc-nqkwv
Feb 25 11:00:04.484: INFO: Got endpoints: latency-svc-q5rvm [752.574307ms]
Feb 25 11:00:04.498: INFO: Created: latency-svc-4jcbs
Feb 25 11:00:04.532: INFO: Got endpoints: latency-svc-tgxqx [748.647244ms]
Feb 25 11:00:04.550: INFO: Created: latency-svc-6tdn4
Feb 25 11:00:04.583: INFO: Got endpoints: latency-svc-zbzz2 [750.36897ms]
Feb 25 11:00:04.599: INFO: Created: latency-svc-kjdzs
Feb 25 11:00:04.634: INFO: Got endpoints: latency-svc-5tq9m [751.460733ms]
Feb 25 11:00:04.649: INFO: Created: latency-svc-dw9vx
Feb 25 11:00:04.682: INFO: Got endpoints: latency-svc-w6pdc [750.91764ms]
Feb 25 11:00:04.698: INFO: Created: latency-svc-5qr6d
Feb 25 11:00:04.732: INFO: Got endpoints: latency-svc-5jghd [749.462225ms]
Feb 25 11:00:04.748: INFO: Created: latency-svc-srz87
Feb 25 11:00:04.785: INFO: Got endpoints: latency-svc-7g4ss [753.04376ms]
Feb 25 11:00:04.801: INFO: Created: latency-svc-dnvsp
Feb 25 11:00:04.831: INFO: Got endpoints: latency-svc-rlpb9 [747.809992ms]
Feb 25 11:00:04.845: INFO: Created: latency-svc-856l4
Feb 25 11:00:04.882: INFO: Got endpoints: latency-svc-76pnd [748.23443ms]
Feb 25 11:00:04.896: INFO: Created: latency-svc-qrsbh
Feb 25 11:00:04.933: INFO: Got endpoints: latency-svc-lzfmp [749.518483ms]
Feb 25 11:00:04.947: INFO: Created: latency-svc-2pwkz
Feb 25 11:00:04.984: INFO: Got endpoints: latency-svc-b2r5l [750.696584ms]
Feb 25 11:00:04.998: INFO: Created: latency-svc-2vv72
Feb 25 11:00:05.038: INFO: Got endpoints: latency-svc-s2wd9 [753.955736ms]
Feb 25 11:00:05.053: INFO: Created: latency-svc-tgnf5
Feb 25 11:00:05.083: INFO: Got endpoints: latency-svc-lb754 [750.221751ms]
Feb 25 11:00:05.098: INFO: Created: latency-svc-bhch8
Feb 25 11:00:05.136: INFO: Got endpoints: latency-svc-c7vc5 [750.682598ms]
Feb 25 11:00:05.150: INFO: Created: latency-svc-wmrxf
Feb 25 11:00:05.181: INFO: Got endpoints: latency-svc-nqkwv [746.965292ms]
Feb 25 11:00:05.197: INFO: Created: latency-svc-qzn58
Feb 25 11:00:05.233: INFO: Got endpoints: latency-svc-4jcbs [748.844438ms]
Feb 25 11:00:05.250: INFO: Created: latency-svc-7jwrd
Feb 25 11:00:05.282: INFO: Got endpoints: latency-svc-6tdn4 [749.787222ms]
Feb 25 11:00:05.296: INFO: Created: latency-svc-6rcrd
Feb 25 11:00:05.336: INFO: Got endpoints: latency-svc-kjdzs [753.101789ms]
Feb 25 11:00:05.350: INFO: Created: latency-svc-x7wtr
Feb 25 11:00:05.387: INFO: Got endpoints: latency-svc-dw9vx [752.481185ms]
Feb 25 11:00:05.402: INFO: Created: latency-svc-hghs4
Feb 25 11:00:05.434: INFO: Got endpoints: latency-svc-5qr6d [751.855079ms]
Feb 25 11:00:05.454: INFO: Created: latency-svc-jk7m8
Feb 25 11:00:05.489: INFO: Got endpoints: latency-svc-srz87 [756.770523ms]
Feb 25 11:00:05.528: INFO: Created: latency-svc-njwrx
Feb 25 11:00:05.533: INFO: Got endpoints: latency-svc-dnvsp [748.492487ms]
Feb 25 11:00:05.547: INFO: Created: latency-svc-p4xc8
Feb 25 11:00:05.582: INFO: Got endpoints: latency-svc-856l4 [751.015287ms]
Feb 25 11:00:05.597: INFO: Created: latency-svc-jzlct
Feb 25 11:00:05.633: INFO: Got endpoints: latency-svc-qrsbh [750.98091ms]
Feb 25 11:00:05.650: INFO: Created: latency-svc-4n64d
Feb 25 11:00:05.688: INFO: Got endpoints: latency-svc-2pwkz [755.2838ms]
Feb 25 11:00:05.703: INFO: Created: latency-svc-8rdz4
Feb 25 11:00:05.733: INFO: Got endpoints: latency-svc-2vv72 [749.137122ms]
Feb 25 11:00:05.750: INFO: Created: latency-svc-ghs92
Feb 25 11:00:05.788: INFO: Got endpoints: latency-svc-tgnf5 [750.724313ms]
Feb 25 11:00:05.808: INFO: Created: latency-svc-mkcqr
Feb 25 11:00:05.832: INFO: Got endpoints: latency-svc-bhch8 [748.3547ms]
Feb 25 11:00:05.846: INFO: Created: latency-svc-m7nlp
Feb 25 11:00:05.885: INFO: Got endpoints: latency-svc-wmrxf [749.28204ms]
Feb 25 11:00:05.901: INFO: Created: latency-svc-4rrnc
Feb 25 11:00:05.931: INFO: Got endpoints: latency-svc-qzn58 [750.208282ms]
Feb 25 11:00:05.946: INFO: Created: latency-svc-lsb87
Feb 25 11:00:05.988: INFO: Got endpoints: latency-svc-7jwrd [755.123943ms]
Feb 25 11:00:06.003: INFO: Created: latency-svc-2jvc2
Feb 25 11:00:06.032: INFO: Got endpoints: latency-svc-6rcrd [750.395011ms]
Feb 25 11:00:06.047: INFO: Created: latency-svc-qnwjq
Feb 25 11:00:06.082: INFO: Got endpoints: latency-svc-x7wtr [745.506301ms]
Feb 25 11:00:06.097: INFO: Created: latency-svc-qtq2n
Feb 25 11:00:06.132: INFO: Got endpoints: latency-svc-hghs4 [745.783856ms]
Feb 25 11:00:06.146: INFO: Created: latency-svc-r9tsm
Feb 25 11:00:06.183: INFO: Got endpoints: latency-svc-jk7m8 [746.959913ms]
Feb 25 11:00:06.198: INFO: Created: latency-svc-wn66h
Feb 25 11:00:06.237: INFO: Got endpoints: latency-svc-njwrx [748.022722ms]
Feb 25 11:00:06.252: INFO: Created: latency-svc-2jnx4
Feb 25 11:00:06.296: INFO: Got endpoints: latency-svc-p4xc8 [762.987525ms]
Feb 25 11:00:06.311: INFO: Created: latency-svc-ntz8h
Feb 25 11:00:06.334: INFO: Got endpoints: latency-svc-jzlct [751.698468ms]
Feb 25 11:00:06.348: INFO: Created: latency-svc-lvrnf
Feb 25 11:00:06.389: INFO: Got endpoints: latency-svc-4n64d [755.593639ms]
Feb 25 11:00:06.407: INFO: Created: latency-svc-5srfn
Feb 25 11:00:06.430: INFO: Got endpoints: latency-svc-8rdz4 [741.957785ms]
Feb 25 11:00:06.447: INFO: Created: latency-svc-f2shg
Feb 25 11:00:06.491: INFO: Got endpoints: latency-svc-ghs92 [757.312869ms]
Feb 25 11:00:06.506: INFO: Created: latency-svc-zpnld
Feb 25 11:00:06.535: INFO: Got endpoints: latency-svc-mkcqr [746.171336ms]
Feb 25 11:00:06.550: INFO: Created: latency-svc-jd2j4
Feb 25 11:00:06.583: INFO: Got endpoints: latency-svc-m7nlp [751.166862ms]
Feb 25 11:00:06.605: INFO: Created: latency-svc-l4vw8
Feb 25 11:00:06.631: INFO: Got endpoints: latency-svc-4rrnc [745.390804ms]
Feb 25 11:00:06.647: INFO: Created: latency-svc-ckw9r
Feb 25 11:00:06.682: INFO: Got endpoints: latency-svc-lsb87 [750.226244ms]
Feb 25 11:00:06.699: INFO: Created: latency-svc-sndww
Feb 25 11:00:06.732: INFO: Got endpoints: latency-svc-2jvc2 [743.854175ms]
Feb 25 11:00:06.747: INFO: Created: latency-svc-8rhwn
Feb 25 11:00:06.783: INFO: Got endpoints: latency-svc-qnwjq [750.469513ms]
Feb 25 11:00:06.798: INFO: Created: latency-svc-dvtrs
Feb 25 11:00:06.833: INFO: Got endpoints: latency-svc-qtq2n [751.709578ms]
Feb 25 11:00:06.848: INFO: Created: latency-svc-22bff
Feb 25 11:00:06.884: INFO: Got endpoints: latency-svc-r9tsm [751.55473ms]
Feb 25 11:00:06.899: INFO: Created: latency-svc-jkznm
Feb 25 11:00:06.935: INFO: Got endpoints: latency-svc-wn66h [751.653851ms]
Feb 25 11:00:06.950: INFO: Created: latency-svc-tq4vh
Feb 25 11:00:06.984: INFO: Got endpoints: latency-svc-2jnx4 [746.579355ms]
Feb 25 11:00:06.998: INFO: Created: latency-svc-pnn55
Feb 25 11:00:07.031: INFO: Got endpoints: latency-svc-ntz8h [734.809523ms]
Feb 25 11:00:07.048: INFO: Created: latency-svc-zqflj
Feb 25 11:00:07.081: INFO: Got endpoints: latency-svc-lvrnf [747.006173ms]
Feb 25 11:00:07.095: INFO: Created: latency-svc-8kmkp
Feb 25 11:00:07.132: INFO: Got endpoints: latency-svc-5srfn [743.3483ms]
Feb 25 11:00:07.150: INFO: Created: latency-svc-r2zph
Feb 25 11:00:07.184: INFO: Got endpoints: latency-svc-f2shg [753.647529ms]
Feb 25 11:00:07.198: INFO: Created: latency-svc-srpcq
Feb 25 11:00:07.235: INFO: Got endpoints: latency-svc-zpnld [743.831848ms]
Feb 25 11:00:07.249: INFO: Created: latency-svc-65ztn
Feb 25 11:00:07.282: INFO: Got endpoints: latency-svc-jd2j4 [747.405439ms]
Feb 25 11:00:07.297: INFO: Created: latency-svc-rbckc
Feb 25 11:00:07.331: INFO: Got endpoints: latency-svc-l4vw8 [748.288339ms]
Feb 25 11:00:07.347: INFO: Created: latency-svc-x2whq
Feb 25 11:00:07.380: INFO: Got endpoints: latency-svc-ckw9r [749.417538ms]
Feb 25 11:00:07.396: INFO: Created: latency-svc-nzvr9
Feb 25 11:00:07.432: INFO: Got endpoints: latency-svc-sndww [749.922935ms]
Feb 25 11:00:07.449: INFO: Created: latency-svc-7s9h2
Feb 25 11:00:07.482: INFO: Got endpoints: latency-svc-8rhwn [750.349179ms]
Feb 25 11:00:07.497: INFO: Created: latency-svc-drpr9
Feb 25 11:00:07.532: INFO: Got endpoints: latency-svc-dvtrs [748.805927ms]
Feb 25 11:00:07.547: INFO: Created: latency-svc-pldgm
Feb 25 11:00:07.582: INFO: Got endpoints: latency-svc-22bff [748.163306ms]
Feb 25 11:00:07.596: INFO: Created: latency-svc-ztxxh
Feb 25 11:00:07.639: INFO: Got endpoints: latency-svc-jkznm [754.887595ms]
Feb 25 11:00:07.655: INFO: Created: latency-svc-4ww5t
Feb 25 11:00:07.683: INFO: Got endpoints: latency-svc-tq4vh [747.671956ms]
Feb 25 11:00:07.697: INFO: Created: latency-svc-xrpsd
Feb 25 11:00:07.730: INFO: Got endpoints: latency-svc-pnn55 [746.760386ms]
Feb 25 11:00:07.746: INFO: Created: latency-svc-gnx5m
Feb 25 11:00:07.781: INFO: Got endpoints: latency-svc-zqflj [750.115825ms]
Feb 25 11:00:07.798: INFO: Created: latency-svc-4gpj8
Feb 25 11:00:07.833: INFO: Got endpoints: latency-svc-8kmkp [751.552453ms]
Feb 25 11:00:07.847: INFO: Created: latency-svc-j9n2d
Feb 25 11:00:07.883: INFO: Got endpoints: latency-svc-r2zph [750.874297ms]
Feb 25 11:00:07.898: INFO: Created: latency-svc-2klp9
Feb 25 11:00:07.934: INFO: Got endpoints: latency-svc-srpcq [750.147443ms]
Feb 25 11:00:07.951: INFO: Created: latency-svc-dkxmk
Feb 25 11:00:07.983: INFO: Got endpoints: latency-svc-65ztn [748.531303ms]
Feb 25 11:00:07.998: INFO: Created: latency-svc-djv8s
Feb 25 11:00:08.032: INFO: Got endpoints: latency-svc-rbckc [749.613479ms]
Feb 25 11:00:08.047: INFO: Created: latency-svc-tdcbs
Feb 25 11:00:08.081: INFO: Got endpoints: latency-svc-x2whq [749.768162ms]
Feb 25 11:00:08.097: INFO: Created: latency-svc-6cs5j
Feb 25 11:00:08.132: INFO: Got endpoints: latency-svc-nzvr9 [751.535714ms]
Feb 25 11:00:08.147: INFO: Created: latency-svc-922z8
Feb 25 11:00:08.181: INFO: Got endpoints: latency-svc-7s9h2 [748.79316ms]
Feb 25 11:00:08.195: INFO: Created: latency-svc-bxnpj
Feb 25 11:00:08.231: INFO: Got endpoints: latency-svc-drpr9 [748.812835ms]
Feb 25 11:00:08.283: INFO: Got endpoints: latency-svc-pldgm [750.867897ms]
Feb 25 11:00:08.336: INFO: Got endpoints: latency-svc-ztxxh [754.766668ms]
Feb 25 11:00:08.383: INFO: Got endpoints: latency-svc-4ww5t [744.465179ms]
Feb 25 11:00:08.434: INFO: Got endpoints: latency-svc-xrpsd [751.144725ms]
Feb 25 11:00:08.483: INFO: Got endpoints: latency-svc-gnx5m [753.093072ms]
Feb 25 11:00:08.531: INFO: Got endpoints: latency-svc-4gpj8 [749.953272ms]
Feb 25 11:00:08.587: INFO: Got endpoints: latency-svc-j9n2d [753.853028ms]
Feb 25 11:00:08.633: INFO: Got endpoints: latency-svc-2klp9 [749.732256ms]
Feb 25 11:00:08.681: INFO: Got endpoints: latency-svc-dkxmk [744.829035ms]
Feb 25 11:00:08.732: INFO: Got endpoints: latency-svc-djv8s [748.334747ms]
Feb 25 11:00:08.781: INFO: Got endpoints: latency-svc-tdcbs [749.291364ms]
Feb 25 11:00:08.833: INFO: Got endpoints: latency-svc-6cs5j [751.473154ms]
Feb 25 11:00:08.882: INFO: Got endpoints: latency-svc-922z8 [750.427048ms]
Feb 25 11:00:08.931: INFO: Got endpoints: latency-svc-bxnpj [750.320227ms]
Feb 25 11:00:08.931: INFO: Latencies: [24.956713ms 36.265434ms 48.906649ms 60.086931ms 72.682887ms 86.868649ms 99.038061ms 112.413613ms 123.027583ms 134.115631ms 147.800924ms 156.823682ms 161.799331ms 166.709997ms 167.896619ms 170.892599ms 171.465769ms 172.038153ms 173.788017ms 175.082878ms 177.527421ms 178.605181ms 178.728641ms 179.996069ms 180.858481ms 181.044254ms 182.793728ms 183.925261ms 185.087994ms 185.675689ms 186.985883ms 187.253582ms 187.621753ms 188.000621ms 188.736199ms 188.867699ms 192.826991ms 194.995784ms 198.851077ms 219.872697ms 260.749491ms 301.324147ms 338.645121ms 374.646501ms 408.675688ms 449.019949ms 495.354859ms 534.423904ms 570.30135ms 606.337552ms 644.945775ms 686.350688ms 724.899291ms 730.381722ms 734.809523ms 734.918947ms 739.053181ms 741.957785ms 742.782252ms 742.842564ms 743.3483ms 743.655242ms 743.713847ms 743.831848ms 743.854175ms 744.456656ms 744.465179ms 744.805042ms 744.829035ms 745.390804ms 745.453777ms 745.506301ms 745.543318ms 745.783856ms 745.878925ms 746.171336ms 746.289537ms 746.579355ms 746.760386ms 746.890699ms 746.959913ms 746.965292ms 747.006173ms 747.055793ms 747.215825ms 747.248444ms 747.405439ms 747.448798ms 747.671956ms 747.809182ms 747.809992ms 748.022722ms 748.163306ms 748.23443ms 748.246108ms 748.288339ms 748.334747ms 748.3547ms 748.492487ms 748.531303ms 748.624654ms 748.647244ms 748.79316ms 748.805927ms 748.812835ms 748.844438ms 748.944781ms 748.985606ms 749.137122ms 749.168711ms 749.259612ms 749.28204ms 749.291364ms 749.335212ms 749.417538ms 749.462225ms 749.518483ms 749.604268ms 749.613479ms 749.732256ms 749.768162ms 749.787222ms 749.922935ms 749.92941ms 749.931078ms 749.953272ms 749.965202ms 750.115825ms 750.147443ms 750.208282ms 750.221751ms 750.226244ms 750.320227ms 750.34797ms 750.349179ms 750.36897ms 750.395011ms 750.427048ms 750.469513ms 750.511168ms 750.682598ms 750.696584ms 750.724313ms 750.867897ms 750.87388ms 750.874297ms 750.905636ms 750.91764ms 750.98091ms 751.015287ms 751.053238ms 751.144725ms 751.166862ms 751.201073ms 751.332886ms 751.451801ms 751.460733ms 751.473154ms 751.535714ms 751.552453ms 751.55473ms 751.631989ms 751.653851ms 751.697394ms 751.698468ms 751.709578ms 751.774374ms 751.855079ms 752.389053ms 752.427293ms 752.481185ms 752.574307ms 752.609344ms 752.673737ms 752.701546ms 753.004359ms 753.04376ms 753.093072ms 753.101789ms 753.629307ms 753.630541ms 753.647529ms 753.853028ms 753.955736ms 754.277238ms 754.766668ms 754.887595ms 755.123943ms 755.2838ms 755.407126ms 755.593639ms 756.770523ms 756.841892ms 757.312869ms 758.746983ms 758.805238ms 761.478614ms 762.987525ms 763.651805ms 774.393111ms]
Feb 25 11:00:08.931: INFO: 50 %ile: 748.624654ms
Feb 25 11:00:08.931: INFO: 90 %ile: 753.630541ms
Feb 25 11:00:08.931: INFO: 99 %ile: 763.651805ms
Feb 25 11:00:08.931: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:08.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1070" for this suite.

• [SLOW TEST:10.905 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":312,"skipped":5577,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:08.949: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7238
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 11:00:09.099: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:15.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7238" for this suite.

• [SLOW TEST:6.211 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":313,"skipped":5582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:15.162: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 11:00:15.325: INFO: Got root ca configmap in namespace "svcaccounts-3753"
Feb 25 11:00:15.336: INFO: Deleted root ca configmap in namespace "svcaccounts-3753"
STEP: waiting for a new root ca configmap created
Feb 25 11:00:15.842: INFO: Recreated root ca configmap in namespace "svcaccounts-3753"
Feb 25 11:00:15.851: INFO: Updated root ca configmap in namespace "svcaccounts-3753"
STEP: waiting for the root ca configmap reconciled
Feb 25 11:00:16.362: INFO: Reconciled root ca configmap in namespace "svcaccounts-3753"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:16.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3753" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":314,"skipped":5615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:16.388: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-5d0e4745-20ee-4f93-b4d2-4282f5ad3fba
STEP: Creating a pod to test consume configMaps
Feb 25 11:00:16.589: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3273e21e-cf97-48f7-a523-29a3e246bae8" in namespace "projected-7970" to be "Succeeded or Failed"
Feb 25 11:00:16.599: INFO: Pod "pod-projected-configmaps-3273e21e-cf97-48f7-a523-29a3e246bae8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.779662ms
Feb 25 11:00:18.611: INFO: Pod "pod-projected-configmaps-3273e21e-cf97-48f7-a523-29a3e246bae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021608853s
STEP: Saw pod success
Feb 25 11:00:18.611: INFO: Pod "pod-projected-configmaps-3273e21e-cf97-48f7-a523-29a3e246bae8" satisfied condition "Succeeded or Failed"
Feb 25 11:00:18.617: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-projected-configmaps-3273e21e-cf97-48f7-a523-29a3e246bae8 container agnhost-container: <nil>
STEP: delete the pod
Feb 25 11:00:18.648: INFO: Waiting for pod pod-projected-configmaps-3273e21e-cf97-48f7-a523-29a3e246bae8 to disappear
Feb 25 11:00:18.654: INFO: Pod pod-projected-configmaps-3273e21e-cf97-48f7-a523-29a3e246bae8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:18.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7970" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5637,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:18.670: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 11:00:18.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d7b3f62-dd6a-43e6-9311-9d0006bc9657" in namespace "downward-api-6562" to be "Succeeded or Failed"
Feb 25 11:00:18.853: INFO: Pod "downwardapi-volume-9d7b3f62-dd6a-43e6-9311-9d0006bc9657": Phase="Pending", Reason="", readiness=false. Elapsed: 6.632515ms
Feb 25 11:00:20.863: INFO: Pod "downwardapi-volume-9d7b3f62-dd6a-43e6-9311-9d0006bc9657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016689958s
STEP: Saw pod success
Feb 25 11:00:20.863: INFO: Pod "downwardapi-volume-9d7b3f62-dd6a-43e6-9311-9d0006bc9657" satisfied condition "Succeeded or Failed"
Feb 25 11:00:20.868: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-9d7b3f62-dd6a-43e6-9311-9d0006bc9657 container client-container: <nil>
STEP: delete the pod
Feb 25 11:00:20.894: INFO: Waiting for pod downwardapi-volume-9d7b3f62-dd6a-43e6-9311-9d0006bc9657 to disappear
Feb 25 11:00:20.898: INFO: Pod downwardapi-volume-9d7b3f62-dd6a-43e6-9311-9d0006bc9657 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:20.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6562" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":5648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:20.914: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3962
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 25 11:00:27.645: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0225 11:00:27.645948      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:27.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3962" for this suite.

• [SLOW TEST:6.748 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":317,"skipped":5674,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:27.662: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 25 11:00:27.826: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2902  fb081e26-9841-4a0b-8368-73038abaedfe 48294 0 2022-02-25 11:00:27 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:aws-cni] [] []  [{e2e.test Update v1 2022-02-25 11:00:27 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwkrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwkrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 25 11:00:27.835: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:00:29.842: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 25 11:00:29.842: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2902 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 11:00:29.842: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Verifying customized DNS server is configured on pod...
Feb 25 11:00:29.921: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2902 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 25 11:00:29.921: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 11:00:30.015: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:30.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2902" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":318,"skipped":5686,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3013
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-c6cae29e-31c6-4d8e-9504-c4cce6bc82a2
STEP: Creating secret with name s-test-opt-upd-4040362f-23f0-491b-b138-91f08d5615eb
STEP: Creating the pod
Feb 25 11:00:30.258: INFO: The status of Pod pod-projected-secrets-14600a9d-73bf-45cb-8eba-be1a51cb0d11 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:00:32.268: INFO: The status of Pod pod-projected-secrets-14600a9d-73bf-45cb-8eba-be1a51cb0d11 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-c6cae29e-31c6-4d8e-9504-c4cce6bc82a2
STEP: Updating secret s-test-opt-upd-4040362f-23f0-491b-b138-91f08d5615eb
STEP: Creating secret with name s-test-opt-create-55b4c60a-7af4-4382-9c34-85df43bcf443
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:34.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3013" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":5701,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:34.401: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0225 11:00:44.616356      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 25 11:00:44.616: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:44.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3243" for this suite.

• [SLOW TEST:10.232 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":320,"skipped":5718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:44.633: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 25 11:00:44.796: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:00:48.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8380" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":5743,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:00:48.768: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 11:00:49.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 11:00:51.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383649, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383649, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383649, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383649, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 11:00:54.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:06.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6320" for this suite.
STEP: Destroying namespace "webhook-6320-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.915 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":322,"skipped":5745,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:06.685: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-6595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Feb 25 11:01:06.845: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Feb 25 11:01:06.854: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 25 11:01:06.854: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Feb 25 11:01:06.868: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 25 11:01:06.868: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Feb 25 11:01:06.885: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 25 11:01:06.886: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Feb 25 11:01:13.947: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:13.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6595" for this suite.

• [SLOW TEST:7.293 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":323,"skipped":5758,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:13.979: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 25 11:01:15.164: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:15.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6024" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":324,"skipped":5767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 11:01:15.389: INFO: The status of Pod pod-secrets-1ddc3072-49a2-418b-96cc-cf7ab75e8788 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:01:17.399: INFO: The status of Pod pod-secrets-1ddc3072-49a2-418b-96cc-cf7ab75e8788 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:17.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4806" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":325,"skipped":5797,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:17.462: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:17.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-460" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":326,"skipped":5803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:17.681: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Feb 25 11:01:17.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9338 create -f -'
Feb 25 11:01:18.763: INFO: stderr: ""
Feb 25 11:01:18.763: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Feb 25 11:01:18.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9338 diff -f -'
Feb 25 11:01:19.810: INFO: rc: 1
Feb 25 11:01:19.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-932286732 --namespace=kubectl-9338 delete -f -'
Feb 25 11:01:19.862: INFO: stderr: ""
Feb 25 11:01:19.862: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:19.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9338" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":327,"skipped":5831,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:19.880: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:20.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7511" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":328,"skipped":5839,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:20.055: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 25 11:01:20.234: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:01:22.242: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 25 11:01:22.264: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:01:24.273: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Feb 25 11:01:24.290: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 25 11:01:24.294: INFO: Pod pod-with-prestop-http-hook still exists
Feb 25 11:01:26.295: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 25 11:01:26.305: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9985" for this suite.

• [SLOW TEST:6.284 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":5840,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:26.339: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Feb 25 11:01:26.513: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:01:28.521: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 25 11:01:28.540: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:01:30.547: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Feb 25 11:01:30.563: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 25 11:01:30.567: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 25 11:01:32.568: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 25 11:01:32.576: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 25 11:01:34.568: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 25 11:01:34.577: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:34.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4249" for this suite.

• [SLOW TEST:8.265 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":5860,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:34.604: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 25 11:01:34.771: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a3cd179-6047-45f7-affa-63313acaae83" in namespace "projected-6176" to be "Succeeded or Failed"
Feb 25 11:01:34.777: INFO: Pod "downwardapi-volume-3a3cd179-6047-45f7-affa-63313acaae83": Phase="Pending", Reason="", readiness=false. Elapsed: 6.45505ms
Feb 25 11:01:36.786: INFO: Pod "downwardapi-volume-3a3cd179-6047-45f7-affa-63313acaae83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014740338s
STEP: Saw pod success
Feb 25 11:01:36.786: INFO: Pod "downwardapi-volume-3a3cd179-6047-45f7-affa-63313acaae83" satisfied condition "Succeeded or Failed"
Feb 25 11:01:36.791: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downwardapi-volume-3a3cd179-6047-45f7-affa-63313acaae83 container client-container: <nil>
STEP: delete the pod
Feb 25 11:01:36.820: INFO: Waiting for pod downwardapi-volume-3a3cd179-6047-45f7-affa-63313acaae83 to disappear
Feb 25 11:01:36.824: INFO: Pod downwardapi-volume-3a3cd179-6047-45f7-affa-63313acaae83 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:36.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6176" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":5862,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:36.838: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-9cf59b56-e140-4acd-afe7-9e21da44b140
STEP: Creating a pod to test consume secrets
Feb 25 11:01:37.011: INFO: Waiting up to 5m0s for pod "pod-secrets-9a4a3b6f-9374-409e-bcdc-fae6a0fe7a13" in namespace "secrets-1314" to be "Succeeded or Failed"
Feb 25 11:01:37.018: INFO: Pod "pod-secrets-9a4a3b6f-9374-409e-bcdc-fae6a0fe7a13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.401566ms
Feb 25 11:01:39.027: INFO: Pod "pod-secrets-9a4a3b6f-9374-409e-bcdc-fae6a0fe7a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015874416s
STEP: Saw pod success
Feb 25 11:01:39.027: INFO: Pod "pod-secrets-9a4a3b6f-9374-409e-bcdc-fae6a0fe7a13" satisfied condition "Succeeded or Failed"
Feb 25 11:01:39.034: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-secrets-9a4a3b6f-9374-409e-bcdc-fae6a0fe7a13 container secret-volume-test: <nil>
STEP: delete the pod
Feb 25 11:01:39.064: INFO: Waiting for pod pod-secrets-9a4a3b6f-9374-409e-bcdc-fae6a0fe7a13 to disappear
Feb 25 11:01:39.069: INFO: Pod pod-secrets-9a4a3b6f-9374-409e-bcdc-fae6a0fe7a13 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:39.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1314" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":5883,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:39.091: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:55.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9102" for this suite.

• [SLOW TEST:16.342 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":333,"skipped":5891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:55.434: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 25 11:01:55.613: INFO: The status of Pod busybox-scheduling-a80f8f1f-3cc8-4783-a2a9-96e1854503a8 is Pending, waiting for it to be Running (with Ready = true)
Feb 25 11:01:57.622: INFO: The status of Pod busybox-scheduling-a80f8f1f-3cc8-4783-a2a9-96e1854503a8 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:01:57.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6737" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":5913,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:01:57.652: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Feb 25 11:01:57.824: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 25 11:02:02.837: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Feb 25 11:02:02.843: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Feb 25 11:02:02.857: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Feb 25 11:02:02.859: INFO: Observed &ReplicaSet event: ADDED
Feb 25 11:02:02.859: INFO: Observed &ReplicaSet event: MODIFIED
Feb 25 11:02:02.860: INFO: Observed &ReplicaSet event: MODIFIED
Feb 25 11:02:02.860: INFO: Observed &ReplicaSet event: MODIFIED
Feb 25 11:02:02.860: INFO: Found replicaset test-rs in namespace replicaset-9788 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 25 11:02:02.861: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Feb 25 11:02:02.861: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 25 11:02:02.873: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Feb 25 11:02:02.875: INFO: Observed &ReplicaSet event: ADDED
Feb 25 11:02:02.876: INFO: Observed &ReplicaSet event: MODIFIED
Feb 25 11:02:02.876: INFO: Observed &ReplicaSet event: MODIFIED
Feb 25 11:02:02.876: INFO: Observed &ReplicaSet event: MODIFIED
Feb 25 11:02:02.876: INFO: Observed replicaset test-rs in namespace replicaset-9788 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 25 11:02:02.876: INFO: Observed &ReplicaSet event: MODIFIED
Feb 25 11:02:02.876: INFO: Found replicaset test-rs in namespace replicaset-9788 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 25 11:02:02.876: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:02:02.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9788" for this suite.

• [SLOW TEST:5.240 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":335,"skipped":5928,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:02:02.891: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6088
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 11:02:03.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 11:02:05.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383723, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383723, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383723, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383723, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 11:02:08.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:02:08.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6088" for this suite.
STEP: Destroying namespace "webhook-6088-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.798 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":336,"skipped":5932,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:02:08.690: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 11:02:09.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 11:02:12.377: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
Feb 25 11:02:12.409: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:02:22.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5336" for this suite.
STEP: Destroying namespace "webhook-5336-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.026 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":337,"skipped":5935,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:02:22.718: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:22.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8394" for this suite.

• [SLOW TEST:60.205 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":5943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:22.924: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 25 11:03:23.096: INFO: Waiting up to 5m0s for pod "pod-0b79d858-7efb-4d8e-b220-baad7b659057" in namespace "emptydir-9780" to be "Succeeded or Failed"
Feb 25 11:03:23.105: INFO: Pod "pod-0b79d858-7efb-4d8e-b220-baad7b659057": Phase="Pending", Reason="", readiness=false. Elapsed: 9.355014ms
Feb 25 11:03:25.114: INFO: Pod "pod-0b79d858-7efb-4d8e-b220-baad7b659057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01856901s
STEP: Saw pod success
Feb 25 11:03:25.114: INFO: Pod "pod-0b79d858-7efb-4d8e-b220-baad7b659057" satisfied condition "Succeeded or Failed"
Feb 25 11:03:25.119: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-0b79d858-7efb-4d8e-b220-baad7b659057 container test-container: <nil>
STEP: delete the pod
Feb 25 11:03:25.148: INFO: Waiting for pod pod-0b79d858-7efb-4d8e-b220-baad7b659057 to disappear
Feb 25 11:03:25.153: INFO: Pod pod-0b79d858-7efb-4d8e-b220-baad7b659057 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:25.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9780" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":339,"skipped":5990,"failed":0}
S
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:25.167: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:27.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-867" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":5991,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:27.374: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 25 11:03:28.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 25 11:03:30.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383808, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383808, loc:(*time.Location)(0xa0aae00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383808, loc:(*time.Location)(0xa0aae00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781383808, loc:(*time.Location)(0xa0aae00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 25 11:03:33.199: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 25 11:03:33.225: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:33.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3377" for this suite.
STEP: Destroying namespace "webhook-3377-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.967 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":341,"skipped":5995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:33.341: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-d17fdf9e-bb1c-4041-99c7-1f60cce43fb0
STEP: Creating a pod to test consume configMaps
Feb 25 11:03:33.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f14b65f-316a-4419-bcf0-b58a9c96908d" in namespace "configmap-6946" to be "Succeeded or Failed"
Feb 25 11:03:33.548: INFO: Pod "pod-configmaps-8f14b65f-316a-4419-bcf0-b58a9c96908d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.835776ms
Feb 25 11:03:35.558: INFO: Pod "pod-configmaps-8f14b65f-316a-4419-bcf0-b58a9c96908d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018336226s
STEP: Saw pod success
Feb 25 11:03:35.558: INFO: Pod "pod-configmaps-8f14b65f-316a-4419-bcf0-b58a9c96908d" satisfied condition "Succeeded or Failed"
Feb 25 11:03:35.563: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod pod-configmaps-8f14b65f-316a-4419-bcf0-b58a9c96908d container agnhost-container: <nil>
STEP: delete the pod
Feb 25 11:03:35.593: INFO: Waiting for pod pod-configmaps-8f14b65f-316a-4419-bcf0-b58a9c96908d to disappear
Feb 25 11:03:35.597: INFO: Pod pod-configmaps-8f14b65f-316a-4419-bcf0-b58a9c96908d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:35.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6946" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":6040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:35.612: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Feb 25 11:03:35.775: INFO: created test-event-1
Feb 25 11:03:35.782: INFO: created test-event-2
Feb 25 11:03:35.788: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Feb 25 11:03:35.792: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Feb 25 11:03:35.822: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:35.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5489" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":343,"skipped":6067,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:35.840: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:47.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8009" for this suite.

• [SLOW TEST:11.243 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":344,"skipped":6074,"failed":0}
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:47.083: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 25 11:03:47.250: INFO: Waiting up to 5m0s for pod "downward-api-42547d52-9c2c-4621-a3f3-dd407db98a09" in namespace "downward-api-3407" to be "Succeeded or Failed"
Feb 25 11:03:47.258: INFO: Pod "downward-api-42547d52-9c2c-4621-a3f3-dd407db98a09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.944994ms
Feb 25 11:03:49.267: INFO: Pod "downward-api-42547d52-9c2c-4621-a3f3-dd407db98a09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016502729s
STEP: Saw pod success
Feb 25 11:03:49.267: INFO: Pod "downward-api-42547d52-9c2c-4621-a3f3-dd407db98a09" satisfied condition "Succeeded or Failed"
Feb 25 11:03:49.271: INFO: Trying to get logs from node ip-10-1-2-179.eu-central-1.compute.internal pod downward-api-42547d52-9c2c-4621-a3f3-dd407db98a09 container dapi-container: <nil>
STEP: delete the pod
Feb 25 11:03:49.296: INFO: Waiting for pod downward-api-42547d52-9c2c-4621-a3f3-dd407db98a09 to disappear
Feb 25 11:03:49.301: INFO: Pod downward-api-42547d52-9c2c-4621-a3f3-dd407db98a09 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:03:49.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3407" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6074,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 25 11:03:49.314: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4521
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 25 11:03:49.465: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 25 11:04:07.165: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
Feb 25 11:04:15.334: INFO: >>> kubeConfig: /tmp/kubeconfig-932286732
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 25 11:04:34.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4521" for this suite.

• [SLOW TEST:44.983 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":346,"skipped":6083,"failed":0}
SSSSSFeb 25 11:04:34.297: INFO: Running AfterSuite actions on all nodes
Feb 25 11:04:34.297: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Feb 25 11:04:34.297: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Feb 25 11:04:34.297: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Feb 25 11:04:34.297: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb 25 11:04:34.297: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb 25 11:04:34.297: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb 25 11:04:34.297: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Feb 25 11:04:34.297: INFO: Running AfterSuite actions on node 1
Feb 25 11:04:34.297: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6088,"failed":0}

Ran 346 of 6434 Specs in 5830.354 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6088 Skipped
PASS

Ginkgo ran 1 suite in 1h37m12.930313499s
Test Suite Passed
