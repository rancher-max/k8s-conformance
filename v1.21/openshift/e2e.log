I1214 12:26:26.769196      20 e2e.go:129] Starting e2e run "4d6bb6f9-012a-453d-afbf-bb2f016ad9be" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1639484784 - Will randomize all specs
Will run 339 of 5770 specs

Dec 14 12:26:26.784: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:26:26.788: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 14 12:26:26.813: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 14 12:26:26.838: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 14 12:26:26.838: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Dec 14 12:26:26.838: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 14 12:26:26.845: INFO: e2e test version: v1.21.6
Dec 14 12:26:26.846: INFO: kube-apiserver version: v1.21.6+c180a7c
Dec 14 12:26:26.846: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:26:26.853: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:26:26.853: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replication-controller
W1214 12:26:27.034406      20 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Dec 14 12:26:27.034: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:26:38.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4429" for this suite.

• [SLOW TEST:11.315 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":1,"skipped":23,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:26:38.168: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:26:38.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7635" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":2,"skipped":34,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:26:38.921: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-7m7s
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 12:26:39.161: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7m7s" in namespace "subpath-4422" to be "Succeeded or Failed"
Dec 14 12:26:39.165: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699574ms
Dec 14 12:26:41.171: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009404478s
Dec 14 12:26:43.181: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020026069s
Dec 14 12:26:45.191: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029644066s
Dec 14 12:26:47.199: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03811885s
Dec 14 12:26:49.215: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 10.05413373s
Dec 14 12:26:51.227: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 12.065710673s
Dec 14 12:26:53.237: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 14.075649008s
Dec 14 12:26:55.252: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 16.09050441s
Dec 14 12:26:57.258: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 18.096802113s
Dec 14 12:26:59.268: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 20.106334742s
Dec 14 12:27:01.272: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 22.110942564s
Dec 14 12:27:03.286: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 24.124389921s
Dec 14 12:27:05.295: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 26.133484742s
Dec 14 12:27:07.305: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Running", Reason="", readiness=true. Elapsed: 28.144184796s
Dec 14 12:27:09.318: INFO: Pod "pod-subpath-test-downwardapi-7m7s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.156313432s
STEP: Saw pod success
Dec 14 12:27:09.318: INFO: Pod "pod-subpath-test-downwardapi-7m7s" satisfied condition "Succeeded or Failed"
Dec 14 12:27:09.322: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-subpath-test-downwardapi-7m7s container test-container-subpath-downwardapi-7m7s: <nil>
STEP: delete the pod
Dec 14 12:27:09.362: INFO: Waiting for pod pod-subpath-test-downwardapi-7m7s to disappear
Dec 14 12:27:09.366: INFO: Pod pod-subpath-test-downwardapi-7m7s no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7m7s
Dec 14 12:27:09.366: INFO: Deleting pod "pod-subpath-test-downwardapi-7m7s" in namespace "subpath-4422"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:27:09.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4422" for this suite.

• [SLOW TEST:30.461 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":3,"skipped":35,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:27:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 12:27:09.627: INFO: Waiting up to 5m0s for pod "pod-48788666-0ffb-4b70-a623-bec19259661c" in namespace "emptydir-8365" to be "Succeeded or Failed"
Dec 14 12:27:09.630: INFO: Pod "pod-48788666-0ffb-4b70-a623-bec19259661c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765777ms
Dec 14 12:27:11.640: INFO: Pod "pod-48788666-0ffb-4b70-a623-bec19259661c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013706463s
Dec 14 12:27:13.646: INFO: Pod "pod-48788666-0ffb-4b70-a623-bec19259661c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019693258s
STEP: Saw pod success
Dec 14 12:27:13.646: INFO: Pod "pod-48788666-0ffb-4b70-a623-bec19259661c" satisfied condition "Succeeded or Failed"
Dec 14 12:27:13.651: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-48788666-0ffb-4b70-a623-bec19259661c container test-container: <nil>
STEP: delete the pod
Dec 14 12:27:13.682: INFO: Waiting for pod pod-48788666-0ffb-4b70-a623-bec19259661c to disappear
Dec 14 12:27:13.687: INFO: Pod pod-48788666-0ffb-4b70-a623-bec19259661c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:27:13.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8365" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":4,"skipped":48,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:27:13.701: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 14 12:27:14.285: INFO: Pod name wrapped-volume-race-c931c2e8-83d1-4ae6-8569-d57893f03a8e: Found 0 pods out of 5
Dec 14 12:27:19.303: INFO: Pod name wrapped-volume-race-c931c2e8-83d1-4ae6-8569-d57893f03a8e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c931c2e8-83d1-4ae6-8569-d57893f03a8e in namespace emptydir-wrapper-1631, will wait for the garbage collector to delete the pods
Dec 14 12:27:23.403: INFO: Deleting ReplicationController wrapped-volume-race-c931c2e8-83d1-4ae6-8569-d57893f03a8e took: 13.67021ms
Dec 14 12:27:23.503: INFO: Terminating ReplicationController wrapped-volume-race-c931c2e8-83d1-4ae6-8569-d57893f03a8e pods took: 100.766699ms
STEP: Creating RC which spawns configmap-volume pods
Dec 14 12:27:38.136: INFO: Pod name wrapped-volume-race-6ad82339-cc8e-4b55-981b-6e4e829c89aa: Found 0 pods out of 5
Dec 14 12:27:43.152: INFO: Pod name wrapped-volume-race-6ad82339-cc8e-4b55-981b-6e4e829c89aa: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6ad82339-cc8e-4b55-981b-6e4e829c89aa in namespace emptydir-wrapper-1631, will wait for the garbage collector to delete the pods
Dec 14 12:27:45.252: INFO: Deleting ReplicationController wrapped-volume-race-6ad82339-cc8e-4b55-981b-6e4e829c89aa took: 11.160544ms
Dec 14 12:27:45.452: INFO: Terminating ReplicationController wrapped-volume-race-6ad82339-cc8e-4b55-981b-6e4e829c89aa pods took: 200.897031ms
STEP: Creating RC which spawns configmap-volume pods
Dec 14 12:27:58.182: INFO: Pod name wrapped-volume-race-a9873137-a7b1-4dbb-9cd4-a85aed91cf69: Found 0 pods out of 5
Dec 14 12:28:03.208: INFO: Pod name wrapped-volume-race-a9873137-a7b1-4dbb-9cd4-a85aed91cf69: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a9873137-a7b1-4dbb-9cd4-a85aed91cf69 in namespace emptydir-wrapper-1631, will wait for the garbage collector to delete the pods
Dec 14 12:28:05.309: INFO: Deleting ReplicationController wrapped-volume-race-a9873137-a7b1-4dbb-9cd4-a85aed91cf69 took: 11.688523ms
Dec 14 12:28:05.414: INFO: Terminating ReplicationController wrapped-volume-race-a9873137-a7b1-4dbb-9cd4-a85aed91cf69 pods took: 105.006156ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:28:15.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1631" for this suite.

• [SLOW TEST:61.947 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":5,"skipped":58,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:28:15.648: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-bf7b2fc6-fcc3-4cd9-8184-2e31bd981040
STEP: Creating a pod to test consume secrets
Dec 14 12:28:15.868: INFO: Waiting up to 5m0s for pod "pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff" in namespace "secrets-7746" to be "Succeeded or Failed"
Dec 14 12:28:15.871: INFO: Pod "pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575866ms
Dec 14 12:28:17.881: INFO: Pod "pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012647506s
Dec 14 12:28:19.887: INFO: Pod "pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019103774s
Dec 14 12:28:21.899: INFO: Pod "pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031453736s
STEP: Saw pod success
Dec 14 12:28:21.899: INFO: Pod "pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff" satisfied condition "Succeeded or Failed"
Dec 14 12:28:21.903: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff container secret-env-test: <nil>
STEP: delete the pod
Dec 14 12:28:21.933: INFO: Waiting for pod pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff to disappear
Dec 14 12:28:21.937: INFO: Pod pod-secrets-3f8e804e-d454-4ce9-8ad6-01e92d4ea9ff no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:28:21.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7746" for this suite.

• [SLOW TEST:6.305 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":6,"skipped":63,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:28:21.952: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Dec 14 12:28:22.146: INFO: Waiting up to 5m0s for pod "var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1" in namespace "var-expansion-2413" to be "Succeeded or Failed"
Dec 14 12:28:22.150: INFO: Pod "var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.9347ms
Dec 14 12:28:24.163: INFO: Pod "var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01726371s
Dec 14 12:28:26.174: INFO: Pod "var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028637903s
STEP: Saw pod success
Dec 14 12:28:26.174: INFO: Pod "var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1" satisfied condition "Succeeded or Failed"
Dec 14 12:28:26.178: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1 container dapi-container: <nil>
STEP: delete the pod
Dec 14 12:28:26.208: INFO: Waiting for pod var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1 to disappear
Dec 14 12:28:26.211: INFO: Pod var-expansion-fc46f63b-0cd7-4737-bbba-132fd6eacba1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:28:26.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2413" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":7,"skipped":67,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:28:26.224: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 12:28:26.432: INFO: The status of Pod pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:28:28.442: INFO: The status of Pod pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:28:30.440: INFO: The status of Pod pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:28:32.444: INFO: The status of Pod pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 12:28:32.987: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906"
Dec 14 12:28:32.987: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906" in namespace "pods-4795" to be "terminated due to deadline exceeded"
Dec 14 12:28:32.992: INFO: Pod "pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906": Phase="Running", Reason="", readiness=true. Elapsed: 4.925036ms
Dec 14 12:28:34.999: INFO: Pod "pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.012213931s
Dec 14 12:28:34.999: INFO: Pod "pod-update-activedeadlineseconds-7a39d40c-4250-4951-8189-fa20b77d6906" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:28:34.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4795" for this suite.

• [SLOW TEST:8.793 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":8,"skipped":73,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:28:35.018: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:28:35.155: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Creating first CR 
Dec 14 12:28:37.841: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-14T12:28:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-14T12:28:37Z]] name:name1 resourceVersion:42184 uid:62ba0f1e-1563-4beb-b27f-5204e0003889] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 14 12:28:47.850: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-14T12:28:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-14T12:28:47Z]] name:name2 resourceVersion:42308 uid:45a35caf-d7e5-43f4-8fe7-901741da6163] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 14 12:28:57.868: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-14T12:28:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-14T12:28:57Z]] name:name1 resourceVersion:42393 uid:62ba0f1e-1563-4beb-b27f-5204e0003889] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 14 12:29:07.885: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-14T12:28:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-14T12:29:07Z]] name:name2 resourceVersion:42489 uid:45a35caf-d7e5-43f4-8fe7-901741da6163] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 14 12:29:17.938: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-14T12:28:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-14T12:28:57Z]] name:name1 resourceVersion:42579 uid:62ba0f1e-1563-4beb-b27f-5204e0003889] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 14 12:29:27.954: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-12-14T12:28:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-12-14T12:29:07Z]] name:name2 resourceVersion:42665 uid:45a35caf-d7e5-43f4-8fe7-901741da6163] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:29:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5965" for this suite.

• [SLOW TEST:63.495 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":9,"skipped":75,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:29:38.513: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-34b844fd-5859-4a95-b313-dce3305a40b4
STEP: Creating a pod to test consume configMaps
Dec 14 12:29:38.988: INFO: Waiting up to 5m0s for pod "pod-configmaps-63d67144-c775-47e9-b53f-789fce809331" in namespace "configmap-5102" to be "Succeeded or Failed"
Dec 14 12:29:38.994: INFO: Pod "pod-configmaps-63d67144-c775-47e9-b53f-789fce809331": Phase="Pending", Reason="", readiness=false. Elapsed: 6.814639ms
Dec 14 12:29:41.004: INFO: Pod "pod-configmaps-63d67144-c775-47e9-b53f-789fce809331": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016481202s
Dec 14 12:29:43.012: INFO: Pod "pod-configmaps-63d67144-c775-47e9-b53f-789fce809331": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024239653s
STEP: Saw pod success
Dec 14 12:29:43.012: INFO: Pod "pod-configmaps-63d67144-c775-47e9-b53f-789fce809331" satisfied condition "Succeeded or Failed"
Dec 14 12:29:43.015: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-configmaps-63d67144-c775-47e9-b53f-789fce809331 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 12:29:43.041: INFO: Waiting for pod pod-configmaps-63d67144-c775-47e9-b53f-789fce809331 to disappear
Dec 14 12:29:43.046: INFO: Pod pod-configmaps-63d67144-c775-47e9-b53f-789fce809331 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:29:43.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5102" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":10,"skipped":89,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:29:43.062: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:29:43.820: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:29:47.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7477" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":11,"skipped":103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:29:47.144: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Dec 14 12:29:47.606: INFO: namespace kubectl-7044
Dec 14 12:29:47.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-7044 create -f -'
Dec 14 12:29:48.770: INFO: stderr: ""
Dec 14 12:29:48.770: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 12:29:49.781: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:29:49.781: INFO: Found 0 / 1
Dec 14 12:29:50.780: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:29:50.780: INFO: Found 0 / 1
Dec 14 12:29:51.776: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:29:51.776: INFO: Found 0 / 1
Dec 14 12:29:52.788: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:29:52.788: INFO: Found 1 / 1
Dec 14 12:29:52.788: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 12:29:52.798: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:29:52.798: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 12:29:52.798: INFO: wait on agnhost-primary startup in kubectl-7044 
Dec 14 12:29:52.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-7044 logs agnhost-primary-kpff8 agnhost-primary'
Dec 14 12:29:52.960: INFO: stderr: ""
Dec 14 12:29:52.960: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 14 12:29:52.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-7044 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 14 12:29:53.116: INFO: stderr: ""
Dec 14 12:29:53.116: INFO: stdout: "service/rm2 exposed\n"
Dec 14 12:29:53.130: INFO: Service rm2 in namespace kubectl-7044 found.
STEP: exposing service
Dec 14 12:29:55.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-7044 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 14 12:29:55.313: INFO: stderr: ""
Dec 14 12:29:55.313: INFO: stdout: "service/rm3 exposed\n"
Dec 14 12:29:55.350: INFO: Service rm3 in namespace kubectl-7044 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:29:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7044" for this suite.

• [SLOW TEST:10.238 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":12,"skipped":126,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:29:57.382: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-a0b00396-06a5-41b9-bb18-06872e7336b2
STEP: Creating a pod to test consume secrets
Dec 14 12:29:57.632: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89" in namespace "projected-5265" to be "Succeeded or Failed"
Dec 14 12:29:57.636: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.343123ms
Dec 14 12:29:59.645: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01323633s
Dec 14 12:30:01.657: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024800372s
Dec 14 12:30:03.667: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035293162s
Dec 14 12:30:05.676: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044520049s
Dec 14 12:30:07.685: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89": Phase="Pending", Reason="", readiness=false. Elapsed: 10.053631641s
Dec 14 12:30:09.694: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.061991958s
STEP: Saw pod success
Dec 14 12:30:09.694: INFO: Pod "pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89" satisfied condition "Succeeded or Failed"
Dec 14 12:30:09.699: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 12:30:09.733: INFO: Waiting for pod pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89 to disappear
Dec 14 12:30:09.737: INFO: Pod pod-projected-secrets-2054c76e-f5a3-4e07-b881-d57f6c70cd89 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:30:09.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5265" for this suite.

• [SLOW TEST:12.369 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":13,"skipped":126,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:30:09.751: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5680
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5680
STEP: creating replication controller externalsvc in namespace services-5680
I1214 12:30:10.046408      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5680, replica count: 2
I1214 12:30:13.098916      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 12:30:16.099945      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 14 12:30:16.140: INFO: Creating new exec pod
Dec 14 12:30:20.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-5680 exec execpodm8xfg -- /bin/sh -x -c nslookup clusterip-service.services-5680.svc.cluster.local'
Dec 14 12:30:20.424: INFO: stderr: "+ nslookup clusterip-service.services-5680.svc.cluster.local\n"
Dec 14 12:30:20.424: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-5680.svc.cluster.local\tcanonical name = externalsvc.services-5680.svc.cluster.local.\nName:\texternalsvc.services-5680.svc.cluster.local\nAddress: 172.30.173.45\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5680, will wait for the garbage collector to delete the pods
Dec 14 12:30:20.489: INFO: Deleting ReplicationController externalsvc took: 9.692195ms
Dec 14 12:30:20.590: INFO: Terminating ReplicationController externalsvc pods took: 100.439577ms
Dec 14 12:30:28.625: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:30:28.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5680" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:18.914 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":14,"skipped":140,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:30:28.665: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 12:30:28.969: INFO: Waiting up to 5m0s for pod "pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b" in namespace "emptydir-377" to be "Succeeded or Failed"
Dec 14 12:30:28.983: INFO: Pod "pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.371532ms
Dec 14 12:30:30.989: INFO: Pod "pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020339631s
Dec 14 12:30:32.996: INFO: Pod "pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027060273s
STEP: Saw pod success
Dec 14 12:30:32.996: INFO: Pod "pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b" satisfied condition "Succeeded or Failed"
Dec 14 12:30:33.000: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b container test-container: <nil>
STEP: delete the pod
Dec 14 12:30:33.027: INFO: Waiting for pod pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b to disappear
Dec 14 12:30:33.032: INFO: Pod pod-eef0653c-31fa-416c-bd7c-6fd9b7ad7b4b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:30:33.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-377" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":15,"skipped":152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:30:33.048: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:30:33.253: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 14 12:30:38.274: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 12:30:44.288: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 14 12:30:46.298: INFO: Creating deployment "test-rollover-deployment"
Dec 14 12:30:46.315: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 14 12:30:48.327: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 14 12:30:48.337: INFO: Ensure that both replica sets have 1 created replica
Dec 14 12:30:48.344: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 14 12:30:48.357: INFO: Updating deployment test-rollover-deployment
Dec 14 12:30:48.357: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 14 12:30:50.369: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 14 12:30:50.375: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 14 12:30:50.384: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 12:30:50.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081848, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:30:52.394: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 12:30:52.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081851, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:30:54.401: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 12:30:54.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081851, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:30:56.396: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 12:30:56.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081851, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:30:58.400: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 12:30:58.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081851, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:31:00.396: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 12:31:00.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081851, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775081846, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:31:02.396: INFO: 
Dec 14 12:31:02.396: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Dec 14 12:31:02.407: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7291  ddc90267-ce8a-4620-b3b8-7c8a73fdfa74 44210 2 2021-12-14 12:30:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-12-14 12:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-12-14 12:31:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008c5568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-12-14 12:30:46 +0000 UTC,LastTransitionTime:2021-12-14 12:30:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-12-14 12:31:01 +0000 UTC,LastTransitionTime:2021-12-14 12:30:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 12:31:02.411: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-7291  458c17eb-8051-49a3-b55f-8c045f73b5ee 44195 2 2021-12-14 12:30:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ddc90267-ce8a-4620-b3b8-7c8a73fdfa74 0xc0005ada20 0xc0005ada21}] []  [{kube-controller-manager Update apps/v1 2021-12-14 12:31:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc90267-ce8a-4620-b3b8-7c8a73fdfa74\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0005adaa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 12:31:02.411: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 14 12:31:02.411: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7291  7510bca5-327c-41f3-847a-8d50925f5cb5 44207 2 2021-12-14 12:30:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ddc90267-ce8a-4620-b3b8-7c8a73fdfa74 0xc0005ad6c7 0xc0005ad6c8}] []  [{e2e.test Update apps/v1 2021-12-14 12:30:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-12-14 12:31:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc90267-ce8a-4620-b3b8-7c8a73fdfa74\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0005ad7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 12:31:02.412: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-7291  1baf1c16-578b-45b5-a2e4-3280e5bc5933 44065 2 2021-12-14 12:30:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ddc90267-ce8a-4620-b3b8-7c8a73fdfa74 0xc0005ad837 0xc0005ad838}] []  [{kube-controller-manager Update apps/v1 2021-12-14 12:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddc90267-ce8a-4620-b3b8-7c8a73fdfa74\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0005ad968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 12:31:02.416: INFO: Pod "test-rollover-deployment-98c5f4599-v6mc5" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-v6mc5 test-rollover-deployment-98c5f4599- deployment-7291  12dd5caa-3e3c-4370-acb6-fcf697bb7275 44104 0 2021-12-14 12:30:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.25"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.25"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 458c17eb-8051-49a3-b55f-8c045f73b5ee 0xc0008c59b7 0xc0008c59b8}] []  [{kube-controller-manager Update v1 2021-12-14 12:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"458c17eb-8051-49a3-b55f-8c045f73b5ee\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 12:30:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 12:30:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqjx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqjx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-4fxp7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:30:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:30:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:30:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:30:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.25,StartTime:2021-12-14 12:30:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 12:30:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://19271584f15c44cf75e29c35310e77134ab53f7185261e30850518608ec916b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:02.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7291" for this suite.

• [SLOW TEST:29.386 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":16,"skipped":225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:02.436: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:02.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-776" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":17,"skipped":269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:02.739: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:31:03.269: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fefa6900-3ab4-45e6-b218-45e05fd92628", Controller:(*bool)(0xc003bb1842), BlockOwnerDeletion:(*bool)(0xc003bb1843)}}
Dec 14 12:31:03.282: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"00eb2859-c46e-41fc-ace8-537c61a89915", Controller:(*bool)(0xc0030cf1e6), BlockOwnerDeletion:(*bool)(0xc0030cf1e7)}}
Dec 14 12:31:03.294: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3b1f9fc4-feb6-46f8-98a1-a1a795efb345", Controller:(*bool)(0xc003bb1b66), BlockOwnerDeletion:(*bool)(0xc003bb1b67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:08.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4523" for this suite.

• [SLOW TEST:5.593 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":18,"skipped":318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:08.332: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:31:08.760: INFO: The status of Pod server-envvars-af7d925d-48fa-4da5-8148-25e92a68ea78 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:31:10.769: INFO: The status of Pod server-envvars-af7d925d-48fa-4da5-8148-25e92a68ea78 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:31:12.770: INFO: The status of Pod server-envvars-af7d925d-48fa-4da5-8148-25e92a68ea78 is Running (Ready = true)
Dec 14 12:31:12.814: INFO: Waiting up to 5m0s for pod "client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540" in namespace "pods-9314" to be "Succeeded or Failed"
Dec 14 12:31:12.818: INFO: Pod "client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540": Phase="Pending", Reason="", readiness=false. Elapsed: 3.620704ms
Dec 14 12:31:14.830: INFO: Pod "client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015662635s
Dec 14 12:31:16.836: INFO: Pod "client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021517194s
Dec 14 12:31:18.845: INFO: Pod "client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030323922s
STEP: Saw pod success
Dec 14 12:31:18.845: INFO: Pod "client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540" satisfied condition "Succeeded or Failed"
Dec 14 12:31:18.848: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540 container env3cont: <nil>
STEP: delete the pod
Dec 14 12:31:18.869: INFO: Waiting for pod client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540 to disappear
Dec 14 12:31:18.873: INFO: Pod client-envvars-2b194c02-8c30-4f8b-b846-955e047e5540 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:18.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9314" for this suite.

• [SLOW TEST:10.553 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":19,"skipped":373,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:18.886: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-32af98a5-73ec-4b89-8d3a-c04d9203f433
STEP: Creating a pod to test consume configMaps
Dec 14 12:31:19.137: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545" in namespace "configmap-9608" to be "Succeeded or Failed"
Dec 14 12:31:19.148: INFO: Pod "pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545": Phase="Pending", Reason="", readiness=false. Elapsed: 11.000485ms
Dec 14 12:31:21.163: INFO: Pod "pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026276734s
Dec 14 12:31:23.173: INFO: Pod "pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03569926s
STEP: Saw pod success
Dec 14 12:31:23.173: INFO: Pod "pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545" satisfied condition "Succeeded or Failed"
Dec 14 12:31:23.176: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 12:31:23.203: INFO: Waiting for pod pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545 to disappear
Dec 14 12:31:23.206: INFO: Pod pod-configmaps-fd8aa59a-f554-4914-baf0-94aec34dc545 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:23.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9608" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":20,"skipped":388,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:23.220: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Dec 14 12:31:23.450: INFO: created test-pod-1
Dec 14 12:31:23.479: INFO: created test-pod-2
Dec 14 12:31:23.512: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:23.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8196" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":21,"skipped":394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:23.645: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Dec 14 12:31:23.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 14 12:31:24.106: INFO: stderr: ""
Dec 14 12:31:24.106: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Dec 14 12:31:24.106: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 14 12:31:24.106: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2206" to be "running and ready, or succeeded"
Dec 14 12:31:24.111: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.517954ms
Dec 14 12:31:26.119: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012410331s
Dec 14 12:31:28.127: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.020972386s
Dec 14 12:31:28.127: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 14 12:31:28.127: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 14 12:31:28.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 logs logs-generator logs-generator'
Dec 14 12:31:28.237: INFO: stderr: ""
Dec 14 12:31:28.237: INFO: stdout: "I1214 12:31:26.979561       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/g8v 364\nI1214 12:31:27.178765       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/nwb 566\nI1214 12:31:27.378142       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/j2n 443\nI1214 12:31:27.578445       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/wtt 527\nI1214 12:31:27.781628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/mjwx 311\nI1214 12:31:27.978878       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/42pw 255\nI1214 12:31:28.178373       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/sg6 337\n"
STEP: limiting log lines
Dec 14 12:31:28.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 logs logs-generator logs-generator --tail=1'
Dec 14 12:31:28.367: INFO: stderr: ""
Dec 14 12:31:28.367: INFO: stdout: "I1214 12:31:28.178373       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/sg6 337\n"
Dec 14 12:31:28.367: INFO: got output "I1214 12:31:28.178373       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/sg6 337\n"
STEP: limiting log bytes
Dec 14 12:31:28.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 logs logs-generator logs-generator --limit-bytes=1'
Dec 14 12:31:28.479: INFO: stderr: ""
Dec 14 12:31:28.479: INFO: stdout: "I"
Dec 14 12:31:28.479: INFO: got output "I"
STEP: exposing timestamps
Dec 14 12:31:28.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 14 12:31:28.599: INFO: stderr: ""
Dec 14 12:31:28.599: INFO: stdout: "2021-12-14T12:31:28.578451187Z I1214 12:31:28.578379       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/4kfc 207\n"
Dec 14 12:31:28.599: INFO: got output "2021-12-14T12:31:28.578451187Z I1214 12:31:28.578379       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/4kfc 207\n"
STEP: restricting to a time range
Dec 14 12:31:31.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 logs logs-generator logs-generator --since=1s'
Dec 14 12:31:31.210: INFO: stderr: ""
Dec 14 12:31:31.210: INFO: stdout: "I1214 12:31:30.378449       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/rbs 258\nI1214 12:31:30.577760       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/2lwd 442\nI1214 12:31:30.778061       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/j44v 362\nI1214 12:31:30.978541       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/wg77 346\nI1214 12:31:31.177788       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/6xj2 288\n"
Dec 14 12:31:31.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 logs logs-generator logs-generator --since=24h'
Dec 14 12:31:31.331: INFO: stderr: ""
Dec 14 12:31:31.331: INFO: stdout: "I1214 12:31:26.979561       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/g8v 364\nI1214 12:31:27.178765       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/nwb 566\nI1214 12:31:27.378142       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/j2n 443\nI1214 12:31:27.578445       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/wtt 527\nI1214 12:31:27.781628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/mjwx 311\nI1214 12:31:27.978878       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/42pw 255\nI1214 12:31:28.178373       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/sg6 337\nI1214 12:31:28.378877       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/fnrw 463\nI1214 12:31:28.578379       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/4kfc 207\nI1214 12:31:28.780894       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/2p5 552\nI1214 12:31:28.978231       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/8v7 202\nI1214 12:31:29.178876       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/nzg 529\nI1214 12:31:29.378355       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/m29 504\nI1214 12:31:29.578821       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/97p 525\nI1214 12:31:29.778249       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/xxf 262\nI1214 12:31:29.978701       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/qz86 548\nI1214 12:31:30.178011       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/2c8 575\nI1214 12:31:30.378449       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/rbs 258\nI1214 12:31:30.577760       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/2lwd 442\nI1214 12:31:30.778061       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/j44v 362\nI1214 12:31:30.978541       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/wg77 346\nI1214 12:31:31.177788       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/6xj2 288\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Dec 14 12:31:31.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2206 delete pod logs-generator'
Dec 14 12:31:37.954: INFO: stderr: ""
Dec 14 12:31:37.954: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:37.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2206" for this suite.

• [SLOW TEST:14.329 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":22,"skipped":424,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:37.975: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:31:38.181: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 12:31:43.191: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Dec 14 12:31:49.223: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Dec 14 12:31:49.237: INFO: observed ReplicaSet test-rs in namespace replicaset-2023 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 12:31:49.285: INFO: observed ReplicaSet test-rs in namespace replicaset-2023 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 12:31:49.325: INFO: observed ReplicaSet test-rs in namespace replicaset-2023 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 12:31:49.349: INFO: observed ReplicaSet test-rs in namespace replicaset-2023 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 12:31:52.153: INFO: observed ReplicaSet test-rs in namespace replicaset-2023 with ReadyReplicas 2, AvailableReplicas 2
Dec 14 12:31:58.997: INFO: observed Replicaset test-rs in namespace replicaset-2023 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:31:58.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2023" for this suite.

• [SLOW TEST:21.053 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":23,"skipped":444,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:31:59.028: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-062101c9-a5f5-4818-9c3e-2f3110bd7acc
STEP: Creating a pod to test consume configMaps
Dec 14 12:31:59.285: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4" in namespace "projected-9481" to be "Succeeded or Failed"
Dec 14 12:31:59.302: INFO: Pod "pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.433403ms
Dec 14 12:32:01.310: INFO: Pod "pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024766427s
Dec 14 12:32:03.319: INFO: Pod "pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033521206s
STEP: Saw pod success
Dec 14 12:32:03.319: INFO: Pod "pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4" satisfied condition "Succeeded or Failed"
Dec 14 12:32:03.324: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 12:32:03.355: INFO: Waiting for pod pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4 to disappear
Dec 14 12:32:03.360: INFO: Pod pod-projected-configmaps-8825c50b-f99e-42de-afa3-567c93c7cda4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:32:03.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9481" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":24,"skipped":456,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:32:03.379: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:32:03.563: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:32:04.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3263" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":25,"skipped":471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:32:04.174: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:32:04.550: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 14 12:32:09.591: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 12:32:09.591: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Dec 14 12:32:09.665: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2613  ab900d72-b104-43e1-9178-3f5f4156743c 45717 1 2021-12-14 12:32:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-12-14 12:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046bbf08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 12:32:09.675: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:32:09.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2613" for this suite.

• [SLOW TEST:5.530 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":26,"skipped":496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:32:09.705: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 14 12:32:10.569: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-719  1af128e1-b2d8-4611-b11f-022f99a2eda7 45758 0 2021-12-14 12:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-14 12:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 12:32:10.569: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-719  1af128e1-b2d8-4611-b11f-022f99a2eda7 45760 0 2021-12-14 12:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-14 12:32:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 12:32:10.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-719  1af128e1-b2d8-4611-b11f-022f99a2eda7 45764 0 2021-12-14 12:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-14 12:32:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 14 12:32:20.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-719  1af128e1-b2d8-4611-b11f-022f99a2eda7 45932 0 2021-12-14 12:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-14 12:32:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 12:32:20.642: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-719  1af128e1-b2d8-4611-b11f-022f99a2eda7 45933 0 2021-12-14 12:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-14 12:32:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 12:32:20.643: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-719  1af128e1-b2d8-4611-b11f-022f99a2eda7 45934 0 2021-12-14 12:32:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-12-14 12:32:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:32:20.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-719" for this suite.

• [SLOW TEST:10.960 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":27,"skipped":522,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:32:20.664: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-793fca44-4e9b-48bb-8020-9447bd7c1c5c
STEP: Creating a pod to test consume configMaps
Dec 14 12:32:21.088: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970" in namespace "projected-6471" to be "Succeeded or Failed"
Dec 14 12:32:21.093: INFO: Pod "pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820789ms
Dec 14 12:32:23.105: INFO: Pod "pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017075838s
Dec 14 12:32:25.116: INFO: Pod "pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028036435s
STEP: Saw pod success
Dec 14 12:32:25.116: INFO: Pod "pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970" satisfied condition "Succeeded or Failed"
Dec 14 12:32:25.120: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 12:32:25.159: INFO: Waiting for pod pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970 to disappear
Dec 14 12:32:25.163: INFO: Pod pod-projected-configmaps-18847804-edde-4561-a867-e68b51d92970 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:32:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6471" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":28,"skipped":522,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:32:25.191: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-lhwz
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 12:32:25.463: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lhwz" in namespace "subpath-6028" to be "Succeeded or Failed"
Dec 14 12:32:25.467: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603042ms
Dec 14 12:32:27.479: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015900064s
Dec 14 12:32:29.486: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022590923s
Dec 14 12:32:31.499: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 6.036270102s
Dec 14 12:32:33.506: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 8.042938109s
Dec 14 12:32:35.514: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 10.05072519s
Dec 14 12:32:37.524: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 12.06055916s
Dec 14 12:32:39.533: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 14.069668345s
Dec 14 12:32:41.540: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 16.076940402s
Dec 14 12:32:43.548: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 18.084926224s
Dec 14 12:32:45.557: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 20.093820403s
Dec 14 12:32:47.569: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 22.105655101s
Dec 14 12:32:49.577: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Running", Reason="", readiness=true. Elapsed: 24.1135525s
Dec 14 12:32:51.590: INFO: Pod "pod-subpath-test-configmap-lhwz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.127039897s
STEP: Saw pod success
Dec 14 12:32:51.590: INFO: Pod "pod-subpath-test-configmap-lhwz" satisfied condition "Succeeded or Failed"
Dec 14 12:32:51.594: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-subpath-test-configmap-lhwz container test-container-subpath-configmap-lhwz: <nil>
STEP: delete the pod
Dec 14 12:32:51.622: INFO: Waiting for pod pod-subpath-test-configmap-lhwz to disappear
Dec 14 12:32:51.626: INFO: Pod pod-subpath-test-configmap-lhwz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lhwz
Dec 14 12:32:51.626: INFO: Deleting pod "pod-subpath-test-configmap-lhwz" in namespace "subpath-6028"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:32:51.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6028" for this suite.

• [SLOW TEST:26.452 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":29,"skipped":536,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:32:51.644: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec 14 12:32:51.936: INFO: Waiting up to 5m0s for pod "downward-api-3848bfb6-db38-4916-afc5-ae71b792034e" in namespace "downward-api-1566" to be "Succeeded or Failed"
Dec 14 12:32:51.942: INFO: Pod "downward-api-3848bfb6-db38-4916-afc5-ae71b792034e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.860061ms
Dec 14 12:32:53.951: INFO: Pod "downward-api-3848bfb6-db38-4916-afc5-ae71b792034e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014371573s
Dec 14 12:32:55.957: INFO: Pod "downward-api-3848bfb6-db38-4916-afc5-ae71b792034e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020226546s
STEP: Saw pod success
Dec 14 12:32:55.957: INFO: Pod "downward-api-3848bfb6-db38-4916-afc5-ae71b792034e" satisfied condition "Succeeded or Failed"
Dec 14 12:32:55.963: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downward-api-3848bfb6-db38-4916-afc5-ae71b792034e container dapi-container: <nil>
STEP: delete the pod
Dec 14 12:32:55.997: INFO: Waiting for pod downward-api-3848bfb6-db38-4916-afc5-ae71b792034e to disappear
Dec 14 12:32:56.003: INFO: Pod downward-api-3848bfb6-db38-4916-afc5-ae71b792034e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:32:56.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1566" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":30,"skipped":543,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:32:56.019: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Dec 14 12:32:58.302: INFO: running pods: 0 < 1
Dec 14 12:33:00.308: INFO: running pods: 0 < 1
Dec 14 12:33:02.317: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:33:04.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3826" for this suite.

• [SLOW TEST:8.346 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":31,"skipped":557,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:33:04.366: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:33:08.641: INFO: Deleting pod "var-expansion-943015ea-c744-49a1-ab81-f46fe36e50bc" in namespace "var-expansion-4616"
Dec 14 12:33:08.651: INFO: Wait up to 5m0s for pod "var-expansion-943015ea-c744-49a1-ab81-f46fe36e50bc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:33:18.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4616" for this suite.

• [SLOW TEST:14.317 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":32,"skipped":584,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:33:18.683: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 14 12:33:18.868: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:33:28.761: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:34:05.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3671" for this suite.

• [SLOW TEST:46.952 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":33,"skipped":598,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:34:05.635: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:34:05.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6392" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":34,"skipped":606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:34:05.913: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-7309
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Dec 14 12:34:06.152: INFO: Found 0 stateful pods, waiting for 3
Dec 14 12:34:16.164: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 12:34:16.164: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 12:34:16.164: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 14 12:34:26.163: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 12:34:26.163: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 12:34:26.163: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 12:34:26.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-7309 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 12:34:26.393: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 12:34:26.393: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 12:34:26.393: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Dec 14 12:34:36.458: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 14 12:34:46.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-7309 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 12:34:46.706: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 12:34:46.706: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 12:34:46.706: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 12:34:56.751: INFO: Waiting for StatefulSet statefulset-7309/ss2 to complete update
Dec 14 12:34:56.751: INFO: Waiting for Pod statefulset-7309/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec 14 12:34:56.751: INFO: Waiting for Pod statefulset-7309/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec 14 12:35:06.762: INFO: Waiting for StatefulSet statefulset-7309/ss2 to complete update
Dec 14 12:35:06.762: INFO: Waiting for Pod statefulset-7309/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec 14 12:35:16.774: INFO: Waiting for StatefulSet statefulset-7309/ss2 to complete update
Dec 14 12:35:16.774: INFO: Waiting for Pod statefulset-7309/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec 14 12:35:26.763: INFO: Waiting for StatefulSet statefulset-7309/ss2 to complete update
STEP: Rolling back to a previous revision
Dec 14 12:35:36.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-7309 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 12:35:36.964: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 12:35:36.964: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 12:35:36.964: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 12:35:47.015: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 14 12:35:57.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-7309 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 12:35:57.288: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 12:35:57.288: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 12:35:57.288: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 12:36:07.335: INFO: Waiting for StatefulSet statefulset-7309/ss2 to complete update
Dec 14 12:36:07.335: INFO: Waiting for Pod statefulset-7309/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec 14 12:36:07.335: INFO: Waiting for Pod statefulset-7309/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec 14 12:36:17.351: INFO: Waiting for StatefulSet statefulset-7309/ss2 to complete update
Dec 14 12:36:17.351: INFO: Waiting for Pod statefulset-7309/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Dec 14 12:36:17.351: INFO: Waiting for Pod statefulset-7309/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Dec 14 12:36:27.352: INFO: Deleting all statefulset in ns statefulset-7309
Dec 14 12:36:27.355: INFO: Scaling statefulset ss2 to 0
Dec 14 12:37:07.379: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 12:37:07.388: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:37:07.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7309" for this suite.

• [SLOW TEST:181.518 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":35,"skipped":674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:37:07.432: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 14 12:37:07.638: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:37:17.425: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:37:53.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2125" for this suite.

• [SLOW TEST:46.318 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":36,"skipped":703,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:37:53.751: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-b6dbb71c-f095-4bd4-a45b-2e683408c814
STEP: Creating a pod to test consume configMaps
Dec 14 12:37:53.986: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8" in namespace "configmap-781" to be "Succeeded or Failed"
Dec 14 12:37:54.000: INFO: Pod "pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.446008ms
Dec 14 12:37:56.008: INFO: Pod "pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021754689s
Dec 14 12:37:58.015: INFO: Pod "pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029253316s
STEP: Saw pod success
Dec 14 12:37:58.015: INFO: Pod "pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8" satisfied condition "Succeeded or Failed"
Dec 14 12:37:58.018: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 12:37:58.064: INFO: Waiting for pod pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8 to disappear
Dec 14 12:37:58.069: INFO: Pod pod-configmaps-5a7c4ecb-8d1e-4647-89ce-054ce157f8b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:37:58.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-781" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":37,"skipped":709,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:37:58.085: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:37:58.276: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-6b3054be-5d93-4e13-8e7e-dae771c7b50b
STEP: Creating the pod
Dec 14 12:37:58.316: INFO: The status of Pod pod-configmaps-ee70cce7-c178-4009-b414-bc487ca7618a is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:38:00.322: INFO: The status of Pod pod-configmaps-ee70cce7-c178-4009-b414-bc487ca7618a is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:38:02.324: INFO: The status of Pod pod-configmaps-ee70cce7-c178-4009-b414-bc487ca7618a is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-6b3054be-5d93-4e13-8e7e-dae771c7b50b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:04.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2226" for this suite.

• [SLOW TEST:6.346 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":38,"skipped":728,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:04.431: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-5835/configmap-test-d67381d6-ae02-4f1c-ba42-7b2f6c2b449b
STEP: Creating a pod to test consume configMaps
Dec 14 12:38:04.655: INFO: Waiting up to 5m0s for pod "pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe" in namespace "configmap-5835" to be "Succeeded or Failed"
Dec 14 12:38:04.660: INFO: Pod "pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.755397ms
Dec 14 12:38:06.666: INFO: Pod "pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010961576s
Dec 14 12:38:08.676: INFO: Pod "pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020990863s
STEP: Saw pod success
Dec 14 12:38:08.676: INFO: Pod "pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe" satisfied condition "Succeeded or Failed"
Dec 14 12:38:08.679: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe container env-test: <nil>
STEP: delete the pod
Dec 14 12:38:08.711: INFO: Waiting for pod pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe to disappear
Dec 14 12:38:08.716: INFO: Pod pod-configmaps-087d1f76-2c86-4216-a4cd-6a271c2805fe no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:08.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5835" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":39,"skipped":733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:08.730: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Dec 14 12:38:08.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-3398 create -f -'
Dec 14 12:38:09.554: INFO: stderr: ""
Dec 14 12:38:09.554: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 12:38:10.562: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:38:10.562: INFO: Found 0 / 1
Dec 14 12:38:11.562: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:38:11.562: INFO: Found 0 / 1
Dec 14 12:38:12.561: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:38:12.561: INFO: Found 0 / 1
Dec 14 12:38:13.564: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:38:13.564: INFO: Found 1 / 1
Dec 14 12:38:13.564: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 14 12:38:13.567: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:38:13.567: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 12:38:13.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-3398 patch pod agnhost-primary-bcfj9 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 14 12:38:13.693: INFO: stderr: ""
Dec 14 12:38:13.693: INFO: stdout: "pod/agnhost-primary-bcfj9 patched\n"
STEP: checking annotations
Dec 14 12:38:13.696: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:38:13.696: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:13.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3398" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":40,"skipped":799,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:13.714: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec 14 12:38:13.995: INFO: Waiting up to 5m0s for pod "downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e" in namespace "downward-api-3281" to be "Succeeded or Failed"
Dec 14 12:38:13.998: INFO: Pod "downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.410399ms
Dec 14 12:38:16.010: INFO: Pod "downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015081913s
Dec 14 12:38:18.018: INFO: Pod "downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023646901s
STEP: Saw pod success
Dec 14 12:38:18.018: INFO: Pod "downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e" satisfied condition "Succeeded or Failed"
Dec 14 12:38:18.023: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e container dapi-container: <nil>
STEP: delete the pod
Dec 14 12:38:18.050: INFO: Waiting for pod downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e to disappear
Dec 14 12:38:18.054: INFO: Pod downward-api-8198f3fd-9db4-4b80-b77e-2e3d3a73a09e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:18.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3281" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":41,"skipped":809,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:18.067: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:22.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4882" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":42,"skipped":829,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:22.310: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 12:38:22.528: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 14 12:38:22.533: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 12:38:22.553: INFO: waiting for watch events with expected annotations
Dec 14 12:38:22.553: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:22.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4367" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":43,"skipped":859,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:22.651: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4746
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 12:38:22.834: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 12:38:22.959: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:38:24.966: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:38:26.968: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:38:28.976: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:38:30.969: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:38:32.978: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 12:38:34.969: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 12:38:36.971: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 12:38:38.968: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 12:38:40.970: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 12:38:42.969: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 12:38:42.977: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 12:38:44.988: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 12:38:44.997: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 12:38:49.053: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 12:38:49.053: INFO: Breadth first check of 10.129.2.31 on host 10.0.132.177...
Dec 14 12:38:49.062: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.42:9080/dial?request=hostname&protocol=udp&host=10.129.2.31&port=8081&tries=1'] Namespace:pod-network-test-4746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 12:38:49.062: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:38:49.180: INFO: Waiting for responses: map[]
Dec 14 12:38:49.180: INFO: reached 10.129.2.31 after 0/1 tries
Dec 14 12:38:49.180: INFO: Breadth first check of 10.131.0.48 on host 10.0.150.199...
Dec 14 12:38:49.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.42:9080/dial?request=hostname&protocol=udp&host=10.131.0.48&port=8081&tries=1'] Namespace:pod-network-test-4746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 12:38:49.187: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:38:49.306: INFO: Waiting for responses: map[]
Dec 14 12:38:49.306: INFO: reached 10.131.0.48 after 0/1 tries
Dec 14 12:38:49.306: INFO: Breadth first check of 10.128.2.41 on host 10.0.169.135...
Dec 14 12:38:49.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.42:9080/dial?request=hostname&protocol=udp&host=10.128.2.41&port=8081&tries=1'] Namespace:pod-network-test-4746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 12:38:49.312: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:38:49.429: INFO: Waiting for responses: map[]
Dec 14 12:38:49.429: INFO: reached 10.128.2.41 after 0/1 tries
Dec 14 12:38:49.429: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:49.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4746" for this suite.

• [SLOW TEST:26.806 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":44,"skipped":875,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:49.457: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec 14 12:38:49.711: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:54.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4627" for this suite.

• [SLOW TEST:5.400 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":45,"skipped":883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:54.858: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:55.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8724" for this suite.
STEP: Destroying namespace "nspatchtest-daed0936-0b1e-484e-9767-941e4f59b194-1703" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":46,"skipped":936,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:55.584: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 14 12:38:55.900: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-811  e99907f8-6938-450b-a3f4-886f0d115517 51396 0 2021-12-14 12:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-14 12:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 12:38:55.900: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-811  e99907f8-6938-450b-a3f4-886f0d115517 51398 0 2021-12-14 12:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-14 12:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 14 12:38:55.950: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-811  e99907f8-6938-450b-a3f4-886f0d115517 51406 0 2021-12-14 12:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-14 12:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 12:38:55.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-811  e99907f8-6938-450b-a3f4-886f0d115517 51412 0 2021-12-14 12:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-12-14 12:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:38:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-811" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":47,"skipped":936,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:38:55.963: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 12:38:56.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47" in namespace "projected-70" to be "Succeeded or Failed"
Dec 14 12:38:56.191: INFO: Pod "downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720844ms
Dec 14 12:38:58.196: INFO: Pod "downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00982548s
Dec 14 12:39:00.206: INFO: Pod "downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019750465s
STEP: Saw pod success
Dec 14 12:39:00.206: INFO: Pod "downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47" satisfied condition "Succeeded or Failed"
Dec 14 12:39:00.209: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47 container client-container: <nil>
STEP: delete the pod
Dec 14 12:39:00.234: INFO: Waiting for pod downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47 to disappear
Dec 14 12:39:00.241: INFO: Pod downwardapi-volume-b3a67a73-69bb-42de-8940-e82561b80e47 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:00.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-70" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":48,"skipped":936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:00.255: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:00.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5893" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":49,"skipped":1027,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:39:01.170: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 12:39:01.205: INFO: The status of Pod pod-logs-websocket-c0e8fe06-08a3-4162-90b8-547623722162 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:39:03.229: INFO: The status of Pod pod-logs-websocket-c0e8fe06-08a3-4162-90b8-547623722162 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:39:05.211: INFO: The status of Pod pod-logs-websocket-c0e8fe06-08a3-4162-90b8-547623722162 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:05.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9216" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":50,"skipped":1033,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:05.260: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 12:39:06.147: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 12:39:08.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082346, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082346, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082346, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082346, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 12:39:11.199: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 14 12:39:11.233: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:11.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3112" for this suite.
STEP: Destroying namespace "webhook-3112-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.118 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":51,"skipped":1041,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9217
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9217
STEP: creating replication controller externalsvc in namespace services-9217
I1214 12:39:11.803074      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9217, replica count: 2
I1214 12:39:14.875382      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 12:39:17.875849      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 14 12:39:17.922: INFO: Creating new exec pod
Dec 14 12:39:21.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9217 exec execpod7gphx -- /bin/sh -x -c nslookup nodeport-service.services-9217.svc.cluster.local'
Dec 14 12:39:22.261: INFO: stderr: "+ nslookup nodeport-service.services-9217.svc.cluster.local\n"
Dec 14 12:39:22.261: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-9217.svc.cluster.local\tcanonical name = externalsvc.services-9217.svc.cluster.local.\nName:\texternalsvc.services-9217.svc.cluster.local\nAddress: 172.30.89.150\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9217, will wait for the garbage collector to delete the pods
Dec 14 12:39:22.335: INFO: Deleting ReplicationController externalsvc took: 17.628321ms
Dec 14 12:39:22.436: INFO: Terminating ReplicationController externalsvc pods took: 100.783476ms
Dec 14 12:39:35.184: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:35.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9217" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:23.838 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":52,"skipped":1062,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:35.223: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 12:39:35.563: INFO: Waiting up to 5m0s for pod "pod-2a9d319b-02b3-45f5-bb05-516b52a5214b" in namespace "emptydir-8225" to be "Succeeded or Failed"
Dec 14 12:39:35.575: INFO: Pod "pod-2a9d319b-02b3-45f5-bb05-516b52a5214b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009028ms
Dec 14 12:39:37.586: INFO: Pod "pod-2a9d319b-02b3-45f5-bb05-516b52a5214b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023341731s
Dec 14 12:39:39.597: INFO: Pod "pod-2a9d319b-02b3-45f5-bb05-516b52a5214b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03418953s
STEP: Saw pod success
Dec 14 12:39:39.597: INFO: Pod "pod-2a9d319b-02b3-45f5-bb05-516b52a5214b" satisfied condition "Succeeded or Failed"
Dec 14 12:39:39.601: INFO: Trying to get logs from node ip-10-0-132-177.ec2.internal pod pod-2a9d319b-02b3-45f5-bb05-516b52a5214b container test-container: <nil>
STEP: delete the pod
Dec 14 12:39:39.638: INFO: Waiting for pod pod-2a9d319b-02b3-45f5-bb05-516b52a5214b to disappear
Dec 14 12:39:39.644: INFO: Pod pod-2a9d319b-02b3-45f5-bb05-516b52a5214b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:39.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8225" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":53,"skipped":1075,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:39.658: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:39:39.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9260 version'
Dec 14 12:39:39.848: INFO: stderr: ""
Dec 14 12:39:39.848: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.6\", GitCommit:\"d921bc6d1810da51177fbd0ed61dc811c5228097\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T17:50:34Z\", GoVersion:\"go1.16.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.6+c180a7c\", GitCommit:\"cefce093e4e5bc9a1916eb5a489ed37c7d467f6f\", GitTreeState:\"clean\", BuildDate:\"2021-12-02T16:34:49Z\", GoVersion:\"go1.16.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:39.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9260" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":54,"skipped":1091,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:39.867: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 12:39:44.118: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:39:44.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7432" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":55,"skipped":1107,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:39:44.163: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-c81c5bdf-a6d5-4ba0-87d1-e1f303bfec40 in namespace container-probe-8425
Dec 14 12:39:48.447: INFO: Started pod test-webserver-c81c5bdf-a6d5-4ba0-87d1-e1f303bfec40 in namespace container-probe-8425
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 12:39:48.451: INFO: Initial restart count of pod test-webserver-c81c5bdf-a6d5-4ba0-87d1-e1f303bfec40 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:43:49.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8425" for this suite.

• [SLOW TEST:245.457 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":56,"skipped":1115,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:43:49.620: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:43:56.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2986" for this suite.

• [SLOW TEST:7.206 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":57,"skipped":1115,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:43:56.827: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-47b2e9c0-0163-4e14-b5b2-bbe844e847b5
STEP: Creating a pod to test consume secrets
Dec 14 12:43:57.124: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d" in namespace "projected-5735" to be "Succeeded or Failed"
Dec 14 12:43:57.129: INFO: Pod "pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.772709ms
Dec 14 12:43:59.140: INFO: Pod "pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016340969s
Dec 14 12:44:01.149: INFO: Pod "pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025052441s
STEP: Saw pod success
Dec 14 12:44:01.149: INFO: Pod "pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d" satisfied condition "Succeeded or Failed"
Dec 14 12:44:01.155: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 12:44:01.197: INFO: Waiting for pod pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d to disappear
Dec 14 12:44:01.203: INFO: Pod pod-projected-secrets-1d09a13c-6e27-4ace-b893-ffc3890d8d5d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:01.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5735" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":58,"skipped":1120,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:01.222: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Dec 14 12:44:01.466: INFO: Waiting up to 5m0s for pod "var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954" in namespace "var-expansion-4744" to be "Succeeded or Failed"
Dec 14 12:44:01.484: INFO: Pod "var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954": Phase="Pending", Reason="", readiness=false. Elapsed: 17.536333ms
Dec 14 12:44:03.495: INFO: Pod "var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028311961s
Dec 14 12:44:05.504: INFO: Pod "var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036986171s
STEP: Saw pod success
Dec 14 12:44:05.504: INFO: Pod "var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954" satisfied condition "Succeeded or Failed"
Dec 14 12:44:05.507: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954 container dapi-container: <nil>
STEP: delete the pod
Dec 14 12:44:05.530: INFO: Waiting for pod var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954 to disappear
Dec 14 12:44:05.534: INFO: Pod var-expansion-4118b85f-8d9c-49e8-b843-56c23bbac954 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:05.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4744" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":59,"skipped":1124,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:05.550: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 12:44:06.117: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 12:44:08.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082646, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082646, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082646, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082646, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 12:44:11.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:44:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:14.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3283" for this suite.
STEP: Destroying namespace "webhook-3283-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.050 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":60,"skipped":1134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:14.602: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 14 12:44:23.665: INFO: Successfully updated pod "adopt-release-dvl9v"
STEP: Checking that the Job readopts the Pod
Dec 14 12:44:23.665: INFO: Waiting up to 15m0s for pod "adopt-release-dvl9v" in namespace "job-3095" to be "adopted"
Dec 14 12:44:23.671: INFO: Pod "adopt-release-dvl9v": Phase="Running", Reason="", readiness=true. Elapsed: 5.20308ms
Dec 14 12:44:25.682: INFO: Pod "adopt-release-dvl9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.016342296s
Dec 14 12:44:25.682: INFO: Pod "adopt-release-dvl9v" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 14 12:44:26.211: INFO: Successfully updated pod "adopt-release-dvl9v"
STEP: Checking that the Job releases the Pod
Dec 14 12:44:26.211: INFO: Waiting up to 15m0s for pod "adopt-release-dvl9v" in namespace "job-3095" to be "released"
Dec 14 12:44:26.215: INFO: Pod "adopt-release-dvl9v": Phase="Running", Reason="", readiness=true. Elapsed: 3.853378ms
Dec 14 12:44:28.225: INFO: Pod "adopt-release-dvl9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.013574434s
Dec 14 12:44:28.225: INFO: Pod "adopt-release-dvl9v" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:28.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3095" for this suite.

• [SLOW TEST:13.640 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":61,"skipped":1189,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:28.242: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 12:44:28.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458" in namespace "downward-api-5018" to be "Succeeded or Failed"
Dec 14 12:44:28.607: INFO: Pod "downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458": Phase="Pending", Reason="", readiness=false. Elapsed: 5.341374ms
Dec 14 12:44:30.612: INFO: Pod "downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010169195s
Dec 14 12:44:32.621: INFO: Pod "downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019313524s
STEP: Saw pod success
Dec 14 12:44:32.621: INFO: Pod "downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458" satisfied condition "Succeeded or Failed"
Dec 14 12:44:32.625: INFO: Trying to get logs from node ip-10-0-132-177.ec2.internal pod downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458 container client-container: <nil>
STEP: delete the pod
Dec 14 12:44:32.654: INFO: Waiting for pod downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458 to disappear
Dec 14 12:44:32.659: INFO: Pod downwardapi-volume-1b06842b-42c2-42d0-a6f3-123c7009c458 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:32.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5018" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":62,"skipped":1206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:32.673: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Dec 14 12:44:32.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 create -f -'
Dec 14 12:44:33.938: INFO: stderr: ""
Dec 14 12:44:33.938: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 12:44:33.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 12:44:34.083: INFO: stderr: ""
Dec 14 12:44:34.083: INFO: stdout: "update-demo-nautilus-h6xm8 update-demo-nautilus-kc79q "
Dec 14 12:44:34.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods update-demo-nautilus-h6xm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 12:44:34.225: INFO: stderr: ""
Dec 14 12:44:34.225: INFO: stdout: ""
Dec 14 12:44:34.225: INFO: update-demo-nautilus-h6xm8 is created but not running
Dec 14 12:44:39.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 12:44:39.471: INFO: stderr: ""
Dec 14 12:44:39.471: INFO: stdout: "update-demo-nautilus-h6xm8 update-demo-nautilus-kc79q "
Dec 14 12:44:39.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods update-demo-nautilus-h6xm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 12:44:39.661: INFO: stderr: ""
Dec 14 12:44:39.661: INFO: stdout: ""
Dec 14 12:44:39.661: INFO: update-demo-nautilus-h6xm8 is created but not running
Dec 14 12:44:44.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 12:44:44.782: INFO: stderr: ""
Dec 14 12:44:44.782: INFO: stdout: "update-demo-nautilus-h6xm8 update-demo-nautilus-kc79q "
Dec 14 12:44:44.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods update-demo-nautilus-h6xm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 12:44:44.874: INFO: stderr: ""
Dec 14 12:44:44.874: INFO: stdout: "true"
Dec 14 12:44:44.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods update-demo-nautilus-h6xm8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 12:44:44.978: INFO: stderr: ""
Dec 14 12:44:44.978: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 12:44:44.978: INFO: validating pod update-demo-nautilus-h6xm8
Dec 14 12:44:44.985: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 12:44:44.985: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 12:44:44.985: INFO: update-demo-nautilus-h6xm8 is verified up and running
Dec 14 12:44:44.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods update-demo-nautilus-kc79q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 12:44:45.089: INFO: stderr: ""
Dec 14 12:44:45.089: INFO: stdout: "true"
Dec 14 12:44:45.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods update-demo-nautilus-kc79q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 12:44:45.184: INFO: stderr: ""
Dec 14 12:44:45.184: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 12:44:45.184: INFO: validating pod update-demo-nautilus-kc79q
Dec 14 12:44:45.193: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 12:44:45.193: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 12:44:45.193: INFO: update-demo-nautilus-kc79q is verified up and running
STEP: using delete to clean up resources
Dec 14 12:44:45.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 delete --grace-period=0 --force -f -'
Dec 14 12:44:45.303: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 12:44:45.304: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 12:44:45.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get rc,svc -l name=update-demo --no-headers'
Dec 14 12:44:45.416: INFO: stderr: "No resources found in kubectl-5496 namespace.\n"
Dec 14 12:44:45.416: INFO: stdout: ""
Dec 14 12:44:45.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5496 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 12:44:45.559: INFO: stderr: ""
Dec 14 12:44:45.559: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:45.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5496" for this suite.

• [SLOW TEST:12.901 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":63,"skipped":1236,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:45.574: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 12:44:46.425: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 12:44:48.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082686, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082686, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082686, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082686, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 12:44:51.467: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:51.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7699" for this suite.
STEP: Destroying namespace "webhook-7699-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.142 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":64,"skipped":1252,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:51.717: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:44:52.041: INFO: Creating deployment "test-recreate-deployment"
Dec 14 12:44:52.053: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 14 12:44:52.063: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Dec 14 12:44:54.077: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 14 12:44:54.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082692, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082692, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082692, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082692, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:44:56.092: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 14 12:44:56.112: INFO: Updating deployment test-recreate-deployment
Dec 14 12:44:56.112: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Dec 14 12:44:56.250: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3299  a677b072-d56f-41d0-9d6e-7b039287ffa6 56523 2 2021-12-14 12:44:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-12-14 12:44:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-12-14 12:44:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003942998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-12-14 12:44:56 +0000 UTC,LastTransitionTime:2021-12-14 12:44:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-12-14 12:44:56 +0000 UTC,LastTransitionTime:2021-12-14 12:44:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 12:44:56.254: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-3299  656d0f8d-bc55-4353-8992-f64f58acf4b6 56521 1 2021-12-14 12:44:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a677b072-d56f-41d0-9d6e-7b039287ffa6 0xc003663fb0 0xc003663fb1}] []  [{kube-controller-manager Update apps/v1 2021-12-14 12:44:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a677b072-d56f-41d0-9d6e-7b039287ffa6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fa8058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 12:44:56.254: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 14 12:44:56.255: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-3299  ff271a39-9050-4d7f-b006-da6fc2dae162 56511 2 2021-12-14 12:44:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a677b072-d56f-41d0-9d6e-7b039287ffa6 0xc003663eb7 0xc003663eb8}] []  [{kube-controller-manager Update apps/v1 2021-12-14 12:44:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a677b072-d56f-41d0-9d6e-7b039287ffa6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003663f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 12:44:56.258: INFO: Pod "test-recreate-deployment-85d47dcb4-w8cr5" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-w8cr5 test-recreate-deployment-85d47dcb4- deployment-3299  238ec85b-b715-4ae5-bee4-3e5f12e263a9 56522 0 2021-12-14 12:44:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 656d0f8d-bc55-4353-8992-f64f58acf4b6 0xc003fa8b27 0xc003fa8b28}] []  [{kube-controller-manager Update v1 2021-12-14 12:44:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"656d0f8d-bc55-4353-8992-f64f58acf4b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 12:44:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9bgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9bgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c37,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-x4cfd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:44:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:44:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:44:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:44:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 12:44:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:44:56.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3299" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":65,"skipped":1263,"failed":0}
SSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:44:56.280: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:44:56.868: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5766da57-f32e-4982-b29b-a260a426886e" in namespace "security-context-test-3393" to be "Succeeded or Failed"
Dec 14 12:44:56.872: INFO: Pod "alpine-nnp-false-5766da57-f32e-4982-b29b-a260a426886e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078923ms
Dec 14 12:44:58.882: INFO: Pod "alpine-nnp-false-5766da57-f32e-4982-b29b-a260a426886e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014522619s
Dec 14 12:45:00.890: INFO: Pod "alpine-nnp-false-5766da57-f32e-4982-b29b-a260a426886e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02266788s
Dec 14 12:45:02.903: INFO: Pod "alpine-nnp-false-5766da57-f32e-4982-b29b-a260a426886e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035108961s
Dec 14 12:45:04.911: INFO: Pod "alpine-nnp-false-5766da57-f32e-4982-b29b-a260a426886e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.042832024s
Dec 14 12:45:04.911: INFO: Pod "alpine-nnp-false-5766da57-f32e-4982-b29b-a260a426886e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:04.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3393" for this suite.

• [SLOW TEST:8.683 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":66,"skipped":1267,"failed":0}
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:04.963: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8815" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":67,"skipped":1267,"failed":0}

------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:05.206: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:45:05.426: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5046
I1214 12:45:05.468361      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5046, replica count: 1
I1214 12:45:06.521441      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 12:45:07.522345      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 12:45:08.522752      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 12:45:08.645: INFO: Created: latency-svc-6pcmf
Dec 14 12:45:08.661: INFO: Got endpoints: latency-svc-6pcmf [37.793137ms]
Dec 14 12:45:08.684: INFO: Created: latency-svc-84d4h
Dec 14 12:45:08.692: INFO: Created: latency-svc-gfgl6
Dec 14 12:45:08.702: INFO: Got endpoints: latency-svc-84d4h [40.716011ms]
Dec 14 12:45:08.703: INFO: Created: latency-svc-7tvgj
Dec 14 12:45:08.710: INFO: Got endpoints: latency-svc-gfgl6 [46.684676ms]
Dec 14 12:45:08.745: INFO: Got endpoints: latency-svc-7tvgj [81.084882ms]
Dec 14 12:45:08.757: INFO: Created: latency-svc-bkdh9
Dec 14 12:45:08.775: INFO: Got endpoints: latency-svc-bkdh9 [112.660637ms]
Dec 14 12:45:08.784: INFO: Created: latency-svc-ph2c7
Dec 14 12:45:08.796: INFO: Got endpoints: latency-svc-ph2c7 [135.250854ms]
Dec 14 12:45:08.805: INFO: Created: latency-svc-nszfd
Dec 14 12:45:08.820: INFO: Created: latency-svc-pvj6p
Dec 14 12:45:08.824: INFO: Got endpoints: latency-svc-nszfd [160.88423ms]
Dec 14 12:45:08.835: INFO: Got endpoints: latency-svc-pvj6p [171.619963ms]
Dec 14 12:45:08.842: INFO: Created: latency-svc-7zvmc
Dec 14 12:45:08.849: INFO: Created: latency-svc-9wv4l
Dec 14 12:45:08.854: INFO: Got endpoints: latency-svc-7zvmc [191.39752ms]
Dec 14 12:45:08.866: INFO: Created: latency-svc-xvb9x
Dec 14 12:45:08.871: INFO: Got endpoints: latency-svc-9wv4l [208.175388ms]
Dec 14 12:45:08.886: INFO: Created: latency-svc-8tvnz
Dec 14 12:45:08.889: INFO: Got endpoints: latency-svc-xvb9x [225.519372ms]
Dec 14 12:45:08.903: INFO: Got endpoints: latency-svc-8tvnz [239.933074ms]
Dec 14 12:45:08.921: INFO: Created: latency-svc-5rcjm
Dec 14 12:45:08.935: INFO: Got endpoints: latency-svc-5rcjm [271.328713ms]
Dec 14 12:45:08.938: INFO: Created: latency-svc-6jxxd
Dec 14 12:45:08.954: INFO: Created: latency-svc-52cvh
Dec 14 12:45:08.957: INFO: Got endpoints: latency-svc-6jxxd [294.405657ms]
Dec 14 12:45:08.968: INFO: Created: latency-svc-mwgjr
Dec 14 12:45:08.980: INFO: Got endpoints: latency-svc-52cvh [318.321179ms]
Dec 14 12:45:08.985: INFO: Got endpoints: latency-svc-mwgjr [321.64558ms]
Dec 14 12:45:08.992: INFO: Created: latency-svc-jkxst
Dec 14 12:45:09.009: INFO: Created: latency-svc-s7tjc
Dec 14 12:45:09.011: INFO: Got endpoints: latency-svc-jkxst [308.638253ms]
Dec 14 12:45:09.028: INFO: Created: latency-svc-cgj49
Dec 14 12:45:09.031: INFO: Created: latency-svc-6ght4
Dec 14 12:45:09.039: INFO: Got endpoints: latency-svc-s7tjc [329.374397ms]
Dec 14 12:45:09.054: INFO: Got endpoints: latency-svc-6ght4 [279.310152ms]
Dec 14 12:45:09.077: INFO: Created: latency-svc-h2gsg
Dec 14 12:45:09.082: INFO: Got endpoints: latency-svc-cgj49 [336.962304ms]
Dec 14 12:45:09.085: INFO: Created: latency-svc-9qrrw
Dec 14 12:45:09.086: INFO: Got endpoints: latency-svc-h2gsg [289.048645ms]
Dec 14 12:45:09.106: INFO: Got endpoints: latency-svc-9qrrw [281.944186ms]
Dec 14 12:45:09.116: INFO: Created: latency-svc-spfrn
Dec 14 12:45:09.123: INFO: Got endpoints: latency-svc-spfrn [288.869152ms]
Dec 14 12:45:09.124: INFO: Created: latency-svc-n5jvt
Dec 14 12:45:09.140: INFO: Got endpoints: latency-svc-n5jvt [286.115432ms]
Dec 14 12:45:09.146: INFO: Created: latency-svc-ssv6c
Dec 14 12:45:09.162: INFO: Got endpoints: latency-svc-ssv6c [290.314144ms]
Dec 14 12:45:09.164: INFO: Created: latency-svc-n8nmz
Dec 14 12:45:09.185: INFO: Got endpoints: latency-svc-n8nmz [295.701024ms]
Dec 14 12:45:09.186: INFO: Created: latency-svc-khl26
Dec 14 12:45:09.191: INFO: Created: latency-svc-q99fm
Dec 14 12:45:09.198: INFO: Got endpoints: latency-svc-khl26 [294.12394ms]
Dec 14 12:45:09.202: INFO: Got endpoints: latency-svc-q99fm [266.9505ms]
Dec 14 12:45:09.216: INFO: Created: latency-svc-5zjs9
Dec 14 12:45:09.229: INFO: Created: latency-svc-d5xfm
Dec 14 12:45:09.230: INFO: Got endpoints: latency-svc-5zjs9 [272.976688ms]
Dec 14 12:45:09.236: INFO: Got endpoints: latency-svc-d5xfm [255.580457ms]
Dec 14 12:45:09.247: INFO: Created: latency-svc-l5bpm
Dec 14 12:45:09.257: INFO: Created: latency-svc-xbjpd
Dec 14 12:45:09.262: INFO: Got endpoints: latency-svc-l5bpm [276.699683ms]
Dec 14 12:45:09.270: INFO: Got endpoints: latency-svc-xbjpd [259.115878ms]
Dec 14 12:45:09.279: INFO: Created: latency-svc-xmqwb
Dec 14 12:45:09.285: INFO: Created: latency-svc-655cc
Dec 14 12:45:09.296: INFO: Got endpoints: latency-svc-xmqwb [256.584041ms]
Dec 14 12:45:09.303: INFO: Got endpoints: latency-svc-655cc [248.879293ms]
Dec 14 12:45:09.311: INFO: Created: latency-svc-kfpk2
Dec 14 12:45:09.329: INFO: Created: latency-svc-c5x9x
Dec 14 12:45:09.341: INFO: Got endpoints: latency-svc-kfpk2 [258.96991ms]
Dec 14 12:45:09.343: INFO: Got endpoints: latency-svc-c5x9x [257.13019ms]
Dec 14 12:45:09.349: INFO: Created: latency-svc-slfk4
Dec 14 12:45:09.367: INFO: Created: latency-svc-xklwk
Dec 14 12:45:09.370: INFO: Got endpoints: latency-svc-slfk4 [264.096409ms]
Dec 14 12:45:09.382: INFO: Created: latency-svc-nf2fq
Dec 14 12:45:09.384: INFO: Created: latency-svc-fwr69
Dec 14 12:45:09.403: INFO: Got endpoints: latency-svc-xklwk [279.754697ms]
Dec 14 12:45:09.404: INFO: Created: latency-svc-zrxnz
Dec 14 12:45:09.409: INFO: Got endpoints: latency-svc-nf2fq [268.619419ms]
Dec 14 12:45:09.409: INFO: Got endpoints: latency-svc-fwr69 [247.192962ms]
Dec 14 12:45:09.417: INFO: Created: latency-svc-5hr4t
Dec 14 12:45:09.419: INFO: Got endpoints: latency-svc-zrxnz [234.526211ms]
Dec 14 12:45:09.428: INFO: Got endpoints: latency-svc-5hr4t [230.787593ms]
Dec 14 12:45:09.434: INFO: Created: latency-svc-xmpr7
Dec 14 12:45:09.454: INFO: Got endpoints: latency-svc-xmpr7 [252.117151ms]
Dec 14 12:45:09.458: INFO: Created: latency-svc-mb6qk
Dec 14 12:45:09.466: INFO: Got endpoints: latency-svc-mb6qk [235.881625ms]
Dec 14 12:45:09.468: INFO: Created: latency-svc-2f4vq
Dec 14 12:45:09.477: INFO: Created: latency-svc-l7hh9
Dec 14 12:45:09.488: INFO: Created: latency-svc-r58sf
Dec 14 12:45:09.491: INFO: Got endpoints: latency-svc-l7hh9 [228.8849ms]
Dec 14 12:45:09.492: INFO: Got endpoints: latency-svc-2f4vq [255.854812ms]
Dec 14 12:45:09.502: INFO: Created: latency-svc-xglzd
Dec 14 12:45:09.525: INFO: Got endpoints: latency-svc-xglzd [229.393135ms]
Dec 14 12:45:09.526: INFO: Got endpoints: latency-svc-r58sf [255.675788ms]
Dec 14 12:45:09.527: INFO: Created: latency-svc-8lrmz
Dec 14 12:45:09.537: INFO: Created: latency-svc-xz4r8
Dec 14 12:45:09.545: INFO: Got endpoints: latency-svc-8lrmz [242.027177ms]
Dec 14 12:45:09.551: INFO: Created: latency-svc-fjg4j
Dec 14 12:45:09.556: INFO: Got endpoints: latency-svc-xz4r8 [215.313029ms]
Dec 14 12:45:09.565: INFO: Created: latency-svc-pjc8n
Dec 14 12:45:09.567: INFO: Got endpoints: latency-svc-fjg4j [223.804057ms]
Dec 14 12:45:09.583: INFO: Got endpoints: latency-svc-pjc8n [212.509354ms]
Dec 14 12:45:09.591: INFO: Created: latency-svc-wc9rd
Dec 14 12:45:09.599: INFO: Got endpoints: latency-svc-wc9rd [195.787371ms]
Dec 14 12:45:09.600: INFO: Created: latency-svc-bk9v8
Dec 14 12:45:09.614: INFO: Got endpoints: latency-svc-bk9v8 [205.12143ms]
Dec 14 12:45:09.620: INFO: Created: latency-svc-8h8d4
Dec 14 12:45:09.628: INFO: Created: latency-svc-bq2kj
Dec 14 12:45:09.640: INFO: Got endpoints: latency-svc-8h8d4 [231.260137ms]
Dec 14 12:45:09.642: INFO: Created: latency-svc-ph68g
Dec 14 12:45:09.647: INFO: Got endpoints: latency-svc-bq2kj [227.699068ms]
Dec 14 12:45:09.653: INFO: Got endpoints: latency-svc-ph68g [224.1413ms]
Dec 14 12:45:09.653: INFO: Created: latency-svc-f7sd5
Dec 14 12:45:09.664: INFO: Got endpoints: latency-svc-f7sd5 [209.569156ms]
Dec 14 12:45:09.668: INFO: Created: latency-svc-lgb2g
Dec 14 12:45:09.674: INFO: Created: latency-svc-dbn4l
Dec 14 12:45:09.676: INFO: Got endpoints: latency-svc-lgb2g [210.012223ms]
Dec 14 12:45:09.685: INFO: Got endpoints: latency-svc-dbn4l [193.405894ms]
Dec 14 12:45:09.687: INFO: Created: latency-svc-twlwm
Dec 14 12:45:09.698: INFO: Got endpoints: latency-svc-twlwm [206.165417ms]
Dec 14 12:45:09.700: INFO: Created: latency-svc-b642w
Dec 14 12:45:09.711: INFO: Created: latency-svc-gl42f
Dec 14 12:45:09.713: INFO: Got endpoints: latency-svc-b642w [187.765691ms]
Dec 14 12:45:09.721: INFO: Got endpoints: latency-svc-gl42f [195.814396ms]
Dec 14 12:45:09.724: INFO: Created: latency-svc-b7vhp
Dec 14 12:45:09.734: INFO: Created: latency-svc-tjxpq
Dec 14 12:45:09.739: INFO: Got endpoints: latency-svc-b7vhp [194.553553ms]
Dec 14 12:45:09.743: INFO: Created: latency-svc-npgv2
Dec 14 12:45:09.745: INFO: Got endpoints: latency-svc-tjxpq [188.448039ms]
Dec 14 12:45:09.755: INFO: Got endpoints: latency-svc-npgv2 [188.050347ms]
Dec 14 12:45:09.765: INFO: Created: latency-svc-xq5dq
Dec 14 12:45:09.765: INFO: Created: latency-svc-q82fx
Dec 14 12:45:09.773: INFO: Got endpoints: latency-svc-q82fx [174.200617ms]
Dec 14 12:45:09.774: INFO: Got endpoints: latency-svc-xq5dq [191.611962ms]
Dec 14 12:45:09.785: INFO: Created: latency-svc-rf7c9
Dec 14 12:45:09.791: INFO: Created: latency-svc-7npgc
Dec 14 12:45:09.793: INFO: Got endpoints: latency-svc-rf7c9 [178.773769ms]
Dec 14 12:45:09.805: INFO: Created: latency-svc-gwxgg
Dec 14 12:45:09.807: INFO: Got endpoints: latency-svc-7npgc [166.963852ms]
Dec 14 12:45:09.820: INFO: Got endpoints: latency-svc-gwxgg [172.643105ms]
Dec 14 12:45:09.821: INFO: Created: latency-svc-4tm6x
Dec 14 12:45:09.833: INFO: Created: latency-svc-xh2wt
Dec 14 12:45:09.836: INFO: Got endpoints: latency-svc-4tm6x [183.311009ms]
Dec 14 12:45:09.842: INFO: Created: latency-svc-q5jvd
Dec 14 12:45:09.846: INFO: Got endpoints: latency-svc-xh2wt [182.826175ms]
Dec 14 12:45:09.859: INFO: Got endpoints: latency-svc-q5jvd [182.832695ms]
Dec 14 12:45:09.867: INFO: Created: latency-svc-qnwzd
Dec 14 12:45:09.873: INFO: Created: latency-svc-br4z4
Dec 14 12:45:09.878: INFO: Got endpoints: latency-svc-qnwzd [193.105541ms]
Dec 14 12:45:09.890: INFO: Got endpoints: latency-svc-br4z4 [192.346762ms]
Dec 14 12:45:09.891: INFO: Created: latency-svc-8f8hh
Dec 14 12:45:09.904: INFO: Created: latency-svc-crhhl
Dec 14 12:45:09.905: INFO: Got endpoints: latency-svc-8f8hh [191.815039ms]
Dec 14 12:45:09.919: INFO: Created: latency-svc-w7tnh
Dec 14 12:45:09.923: INFO: Got endpoints: latency-svc-crhhl [202.35757ms]
Dec 14 12:45:09.926: INFO: Created: latency-svc-p8jbv
Dec 14 12:45:09.927: INFO: Got endpoints: latency-svc-w7tnh [187.820279ms]
Dec 14 12:45:09.942: INFO: Got endpoints: latency-svc-p8jbv [197.319137ms]
Dec 14 12:45:09.948: INFO: Created: latency-svc-4wnwh
Dec 14 12:45:09.959: INFO: Created: latency-svc-4b6ch
Dec 14 12:45:09.963: INFO: Got endpoints: latency-svc-4wnwh [208.798765ms]
Dec 14 12:45:09.974: INFO: Got endpoints: latency-svc-4b6ch [200.257342ms]
Dec 14 12:45:09.978: INFO: Created: latency-svc-42n4b
Dec 14 12:45:09.980: INFO: Created: latency-svc-5fwvz
Dec 14 12:45:09.990: INFO: Created: latency-svc-hshkg
Dec 14 12:45:09.993: INFO: Got endpoints: latency-svc-42n4b [218.627666ms]
Dec 14 12:45:09.999: INFO: Created: latency-svc-qrdqr
Dec 14 12:45:10.015: INFO: Created: latency-svc-ckqh9
Dec 14 12:45:10.021: INFO: Created: latency-svc-ww4s2
Dec 14 12:45:10.036: INFO: Got endpoints: latency-svc-5fwvz [243.366681ms]
Dec 14 12:45:10.041: INFO: Created: latency-svc-pj8xj
Dec 14 12:45:10.044: INFO: Created: latency-svc-m2ds4
Dec 14 12:45:10.063: INFO: Created: latency-svc-p2gmp
Dec 14 12:45:10.063: INFO: Created: latency-svc-hxdpn
Dec 14 12:45:10.077: INFO: Created: latency-svc-fb2pf
Dec 14 12:45:10.082: INFO: Got endpoints: latency-svc-qrdqr [262.664845ms]
Dec 14 12:45:10.087: INFO: Got endpoints: latency-svc-ckqh9 [94.006688ms]
Dec 14 12:45:10.088: INFO: Created: latency-svc-nvr4p
Dec 14 12:45:10.090: INFO: Got endpoints: latency-svc-ww4s2 [254.090734ms]
Dec 14 12:45:10.092: INFO: Got endpoints: latency-svc-hshkg [284.521933ms]
Dec 14 12:45:10.119: INFO: Created: latency-svc-ml4j5
Dec 14 12:45:10.122: INFO: Got endpoints: latency-svc-hxdpn [244.031999ms]
Dec 14 12:45:10.122: INFO: Got endpoints: latency-svc-pj8xj [275.13913ms]
Dec 14 12:45:10.127: INFO: Got endpoints: latency-svc-p2gmp [236.551776ms]
Dec 14 12:45:10.130: INFO: Got endpoints: latency-svc-m2ds4 [271.44369ms]
Dec 14 12:45:10.135: INFO: Got endpoints: latency-svc-fb2pf [229.530396ms]
Dec 14 12:45:10.137: INFO: Created: latency-svc-jznws
Dec 14 12:45:10.138: INFO: Got endpoints: latency-svc-ml4j5 [195.635206ms]
Dec 14 12:45:10.139: INFO: Got endpoints: latency-svc-nvr4p [215.061758ms]
Dec 14 12:45:10.144: INFO: Got endpoints: latency-svc-jznws [216.312755ms]
Dec 14 12:45:10.157: INFO: Created: latency-svc-9q9bz
Dec 14 12:45:10.164: INFO: Created: latency-svc-x7n5r
Dec 14 12:45:10.166: INFO: Got endpoints: latency-svc-9q9bz [202.138875ms]
Dec 14 12:45:10.174: INFO: Got endpoints: latency-svc-x7n5r [200.781708ms]
Dec 14 12:45:10.185: INFO: Created: latency-svc-mz4sx
Dec 14 12:45:10.194: INFO: Created: latency-svc-mmmf5
Dec 14 12:45:10.205: INFO: Got endpoints: latency-svc-mz4sx [168.581607ms]
Dec 14 12:45:10.210: INFO: Created: latency-svc-ph42t
Dec 14 12:45:10.211: INFO: Got endpoints: latency-svc-mmmf5 [128.549924ms]
Dec 14 12:45:10.221: INFO: Created: latency-svc-p8k9g
Dec 14 12:45:10.230: INFO: Got endpoints: latency-svc-ph42t [142.981457ms]
Dec 14 12:45:10.242: INFO: Created: latency-svc-shk9h
Dec 14 12:45:10.244: INFO: Created: latency-svc-qjlts
Dec 14 12:45:10.263: INFO: Got endpoints: latency-svc-p8k9g [173.193718ms]
Dec 14 12:45:10.274: INFO: Created: latency-svc-zqskl
Dec 14 12:45:10.279: INFO: Got endpoints: latency-svc-shk9h [186.891967ms]
Dec 14 12:45:10.291: INFO: Got endpoints: latency-svc-qjlts [169.037311ms]
Dec 14 12:45:10.291: INFO: Created: latency-svc-mcsgc
Dec 14 12:45:10.313: INFO: Created: latency-svc-5ztwc
Dec 14 12:45:10.324: INFO: Created: latency-svc-nlf6k
Dec 14 12:45:10.340: INFO: Created: latency-svc-fs4kb
Dec 14 12:45:10.343: INFO: Created: latency-svc-m2smx
Dec 14 12:45:10.360: INFO: Created: latency-svc-g8ttk
Dec 14 12:45:10.363: INFO: Created: latency-svc-vcdmk
Dec 14 12:45:10.371: INFO: Got endpoints: latency-svc-zqskl [248.430089ms]
Dec 14 12:45:10.372: INFO: Got endpoints: latency-svc-mcsgc [244.662759ms]
Dec 14 12:45:10.380: INFO: Got endpoints: latency-svc-5ztwc [250.287554ms]
Dec 14 12:45:10.384: INFO: Got endpoints: latency-svc-nlf6k [249.536491ms]
Dec 14 12:45:10.385: INFO: Created: latency-svc-4fd8h
Dec 14 12:45:10.392: INFO: Created: latency-svc-78kh2
Dec 14 12:45:10.411: INFO: Got endpoints: latency-svc-fs4kb [273.460935ms]
Dec 14 12:45:10.417: INFO: Created: latency-svc-8hbmc
Dec 14 12:45:10.417: INFO: Got endpoints: latency-svc-m2smx [278.109456ms]
Dec 14 12:45:10.417: INFO: Got endpoints: latency-svc-vcdmk [273.304528ms]
Dec 14 12:45:10.417: INFO: Got endpoints: latency-svc-4fd8h [242.725236ms]
Dec 14 12:45:10.420: INFO: Created: latency-svc-86fmq
Dec 14 12:45:10.427: INFO: Got endpoints: latency-svc-g8ttk [261.514916ms]
Dec 14 12:45:10.437: INFO: Created: latency-svc-b4285
Dec 14 12:45:10.444: INFO: Created: latency-svc-7q7sq
Dec 14 12:45:10.447: INFO: Got endpoints: latency-svc-78kh2 [241.659653ms]
Dec 14 12:45:10.451: INFO: Got endpoints: latency-svc-8hbmc [220.564589ms]
Dec 14 12:45:10.453: INFO: Got endpoints: latency-svc-86fmq [241.825779ms]
Dec 14 12:45:10.526: INFO: Got endpoints: latency-svc-b4285 [262.862037ms]
Dec 14 12:45:10.530: INFO: Got endpoints: latency-svc-7q7sq [250.576211ms]
Dec 14 12:45:10.547: INFO: Created: latency-svc-4x55r
Dec 14 12:45:10.554: INFO: Created: latency-svc-sxcsj
Dec 14 12:45:10.564: INFO: Got endpoints: latency-svc-sxcsj [272.857229ms]
Dec 14 12:45:10.577: INFO: Got endpoints: latency-svc-4x55r [206.295718ms]
Dec 14 12:45:10.582: INFO: Created: latency-svc-5bkfd
Dec 14 12:45:10.589: INFO: Created: latency-svc-7dndd
Dec 14 12:45:10.604: INFO: Got endpoints: latency-svc-5bkfd [232.352337ms]
Dec 14 12:45:10.612: INFO: Created: latency-svc-fgqg9
Dec 14 12:45:10.625: INFO: Got endpoints: latency-svc-fgqg9 [214.100784ms]
Dec 14 12:45:10.634: INFO: Got endpoints: latency-svc-7dndd [253.154747ms]
Dec 14 12:45:10.638: INFO: Created: latency-svc-sdpkr
Dec 14 12:45:10.652: INFO: Got endpoints: latency-svc-sdpkr [267.36069ms]
Dec 14 12:45:10.656: INFO: Created: latency-svc-zsp7k
Dec 14 12:45:10.665: INFO: Created: latency-svc-x6x8n
Dec 14 12:45:10.671: INFO: Got endpoints: latency-svc-zsp7k [254.324361ms]
Dec 14 12:45:10.677: INFO: Created: latency-svc-d94wm
Dec 14 12:45:10.684: INFO: Got endpoints: latency-svc-d94wm [266.664438ms]
Dec 14 12:45:10.685: INFO: Got endpoints: latency-svc-x6x8n [267.822089ms]
Dec 14 12:45:10.687: INFO: Created: latency-svc-tjh8w
Dec 14 12:45:10.706: INFO: Got endpoints: latency-svc-tjh8w [278.338963ms]
Dec 14 12:45:10.714: INFO: Created: latency-svc-4v289
Dec 14 12:45:10.716: INFO: Created: latency-svc-k64ws
Dec 14 12:45:10.743: INFO: Got endpoints: latency-svc-k64ws [291.962163ms]
Dec 14 12:45:10.743: INFO: Got endpoints: latency-svc-4v289 [296.5002ms]
Dec 14 12:45:10.763: INFO: Created: latency-svc-pbqkc
Dec 14 12:45:10.779: INFO: Got endpoints: latency-svc-pbqkc [325.920428ms]
Dec 14 12:45:10.782: INFO: Created: latency-svc-2f7qc
Dec 14 12:45:10.793: INFO: Got endpoints: latency-svc-2f7qc [267.114647ms]
Dec 14 12:45:10.809: INFO: Created: latency-svc-kn9hl
Dec 14 12:45:10.812: INFO: Created: latency-svc-7bxpk
Dec 14 12:45:10.828: INFO: Got endpoints: latency-svc-kn9hl [298.152376ms]
Dec 14 12:45:10.830: INFO: Created: latency-svc-pgmdc
Dec 14 12:45:10.833: INFO: Got endpoints: latency-svc-7bxpk [269.163631ms]
Dec 14 12:45:10.842: INFO: Created: latency-svc-hlzrw
Dec 14 12:45:10.854: INFO: Got endpoints: latency-svc-hlzrw [250.297626ms]
Dec 14 12:45:10.855: INFO: Got endpoints: latency-svc-pgmdc [277.933687ms]
Dec 14 12:45:10.866: INFO: Created: latency-svc-shkdw
Dec 14 12:45:10.879: INFO: Created: latency-svc-vgdxf
Dec 14 12:45:10.882: INFO: Got endpoints: latency-svc-shkdw [256.549886ms]
Dec 14 12:45:10.892: INFO: Got endpoints: latency-svc-vgdxf [257.912967ms]
Dec 14 12:45:10.902: INFO: Created: latency-svc-bnx7n
Dec 14 12:45:10.911: INFO: Got endpoints: latency-svc-bnx7n [258.777432ms]
Dec 14 12:45:10.912: INFO: Created: latency-svc-f2ktq
Dec 14 12:45:10.930: INFO: Created: latency-svc-2w4tj
Dec 14 12:45:10.935: INFO: Got endpoints: latency-svc-f2ktq [263.444727ms]
Dec 14 12:45:10.944: INFO: Created: latency-svc-hhrwj
Dec 14 12:45:10.955: INFO: Created: latency-svc-q7jm2
Dec 14 12:45:10.959: INFO: Got endpoints: latency-svc-2w4tj [273.88577ms]
Dec 14 12:45:10.965: INFO: Got endpoints: latency-svc-hhrwj [280.983311ms]
Dec 14 12:45:10.974: INFO: Created: latency-svc-7h26v
Dec 14 12:45:10.986: INFO: Got endpoints: latency-svc-q7jm2 [280.154491ms]
Dec 14 12:45:10.991: INFO: Created: latency-svc-kjmkn
Dec 14 12:45:11.013: INFO: Got endpoints: latency-svc-7h26v [270.263247ms]
Dec 14 12:45:11.016: INFO: Created: latency-svc-5rtzm
Dec 14 12:45:11.022: INFO: Got endpoints: latency-svc-kjmkn [279.338477ms]
Dec 14 12:45:11.025: INFO: Created: latency-svc-jt5ns
Dec 14 12:45:11.034: INFO: Got endpoints: latency-svc-5rtzm [255.635712ms]
Dec 14 12:45:11.040: INFO: Created: latency-svc-zb922
Dec 14 12:45:11.049: INFO: Created: latency-svc-qw766
Dec 14 12:45:11.056: INFO: Got endpoints: latency-svc-jt5ns [262.893ms]
Dec 14 12:45:11.057: INFO: Got endpoints: latency-svc-zb922 [229.285849ms]
Dec 14 12:45:11.075: INFO: Got endpoints: latency-svc-qw766 [242.519684ms]
Dec 14 12:45:11.077: INFO: Created: latency-svc-ps2v9
Dec 14 12:45:11.089: INFO: Created: latency-svc-nq5c5
Dec 14 12:45:11.093: INFO: Got endpoints: latency-svc-ps2v9 [238.005109ms]
Dec 14 12:45:11.100: INFO: Created: latency-svc-794ms
Dec 14 12:45:11.109: INFO: Created: latency-svc-8jshx
Dec 14 12:45:11.120: INFO: Got endpoints: latency-svc-nq5c5 [265.421391ms]
Dec 14 12:45:11.123: INFO: Created: latency-svc-twnln
Dec 14 12:45:11.133: INFO: Got endpoints: latency-svc-8jshx [240.919377ms]
Dec 14 12:45:11.135: INFO: Got endpoints: latency-svc-794ms [252.755067ms]
Dec 14 12:45:11.137: INFO: Created: latency-svc-j5jk9
Dec 14 12:45:11.152: INFO: Got endpoints: latency-svc-j5jk9 [217.090449ms]
Dec 14 12:45:11.155: INFO: Created: latency-svc-gzmbl
Dec 14 12:45:11.160: INFO: Got endpoints: latency-svc-twnln [249.269832ms]
Dec 14 12:45:11.169: INFO: Created: latency-svc-vtmjs
Dec 14 12:45:11.176: INFO: Created: latency-svc-sr4hx
Dec 14 12:45:11.183: INFO: Created: latency-svc-5dv5b
Dec 14 12:45:11.194: INFO: Created: latency-svc-l6p74
Dec 14 12:45:11.211: INFO: Got endpoints: latency-svc-sr4hx [224.759571ms]
Dec 14 12:45:11.211: INFO: Created: latency-svc-tvjlk
Dec 14 12:45:11.220: INFO: Got endpoints: latency-svc-vtmjs [255.105844ms]
Dec 14 12:45:11.226: INFO: Got endpoints: latency-svc-gzmbl [266.977413ms]
Dec 14 12:45:11.228: INFO: Got endpoints: latency-svc-5dv5b [214.687046ms]
Dec 14 12:45:11.229: INFO: Got endpoints: latency-svc-l6p74 [206.680272ms]
Dec 14 12:45:11.233: INFO: Created: latency-svc-lzb2q
Dec 14 12:45:11.242: INFO: Created: latency-svc-qrw6z
Dec 14 12:45:11.255: INFO: Created: latency-svc-htq2l
Dec 14 12:45:11.266: INFO: Got endpoints: latency-svc-tvjlk [231.225799ms]
Dec 14 12:45:11.269: INFO: Got endpoints: latency-svc-lzb2q [212.642107ms]
Dec 14 12:45:11.273: INFO: Got endpoints: latency-svc-qrw6z [215.910268ms]
Dec 14 12:45:11.279: INFO: Got endpoints: latency-svc-htq2l [203.170259ms]
Dec 14 12:45:11.294: INFO: Created: latency-svc-6xfxk
Dec 14 12:45:11.306: INFO: Created: latency-svc-9v6cf
Dec 14 12:45:11.313: INFO: Created: latency-svc-n2wvk
Dec 14 12:45:11.314: INFO: Got endpoints: latency-svc-6xfxk [221.541123ms]
Dec 14 12:45:11.321: INFO: Got endpoints: latency-svc-9v6cf [200.474539ms]
Dec 14 12:45:11.326: INFO: Created: latency-svc-mtxmj
Dec 14 12:45:11.336: INFO: Created: latency-svc-7pqnt
Dec 14 12:45:11.337: INFO: Got endpoints: latency-svc-n2wvk [204.493313ms]
Dec 14 12:45:11.345: INFO: Got endpoints: latency-svc-mtxmj [209.990732ms]
Dec 14 12:45:11.372: INFO: Got endpoints: latency-svc-7pqnt [220.080292ms]
Dec 14 12:45:11.375: INFO: Created: latency-svc-6q5kv
Dec 14 12:45:11.384: INFO: Created: latency-svc-dn8lg
Dec 14 12:45:11.392: INFO: Created: latency-svc-qfbxw
Dec 14 12:45:11.396: INFO: Got endpoints: latency-svc-dn8lg [185.570907ms]
Dec 14 12:45:11.401: INFO: Got endpoints: latency-svc-6q5kv [240.831493ms]
Dec 14 12:45:11.408: INFO: Got endpoints: latency-svc-qfbxw [187.607321ms]
Dec 14 12:45:11.426: INFO: Created: latency-svc-ntrcr
Dec 14 12:45:11.446: INFO: Created: latency-svc-ffhx7
Dec 14 12:45:11.455: INFO: Created: latency-svc-rvrnp
Dec 14 12:45:11.465: INFO: Created: latency-svc-rs75f
Dec 14 12:45:11.470: INFO: Got endpoints: latency-svc-ffhx7 [242.459924ms]
Dec 14 12:45:11.474: INFO: Created: latency-svc-6c849
Dec 14 12:45:11.474: INFO: Got endpoints: latency-svc-ntrcr [248.606306ms]
Dec 14 12:45:11.505: INFO: Got endpoints: latency-svc-rvrnp [275.336246ms]
Dec 14 12:45:11.505: INFO: Got endpoints: latency-svc-rs75f [236.152913ms]
Dec 14 12:45:11.511: INFO: Created: latency-svc-pbw2x
Dec 14 12:45:11.514: INFO: Got endpoints: latency-svc-6c849 [248.082492ms]
Dec 14 12:45:11.521: INFO: Created: latency-svc-mcgdf
Dec 14 12:45:11.528: INFO: Created: latency-svc-zdcpm
Dec 14 12:45:11.528: INFO: Got endpoints: latency-svc-pbw2x [255.403979ms]
Dec 14 12:45:11.541: INFO: Got endpoints: latency-svc-mcgdf [261.833824ms]
Dec 14 12:45:11.547: INFO: Got endpoints: latency-svc-zdcpm [232.513746ms]
Dec 14 12:45:11.551: INFO: Created: latency-svc-xm7l7
Dec 14 12:45:11.577: INFO: Created: latency-svc-vhnmk
Dec 14 12:45:11.579: INFO: Got endpoints: latency-svc-xm7l7 [258.395311ms]
Dec 14 12:45:11.580: INFO: Got endpoints: latency-svc-vhnmk [243.246875ms]
Dec 14 12:45:11.590: INFO: Created: latency-svc-76r9p
Dec 14 12:45:11.601: INFO: Created: latency-svc-tgb7d
Dec 14 12:45:11.611: INFO: Created: latency-svc-c99sk
Dec 14 12:45:11.621: INFO: Created: latency-svc-snwtm
Dec 14 12:45:11.625: INFO: Got endpoints: latency-svc-c99sk [228.449067ms]
Dec 14 12:45:11.626: INFO: Created: latency-svc-r2jkt
Dec 14 12:45:11.628: INFO: Got endpoints: latency-svc-76r9p [283.550761ms]
Dec 14 12:45:11.636: INFO: Got endpoints: latency-svc-tgb7d [264.268597ms]
Dec 14 12:45:11.640: INFO: Created: latency-svc-dd6ff
Dec 14 12:45:11.641: INFO: Got endpoints: latency-svc-r2jkt [233.514435ms]
Dec 14 12:45:11.652: INFO: Created: latency-svc-szjgq
Dec 14 12:45:11.658: INFO: Got endpoints: latency-svc-snwtm [257.387351ms]
Dec 14 12:45:11.669: INFO: Created: latency-svc-zm9tx
Dec 14 12:45:11.681: INFO: Got endpoints: latency-svc-szjgq [206.764184ms]
Dec 14 12:45:11.685: INFO: Created: latency-svc-h7hjw
Dec 14 12:45:11.700: INFO: Created: latency-svc-lj5m9
Dec 14 12:45:11.702: INFO: Got endpoints: latency-svc-dd6ff [231.570734ms]
Dec 14 12:45:11.719: INFO: Got endpoints: latency-svc-h7hjw [213.352322ms]
Dec 14 12:45:11.722: INFO: Got endpoints: latency-svc-lj5m9 [207.820655ms]
Dec 14 12:45:11.723: INFO: Got endpoints: latency-svc-zm9tx [218.019381ms]
Dec 14 12:45:11.724: INFO: Created: latency-svc-nd6ql
Dec 14 12:45:11.748: INFO: Got endpoints: latency-svc-nd6ql [219.738387ms]
Dec 14 12:45:11.758: INFO: Created: latency-svc-cjdhc
Dec 14 12:45:11.760: INFO: Created: latency-svc-m2t8n
Dec 14 12:45:11.765: INFO: Got endpoints: latency-svc-cjdhc [224.662147ms]
Dec 14 12:45:11.771: INFO: Created: latency-svc-xlvpv
Dec 14 12:45:11.784: INFO: Created: latency-svc-t88qw
Dec 14 12:45:11.784: INFO: Got endpoints: latency-svc-m2t8n [237.554605ms]
Dec 14 12:45:11.802: INFO: Got endpoints: latency-svc-xlvpv [222.737477ms]
Dec 14 12:45:11.806: INFO: Got endpoints: latency-svc-t88qw [225.407741ms]
Dec 14 12:45:11.810: INFO: Created: latency-svc-x8r98
Dec 14 12:45:11.819: INFO: Created: latency-svc-fxrt8
Dec 14 12:45:11.834: INFO: Created: latency-svc-7bbzf
Dec 14 12:45:11.842: INFO: Got endpoints: latency-svc-fxrt8 [213.499654ms]
Dec 14 12:45:11.848: INFO: Got endpoints: latency-svc-7bbzf [211.639629ms]
Dec 14 12:45:11.848: INFO: Got endpoints: latency-svc-x8r98 [223.129003ms]
Dec 14 12:45:11.854: INFO: Created: latency-svc-74hjz
Dec 14 12:45:11.866: INFO: Got endpoints: latency-svc-74hjz [224.490485ms]
Dec 14 12:45:11.866: INFO: Latencies: [40.716011ms 46.684676ms 81.084882ms 94.006688ms 112.660637ms 128.549924ms 135.250854ms 142.981457ms 160.88423ms 166.963852ms 168.581607ms 169.037311ms 171.619963ms 172.643105ms 173.193718ms 174.200617ms 178.773769ms 182.826175ms 182.832695ms 183.311009ms 185.570907ms 186.891967ms 187.607321ms 187.765691ms 187.820279ms 188.050347ms 188.448039ms 191.39752ms 191.611962ms 191.815039ms 192.346762ms 193.105541ms 193.405894ms 194.553553ms 195.635206ms 195.787371ms 195.814396ms 197.319137ms 200.257342ms 200.474539ms 200.781708ms 202.138875ms 202.35757ms 203.170259ms 204.493313ms 205.12143ms 206.165417ms 206.295718ms 206.680272ms 206.764184ms 207.820655ms 208.175388ms 208.798765ms 209.569156ms 209.990732ms 210.012223ms 211.639629ms 212.509354ms 212.642107ms 213.352322ms 213.499654ms 214.100784ms 214.687046ms 215.061758ms 215.313029ms 215.910268ms 216.312755ms 217.090449ms 218.019381ms 218.627666ms 219.738387ms 220.080292ms 220.564589ms 221.541123ms 222.737477ms 223.129003ms 223.804057ms 224.1413ms 224.490485ms 224.662147ms 224.759571ms 225.407741ms 225.519372ms 227.699068ms 228.449067ms 228.8849ms 229.285849ms 229.393135ms 229.530396ms 230.787593ms 231.225799ms 231.260137ms 231.570734ms 232.352337ms 232.513746ms 233.514435ms 234.526211ms 235.881625ms 236.152913ms 236.551776ms 237.554605ms 238.005109ms 239.933074ms 240.831493ms 240.919377ms 241.659653ms 241.825779ms 242.027177ms 242.459924ms 242.519684ms 242.725236ms 243.246875ms 243.366681ms 244.031999ms 244.662759ms 247.192962ms 248.082492ms 248.430089ms 248.606306ms 248.879293ms 249.269832ms 249.536491ms 250.287554ms 250.297626ms 250.576211ms 252.117151ms 252.755067ms 253.154747ms 254.090734ms 254.324361ms 255.105844ms 255.403979ms 255.580457ms 255.635712ms 255.675788ms 255.854812ms 256.549886ms 256.584041ms 257.13019ms 257.387351ms 257.912967ms 258.395311ms 258.777432ms 258.96991ms 259.115878ms 261.514916ms 261.833824ms 262.664845ms 262.862037ms 262.893ms 263.444727ms 264.096409ms 264.268597ms 265.421391ms 266.664438ms 266.9505ms 266.977413ms 267.114647ms 267.36069ms 267.822089ms 268.619419ms 269.163631ms 270.263247ms 271.328713ms 271.44369ms 272.857229ms 272.976688ms 273.304528ms 273.460935ms 273.88577ms 275.13913ms 275.336246ms 276.699683ms 277.933687ms 278.109456ms 278.338963ms 279.310152ms 279.338477ms 279.754697ms 280.154491ms 280.983311ms 281.944186ms 283.550761ms 284.521933ms 286.115432ms 288.869152ms 289.048645ms 290.314144ms 291.962163ms 294.12394ms 294.405657ms 295.701024ms 296.5002ms 298.152376ms 308.638253ms 318.321179ms 321.64558ms 325.920428ms 329.374397ms 336.962304ms]
Dec 14 12:45:11.866: INFO: 50 %ile: 237.554605ms
Dec 14 12:45:11.866: INFO: 90 %ile: 280.983311ms
Dec 14 12:45:11.866: INFO: 99 %ile: 329.374397ms
Dec 14 12:45:11.866: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:11.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5046" for this suite.

• [SLOW TEST:6.685 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":68,"skipped":1267,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:11.891: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-5d252e76-d892-401f-a41a-5c09f2839669
STEP: Creating a pod to test consume configMaps
Dec 14 12:45:12.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7" in namespace "projected-980" to be "Succeeded or Failed"
Dec 14 12:45:12.181: INFO: Pod "pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.441019ms
Dec 14 12:45:14.192: INFO: Pod "pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028850982s
Dec 14 12:45:16.200: INFO: Pod "pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037755015s
Dec 14 12:45:18.217: INFO: Pod "pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054131982s
STEP: Saw pod success
Dec 14 12:45:18.217: INFO: Pod "pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7" satisfied condition "Succeeded or Failed"
Dec 14 12:45:18.223: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 12:45:18.258: INFO: Waiting for pod pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7 to disappear
Dec 14 12:45:18.265: INFO: Pod pod-projected-configmaps-bf7c28b7-2593-4013-a175-da83310adfe7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:18.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-980" for this suite.

• [SLOW TEST:6.396 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":69,"skipped":1285,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:18.287: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:45:18.478: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 12:45:18.523: INFO: The status of Pod pod-exec-websocket-fc280216-49ad-497c-9772-0b9fc1e88654 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:45:20.534: INFO: The status of Pod pod-exec-websocket-fc280216-49ad-497c-9772-0b9fc1e88654 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:45:22.532: INFO: The status of Pod pod-exec-websocket-fc280216-49ad-497c-9772-0b9fc1e88654 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:22.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8909" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":70,"skipped":1286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:22.983: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 12:45:23.244: INFO: Waiting up to 5m0s for pod "pod-30881d22-58fa-4fb4-a519-616a6ca0911e" in namespace "emptydir-1700" to be "Succeeded or Failed"
Dec 14 12:45:23.250: INFO: Pod "pod-30881d22-58fa-4fb4-a519-616a6ca0911e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585002ms
Dec 14 12:45:25.263: INFO: Pod "pod-30881d22-58fa-4fb4-a519-616a6ca0911e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019204784s
Dec 14 12:45:27.271: INFO: Pod "pod-30881d22-58fa-4fb4-a519-616a6ca0911e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027865455s
STEP: Saw pod success
Dec 14 12:45:27.272: INFO: Pod "pod-30881d22-58fa-4fb4-a519-616a6ca0911e" satisfied condition "Succeeded or Failed"
Dec 14 12:45:27.275: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-30881d22-58fa-4fb4-a519-616a6ca0911e container test-container: <nil>
STEP: delete the pod
Dec 14 12:45:27.304: INFO: Waiting for pod pod-30881d22-58fa-4fb4-a519-616a6ca0911e to disappear
Dec 14 12:45:27.312: INFO: Pod pod-30881d22-58fa-4fb4-a519-616a6ca0911e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1700" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":71,"skipped":1312,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:27.332: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:45:27.599: INFO: The status of Pod pod-secrets-2840d2f9-6479-476d-88d8-57bfdadd83a0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:45:29.607: INFO: The status of Pod pod-secrets-2840d2f9-6479-476d-88d8-57bfdadd83a0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:45:31.610: INFO: The status of Pod pod-secrets-2840d2f9-6479-476d-88d8-57bfdadd83a0 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:31.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8848" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":72,"skipped":1321,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:31.680: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Dec 14 12:45:31.848: INFO: created test-event-1
Dec 14 12:45:31.858: INFO: created test-event-2
Dec 14 12:45:31.866: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec 14 12:45:31.871: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec 14 12:45:31.917: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:31.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6431" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":73,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:31.936: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:34.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2902" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":74,"skipped":1357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:34.252: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:45:34.493: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 14 12:45:34.511: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 12:45:39.522: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 12:45:39.522: INFO: Creating deployment "test-rolling-update-deployment"
Dec 14 12:45:39.529: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 14 12:45:39.538: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 14 12:45:41.547: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 14 12:45:41.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082739, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082739, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082739, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775082739, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 12:45:43.558: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Dec 14 12:45:43.569: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7349  5aaab927-a452-4182-ab53-be7dd0341fa0 59436 1 2021-12-14 12:45:39 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-12-14 12:45:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-12-14 12:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a187bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-12-14 12:45:39 +0000 UTC,LastTransitionTime:2021-12-14 12:45:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-12-14 12:45:42 +0000 UTC,LastTransitionTime:2021-12-14 12:45:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 12:45:43.574: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-7349  42562eec-68f4-43b6-afd3-7ac624cd349f 59423 1 2021-12-14 12:45:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5aaab927-a452-4182-ab53-be7dd0341fa0 0xc00a1b1857 0xc00a1b1858}] []  [{kube-controller-manager Update apps/v1 2021-12-14 12:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5aaab927-a452-4182-ab53-be7dd0341fa0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a1b18e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 12:45:43.574: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 14 12:45:43.574: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7349  6607f5ca-5501-4be9-8be0-1a47e9ec829a 59433 2 2021-12-14 12:45:34 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5aaab927-a452-4182-ab53-be7dd0341fa0 0xc00a1b1747 0xc00a1b1748}] []  [{e2e.test Update apps/v1 2021-12-14 12:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-12-14 12:45:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5aaab927-a452-4182-ab53-be7dd0341fa0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00a1b17e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 12:45:43.579: INFO: Pod "test-rolling-update-deployment-585b757574-jz856" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-jz856 test-rolling-update-deployment-585b757574- deployment-7349  283db8c2-8095-4224-9e23-6f414b7b935c 59422 0 2021-12-14 12:45:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.56"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.56"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 42562eec-68f4-43b6-afd3-7ac624cd349f 0xc00a187f77 0xc00a187f78}] []  [{kube-controller-manager Update v1 2021-12-14 12:45:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42562eec-68f4-43b6-afd3-7ac624cd349f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 12:45:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 12:45:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6p5lw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6p5lw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-hljl4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:45:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:45:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 12:45:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.56,StartTime:2021-12-14 12:45:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 12:45:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://3559f052db582be5f37c4ce19654e59cd1d0b533c79c132f35132a0494926bf9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:45:43.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7349" for this suite.

• [SLOW TEST:9.342 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":75,"skipped":1400,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:45:43.595: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-1400
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1400
STEP: Creating statefulset with conflicting port in namespace statefulset-1400
STEP: Waiting until pod test-pod will start running in namespace statefulset-1400
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1400
Dec 14 12:45:47.914: INFO: Observed stateful pod in namespace: statefulset-1400, name: ss-0, uid: 526ed9a4-b922-4fb9-8dd0-04e82372f32a, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 12:45:47.933: INFO: Observed stateful pod in namespace: statefulset-1400, name: ss-0, uid: 526ed9a4-b922-4fb9-8dd0-04e82372f32a, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 12:45:47.940: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1400
STEP: Removing pod with conflicting port in namespace statefulset-1400
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1400 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Dec 14 12:45:51.987: INFO: Deleting all statefulset in ns statefulset-1400
Dec 14 12:45:51.991: INFO: Scaling statefulset ss to 0
Dec 14 12:46:02.043: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 12:46:02.047: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:46:02.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1400" for this suite.

• [SLOW TEST:18.487 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":76,"skipped":1407,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:46:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5541 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5541;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5541 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5541;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5541.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5541.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5541.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5541.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5541.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5541.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5541.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5541.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5541.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 136.60.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.60.136_udp@PTR;check="$$(dig +tcp +noall +answer +search 136.60.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.60.136_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5541 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5541;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5541 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5541;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5541.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5541.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5541.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5541.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5541.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5541.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5541.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5541.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5541.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5541.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 136.60.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.60.136_udp@PTR;check="$$(dig +tcp +noall +answer +search 136.60.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.60.136_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 12:46:22.379: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5541/dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893: the server could not find the requested resource (get pods dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893)
Dec 14 12:46:22.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5541 from pod dns-5541/dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893: the server could not find the requested resource (get pods dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893)
Dec 14 12:46:22.412: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5541.svc from pod dns-5541/dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893: the server could not find the requested resource (get pods dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893)
Dec 14 12:46:22.465: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5541/dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893: the server could not find the requested resource (get pods dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893)
Dec 14 12:46:22.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-5541.svc from pod dns-5541/dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893: the server could not find the requested resource (get pods dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893)
Dec 14 12:46:22.530: INFO: Lookups using dns-5541/dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893 failed for: [wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-5541 wheezy_tcp@_http._tcp.dns-test-service.dns-5541.svc jessie_tcp@dns-test-service jessie_tcp@dns-test-service.dns-5541.svc]

Dec 14 12:46:27.899: INFO: DNS probes using dns-5541/dns-test-c2909637-a2cf-40c1-8ae2-415e90d6e893 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:46:28.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5541" for this suite.

• [SLOW TEST:25.965 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":77,"skipped":1409,"failed":0}
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:46:28.047: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:46:28.424: INFO: Creating ReplicaSet my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa
Dec 14 12:46:28.440: INFO: Pod name my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa: Found 0 pods out of 1
Dec 14 12:46:33.448: INFO: Pod name my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa: Found 1 pods out of 1
Dec 14 12:46:33.448: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa" is running
Dec 14 12:46:37.461: INFO: Pod "my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa-87llr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:46:28 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:46:28 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:46:28 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:46:28 +0000 UTC Reason: Message:}])
Dec 14 12:46:37.461: INFO: Trying to dial the pod
Dec 14 12:46:42.489: INFO: Controller my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa: Got expected result from replica 1 [my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa-87llr]: "my-hostname-basic-8b312141-0cb8-48fd-bc23-a1502f7f18fa-87llr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:46:42.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2361" for this suite.

• [SLOW TEST:14.467 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":78,"skipped":1409,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:46:42.514: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-782
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-782
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-782
Dec 14 12:46:42.765: INFO: Found 0 stateful pods, waiting for 1
Dec 14 12:46:52.775: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 14 12:46:52.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 12:46:53.263: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 12:46:53.263: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 12:46:53.263: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 12:46:53.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 12:47:03.278: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 12:47:03.278: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 12:47:03.304: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997878s
Dec 14 12:47:04.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995080319s
Dec 14 12:47:05.319: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988367548s
Dec 14 12:47:06.331: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980181149s
Dec 14 12:47:07.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.967793975s
Dec 14 12:47:08.359: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955291193s
Dec 14 12:47:09.369: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.940690263s
Dec 14 12:47:10.375: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.930724977s
Dec 14 12:47:11.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.924092727s
Dec 14 12:47:12.391: INFO: Verifying statefulset ss doesn't scale past 1 for another 915.41489ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-782
Dec 14 12:47:13.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 12:47:13.645: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 12:47:13.645: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 12:47:13.645: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 12:47:13.650: INFO: Found 1 stateful pods, waiting for 3
Dec 14 12:47:23.658: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 12:47:23.658: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 12:47:23.658: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 14 12:47:23.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 12:47:23.940: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 12:47:23.940: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 12:47:23.940: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 12:47:23.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 12:47:24.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 12:47:24.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 12:47:24.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 12:47:24.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 12:47:24.368: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 12:47:24.368: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 12:47:24.368: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 12:47:24.368: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 12:47:24.374: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 14 12:47:34.394: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 12:47:34.394: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 12:47:34.394: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 12:47:34.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999804s
Dec 14 12:47:35.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995964972s
Dec 14 12:47:36.432: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988180098s
Dec 14 12:47:37.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979942411s
Dec 14 12:47:38.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970614264s
Dec 14 12:47:39.456: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964165604s
Dec 14 12:47:40.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956434367s
Dec 14 12:47:41.472: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.949705973s
Dec 14 12:47:42.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939996638s
Dec 14 12:47:43.488: INFO: Verifying statefulset ss doesn't scale past 3 for another 932.354599ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-782
Dec 14 12:47:44.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 12:47:44.703: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 12:47:44.703: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 12:47:44.703: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 12:47:44.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 12:47:44.960: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 12:47:44.960: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 12:47:44.960: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 12:47:44.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-782 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 12:47:45.190: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 12:47:45.190: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 12:47:45.190: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 12:47:45.190: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Dec 14 12:48:25.232: INFO: Deleting all statefulset in ns statefulset-782
Dec 14 12:48:25.242: INFO: Scaling statefulset ss to 0
Dec 14 12:48:25.267: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 12:48:25.270: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:48:25.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-782" for this suite.

• [SLOW TEST:102.791 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":79,"skipped":1421,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:48:25.305: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec 14 12:48:25.572: INFO: Waiting up to 5m0s for pod "downward-api-f7be6075-4779-4c58-967a-8c794aef596d" in namespace "downward-api-790" to be "Succeeded or Failed"
Dec 14 12:48:25.576: INFO: Pod "downward-api-f7be6075-4779-4c58-967a-8c794aef596d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16284ms
Dec 14 12:48:27.584: INFO: Pod "downward-api-f7be6075-4779-4c58-967a-8c794aef596d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011576382s
Dec 14 12:48:29.590: INFO: Pod "downward-api-f7be6075-4779-4c58-967a-8c794aef596d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017142368s
STEP: Saw pod success
Dec 14 12:48:29.590: INFO: Pod "downward-api-f7be6075-4779-4c58-967a-8c794aef596d" satisfied condition "Succeeded or Failed"
Dec 14 12:48:29.594: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downward-api-f7be6075-4779-4c58-967a-8c794aef596d container dapi-container: <nil>
STEP: delete the pod
Dec 14 12:48:29.635: INFO: Waiting for pod downward-api-f7be6075-4779-4c58-967a-8c794aef596d to disappear
Dec 14 12:48:29.639: INFO: Pod downward-api-f7be6075-4779-4c58-967a-8c794aef596d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:48:29.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-790" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":80,"skipped":1423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:48:29.653: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:48:29.861: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-a8d9467e-a658-4c18-b273-738bf1ca5993
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:48:33.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5386" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":81,"skipped":1450,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:48:33.951: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-7310/configmap-test-1a1d063a-5603-4120-b6db-da9608929df3
STEP: Creating a pod to test consume configMaps
Dec 14 12:48:34.200: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342" in namespace "configmap-7310" to be "Succeeded or Failed"
Dec 14 12:48:34.204: INFO: Pod "pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234887ms
Dec 14 12:48:36.219: INFO: Pod "pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018966792s
Dec 14 12:48:38.234: INFO: Pod "pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034081757s
STEP: Saw pod success
Dec 14 12:48:38.234: INFO: Pod "pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342" satisfied condition "Succeeded or Failed"
Dec 14 12:48:38.239: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342 container env-test: <nil>
STEP: delete the pod
Dec 14 12:48:38.272: INFO: Waiting for pod pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342 to disappear
Dec 14 12:48:38.277: INFO: Pod pod-configmaps-5d4a9bbf-c5c2-4b07-a659-dd241367a342 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:48:38.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7310" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":82,"skipped":1465,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:48:38.290: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:48:43.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4919" for this suite.

• [SLOW TEST:5.306 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":83,"skipped":1483,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:48:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 14 12:48:43.848: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 12:49:44.111: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:49:44.120: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:49:44.445: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Dec 14 12:49:44.453: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:49:44.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9747" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:49:44.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4334" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.016 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":84,"skipped":1486,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:49:44.612: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:49:44.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6116" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":85,"skipped":1501,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:49:44.862: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 14 12:49:45.067: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 14 12:50:19.867: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 12:50:30.203: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:51:07.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7432" for this suite.

• [SLOW TEST:82.807 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":86,"skipped":1519,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:51:07.670: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 14 12:51:07.841: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 12:51:07.854: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 12:51:07.860: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-177.ec2.internal before test
Dec 14 12:51:07.906: INFO: aws-ebs-csi-driver-node-2t5xl from openshift-cluster-csi-drivers started at 2021-12-14 11:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 12:51:07.906: INFO: tuned-ckp8j from openshift-cluster-node-tuning-operator started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container tuned ready: true, restart count 0
Dec 14 12:51:07.906: INFO: dns-default-xq8ng from openshift-dns started at 2021-12-14 11:52:45 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container dns ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.906: INFO: node-resolver-5tjmp from openshift-dns started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 12:51:07.906: INFO: node-ca-gb64k from openshift-image-registry started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 12:51:07.906: INFO: ingress-canary-d5qjx from openshift-ingress-canary started at 2021-12-14 11:52:45 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 12:51:07.906: INFO: machine-config-daemon-98m5c from openshift-machine-config-operator started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 12:51:07.906: INFO: node-exporter-f89s7 from openshift-monitoring started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 12:51:07.906: INFO: multus-7jp7s from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 12:51:07.906: INFO: multus-additional-cni-plugins-6kws7 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 12:51:07.906: INFO: network-metrics-daemon-8sqs4 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 12:51:07.906: INFO: network-check-target-hpkw5 from openshift-network-diagnostics started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 12:51:07.906: INFO: sdn-7t2qj from openshift-sdn started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container sdn ready: true, restart count 0
Dec 14 12:51:07.906: INFO: sonobuoy from sonobuoy started at 2021-12-14 12:26:18 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 12:51:07.906: INFO: sonobuoy-e2e-job-1d421b73e370443b from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container e2e ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 12:51:07.906: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-hgvlc from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.906: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 12:51:07.906: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-150-199.ec2.internal before test
Dec 14 12:51:07.952: INFO: aws-ebs-csi-driver-node-6djjl from openshift-cluster-csi-drivers started at 2021-12-14 11:50:28 +0000 UTC (3 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 12:51:07.952: INFO: tuned-btrkv from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container tuned ready: true, restart count 0
Dec 14 12:51:07.952: INFO: dns-default-hk5vj from openshift-dns started at 2021-12-14 11:51:18 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container dns ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.952: INFO: node-resolver-zjfkl from openshift-dns started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 12:51:07.952: INFO: image-registry-669979f5d8-66fds from openshift-image-registry started at 2021-12-14 11:51:20 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container registry ready: true, restart count 0
Dec 14 12:51:07.952: INFO: node-ca-x5d9t from openshift-image-registry started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 12:51:07.952: INFO: ingress-canary-vth2d from openshift-ingress-canary started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 12:51:07.952: INFO: router-default-7fbf56f75b-ncphj from openshift-ingress started at 2021-12-14 11:51:19 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container router ready: true, restart count 0
Dec 14 12:51:07.952: INFO: machine-config-daemon-t7xdp from openshift-machine-config-operator started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 12:51:07.952: INFO: certified-operators-nt7k6 from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 12:51:07.952: INFO: community-operators-g4pdk from openshift-marketplace started at 2021-12-14 11:51:21 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 12:51:07.952: INFO: redhat-marketplace-qmxvd from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 12:51:07.952: INFO: redhat-operators-648jb from openshift-marketplace started at 2021-12-14 11:51:21 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 12:51:07.952: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-12-14 11:51:57 +0000 UTC (5 container statuses recorded)
Dec 14 12:51:07.952: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.952: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: kube-state-metrics-78d545c868-nnjp9 from openshift-monitoring started at 2021-12-14 11:51:22 +0000 UTC (3 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 14 12:51:07.953: INFO: node-exporter-2xvt4 from openshift-monitoring started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 12:51:07.953: INFO: openshift-state-metrics-85bc7c8d6c-jws2g from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 14 12:51:07.953: INFO: prometheus-adapter-74475585f4-skfcj from openshift-monitoring started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 12:51:07.953: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-12-14 11:52:02 +0000 UTC (7 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 12:51:07.953: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 12:51:07.953: INFO: telemeter-client-f96747c6d-sj25j from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container reload ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 14 12:51:07.953: INFO: thanos-querier-74cd47ff6f-wdf79 from openshift-monitoring started at 2021-12-14 11:52:00 +0000 UTC (5 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 12:51:07.953: INFO: multus-6drw6 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 12:51:07.953: INFO: multus-additional-cni-plugins-p2nx7 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 12:51:07.953: INFO: network-metrics-daemon-dvsjd from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 12:51:07.953: INFO: network-check-source-579795dc4b-k549q from openshift-network-diagnostics started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container check-endpoints ready: true, restart count 0
Dec 14 12:51:07.953: INFO: network-check-target-lrvdq from openshift-network-diagnostics started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 12:51:07.953: INFO: sdn-mwhj9 from openshift-sdn started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container sdn ready: true, restart count 0
Dec 14 12:51:07.953: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-2mpvh from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.953: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 12:51:07.953: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-169-135.ec2.internal before test
Dec 14 12:51:07.996: INFO: aws-ebs-csi-driver-node-mzk2q from openshift-cluster-csi-drivers started at 2021-12-14 11:50:39 +0000 UTC (3 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 12:51:07.996: INFO: tuned-fqln7 from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container tuned ready: true, restart count 0
Dec 14 12:51:07.996: INFO: dns-default-6kr7s from openshift-dns started at 2021-12-14 11:51:29 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container dns ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: node-resolver-tz2bj from openshift-dns started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 12:51:07.996: INFO: image-registry-669979f5d8-2zmqt from openshift-image-registry started at 2021-12-14 11:51:31 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container registry ready: true, restart count 0
Dec 14 12:51:07.996: INFO: node-ca-plgtj from openshift-image-registry started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 12:51:07.996: INFO: ingress-canary-jjs2l from openshift-ingress-canary started at 2021-12-14 11:51:29 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 12:51:07.996: INFO: router-default-7fbf56f75b-59vcg from openshift-ingress started at 2021-12-14 11:51:29 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container router ready: true, restart count 0
Dec 14 12:51:07.996: INFO: machine-config-daemon-rwqcs from openshift-machine-config-operator started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-12-14 11:51:57 +0000 UTC (5 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-12-14 11:51:58 +0000 UTC (5 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: grafana-d5d47878d-fb2st from openshift-monitoring started at 2021-12-14 11:51:57 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container grafana ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: node-exporter-rvqjd from openshift-monitoring started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 12:51:07.996: INFO: prometheus-adapter-74475585f4-nr6cs from openshift-monitoring started at 2021-12-14 11:52:38 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 12:51:07.996: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-12-14 11:52:02 +0000 UTC (7 container statuses recorded)
Dec 14 12:51:07.996: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.996: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 12:51:07.997: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 12:51:07.997: INFO: thanos-querier-74cd47ff6f-tdrv7 from openshift-monitoring started at 2021-12-14 11:52:00 +0000 UTC (5 container statuses recorded)
Dec 14 12:51:07.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 12:51:07.997: INFO: multus-additional-cni-plugins-kjrl5 from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.997: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 12:51:07.997: INFO: multus-rmr4w from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.997: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 12:51:07.997: INFO: network-metrics-daemon-g4ppk from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 12:51:07.997: INFO: network-check-target-br6mv from openshift-network-diagnostics started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 12:51:07.997: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 12:51:07.997: INFO: sdn-jbrq7 from openshift-sdn started at 2021-12-14 11:50:40 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container sdn ready: true, restart count 0
Dec 14 12:51:07.997: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-7qgl9 from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 12:51:07.997: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 12:51:07.997: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d52b9007-164d-4e38-824a-bc0e151fa2f8 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d52b9007-164d-4e38-824a-bc0e151fa2f8 off the node ip-10-0-169-135.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d52b9007-164d-4e38-824a-bc0e151fa2f8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:51:16.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3440" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.492 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":87,"skipped":1523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:51:16.163: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 12:51:16.800: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Dec 14 12:51:18.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083076, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083076, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083076, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083076, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 12:51:21.842: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:51:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:51:25.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6926" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.976 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":88,"skipped":1546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:51:25.139: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 12:51:26.714: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 12:51:28.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083086, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083086, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083086, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083086, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 12:51:31.753: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 14 12:51:35.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=webhook-3509 attach --namespace=webhook-3509 to-be-attached-pod -i -c=container1'
Dec 14 12:51:35.961: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:51:35.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3509" for this suite.
STEP: Destroying namespace "webhook-3509-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.933 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":89,"skipped":1577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:51:36.073: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 12:51:37.260: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 12:51:39.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083097, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083097, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083097, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083097, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 12:51:42.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:51:42.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3446" for this suite.
STEP: Destroying namespace "webhook-3446-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.565 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":90,"skipped":1599,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:51:42.638: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:51:43.008: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 12:51:52.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-6492 --namespace=crd-publish-openapi-6492 create -f -'
Dec 14 12:51:53.769: INFO: stderr: ""
Dec 14 12:51:53.769: INFO: stdout: "e2e-test-crd-publish-openapi-3254-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 12:51:53.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-6492 --namespace=crd-publish-openapi-6492 delete e2e-test-crd-publish-openapi-3254-crds test-cr'
Dec 14 12:51:53.927: INFO: stderr: ""
Dec 14 12:51:53.927: INFO: stdout: "e2e-test-crd-publish-openapi-3254-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 14 12:51:53.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-6492 --namespace=crd-publish-openapi-6492 apply -f -'
Dec 14 12:51:54.716: INFO: stderr: ""
Dec 14 12:51:54.716: INFO: stdout: "e2e-test-crd-publish-openapi-3254-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 12:51:54.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-6492 --namespace=crd-publish-openapi-6492 delete e2e-test-crd-publish-openapi-3254-crds test-cr'
Dec 14 12:51:54.833: INFO: stderr: ""
Dec 14 12:51:54.833: INFO: stdout: "e2e-test-crd-publish-openapi-3254-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 12:51:54.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-6492 explain e2e-test-crd-publish-openapi-3254-crds'
Dec 14 12:51:55.479: INFO: stderr: ""
Dec 14 12:51:55.479: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3254-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:52:04.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6492" for this suite.

• [SLOW TEST:22.148 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":91,"skipped":1601,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:52:04.786: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W1214 12:52:04.978472      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:54:01.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3853" for this suite.

• [SLOW TEST:116.250 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":92,"skipped":1622,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:54:01.037: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:54:01.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1155" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":93,"skipped":1636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:54:01.395: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-f0816881-8c82-48ed-ae31-d3a5d73a63b8
STEP: Creating a pod to test consume configMaps
Dec 14 12:54:01.724: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6" in namespace "configmap-575" to be "Succeeded or Failed"
Dec 14 12:54:01.728: INFO: Pod "pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924133ms
Dec 14 12:54:03.739: INFO: Pod "pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015372372s
Dec 14 12:54:05.747: INFO: Pod "pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022865777s
Dec 14 12:54:07.763: INFO: Pod "pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038973639s
STEP: Saw pod success
Dec 14 12:54:07.763: INFO: Pod "pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6" satisfied condition "Succeeded or Failed"
Dec 14 12:54:07.767: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 12:54:07.830: INFO: Waiting for pod pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6 to disappear
Dec 14 12:54:07.842: INFO: Pod pod-configmaps-3c7b55f8-f980-4a10-96c2-35106c4896e6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:54:07.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-575" for this suite.

• [SLOW TEST:6.463 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":94,"skipped":1673,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:54:07.858: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:54:08.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6754 create -f -'
Dec 14 12:54:08.805: INFO: stderr: ""
Dec 14 12:54:08.805: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 14 12:54:08.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6754 create -f -'
Dec 14 12:54:09.556: INFO: stderr: ""
Dec 14 12:54:09.557: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 12:54:10.567: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:10.567: INFO: Found 0 / 1
Dec 14 12:54:11.566: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:11.566: INFO: Found 0 / 1
Dec 14 12:54:12.565: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:12.565: INFO: Found 0 / 1
Dec 14 12:54:13.570: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:13.570: INFO: Found 0 / 1
Dec 14 12:54:14.568: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:14.568: INFO: Found 0 / 1
Dec 14 12:54:15.564: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:15.564: INFO: Found 0 / 1
Dec 14 12:54:16.564: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:16.564: INFO: Found 1 / 1
Dec 14 12:54:16.564: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 12:54:16.569: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 12:54:16.569: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 12:54:16.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6754 describe pod agnhost-primary-q5z76'
Dec 14 12:54:16.708: INFO: stderr: ""
Dec 14 12:54:16.708: INFO: stdout: "Name:         agnhost-primary-q5z76\nNamespace:    kubectl-6754\nPriority:     0\nNode:         ip-10-0-150-199.ec2.internal/10.0.150.199\nStart Time:   Tue, 14 Dec 2021 12:54:08 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"openshift-sdn\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.131.0.68\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"openshift-sdn\",\n                    \"interface\": \"eth0\",\n                    \"ips\": [\n                        \"10.131.0.68\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           10.131.0.68\nIPs:\n  IP:           10.131.0.68\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://eefba08216e9ca6b29ed6ef67f583b57e5a7a91c98f04d5842ad8be061d9051b\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 14 Dec 2021 12:54:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rs9b5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rs9b5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       7s    default-scheduler  Successfully assigned kubectl-6754/agnhost-primary-q5z76 to ip-10-0-150-199.ec2.internal\n  Normal  AddedInterface  5s    multus             Add eth0 [10.131.0.68/23] from openshift-sdn\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Dec 14 12:54:16.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6754 describe rc agnhost-primary'
Dec 14 12:54:16.827: INFO: stderr: ""
Dec 14 12:54:16.827: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6754\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: agnhost-primary-q5z76\n"
Dec 14 12:54:16.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6754 describe service agnhost-primary'
Dec 14 12:54:16.943: INFO: stderr: ""
Dec 14 12:54:16.943: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6754\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.161.104\nIPs:               172.30.161.104\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.131.0.68:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 14 12:54:16.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6754 describe node ip-10-0-132-177.ec2.internal'
Dec 14 12:54:17.250: INFO: stderr: ""
Dec 14 12:54:17.250: INFO: stdout: "Name:               ip-10-0-132-177.ec2.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-132-177\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=m4.large\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-1a\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0dae2f00b5bac3c5e\"}\n                    machine.openshift.io/machine: openshift-machine-api/maszulik-zzbxf-worker-us-east-1a-rbl5v\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-e1e4de1d00d081376745a07125c8009d\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-e1e4de1d00d081376745a07125c8009d\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 14 Dec 2021 11:51:55 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-132-177.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 14 Dec 2021 12:54:12 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 14 Dec 2021 12:50:02 +0000   Tue, 14 Dec 2021 11:51:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 14 Dec 2021 12:50:02 +0000   Tue, 14 Dec 2021 11:51:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 14 Dec 2021 12:50:02 +0000   Tue, 14 Dec 2021 11:51:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 14 Dec 2021 12:50:02 +0000   Tue, 14 Dec 2021 11:52:45 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.132.177\n  Hostname:     ip-10-0-132-177.ec2.internal\n  InternalDNS:  ip-10-0-132-177.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         2\n  ephemeral-storage:           125293548Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      8149576Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         1500m\n  ephemeral-storage:           115470533646\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      6998600Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                             f3d8bbe4d3a248188b1f8b81b60f3dc4\n  System UUID:                            ec2cbacf-006d-f755-84e6-574a9eccc3ad\n  Boot ID:                                34ae343c-6c4b-4f58-bc81-2d9058098258\n  Kernel Version:                         4.18.0-305.28.1.el8_4.x86_64\n  OS Image:                               Red Hat Enterprise Linux CoreOS 48.84.202112022303-0 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.21.4-5.rhaos4.8.gitc845cf4.el8\n  Kubelet Version:                        v1.21.6+c180a7c\n  Kube-Proxy Version:                     v1.21.6+c180a7c\nProviderID:                               aws:///us-east-1a/i-0dae2f00b5bac3c5e\nNon-terminated Pods:                      (16 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  openshift-cluster-csi-drivers           aws-ebs-csi-driver-node-2t5xl                              30m (2%)      0 (0%)      150Mi (2%)       0 (0%)         62m\n  openshift-cluster-node-tuning-operator  tuned-ckp8j                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         62m\n  openshift-dns                           dns-default-xq8ng                                          60m (4%)      0 (0%)      110Mi (1%)       0 (0%)         61m\n  openshift-dns                           node-resolver-5tjmp                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         62m\n  openshift-image-registry                node-ca-gb64k                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         62m\n  openshift-ingress-canary                ingress-canary-d5qjx                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         61m\n  openshift-machine-config-operator       machine-config-daemon-98m5c                                40m (2%)      0 (0%)      100Mi (1%)       0 (0%)         62m\n  openshift-monitoring                    node-exporter-f89s7                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         62m\n  openshift-multus                        multus-7jp7s                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         62m\n  openshift-multus                        multus-additional-cni-plugins-6kws7                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         62m\n  openshift-multus                        network-metrics-daemon-8sqs4                               20m (1%)      0 (0%)      120Mi (1%)       0 (0%)         62m\n  openshift-network-diagnostics           network-check-target-hpkw5                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         62m\n  openshift-sdn                           sdn-7t2qj                                                  110m (7%)     0 (0%)      220Mi (3%)       0 (0%)         62m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  sonobuoy                                sonobuoy-e2e-job-1d421b73e370443b                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-hgvlc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         334m (22%)   0 (0%)\n  memory                      938Mi (13%)  0 (0%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  hugepages-1Gi               0 (0%)       0 (0%)\n  hugepages-2Mi               0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\nEvents:\n  Type    Reason                   Age                From     Message\n  ----    ------                   ----               ----     -------\n  Normal  Starting                 62m                kubelet  Starting kubelet.\n  Normal  NodeHasSufficientMemory  62m (x2 over 62m)  kubelet  Node ip-10-0-132-177.ec2.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    62m (x2 over 62m)  kubelet  Node ip-10-0-132-177.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     62m (x2 over 62m)  kubelet  Node ip-10-0-132-177.ec2.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  62m                kubelet  Updated Node Allocatable limit across pods\n  Normal  NodeReady                61m                kubelet  Node ip-10-0-132-177.ec2.internal status is now: NodeReady\n"
Dec 14 12:54:17.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6754 describe namespace kubectl-6754'
Dec 14 12:54:17.366: INFO: stderr: ""
Dec 14 12:54:17.366: INFO: stdout: "Name:         kubectl-6754\nLabels:       e2e-framework=kubectl\n              e2e-run=4d6bb6f9-012a-453d-afbf-bb2f016ad9be\n              kubernetes.io/metadata.name=kubectl-6754\nAnnotations:  openshift.io/sa.scc.mcs: s0:c41,c20\n              openshift.io/sa.scc.supplemental-groups: 1001680000/10000\n              openshift.io/sa.scc.uid-range: 1001680000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:54:17.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6754" for this suite.

• [SLOW TEST:9.541 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1084
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":95,"skipped":1679,"failed":0}
S
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:54:17.399: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2328, will wait for the garbage collector to delete the pods
Dec 14 12:54:23.638: INFO: Deleting Job.batch foo took: 14.516968ms
Dec 14 12:54:23.740: INFO: Terminating Job.batch foo pods took: 101.160294ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:54:58.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2328" for this suite.

• [SLOW TEST:41.167 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":96,"skipped":1680,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:54:58.566: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-5d653da3-8fb5-491a-9899-4f616734590f
STEP: Creating secret with name secret-projected-all-test-volume-d98a9730-ec06-48d0-a285-637037c92f5e
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 14 12:54:58.805: INFO: Waiting up to 5m0s for pod "projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339" in namespace "projected-1243" to be "Succeeded or Failed"
Dec 14 12:54:58.810: INFO: Pod "projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.952044ms
Dec 14 12:55:00.819: INFO: Pod "projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013596812s
Dec 14 12:55:02.828: INFO: Pod "projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022883407s
STEP: Saw pod success
Dec 14 12:55:02.828: INFO: Pod "projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339" satisfied condition "Succeeded or Failed"
Dec 14 12:55:02.832: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 14 12:55:02.872: INFO: Waiting for pod projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339 to disappear
Dec 14 12:55:02.875: INFO: Pod projected-volume-6f2418fe-a693-43ad-a15b-ce292f482339 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:55:02.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1243" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":97,"skipped":1693,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:55:02.890: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec 14 12:55:03.184: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:05.193: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:07.194: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec 14 12:55:07.223: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:09.232: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:11.234: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 12:55:11.256: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 12:55:11.261: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 12:55:13.262: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 12:55:13.269: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 12:55:15.262: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 12:55:15.270: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:55:15.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1073" for this suite.

• [SLOW TEST:12.399 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":98,"skipped":1696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:55:15.289: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-9165
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9165 to expose endpoints map[]
Dec 14 12:55:15.512: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 14 12:55:16.525: INFO: successfully validated that service multi-endpoint-test in namespace services-9165 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9165
Dec 14 12:55:16.551: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:18.557: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:20.560: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9165 to expose endpoints map[pod1:[100]]
Dec 14 12:55:20.577: INFO: successfully validated that service multi-endpoint-test in namespace services-9165 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9165
Dec 14 12:55:20.604: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:22.609: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:55:24.624: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9165 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 14 12:55:24.644: INFO: successfully validated that service multi-endpoint-test in namespace services-9165 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-9165
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9165 to expose endpoints map[pod2:[101]]
Dec 14 12:55:25.678: INFO: successfully validated that service multi-endpoint-test in namespace services-9165 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9165
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9165 to expose endpoints map[]
Dec 14 12:55:26.708: INFO: successfully validated that service multi-endpoint-test in namespace services-9165 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:55:26.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9165" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:11.477 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":99,"skipped":1731,"failed":0}
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:55:26.766: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 in namespace container-probe-1802
Dec 14 12:55:33.159: INFO: Started pod liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 in namespace container-probe-1802
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 12:55:33.165: INFO: Initial restart count of pod liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 is 0
Dec 14 12:55:49.244: INFO: Restart count of pod container-probe-1802/liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 is now 1 (16.078768212s elapsed)
Dec 14 12:56:09.333: INFO: Restart count of pod container-probe-1802/liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 is now 2 (36.167996381s elapsed)
Dec 14 12:56:29.439: INFO: Restart count of pod container-probe-1802/liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 is now 3 (56.273397841s elapsed)
Dec 14 12:56:49.537: INFO: Restart count of pod container-probe-1802/liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 is now 4 (1m16.37173374s elapsed)
Dec 14 12:57:49.840: INFO: Restart count of pod container-probe-1802/liveness-53b49051-0945-4ba5-a7c2-9c8c7f653d67 is now 5 (2m16.674974115s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:57:49.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1802" for this suite.

• [SLOW TEST:143.106 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":100,"skipped":1731,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:57:49.872: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 14 12:57:50.083: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8531  c4e01051-1cae-412f-a9b2-953223017051 69171 0 2021-12-14 12:57:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-12-14 12:57:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 12:57:50.083: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8531  c4e01051-1cae-412f-a9b2-953223017051 69174 0 2021-12-14 12:57:50 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-12-14 12:57:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:57:50.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8531" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":101,"skipped":1745,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:57:50.096: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 12:57:50.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7" in namespace "projected-507" to be "Succeeded or Failed"
Dec 14 12:57:50.341: INFO: Pod "downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.765299ms
Dec 14 12:57:52.352: INFO: Pod "downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016953306s
Dec 14 12:57:54.362: INFO: Pod "downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026816458s
STEP: Saw pod success
Dec 14 12:57:54.362: INFO: Pod "downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7" satisfied condition "Succeeded or Failed"
Dec 14 12:57:54.367: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7 container client-container: <nil>
STEP: delete the pod
Dec 14 12:57:54.413: INFO: Waiting for pod downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7 to disappear
Dec 14 12:57:54.417: INFO: Pod downwardapi-volume-b241226c-df01-4c6c-b70d-4050800bdee7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:57:54.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-507" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":102,"skipped":1747,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:57:54.435: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:57:58.684: INFO: Deleting pod "var-expansion-cfe9a0df-ca90-45f7-af87-58d100b7184a" in namespace "var-expansion-1392"
Dec 14 12:57:58.699: INFO: Wait up to 5m0s for pod "var-expansion-cfe9a0df-ca90-45f7-af87-58d100b7184a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:58:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1392" for this suite.

• [SLOW TEST:14.294 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":103,"skipped":1747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:58:08.729: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 12:58:08.980: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe" in namespace "downward-api-980" to be "Succeeded or Failed"
Dec 14 12:58:08.985: INFO: Pod "downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.302991ms
Dec 14 12:58:10.992: INFO: Pod "downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01219282s
Dec 14 12:58:12.999: INFO: Pod "downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018791274s
STEP: Saw pod success
Dec 14 12:58:12.999: INFO: Pod "downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe" satisfied condition "Succeeded or Failed"
Dec 14 12:58:13.003: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe container client-container: <nil>
STEP: delete the pod
Dec 14 12:58:13.026: INFO: Waiting for pod downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe to disappear
Dec 14 12:58:13.031: INFO: Pod downwardapi-volume-bcc709a0-e092-43aa-b79a-023ca59cabfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:58:13.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-980" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":104,"skipped":1770,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:58:13.051: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a
Dec 14 12:58:13.231: INFO: Pod name my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a: Found 0 pods out of 1
Dec 14 12:58:18.255: INFO: Pod name my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a: Found 1 pods out of 1
Dec 14 12:58:18.255: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a" are running
Dec 14 12:58:18.259: INFO: Pod "my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a-r9cfm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:58:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:58:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:58:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-12-14 12:58:13 +0000 UTC Reason: Message:}])
Dec 14 12:58:18.259: INFO: Trying to dial the pod
Dec 14 12:58:23.279: INFO: Controller my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a: Got expected result from replica 1 [my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a-r9cfm]: "my-hostname-basic-2879dbec-41d6-4232-a6b3-56e147c2b60a-r9cfm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:58:23.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1476" for this suite.

• [SLOW TEST:10.244 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":105,"skipped":1783,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:58:23.295: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 12:58:24.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 12:58:26.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083504, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083504, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083504, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083504, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 12:58:29.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:58:29.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9396" for this suite.
STEP: Destroying namespace "webhook-9396-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.355 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":106,"skipped":1793,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:58:29.650: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4655
STEP: creating service affinity-clusterip in namespace services-4655
STEP: creating replication controller affinity-clusterip in namespace services-4655
I1214 12:58:29.952638      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4655, replica count: 3
I1214 12:58:33.011914      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 12:58:36.012326      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 12:58:36.030: INFO: Creating new exec pod
Dec 14 12:58:41.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-4655 exec execpod-affinityfjwsf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Dec 14 12:58:41.293: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 14 12:58:41.293: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 12:58:41.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-4655 exec execpod-affinityfjwsf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.61.79 80'
Dec 14 12:58:41.497: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.61.79 80\nConnection to 172.30.61.79 80 port [tcp/http] succeeded!\n"
Dec 14 12:58:41.497: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 12:58:41.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-4655 exec execpod-affinityfjwsf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.61.79:80/ ; done'
Dec 14 12:58:41.778: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.79:80/\n"
Dec 14 12:58:41.778: INFO: stdout: "\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g\naffinity-clusterip-q6h9g"
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Received response from host: affinity-clusterip-q6h9g
Dec 14 12:58:41.778: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4655, will wait for the garbage collector to delete the pods
Dec 14 12:58:41.863: INFO: Deleting ReplicationController affinity-clusterip took: 8.337288ms
Dec 14 12:58:41.965: INFO: Terminating ReplicationController affinity-clusterip pods took: 102.718973ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:58:55.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4655" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:25.582 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":107,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:58:55.232: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 12:58:55.651: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:55.651: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:55.652: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:58:55.656: INFO: Number of nodes with available pods: 0
Dec 14 12:58:55.656: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 12:58:56.665: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:56.665: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:56.665: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:58:56.669: INFO: Number of nodes with available pods: 0
Dec 14 12:58:56.669: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 12:58:57.666: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:57.666: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:57.666: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:58:57.671: INFO: Number of nodes with available pods: 0
Dec 14 12:58:57.671: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 12:58:58.665: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:58.665: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:58.665: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:58:58.670: INFO: Number of nodes with available pods: 0
Dec 14 12:58:58.670: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 12:58:59.668: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:59.668: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:58:59.668: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:58:59.674: INFO: Number of nodes with available pods: 2
Dec 14 12:58:59.674: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 12:59:00.664: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:00.665: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:00.665: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:00.669: INFO: Number of nodes with available pods: 3
Dec 14 12:59:00.669: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 14 12:59:00.695: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:00.695: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:00.695: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:00.699: INFO: Number of nodes with available pods: 2
Dec 14 12:59:00.699: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:01.708: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:01.708: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:01.708: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:01.713: INFO: Number of nodes with available pods: 2
Dec 14 12:59:01.713: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:02.717: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:02.718: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:02.718: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:02.723: INFO: Number of nodes with available pods: 2
Dec 14 12:59:02.723: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:03.709: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:03.709: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:03.709: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:03.714: INFO: Number of nodes with available pods: 2
Dec 14 12:59:03.714: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:04.711: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:04.711: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:04.711: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:04.715: INFO: Number of nodes with available pods: 2
Dec 14 12:59:04.715: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:05.708: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:05.708: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:05.708: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:05.714: INFO: Number of nodes with available pods: 2
Dec 14 12:59:05.714: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:06.716: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:06.716: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:06.716: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:06.720: INFO: Number of nodes with available pods: 2
Dec 14 12:59:06.720: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:07.710: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:07.710: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:07.710: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:07.717: INFO: Number of nodes with available pods: 2
Dec 14 12:59:07.717: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:08.710: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:08.710: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:08.710: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:08.714: INFO: Number of nodes with available pods: 2
Dec 14 12:59:08.714: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:09.709: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:09.709: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:09.709: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:09.716: INFO: Number of nodes with available pods: 2
Dec 14 12:59:09.716: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:10.713: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:10.713: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:10.713: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:10.718: INFO: Number of nodes with available pods: 2
Dec 14 12:59:10.718: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:11.707: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:11.708: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:11.708: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:11.716: INFO: Number of nodes with available pods: 2
Dec 14 12:59:11.716: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:12.711: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:12.711: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:12.711: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:12.716: INFO: Number of nodes with available pods: 2
Dec 14 12:59:12.716: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:13.707: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:13.708: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:13.708: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:13.713: INFO: Number of nodes with available pods: 2
Dec 14 12:59:13.713: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:14.715: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:14.715: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:14.715: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:14.719: INFO: Number of nodes with available pods: 2
Dec 14 12:59:14.719: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 12:59:15.717: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:15.717: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 12:59:15.717: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 12:59:15.731: INFO: Number of nodes with available pods: 3
Dec 14 12:59:15.731: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1936, will wait for the garbage collector to delete the pods
Dec 14 12:59:15.814: INFO: Deleting DaemonSet.extensions daemon-set took: 12.765436ms
Dec 14 12:59:15.922: INFO: Terminating DaemonSet.extensions daemon-set pods took: 107.696492ms
Dec 14 12:59:28.647: INFO: Number of nodes with available pods: 0
Dec 14 12:59:28.647: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 12:59:28.652: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70844"},"items":null}

Dec 14 12:59:28.656: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70844"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:59:28.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1936" for this suite.

• [SLOW TEST:33.483 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":108,"skipped":1855,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:59:28.716: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Dec 14 12:59:28.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5873 api-versions'
Dec 14 12:59:28.999: INFO: stderr: ""
Dec 14 12:59:28.999: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.openshift.io/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:59:28.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5873" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":109,"skipped":1873,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:59:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 12:59:29.254: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6" in namespace "projected-4659" to be "Succeeded or Failed"
Dec 14 12:59:29.262: INFO: Pod "downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.371313ms
Dec 14 12:59:31.271: INFO: Pod "downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017192746s
Dec 14 12:59:33.292: INFO: Pod "downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037965298s
STEP: Saw pod success
Dec 14 12:59:33.292: INFO: Pod "downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6" satisfied condition "Succeeded or Failed"
Dec 14 12:59:33.298: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6 container client-container: <nil>
STEP: delete the pod
Dec 14 12:59:33.341: INFO: Waiting for pod downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6 to disappear
Dec 14 12:59:33.346: INFO: Pod downwardapi-volume-b8df8486-f1f8-4002-b95a-6ca2ea7df0f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:59:33.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4659" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":110,"skipped":1874,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:59:33.361: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:59:33.666: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 12:59:43.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-9245 --namespace=crd-publish-openapi-9245 create -f -'
Dec 14 12:59:44.502: INFO: stderr: ""
Dec 14 12:59:44.502: INFO: stdout: "e2e-test-crd-publish-openapi-1096-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 12:59:44.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-9245 --namespace=crd-publish-openapi-9245 delete e2e-test-crd-publish-openapi-1096-crds test-cr'
Dec 14 12:59:44.672: INFO: stderr: ""
Dec 14 12:59:44.672: INFO: stdout: "e2e-test-crd-publish-openapi-1096-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 14 12:59:44.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-9245 --namespace=crd-publish-openapi-9245 apply -f -'
Dec 14 12:59:45.354: INFO: stderr: ""
Dec 14 12:59:45.355: INFO: stdout: "e2e-test-crd-publish-openapi-1096-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 12:59:45.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-9245 --namespace=crd-publish-openapi-9245 delete e2e-test-crd-publish-openapi-1096-crds test-cr'
Dec 14 12:59:45.473: INFO: stderr: ""
Dec 14 12:59:45.473: INFO: stdout: "e2e-test-crd-publish-openapi-1096-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 12:59:45.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-9245 explain e2e-test-crd-publish-openapi-1096-crds'
Dec 14 12:59:45.932: INFO: stderr: ""
Dec 14 12:59:45.932: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1096-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 12:59:55.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9245" for this suite.

• [SLOW TEST:22.265 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":111,"skipped":1876,"failed":0}
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 12:59:55.626: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 12:59:55.850: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-29043040-169b-4b97-8597-8cf39b4674c3
STEP: Creating configMap with name cm-test-opt-upd-b6b8456c-793b-468d-baca-c48b213fa49a
STEP: Creating the pod
Dec 14 12:59:55.902: INFO: The status of Pod pod-configmaps-3848b7c3-0ec2-4954-9648-e61f3cb656e5 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:59:57.908: INFO: The status of Pod pod-configmaps-3848b7c3-0ec2-4954-9648-e61f3cb656e5 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 12:59:59.910: INFO: The status of Pod pod-configmaps-3848b7c3-0ec2-4954-9648-e61f3cb656e5 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:00:01.912: INFO: The status of Pod pod-configmaps-3848b7c3-0ec2-4954-9648-e61f3cb656e5 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-29043040-169b-4b97-8597-8cf39b4674c3
STEP: Updating configmap cm-test-opt-upd-b6b8456c-793b-468d-baca-c48b213fa49a
STEP: Creating configMap with name cm-test-opt-create-ffd27415-8fd6-4ad2-87f8-c1cf2e6bd3bb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:01:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8349" for this suite.

• [SLOW TEST:84.824 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":112,"skipped":1876,"failed":0}
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:01:20.450: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:01:30.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5774" for this suite.

• [SLOW TEST:10.336 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":113,"skipped":1876,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:01:30.786: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Dec 14 13:01:31.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-2493 cluster-info'
Dec 14 13:01:31.122: INFO: stderr: ""
Dec 14 13:01:31.122: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:01:31.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2493" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":114,"skipped":1894,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:01:31.141: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:01:31.352: INFO: The status of Pod busybox-readonly-fsa126b7b9-9bb2-4af4-9c6d-3c61353e6603 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:01:33.361: INFO: The status of Pod busybox-readonly-fsa126b7b9-9bb2-4af4-9c6d-3c61353e6603 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:01:35.361: INFO: The status of Pod busybox-readonly-fsa126b7b9-9bb2-4af4-9c6d-3c61353e6603 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:01:35.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9846" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":115,"skipped":1897,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:01:35.392: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5d1ca0b8-56f9-4329-a8a3-bf3618334bdb
STEP: Creating a pod to test consume secrets
Dec 14 13:01:35.586: INFO: Waiting up to 5m0s for pod "pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44" in namespace "secrets-9190" to be "Succeeded or Failed"
Dec 14 13:01:35.595: INFO: Pod "pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44": Phase="Pending", Reason="", readiness=false. Elapsed: 9.040019ms
Dec 14 13:01:37.604: INFO: Pod "pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017794254s
Dec 14 13:01:39.612: INFO: Pod "pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025539239s
STEP: Saw pod success
Dec 14 13:01:39.612: INFO: Pod "pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44" satisfied condition "Succeeded or Failed"
Dec 14 13:01:39.616: INFO: Trying to get logs from node ip-10-0-150-199.ec2.internal pod pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:01:39.677: INFO: Waiting for pod pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44 to disappear
Dec 14 13:01:39.685: INFO: Pod pod-secrets-8616df7d-4c63-41ea-a8d3-c6a5a77fbd44 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:01:39.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9190" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":116,"skipped":1909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:01:39.699: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:01:51.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5992" for this suite.

• [SLOW TEST:11.361 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":117,"skipped":1944,"failed":0}
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:01:51.060: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec 14 13:01:51.237: INFO: PodSpec: initContainers in spec.initContainers
Dec 14 13:02:43.738: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a19e6b2d-5281-40c0-a978-30765e7362dd", GenerateName:"", Namespace:"init-container-8457", SelfLink:"", UID:"7de6731e-03ff-498f-af8f-6fddb985c65d", ResourceVersion:"73531", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63775083711, loc:(*time.Location)(0x9e1f020)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"237038725"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.81\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.81\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0040ea168), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0040ea198)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0040ea1c8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0040ea1e0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0040ea1f8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0040ea210)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-tb665", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0073b82a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tb665", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003406a20), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tb665", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003406de0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-tb665", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003406720), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00821c458), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-150-199.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003a06150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00821c510)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00821c530)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00821c54c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00821c550), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0052980c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083711, loc:(*time.Location)(0x9e1f020)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083711, loc:(*time.Location)(0x9e1f020)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083711, loc:(*time.Location)(0x9e1f020)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775083711, loc:(*time.Location)(0x9e1f020)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.150.199", PodIP:"10.131.0.81", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.131.0.81"}}, StartTime:(*v1.Time)(0xc0040ea240), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a06230)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a062a0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:244bdbdf4b8d368b5836e9d2c7808a280a73ad72ae321d644e9f220da503218f", ContainerID:"cri-o://9d864fd0731d2a2924555a4ae9fb2b7714c4a2fa56676d192652b5f92def56f2", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0073b8420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0073b83c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00821c5cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:02:43.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8457" for this suite.

• [SLOW TEST:52.697 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":118,"skipped":1944,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:02:43.758: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-d55d27f2-186e-42eb-94b4-9cd4f6c66a34
STEP: Creating a pod to test consume configMaps
Dec 14 13:02:44.074: INFO: Waiting up to 5m0s for pod "pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc" in namespace "configmap-5305" to be "Succeeded or Failed"
Dec 14 13:02:44.079: INFO: Pod "pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.052456ms
Dec 14 13:02:46.086: INFO: Pod "pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012353453s
Dec 14 13:02:48.092: INFO: Pod "pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018342197s
STEP: Saw pod success
Dec 14 13:02:48.092: INFO: Pod "pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc" satisfied condition "Succeeded or Failed"
Dec 14 13:02:48.097: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:02:48.125: INFO: Waiting for pod pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc to disappear
Dec 14 13:02:48.130: INFO: Pod pod-configmaps-d9e3448e-b7d3-4061-b4ca-32436f0f78bc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:02:48.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5305" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":119,"skipped":1948,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:02:48.143: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec 14 13:02:48.325: INFO: Waiting up to 5m0s for pod "downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3" in namespace "downward-api-6840" to be "Succeeded or Failed"
Dec 14 13:02:48.331: INFO: Pod "downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837332ms
Dec 14 13:02:50.340: INFO: Pod "downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014832755s
Dec 14 13:02:52.346: INFO: Pod "downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0214373s
STEP: Saw pod success
Dec 14 13:02:52.347: INFO: Pod "downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3" satisfied condition "Succeeded or Failed"
Dec 14 13:02:52.350: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:02:52.376: INFO: Waiting for pod downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3 to disappear
Dec 14 13:02:52.380: INFO: Pod downward-api-212faccc-bc1c-4e17-ae2b-6cfe175b2ce3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:02:52.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6840" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":120,"skipped":1965,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:02:52.393: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec 14 13:02:52.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-1451 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Dec 14 13:02:52.679: INFO: stderr: ""
Dec 14 13:02:52.679: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Dec 14 13:02:52.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-1451 delete pods e2e-test-httpd-pod'
Dec 14 13:02:58.519: INFO: stderr: ""
Dec 14 13:02:58.519: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:02:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1451" for this suite.

• [SLOW TEST:6.151 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":121,"skipped":1977,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:02:58.545: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1214 13:02:59.944876      20 metrics_grabber.go:102] Can't find any pods in namespace kube-system to grab metrics from
W1214 13:02:59.944904      20 metrics_grabber.go:138] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 13:02:59.944: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:02:59.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3806" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":122,"skipped":1996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:02:59.958: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Dec 14 13:03:00.185: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 13:04:00.424: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:04:00.433: INFO: Starting informer...
STEP: Starting pods...
Dec 14 13:04:00.665: INFO: Pod1 is running on ip-10-0-169-135.ec2.internal. Tainting Node
Dec 14 13:04:04.901: INFO: Pod2 is running on ip-10-0-169-135.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 14 13:04:18.676: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 14 13:04:38.550: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:04:38.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7953" for this suite.

• [SLOW TEST:98.678 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":123,"skipped":2046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:04:38.637: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Dec 14 13:04:39.148: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:04:41.155: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:04:43.167: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.132.177 on the node which pod1 resides and expect scheduled
Dec 14 13:04:43.264: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:04:45.275: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:04:47.272: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.132.177 but use UDP protocol on the node which pod2 resides
Dec 14 13:04:47.295: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:04:49.305: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:04:51.303: INFO: The status of Pod pod3 is Running (Ready = true)
Dec 14 13:04:51.339: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:04:53.351: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Dec 14 13:04:53.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.132.177 http://127.0.0.1:54323/hostname] Namespace:hostport-1932 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:04:53.354: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.132.177, port: 54323
Dec 14 13:04:53.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.132.177:54323/hostname] Namespace:hostport-1932 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:04:53.486: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.132.177, port: 54323 UDP
Dec 14 13:04:53.596: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.132.177 54323] Namespace:hostport-1932 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:04:53.596: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:04:58.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1932" for this suite.

• [SLOW TEST:20.121 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":124,"skipped":2088,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:04:58.758: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec 14 13:04:59.011: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:05:01.018: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:05:03.024: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec 14 13:05:03.054: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:05:05.061: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:05:07.062: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec 14 13:05:07.078: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:05:07.083: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:05:09.084: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:05:09.096: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:05:11.083: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:05:11.097: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:05:13.083: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:05:13.093: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:05:15.083: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:05:15.090: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:05:17.084: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:05:17.092: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 13:05:19.084: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 13:05:19.091: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:05:19.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9998" for this suite.

• [SLOW TEST:20.365 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":125,"skipped":2107,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:05:19.123: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:05:19.528: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd" in namespace "projected-8165" to be "Succeeded or Failed"
Dec 14 13:05:19.546: INFO: Pod "downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.014708ms
Dec 14 13:05:21.554: INFO: Pod "downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02662154s
Dec 14 13:05:23.564: INFO: Pod "downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036345781s
STEP: Saw pod success
Dec 14 13:05:23.564: INFO: Pod "downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd" satisfied condition "Succeeded or Failed"
Dec 14 13:05:23.567: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd container client-container: <nil>
STEP: delete the pod
Dec 14 13:05:23.591: INFO: Waiting for pod downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd to disappear
Dec 14 13:05:23.597: INFO: Pod downwardapi-volume-0d1fc99d-c967-4497-8cfd-fa0794c46cbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:05:23.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8165" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":126,"skipped":2107,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:05:23.612: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-vsjf
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 13:05:23.872: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vsjf" in namespace "subpath-5080" to be "Succeeded or Failed"
Dec 14 13:05:23.878: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73809ms
Dec 14 13:05:25.886: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01364845s
Dec 14 13:05:27.907: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 4.034842679s
Dec 14 13:05:29.912: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 6.039806639s
Dec 14 13:05:31.923: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 8.050926949s
Dec 14 13:05:33.933: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 10.061040898s
Dec 14 13:05:35.941: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 12.068689952s
Dec 14 13:05:37.950: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 14.078594996s
Dec 14 13:05:39.958: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 16.086112356s
Dec 14 13:05:41.968: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 18.095863748s
Dec 14 13:05:43.975: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 20.103305207s
Dec 14 13:05:45.984: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Running", Reason="", readiness=true. Elapsed: 22.111674085s
Dec 14 13:05:47.992: INFO: Pod "pod-subpath-test-secret-vsjf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.120174494s
STEP: Saw pod success
Dec 14 13:05:47.992: INFO: Pod "pod-subpath-test-secret-vsjf" satisfied condition "Succeeded or Failed"
Dec 14 13:05:47.996: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-subpath-test-secret-vsjf container test-container-subpath-secret-vsjf: <nil>
STEP: delete the pod
Dec 14 13:05:48.030: INFO: Waiting for pod pod-subpath-test-secret-vsjf to disappear
Dec 14 13:05:48.034: INFO: Pod pod-subpath-test-secret-vsjf no longer exists
STEP: Deleting pod pod-subpath-test-secret-vsjf
Dec 14 13:05:48.034: INFO: Deleting pod "pod-subpath-test-secret-vsjf" in namespace "subpath-5080"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:05:48.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5080" for this suite.

• [SLOW TEST:24.439 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":127,"skipped":2110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:05:48.052: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 13:05:49.269: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 13:05:49.286: INFO: waiting for watch events with expected annotations
Dec 14 13:05:49.286: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:05:49.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6015" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":128,"skipped":2134,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:05:49.384: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-435
STEP: creating service affinity-clusterip-transition in namespace services-435
STEP: creating replication controller affinity-clusterip-transition in namespace services-435
I1214 13:05:49.625962      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-435, replica count: 3
I1214 13:05:52.678936      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:05:55.680068      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:05:55.690: INFO: Creating new exec pod
Dec 14 13:06:00.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-435 exec execpod-affinitybsgb9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Dec 14 13:06:01.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 14 13:06:01.053: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:06:01.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-435 exec execpod-affinitybsgb9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.126.33 80'
Dec 14 13:06:01.261: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.126.33 80\nConnection to 172.30.126.33 80 port [tcp/http] succeeded!\n"
Dec 14 13:06:01.261: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:06:01.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-435 exec execpod-affinitybsgb9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.126.33:80/ ; done'
Dec 14 13:06:01.755: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n"
Dec 14 13:06:01.755: INFO: stdout: "\naffinity-clusterip-transition-8b762\naffinity-clusterip-transition-2774v\naffinity-clusterip-transition-8b762\naffinity-clusterip-transition-8b762\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-2774v\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-2774v\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-2774v\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-8b762\naffinity-clusterip-transition-8b762\naffinity-clusterip-transition-2774v"
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-8b762
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-2774v
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-8b762
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-8b762
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-2774v
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-2774v
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-2774v
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-8b762
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-8b762
Dec 14 13:06:01.755: INFO: Received response from host: affinity-clusterip-transition-2774v
Dec 14 13:06:01.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-435 exec execpod-affinitybsgb9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.126.33:80/ ; done'
Dec 14 13:06:02.261: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.126.33:80/\n"
Dec 14 13:06:02.261: INFO: stdout: "\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm\naffinity-clusterip-transition-sm6vm"
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Received response from host: affinity-clusterip-transition-sm6vm
Dec 14 13:06:02.261: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-435, will wait for the garbage collector to delete the pods
Dec 14 13:06:02.355: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.650593ms
Dec 14 13:06:02.456: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.349243ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:06:15.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-435" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:25.857 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":129,"skipped":2144,"failed":0}
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:06:15.241: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:06:15.564: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 14 13:06:17.627: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:06:18.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3528" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":130,"skipped":2144,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:06:18.651: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-853e6ac2-a1bd-4839-89c4-8a5bf9bedf4e
STEP: Creating a pod to test consume secrets
Dec 14 13:06:18.989: INFO: Waiting up to 5m0s for pod "pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee" in namespace "secrets-9041" to be "Succeeded or Failed"
Dec 14 13:06:19.001: INFO: Pod "pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.249939ms
Dec 14 13:06:21.009: INFO: Pod "pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019923994s
Dec 14 13:06:23.019: INFO: Pod "pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029715936s
STEP: Saw pod success
Dec 14 13:06:23.019: INFO: Pod "pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee" satisfied condition "Succeeded or Failed"
Dec 14 13:06:23.023: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:06:23.058: INFO: Waiting for pod pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee to disappear
Dec 14 13:06:23.065: INFO: Pod pod-secrets-ae434003-a67c-4c54-8c01-95572210d0ee no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:06:23.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9041" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":131,"skipped":2152,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:06:23.081: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:06:23.275: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-be287db9-ea88-4cec-ab0e-c96970af9963
STEP: Creating the pod
Dec 14 13:06:23.326: INFO: The status of Pod pod-projected-configmaps-15ac977b-396d-4560-b82f-1aa0dea3c844 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:06:25.336: INFO: The status of Pod pod-projected-configmaps-15ac977b-396d-4560-b82f-1aa0dea3c844 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:06:27.334: INFO: The status of Pod pod-projected-configmaps-15ac977b-396d-4560-b82f-1aa0dea3c844 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-be287db9-ea88-4cec-ab0e-c96970af9963
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:06:29.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9340" for this suite.

• [SLOW TEST:6.319 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":132,"skipped":2156,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:06:29.400: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Dec 14 13:06:29.580: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:24.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8842" for this suite.

• [SLOW TEST:55.497 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":133,"skipped":2162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:24.898: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:07:25.194: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-969c5a97-c5b3-4a76-bec3-daab78cc60c7" in namespace "security-context-test-3285" to be "Succeeded or Failed"
Dec 14 13:07:25.204: INFO: Pod "busybox-readonly-false-969c5a97-c5b3-4a76-bec3-daab78cc60c7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.610702ms
Dec 14 13:07:27.213: INFO: Pod "busybox-readonly-false-969c5a97-c5b3-4a76-bec3-daab78cc60c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018235649s
Dec 14 13:07:29.302: INFO: Pod "busybox-readonly-false-969c5a97-c5b3-4a76-bec3-daab78cc60c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081961719s
Dec 14 13:07:29.302: INFO: Pod "busybox-readonly-false-969c5a97-c5b3-4a76-bec3-daab78cc60c7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:29.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3285" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":134,"skipped":2200,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:29.461: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:07:29.606: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:36.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6314" for this suite.

• [SLOW TEST:7.525 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":135,"skipped":2209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:36.986: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:37.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1851" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":136,"skipped":2257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:37.627: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:07:38.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe" in namespace "downward-api-375" to be "Succeeded or Failed"
Dec 14 13:07:38.058: INFO: Pod "downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.615292ms
Dec 14 13:07:40.069: INFO: Pod "downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017222742s
Dec 14 13:07:42.081: INFO: Pod "downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028919405s
STEP: Saw pod success
Dec 14 13:07:42.081: INFO: Pod "downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe" satisfied condition "Succeeded or Failed"
Dec 14 13:07:42.085: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe container client-container: <nil>
STEP: delete the pod
Dec 14 13:07:42.116: INFO: Waiting for pod downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe to disappear
Dec 14 13:07:42.130: INFO: Pod downwardapi-volume-884ae3a2-699f-45c3-93da-e4e1c04badbe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:42.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-375" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":137,"skipped":2299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:42.149: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 13:07:42.597: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 13:07:42.632: INFO: waiting for watch events with expected annotations
Dec 14 13:07:42.632: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:42.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5559" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":138,"skipped":2329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Dec 14 13:07:43.341: INFO: The status of Pod pod-hostip-df2dcaa2-3d32-415b-9603-e05b9c08130f is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:07:45.354: INFO: The status of Pod pod-hostip-df2dcaa2-3d32-415b-9603-e05b9c08130f is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:07:47.372: INFO: The status of Pod pod-hostip-df2dcaa2-3d32-415b-9603-e05b9c08130f is Running (Ready = true)
Dec 14 13:07:47.411: INFO: Pod pod-hostip-df2dcaa2-3d32-415b-9603-e05b9c08130f has hostIP: 10.0.169.135
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:47.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2271" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":139,"skipped":2351,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:47.468: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:07:47.940: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-11eff293-2d06-4b38-a28b-0d5cb1e09d02
STEP: Creating secret with name s-test-opt-upd-90e045bc-5c1a-4cd8-b64f-6969151f5cd6
STEP: Creating the pod
Dec 14 13:07:47.989: INFO: The status of Pod pod-secrets-013242b9-0b0c-4867-92a3-01c44aefc3d1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:07:50.000: INFO: The status of Pod pod-secrets-013242b9-0b0c-4867-92a3-01c44aefc3d1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:07:51.998: INFO: The status of Pod pod-secrets-013242b9-0b0c-4867-92a3-01c44aefc3d1 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-11eff293-2d06-4b38-a28b-0d5cb1e09d02
STEP: Updating secret s-test-opt-upd-90e045bc-5c1a-4cd8-b64f-6969151f5cd6
STEP: Creating secret with name s-test-opt-create-9d20e719-b48d-4ba9-9a02-184c655bc95a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:07:56.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5285" for this suite.

• [SLOW TEST:8.649 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":140,"skipped":2370,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:07:56.117: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1214 13:08:06.379425      20 metrics_grabber.go:102] Can't find any pods in namespace kube-system to grab metrics from
W1214 13:08:06.379453      20 metrics_grabber.go:138] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 13:08:06.379: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:06.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-543" for this suite.

• [SLOW TEST:10.288 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":141,"skipped":2376,"failed":0}
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:06.405: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:08:06.749: INFO: The status of Pod busybox-scheduling-5751d419-a38d-4ba4-8d02-f78fc8c0c18c is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:08:08.759: INFO: The status of Pod busybox-scheduling-5751d419-a38d-4ba4-8d02-f78fc8c0c18c is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:08:10.755: INFO: The status of Pod busybox-scheduling-5751d419-a38d-4ba4-8d02-f78fc8c0c18c is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:10.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4097" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":142,"skipped":2377,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:10.791: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 13:08:11.085: INFO: Waiting up to 5m0s for pod "pod-f29ebbb2-4093-4cea-850d-b553a66baff8" in namespace "emptydir-3663" to be "Succeeded or Failed"
Dec 14 13:08:11.090: INFO: Pod "pod-f29ebbb2-4093-4cea-850d-b553a66baff8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.00587ms
Dec 14 13:08:13.100: INFO: Pod "pod-f29ebbb2-4093-4cea-850d-b553a66baff8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015050034s
Dec 14 13:08:15.111: INFO: Pod "pod-f29ebbb2-4093-4cea-850d-b553a66baff8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026852349s
STEP: Saw pod success
Dec 14 13:08:15.111: INFO: Pod "pod-f29ebbb2-4093-4cea-850d-b553a66baff8" satisfied condition "Succeeded or Failed"
Dec 14 13:08:15.117: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-f29ebbb2-4093-4cea-850d-b553a66baff8 container test-container: <nil>
STEP: delete the pod
Dec 14 13:08:15.143: INFO: Waiting for pod pod-f29ebbb2-4093-4cea-850d-b553a66baff8 to disappear
Dec 14 13:08:15.148: INFO: Pod pod-f29ebbb2-4093-4cea-850d-b553a66baff8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:15.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3663" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":143,"skipped":2381,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:15.162: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:08:15.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89" in namespace "downward-api-6934" to be "Succeeded or Failed"
Dec 14 13:08:15.422: INFO: Pod "downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.668207ms
Dec 14 13:08:17.428: INFO: Pod "downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012931873s
Dec 14 13:08:19.439: INFO: Pod "downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023887063s
STEP: Saw pod success
Dec 14 13:08:19.439: INFO: Pod "downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89" satisfied condition "Succeeded or Failed"
Dec 14 13:08:19.443: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89 container client-container: <nil>
STEP: delete the pod
Dec 14 13:08:19.478: INFO: Waiting for pod downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89 to disappear
Dec 14 13:08:19.482: INFO: Pod downwardapi-volume-cd6b7fb0-ba17-43fa-ba65-5624fbdd2f89 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:19.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6934" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":144,"skipped":2381,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:19.498: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:08:19.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b" in namespace "projected-8649" to be "Succeeded or Failed"
Dec 14 13:08:19.771: INFO: Pod "downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.785307ms
Dec 14 13:08:21.788: INFO: Pod "downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023091318s
Dec 14 13:08:23.799: INFO: Pod "downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034150625s
STEP: Saw pod success
Dec 14 13:08:23.799: INFO: Pod "downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b" satisfied condition "Succeeded or Failed"
Dec 14 13:08:23.804: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b container client-container: <nil>
STEP: delete the pod
Dec 14 13:08:23.843: INFO: Waiting for pod downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b to disappear
Dec 14 13:08:23.852: INFO: Pod downwardapi-volume-71ba57af-b26d-41c1-95f8-6e968e35db4b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:23.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8649" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":145,"skipped":2384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:23.870: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Dec 14 13:08:24.070: INFO: Major version: 1
STEP: Confirm minor version
Dec 14 13:08:24.070: INFO: cleanMinorVersion: 21
Dec 14 13:08:24.070: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:24.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6780" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":146,"skipped":2446,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:24.091: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Dec 14 13:08:24.389: INFO: Waiting up to 5m0s for pod "downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910" in namespace "downward-api-6888" to be "Succeeded or Failed"
Dec 14 13:08:24.395: INFO: Pod "downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910": Phase="Pending", Reason="", readiness=false. Elapsed: 6.002915ms
Dec 14 13:08:26.404: INFO: Pod "downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014272344s
Dec 14 13:08:28.426: INFO: Pod "downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036490476s
STEP: Saw pod success
Dec 14 13:08:28.426: INFO: Pod "downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910" satisfied condition "Succeeded or Failed"
Dec 14 13:08:28.436: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910 container dapi-container: <nil>
STEP: delete the pod
Dec 14 13:08:28.502: INFO: Waiting for pod downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910 to disappear
Dec 14 13:08:28.506: INFO: Pod downward-api-76b4f91a-842e-46dd-8f20-76bdeec13910 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:28.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6888" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":147,"skipped":2461,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:28.522: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 13:08:32.825: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:32.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5739" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":148,"skipped":2463,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:32.862: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Dec 14 13:08:37.079: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8501 PodName:pod-sharedvolume-6be59da2-edcd-4cfe-ae2e-adbf8f424088 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:08:37.079: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:08:37.186: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:37.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8501" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":149,"skipped":2476,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:37.204: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:08:37.405: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-77f7fd3d-b756-4d2c-9e80-f6c98e98c205" in namespace "security-context-test-5765" to be "Succeeded or Failed"
Dec 14 13:08:37.409: INFO: Pod "busybox-privileged-false-77f7fd3d-b756-4d2c-9e80-f6c98e98c205": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815238ms
Dec 14 13:08:39.419: INFO: Pod "busybox-privileged-false-77f7fd3d-b756-4d2c-9e80-f6c98e98c205": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013851055s
Dec 14 13:08:41.429: INFO: Pod "busybox-privileged-false-77f7fd3d-b756-4d2c-9e80-f6c98e98c205": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024204422s
Dec 14 13:08:41.429: INFO: Pod "busybox-privileged-false-77f7fd3d-b756-4d2c-9e80-f6c98e98c205" satisfied condition "Succeeded or Failed"
Dec 14 13:08:41.463: INFO: Got logs for pod "busybox-privileged-false-77f7fd3d-b756-4d2c-9e80-f6c98e98c205": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:08:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5765" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":150,"skipped":2484,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:08:41.486: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:08:41.678: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 14 13:08:51.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 create -f -'
Dec 14 13:08:52.761: INFO: stderr: ""
Dec 14 13:08:52.761: INFO: stdout: "e2e-test-crd-publish-openapi-633-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 13:08:52.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 delete e2e-test-crd-publish-openapi-633-crds test-foo'
Dec 14 13:08:52.913: INFO: stderr: ""
Dec 14 13:08:52.914: INFO: stdout: "e2e-test-crd-publish-openapi-633-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 14 13:08:52.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 apply -f -'
Dec 14 13:08:53.576: INFO: stderr: ""
Dec 14 13:08:53.576: INFO: stdout: "e2e-test-crd-publish-openapi-633-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 13:08:53.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 delete e2e-test-crd-publish-openapi-633-crds test-foo'
Dec 14 13:08:53.724: INFO: stderr: ""
Dec 14 13:08:53.724: INFO: stdout: "e2e-test-crd-publish-openapi-633-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 14 13:08:53.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 create -f -'
Dec 14 13:08:54.196: INFO: rc: 1
Dec 14 13:08:54.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 apply -f -'
Dec 14 13:08:54.769: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 14 13:08:54.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 create -f -'
Dec 14 13:08:55.399: INFO: rc: 1
Dec 14 13:08:55.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 --namespace=crd-publish-openapi-4137 apply -f -'
Dec 14 13:08:56.060: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 14 13:08:56.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 explain e2e-test-crd-publish-openapi-633-crds'
Dec 14 13:08:56.825: INFO: stderr: ""
Dec 14 13:08:56.826: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-633-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 14 13:08:56.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 explain e2e-test-crd-publish-openapi-633-crds.metadata'
Dec 14 13:08:57.308: INFO: stderr: ""
Dec 14 13:08:57.308: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-633-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 14 13:08:57.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 explain e2e-test-crd-publish-openapi-633-crds.spec'
Dec 14 13:08:59.843: INFO: stderr: ""
Dec 14 13:08:59.843: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-633-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 14 13:08:59.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 explain e2e-test-crd-publish-openapi-633-crds.spec.bars'
Dec 14 13:09:00.526: INFO: stderr: ""
Dec 14 13:09:00.526: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-633-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 14 13:09:00.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-4137 explain e2e-test-crd-publish-openapi-633-crds.spec.bars2'
Dec 14 13:09:01.037: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:09:11.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4137" for this suite.

• [SLOW TEST:29.777 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":151,"skipped":2502,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:09:11.264: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-2035c6a6-43b0-4385-b0c4-3b0c89b996f4
STEP: Creating a pod to test consume secrets
Dec 14 13:09:11.495: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532" in namespace "projected-2034" to be "Succeeded or Failed"
Dec 14 13:09:11.500: INFO: Pod "pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532": Phase="Pending", Reason="", readiness=false. Elapsed: 4.482538ms
Dec 14 13:09:13.522: INFO: Pod "pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027148645s
Dec 14 13:09:15.539: INFO: Pod "pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04379069s
STEP: Saw pod success
Dec 14 13:09:15.539: INFO: Pod "pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532" satisfied condition "Succeeded or Failed"
Dec 14 13:09:15.549: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:09:15.588: INFO: Waiting for pod pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532 to disappear
Dec 14 13:09:15.594: INFO: Pod pod-projected-secrets-d3148332-669a-46af-baa8-40d81d45c532 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:09:15.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2034" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":152,"skipped":2523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:09:15.618: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6032.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6032.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6032.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6032.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6032.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6032.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:09:19.934: INFO: DNS probes using dns-6032/dns-test-ff75ea95-5904-4674-a13d-2aca519f2515 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:09:19.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6032" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":153,"skipped":2549,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:09:19.970: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Dec 14 13:09:20.161: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:20.161: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:20.196: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:20.196: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:20.218: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:20.218: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:20.334: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:20.334: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 13:09:23.531: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 13:09:23.531: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 13:09:23.862: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Dec 14 13:09:23.898: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 0
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.901: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.914: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.915: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.939: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.939: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:23.979: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:23.979: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:24.006: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:24.006: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:27.634: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:27.634: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:27.677: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
STEP: listing Deployments
Dec 14 13:09:27.731: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Dec 14 13:09:27.912: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Dec 14 13:09:28.028: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:28.028: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:28.028: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:28.031: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:28.031: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:28.032: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:30.906: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:30.935: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:30.955: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:30.970: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:31.043: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 13:09:34.065: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Dec 14 13:09:34.158: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:34.158: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:34.158: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:34.158: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:34.158: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:34.158: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 1
Dec 14 13:09:34.159: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:34.159: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:34.159: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:34.159: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:34.159: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 2
Dec 14 13:09:34.159: INFO: observed Deployment test-deployment in namespace deployment-359 with ReadyReplicas 3
STEP: deleting the Deployment
Dec 14 13:09:34.175: INFO: observed event type MODIFIED
Dec 14 13:09:34.175: INFO: observed event type MODIFIED
Dec 14 13:09:34.175: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
Dec 14 13:09:34.176: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Dec 14 13:09:34.179: INFO: Log out all the ReplicaSets if there is no deployment created
Dec 14 13:09:34.184: INFO: ReplicaSet "test-deployment-748588b7cd":
&ReplicaSet{ObjectMeta:{test-deployment-748588b7cd  deployment-359  9e55ab2d-f031-4b80-a4da-2c0c3d8177fb 80956 4 2021-12-14 13:09:23 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment b7551d6b-8ff4-4781-bba3-3b71d007ff5b 0xc0061a1a57 0xc0061a1a58}] []  [{kube-controller-manager Update apps/v1 2021-12-14 13:09:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7551d6b-8ff4-4781-bba3-3b71d007ff5b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 748588b7cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.4.1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0061a1ac0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 14 13:09:34.193: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-359  9a5a5524-42ea-434e-a4ae-bb18f79a0c9c 80829 3 2021-12-14 13:09:20 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment b7551d6b-8ff4-4781-bba3-3b71d007ff5b 0xc0061a1b27 0xc0061a1b28}] []  [{kube-controller-manager Update apps/v1 2021-12-14 13:09:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7551d6b-8ff4-4781-bba3-3b71d007ff5b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0061a1b90 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 14 13:09:34.197: INFO: ReplicaSet "test-deployment-85d87c6f4b":
&ReplicaSet{ObjectMeta:{test-deployment-85d87c6f4b  deployment-359  b0f76f56-e7b0-4197-9bdc-8bf591f0f61e 80944 2 2021-12-14 13:09:27 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment b7551d6b-8ff4-4781-bba3-3b71d007ff5b 0xc0061a1bf7 0xc0061a1bf8}] []  [{kube-controller-manager Update apps/v1 2021-12-14 13:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7551d6b-8ff4-4781-bba3-3b71d007ff5b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 85d87c6f4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0061a1c70 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec 14 13:09:34.202: INFO: pod: "test-deployment-85d87c6f4b-5br8b":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-5br8b test-deployment-85d87c6f4b- deployment-359  10e4c801-da52-4aba-b5f1-17fc80db206f 80891 0 2021-12-14 13:09:27 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.116"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.116"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b b0f76f56-e7b0-4197-9bdc-8bf591f0f61e 0xc00615ecf7 0xc00615ecf8}] []  [{kube-controller-manager Update v1 2021-12-14 13:09:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0f76f56-e7b0-4197-9bdc-8bf591f0f61e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:09:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-12-14 13:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4fpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4fpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-v8bv9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.116,StartTime:2021-12-14 13:09:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:09:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://ca7c880460c0b165e1a6bbf5249418f1a71c0d4673f7ad8dc666eb3543ac5e92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 14 13:09:34.202: INFO: pod: "test-deployment-85d87c6f4b-sr2zc":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-sr2zc test-deployment-85d87c6f4b- deployment-359  c5f37ee9-9d57-4ed0-980f-2da88ac65ce0 80943 0 2021-12-14 13:09:30 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.93"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.93"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b b0f76f56-e7b0-4197-9bdc-8bf591f0f61e 0xc00615ef77 0xc00615ef78}] []  [{kube-controller-manager Update v1 2021-12-14 13:09:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0f76f56-e7b0-4197-9bdc-8bf591f0f61e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:09:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:09:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4h54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4h54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c48,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-v8bv9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:09:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:10.131.0.93,StartTime:2021-12-14 13:09:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:09:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://4e8c13a1da3c421b4ed76e600300e12f316a9dd62ea2895508c1a1a1304aa766,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:09:34.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-359" for this suite.

• [SLOW TEST:14.248 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":154,"skipped":2552,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:09:34.218: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:09:35.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:09:37.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084175, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084175, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084175, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084175, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:09:40.855: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:09:40.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8719" for this suite.
STEP: Destroying namespace "webhook-8719-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.926 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":155,"skipped":2556,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:09:41.145: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 13:09:41.568: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 14 13:09:41.579: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 13:09:41.599: INFO: waiting for watch events with expected annotations
Dec 14 13:09:41.599: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:09:41.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4682" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":156,"skipped":2558,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:09:41.663: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7208.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7208.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7208.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7208.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:09:45.950: INFO: DNS probes using dns-test-c267f073-55e9-4578-bb24-19a60d813d53 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7208.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7208.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7208.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7208.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:09:50.043: INFO: File wheezy_udp@dns-test-service-3.dns-7208.svc.cluster.local from pod  dns-7208/dns-test-c8286628-e7d9-4035-9c1b-fe78564aabd7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 13:09:50.048: INFO: File jessie_udp@dns-test-service-3.dns-7208.svc.cluster.local from pod  dns-7208/dns-test-c8286628-e7d9-4035-9c1b-fe78564aabd7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 13:09:50.048: INFO: Lookups using dns-7208/dns-test-c8286628-e7d9-4035-9c1b-fe78564aabd7 failed for: [wheezy_udp@dns-test-service-3.dns-7208.svc.cluster.local jessie_udp@dns-test-service-3.dns-7208.svc.cluster.local]

Dec 14 13:09:55.064: INFO: DNS probes using dns-test-c8286628-e7d9-4035-9c1b-fe78564aabd7 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7208.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7208.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7208.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7208.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:09:59.174: INFO: DNS probes using dns-test-e3a51c3e-9c64-476e-ba45-a3edc2972910 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:09:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7208" for this suite.

• [SLOW TEST:17.598 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":157,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:09:59.263: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-211ce519-6b42-49ad-8393-ddfc4e3af7df
STEP: Creating a pod to test consume configMaps
Dec 14 13:09:59.705: INFO: Waiting up to 5m0s for pod "pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b" in namespace "configmap-2468" to be "Succeeded or Failed"
Dec 14 13:09:59.708: INFO: Pod "pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.471127ms
Dec 14 13:10:01.717: INFO: Pod "pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011880277s
Dec 14 13:10:03.724: INFO: Pod "pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019069069s
STEP: Saw pod success
Dec 14 13:10:03.724: INFO: Pod "pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b" satisfied condition "Succeeded or Failed"
Dec 14 13:10:03.728: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:10:03.753: INFO: Waiting for pod pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b to disappear
Dec 14 13:10:03.758: INFO: Pod pod-configmaps-d5a3456f-c66e-46ac-b7d0-f3512524686b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:10:03.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2468" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":158,"skipped":2574,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:10:03.773: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec 14 13:10:03.961: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:10:09.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-143" for this suite.

• [SLOW TEST:5.350 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":159,"skipped":2578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:10:09.123: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-dqcl
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 13:10:09.469: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dqcl" in namespace "subpath-9388" to be "Succeeded or Failed"
Dec 14 13:10:09.473: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.92968ms
Dec 14 13:10:11.480: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011209751s
Dec 14 13:10:13.488: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 4.019017917s
Dec 14 13:10:15.498: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 6.028926585s
Dec 14 13:10:17.507: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 8.037367556s
Dec 14 13:10:19.521: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 10.052083207s
Dec 14 13:10:21.534: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 12.06436662s
Dec 14 13:10:23.539: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 14.070225911s
Dec 14 13:10:25.548: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 16.07913828s
Dec 14 13:10:27.560: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 18.090805308s
Dec 14 13:10:29.572: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 20.102396066s
Dec 14 13:10:31.579: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Running", Reason="", readiness=true. Elapsed: 22.110031428s
Dec 14 13:10:33.585: INFO: Pod "pod-subpath-test-projected-dqcl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.115510449s
STEP: Saw pod success
Dec 14 13:10:33.585: INFO: Pod "pod-subpath-test-projected-dqcl" satisfied condition "Succeeded or Failed"
Dec 14 13:10:33.588: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-subpath-test-projected-dqcl container test-container-subpath-projected-dqcl: <nil>
STEP: delete the pod
Dec 14 13:10:33.624: INFO: Waiting for pod pod-subpath-test-projected-dqcl to disappear
Dec 14 13:10:33.627: INFO: Pod pod-subpath-test-projected-dqcl no longer exists
STEP: Deleting pod pod-subpath-test-projected-dqcl
Dec 14 13:10:33.627: INFO: Deleting pod "pod-subpath-test-projected-dqcl" in namespace "subpath-9388"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:10:33.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9388" for this suite.

• [SLOW TEST:24.525 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":160,"skipped":2636,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:10:33.648: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9631
Dec 14 13:10:33.872: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:10:35.879: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 14 13:10:35.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 13:10:36.096: INFO: rc: 7
Dec 14 13:10:36.111: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:10:36.116: INFO: Pod kube-proxy-mode-detector no longer exists
Dec 14 13:10:36.116: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-9631
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9631
I1214 13:10:36.137485      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9631, replica count: 3
I1214 13:10:39.188922      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:10:42.194893      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:10:42.208: INFO: Creating new exec pod
Dec 14 13:10:47.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Dec 14 13:10:47.487: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 13:10:47.487: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:10:47.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.214.103 80'
Dec 14 13:10:47.747: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.214.103 80\nConnection to 172.30.214.103 80 port [tcp/http] succeeded!\n"
Dec 14 13:10:47.747: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:10:47.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.214.103:80/ ; done'
Dec 14 13:10:48.080: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n"
Dec 14 13:10:48.080: INFO: stdout: "\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g\naffinity-clusterip-timeout-4pg9g"
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Received response from host: affinity-clusterip-timeout-4pg9g
Dec 14 13:10:48.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.214.103:80/'
Dec 14 13:10:48.279: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n"
Dec 14 13:10:48.279: INFO: stdout: "affinity-clusterip-timeout-4pg9g"
Dec 14 13:11:08.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.214.103:80/'
Dec 14 13:11:08.581: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n"
Dec 14 13:11:08.582: INFO: stdout: "affinity-clusterip-timeout-4pg9g"
Dec 14 13:11:28.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.214.103:80/'
Dec 14 13:11:28.920: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n"
Dec 14 13:11:28.920: INFO: stdout: "affinity-clusterip-timeout-4pg9g"
Dec 14 13:11:48.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.214.103:80/'
Dec 14 13:11:49.153: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n"
Dec 14 13:11:49.153: INFO: stdout: "affinity-clusterip-timeout-4pg9g"
Dec 14 13:12:09.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-9631 exec execpod-affinityb45fc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.214.103:80/'
Dec 14 13:12:09.370: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.214.103:80/\n"
Dec 14 13:12:09.370: INFO: stdout: "affinity-clusterip-timeout-jp7ck"
Dec 14 13:12:09.370: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9631, will wait for the garbage collector to delete the pods
Dec 14 13:12:09.449: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.766695ms
Dec 14 13:12:09.550: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.058503ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:12:18.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9631" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:104.959 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":161,"skipped":2645,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:12:18.608: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-2fe2c0a9-6944-4573-9769-4044c3767a57
STEP: Creating a pod to test consume configMaps
Dec 14 13:12:19.243: INFO: Waiting up to 5m0s for pod "pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25" in namespace "configmap-9507" to be "Succeeded or Failed"
Dec 14 13:12:19.247: INFO: Pod "pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925403ms
Dec 14 13:12:21.255: INFO: Pod "pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011469238s
Dec 14 13:12:23.261: INFO: Pod "pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017135673s
STEP: Saw pod success
Dec 14 13:12:23.261: INFO: Pod "pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25" satisfied condition "Succeeded or Failed"
Dec 14 13:12:23.265: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:12:23.294: INFO: Waiting for pod pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25 to disappear
Dec 14 13:12:23.298: INFO: Pod pod-configmaps-f390048e-48f0-4235-92d3-0684b5c41f25 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:12:23.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9507" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":162,"skipped":2655,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:12:23.311: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:12:41.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-275" for this suite.
STEP: Destroying namespace "nsdeletetest-5783" for this suite.
Dec 14 13:12:41.315: INFO: Namespace nsdeletetest-5783 was already deleted
STEP: Destroying namespace "nsdeletetest-960" for this suite.

• [SLOW TEST:18.025 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":163,"skipped":2658,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:12:41.337: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-866c96c7-278d-4ecc-857e-bb021bd6ddc6
STEP: Creating a pod to test consume secrets
Dec 14 13:12:41.584: INFO: Waiting up to 5m0s for pod "pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a" in namespace "secrets-62" to be "Succeeded or Failed"
Dec 14 13:12:41.589: INFO: Pod "pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555854ms
Dec 14 13:12:43.601: INFO: Pod "pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016956529s
Dec 14 13:12:45.617: INFO: Pod "pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032341004s
STEP: Saw pod success
Dec 14 13:12:45.617: INFO: Pod "pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a" satisfied condition "Succeeded or Failed"
Dec 14 13:12:45.621: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:12:45.644: INFO: Waiting for pod pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a to disappear
Dec 14 13:12:45.650: INFO: Pod pod-secrets-8a9b55b3-ef53-4dd3-85b3-90c2933ecd1a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:12:45.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-62" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":164,"skipped":2667,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:12:45.664: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Dec 14 13:13:06.083: INFO: EndpointSlice for Service endpointslice-9502/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:13:16.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9502" for this suite.

• [SLOW TEST:30.461 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":165,"skipped":2673,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:13:16.125: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Dec 14 13:13:16.381: INFO: Waiting up to 5m0s for pod "client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96" in namespace "containers-5441" to be "Succeeded or Failed"
Dec 14 13:13:16.399: INFO: Pod "client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96": Phase="Pending", Reason="", readiness=false. Elapsed: 18.110598ms
Dec 14 13:13:18.406: INFO: Pod "client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024970945s
Dec 14 13:13:20.413: INFO: Pod "client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031369008s
STEP: Saw pod success
Dec 14 13:13:20.413: INFO: Pod "client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96" satisfied condition "Succeeded or Failed"
Dec 14 13:13:20.416: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:13:20.439: INFO: Waiting for pod client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96 to disappear
Dec 14 13:13:20.444: INFO: Pod client-containers-f8176e4d-9f9e-444b-8f4b-29d8ef7f3b96 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:13:20.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5441" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":166,"skipped":2690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:13:20.459: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2932
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-2932
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2932
Dec 14 13:13:20.756: INFO: Found 0 stateful pods, waiting for 1
Dec 14 13:13:30.797: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 14 13:13:30.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:13:31.012: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:13:31.012: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:13:31.012: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:13:31.018: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 13:13:41.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:13:41.024: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:13:41.045: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:13:41.045: INFO: ss-0  ip-10-0-169-135.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  }]
Dec 14 13:13:41.045: INFO: 
Dec 14 13:13:41.045: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 14 13:13:42.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99452373s
Dec 14 13:13:43.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985609144s
Dec 14 13:13:44.069: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978378546s
Dec 14 13:13:45.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969919263s
Dec 14 13:13:46.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961546999s
Dec 14 13:13:47.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952348702s
Dec 14 13:13:48.104: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944757605s
Dec 14 13:13:49.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936059056s
Dec 14 13:13:50.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 930.099543ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2932
Dec 14 13:13:51.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:13:51.351: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 13:13:51.351: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:13:51.351: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:13:51.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:13:51.558: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 13:13:51.558: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:13:51.558: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:13:51.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:13:51.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 13:13:51.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 13:13:51.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 13:13:51.797: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 14 13:14:01.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:14:01.807: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 13:14:01.807: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 14 13:14:01.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:14:02.057: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:14:02.057: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:14:02.057: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:14:02.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:14:02.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:14:02.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:14:02.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:14:02.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 13:14:02.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 13:14:02.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 13:14:02.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 13:14:02.570: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:14:02.577: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 14 13:14:12.593: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:14:12.593: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:14:12.593: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 13:14:12.610: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:12.610: INFO: ss-0  ip-10-0-169-135.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  }]
Dec 14 13:14:12.610: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:12.610: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:12.610: INFO: 
Dec 14 13:14:12.610: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:14:13.620: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:13.620: INFO: ss-0  ip-10-0-169-135.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  }]
Dec 14 13:14:13.620: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:13.620: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:13.620: INFO: 
Dec 14 13:14:13.620: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:14:14.628: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:14.628: INFO: ss-0  ip-10-0-169-135.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  }]
Dec 14 13:14:14.628: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:14.628: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:14.628: INFO: 
Dec 14 13:14:14.628: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:14:15.636: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:15.636: INFO: ss-0  ip-10-0-169-135.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  }]
Dec 14 13:14:15.636: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:15.636: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:15.636: INFO: 
Dec 14 13:14:15.636: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:14:16.643: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:16.643: INFO: ss-0  ip-10-0-169-135.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  }]
Dec 14 13:14:16.643: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:16.643: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:16.643: INFO: 
Dec 14 13:14:16.643: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:14:17.652: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:17.652: INFO: ss-0  ip-10-0-169-135.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:20 +0000 UTC  }]
Dec 14 13:14:17.652: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:17.652: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:17.652: INFO: 
Dec 14 13:14:17.652: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 13:14:18.660: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:18.660: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:18.660: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:18.660: INFO: 
Dec 14 13:14:18.660: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 14 13:14:19.671: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:19.671: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:19.671: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:19.671: INFO: 
Dec 14 13:14:19.671: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 14 13:14:20.680: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:20.680: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:20.680: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:20.680: INFO: 
Dec 14 13:14:20.680: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 14 13:14:21.690: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Dec 14 13:14:21.690: INFO: ss-1  ip-10-0-150-199.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:21.690: INFO: ss-2  ip-10-0-132-177.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:14:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:13:41 +0000 UTC  }]
Dec 14 13:14:21.690: INFO: 
Dec 14 13:14:21.691: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2932
Dec 14 13:14:22.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:14:22.835: INFO: rc: 1
Dec 14 13:14:22.835: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 14 13:14:32.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:14:32.935: INFO: rc: 1
Dec 14 13:14:32.935: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:14:42.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:14:43.056: INFO: rc: 1
Dec 14 13:14:43.056: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:14:53.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:14:53.174: INFO: rc: 1
Dec 14 13:14:53.174: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:15:03.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:15:03.420: INFO: rc: 1
Dec 14 13:15:03.420: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:15:13.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:15:13.540: INFO: rc: 1
Dec 14 13:15:13.540: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:15:23.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:15:23.671: INFO: rc: 1
Dec 14 13:15:23.672: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:15:33.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:15:33.786: INFO: rc: 1
Dec 14 13:15:33.786: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:15:43.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:15:43.940: INFO: rc: 1
Dec 14 13:15:43.940: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:15:53.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:15:54.053: INFO: rc: 1
Dec 14 13:15:54.053: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:16:04.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:16:04.184: INFO: rc: 1
Dec 14 13:16:04.185: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:16:14.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:16:14.302: INFO: rc: 1
Dec 14 13:16:14.303: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:16:24.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:16:24.404: INFO: rc: 1
Dec 14 13:16:24.404: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:16:34.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:16:34.503: INFO: rc: 1
Dec 14 13:16:34.503: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:16:44.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:16:44.605: INFO: rc: 1
Dec 14 13:16:44.605: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:16:54.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:16:54.732: INFO: rc: 1
Dec 14 13:16:54.732: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:17:04.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:17:04.836: INFO: rc: 1
Dec 14 13:17:04.837: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:17:14.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:17:14.975: INFO: rc: 1
Dec 14 13:17:14.975: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:17:24.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:17:25.099: INFO: rc: 1
Dec 14 13:17:25.100: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:17:35.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:17:35.209: INFO: rc: 1
Dec 14 13:17:35.209: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:17:45.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:17:45.309: INFO: rc: 1
Dec 14 13:17:45.310: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:17:55.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:17:55.418: INFO: rc: 1
Dec 14 13:17:55.419: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:18:05.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:18:05.560: INFO: rc: 1
Dec 14 13:18:05.560: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:18:15.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:18:15.689: INFO: rc: 1
Dec 14 13:18:15.689: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:18:25.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:18:25.889: INFO: rc: 1
Dec 14 13:18:25.889: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:18:35.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:18:35.987: INFO: rc: 1
Dec 14 13:18:35.987: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:18:45.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:18:46.088: INFO: rc: 1
Dec 14 13:18:46.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:18:56.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:18:56.521: INFO: rc: 1
Dec 14 13:18:56.521: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:19:06.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:19:06.636: INFO: rc: 1
Dec 14 13:19:06.636: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:19:16.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:19:16.779: INFO: rc: 1
Dec 14 13:19:16.780: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Dec 14 13:19:26.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=statefulset-2932 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 13:19:26.886: INFO: rc: 1
Dec 14 13:19:26.886: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Dec 14 13:19:26.886: INFO: Scaling statefulset ss to 0
Dec 14 13:19:26.908: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Dec 14 13:19:26.911: INFO: Deleting all statefulset in ns statefulset-2932
Dec 14 13:19:26.915: INFO: Scaling statefulset ss to 0
Dec 14 13:19:26.928: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:19:26.931: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:19:26.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2932" for this suite.

• [SLOW TEST:366.508 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":167,"skipped":2717,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:19:26.967: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:19:28.682: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 13:19:30.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084768, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084768, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084768, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084768, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:19:33.733: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:19:33.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2320" for this suite.
STEP: Destroying namespace "webhook-2320-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.994 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":168,"skipped":2721,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:19:33.961: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:19:34.247: INFO: Got root ca configmap in namespace "svcaccounts-6485"
Dec 14 13:19:34.261: INFO: Deleted root ca configmap in namespace "svcaccounts-6485"
STEP: waiting for a new root ca configmap created
Dec 14 13:19:34.766: INFO: Recreated root ca configmap in namespace "svcaccounts-6485"
Dec 14 13:19:34.774: INFO: Updated root ca configmap in namespace "svcaccounts-6485"
STEP: waiting for the root ca configmap reconciled
Dec 14 13:19:35.280: INFO: Reconciled root ca configmap in namespace "svcaccounts-6485"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:19:35.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6485" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":169,"skipped":2723,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:19:35.299: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-c2947e28-9e9a-449a-b2b8-ef532d57de3a in namespace container-probe-9555
Dec 14 13:19:39.555: INFO: Started pod busybox-c2947e28-9e9a-449a-b2b8-ef532d57de3a in namespace container-probe-9555
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:19:39.560: INFO: Initial restart count of pod busybox-c2947e28-9e9a-449a-b2b8-ef532d57de3a is 0
Dec 14 13:20:27.839: INFO: Restart count of pod container-probe-9555/busybox-c2947e28-9e9a-449a-b2b8-ef532d57de3a is now 1 (48.279013577s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:20:27.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9555" for this suite.

• [SLOW TEST:52.591 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":170,"skipped":2732,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:20:27.890: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:20:28.613: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a" in namespace "downward-api-5869" to be "Succeeded or Failed"
Dec 14 13:20:28.618: INFO: Pod "downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.32839ms
Dec 14 13:20:30.630: INFO: Pod "downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016422049s
Dec 14 13:20:32.640: INFO: Pod "downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027106124s
STEP: Saw pod success
Dec 14 13:20:32.641: INFO: Pod "downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a" satisfied condition "Succeeded or Failed"
Dec 14 13:20:32.645: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a container client-container: <nil>
STEP: delete the pod
Dec 14 13:20:32.687: INFO: Waiting for pod downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a to disappear
Dec 14 13:20:32.699: INFO: Pod downwardapi-volume-4f079b51-8b92-4985-a5d8-a901e088f37a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:20:32.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5869" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":171,"skipped":2751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:20:32.716: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:20:34.081: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Dec 14 13:20:36.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084834, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084834, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084834, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775084834, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:20:39.116: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:20:39.122: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:20:42.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5110" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.066 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":172,"skipped":2783,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:20:42.782: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Dec 14 13:20:43.571: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Dec 14 13:20:43.620: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:20:43.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6368" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":173,"skipped":2784,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:20:43.763: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Dec 14 13:20:44.290: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 14 13:20:44.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 create -f -'
Dec 14 13:20:45.404: INFO: stderr: ""
Dec 14 13:20:45.404: INFO: stdout: "service/agnhost-replica created\n"
Dec 14 13:20:45.405: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 14 13:20:45.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 create -f -'
Dec 14 13:20:46.223: INFO: stderr: ""
Dec 14 13:20:46.223: INFO: stdout: "service/agnhost-primary created\n"
Dec 14 13:20:46.224: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 14 13:20:46.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 create -f -'
Dec 14 13:20:47.318: INFO: stderr: ""
Dec 14 13:20:47.318: INFO: stdout: "service/frontend created\n"
Dec 14 13:20:47.319: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 14 13:20:47.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 create -f -'
Dec 14 13:20:48.045: INFO: stderr: ""
Dec 14 13:20:48.045: INFO: stdout: "deployment.apps/frontend created\n"
Dec 14 13:20:48.045: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 13:20:48.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 create -f -'
Dec 14 13:20:48.990: INFO: stderr: ""
Dec 14 13:20:48.990: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 14 13:20:48.990: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 13:20:48.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 create -f -'
Dec 14 13:20:49.499: INFO: stderr: ""
Dec 14 13:20:49.499: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec 14 13:20:49.499: INFO: Waiting for all frontend pods to be Running.
Dec 14 13:20:54.551: INFO: Waiting for frontend to serve content.
Dec 14 13:20:54.569: INFO: Trying to add a new entry to the guestbook.
Dec 14 13:20:54.592: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 14 13:20:54.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 delete --grace-period=0 --force -f -'
Dec 14 13:20:54.729: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:20:54.729: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:20:54.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 delete --grace-period=0 --force -f -'
Dec 14 13:20:55.081: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:20:55.081: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:20:55.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 delete --grace-period=0 --force -f -'
Dec 14 13:20:55.240: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:20:55.240: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:20:55.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 delete --grace-period=0 --force -f -'
Dec 14 13:20:55.336: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:20:55.337: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:20:55.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 delete --grace-period=0 --force -f -'
Dec 14 13:20:55.493: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:20:55.493: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 13:20:55.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-4610 delete --grace-period=0 --force -f -'
Dec 14 13:20:55.691: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:20:55.691: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:20:55.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4610" for this suite.

• [SLOW TEST:11.975 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":174,"skipped":2796,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:20:55.738: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:20:56.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6408" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":175,"skipped":2798,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:20:56.541: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:20:57.015: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 14 13:21:07.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-3174 --namespace=crd-publish-openapi-3174 create -f -'
Dec 14 13:21:09.460: INFO: stderr: ""
Dec 14 13:21:09.460: INFO: stdout: "e2e-test-crd-publish-openapi-8626-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 13:21:09.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-3174 --namespace=crd-publish-openapi-3174 delete e2e-test-crd-publish-openapi-8626-crds test-cr'
Dec 14 13:21:09.606: INFO: stderr: ""
Dec 14 13:21:09.606: INFO: stdout: "e2e-test-crd-publish-openapi-8626-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 14 13:21:09.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-3174 --namespace=crd-publish-openapi-3174 apply -f -'
Dec 14 13:21:10.385: INFO: stderr: ""
Dec 14 13:21:10.385: INFO: stdout: "e2e-test-crd-publish-openapi-8626-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 13:21:10.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-3174 --namespace=crd-publish-openapi-3174 delete e2e-test-crd-publish-openapi-8626-crds test-cr'
Dec 14 13:21:10.506: INFO: stderr: ""
Dec 14 13:21:10.506: INFO: stdout: "e2e-test-crd-publish-openapi-8626-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 14 13:21:10.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=crd-publish-openapi-3174 explain e2e-test-crd-publish-openapi-8626-crds'
Dec 14 13:21:11.206: INFO: stderr: ""
Dec 14 13:21:11.206: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8626-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:21:20.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3174" for this suite.

• [SLOW TEST:24.216 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":176,"skipped":2827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:21:20.757: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7054.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7054.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7054.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7054.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7054.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.43.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.43.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.43.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.43.175_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7054.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7054.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7054.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7054.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7054.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7054.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 175.43.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.43.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.43.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.43.175_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 13:21:25.068: INFO: Unable to read wheezy_udp@dns-test-service.dns-7054.svc.cluster.local from pod dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879: the server could not find the requested resource (get pods dns-test-be056798-6548-4bd1-aba3-252fd8c91879)
Dec 14 13:21:25.078: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local from pod dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879: the server could not find the requested resource (get pods dns-test-be056798-6548-4bd1-aba3-252fd8c91879)
Dec 14 13:21:25.083: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local from pod dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879: the server could not find the requested resource (get pods dns-test-be056798-6548-4bd1-aba3-252fd8c91879)
Dec 14 13:21:25.115: INFO: Unable to read jessie_udp@dns-test-service.dns-7054.svc.cluster.local from pod dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879: the server could not find the requested resource (get pods dns-test-be056798-6548-4bd1-aba3-252fd8c91879)
Dec 14 13:21:25.120: INFO: Unable to read jessie_tcp@dns-test-service.dns-7054.svc.cluster.local from pod dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879: the server could not find the requested resource (get pods dns-test-be056798-6548-4bd1-aba3-252fd8c91879)
Dec 14 13:21:25.124: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local from pod dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879: the server could not find the requested resource (get pods dns-test-be056798-6548-4bd1-aba3-252fd8c91879)
Dec 14 13:21:25.129: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local from pod dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879: the server could not find the requested resource (get pods dns-test-be056798-6548-4bd1-aba3-252fd8c91879)
Dec 14 13:21:25.156: INFO: Lookups using dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879 failed for: [wheezy_udp@dns-test-service.dns-7054.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local jessie_udp@dns-test-service.dns-7054.svc.cluster.local jessie_tcp@dns-test-service.dns-7054.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7054.svc.cluster.local]

Dec 14 13:21:30.257: INFO: DNS probes using dns-7054/dns-test-be056798-6548-4bd1-aba3-252fd8c91879 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:21:30.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7054" for this suite.

• [SLOW TEST:9.602 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":177,"skipped":2877,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:21:30.359: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 14 13:21:30.629: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90214 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:21:30.629: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90214 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 14 13:21:40.653: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90354 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:21:40.653: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90354 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 14 13:21:50.689: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90446 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:21:50.689: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90446 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 14 13:22:00.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90544 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:22:00.700: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4810  f4924b8a-2dc0-4ec3-9185-7e45029f6cca 90544 0 2021-12-14 13:21:30 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-12-14 13:21:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 14 13:22:10.717: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4810  8f49d77f-a467-43dd-ba5f-790d05f6f8dd 90642 0 2021-12-14 13:22:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-14 13:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:22:10.717: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4810  8f49d77f-a467-43dd-ba5f-790d05f6f8dd 90642 0 2021-12-14 13:22:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-14 13:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 14 13:22:20.729: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4810  8f49d77f-a467-43dd-ba5f-790d05f6f8dd 90736 0 2021-12-14 13:22:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-14 13:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 13:22:20.729: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4810  8f49d77f-a467-43dd-ba5f-790d05f6f8dd 90736 0 2021-12-14 13:22:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-12-14 13:22:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:22:30.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4810" for this suite.

• [SLOW TEST:60.400 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":178,"skipped":2885,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:22:30.759: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Dec 14 13:22:30.974: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-8918 proxy --unix-socket=/tmp/kubectl-proxy-unix310635606/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:22:31.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8918" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":179,"skipped":2890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:22:31.093: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W1214 13:22:31.303785      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:01.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8838" for this suite.

• [SLOW TEST:330.277 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":180,"skipped":2916,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:01.370: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:28:01.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c" in namespace "projected-5372" to be "Succeeded or Failed"
Dec 14 13:28:01.670: INFO: Pod "downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.292443ms
Dec 14 13:28:03.675: INFO: Pod "downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023702531s
Dec 14 13:28:05.683: INFO: Pod "downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03190509s
STEP: Saw pod success
Dec 14 13:28:05.683: INFO: Pod "downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c" satisfied condition "Succeeded or Failed"
Dec 14 13:28:05.687: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c container client-container: <nil>
STEP: delete the pod
Dec 14 13:28:05.718: INFO: Waiting for pod downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c to disappear
Dec 14 13:28:05.723: INFO: Pod downwardapi-volume-d800ec87-aaa3-4637-b881-ce12f9d8675c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:05.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5372" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":181,"skipped":2935,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:05.738: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 14 13:28:05.949: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 14 13:28:10.957: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:11.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2893" for this suite.

• [SLOW TEST:6.273 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":182,"skipped":2941,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:12.012: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:28:12.350: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39" in namespace "projected-8014" to be "Succeeded or Failed"
Dec 14 13:28:12.361: INFO: Pod "downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.799837ms
Dec 14 13:28:14.370: INFO: Pod "downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01980284s
Dec 14 13:28:16.378: INFO: Pod "downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028590823s
STEP: Saw pod success
Dec 14 13:28:16.378: INFO: Pod "downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39" satisfied condition "Succeeded or Failed"
Dec 14 13:28:16.383: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39 container client-container: <nil>
STEP: delete the pod
Dec 14 13:28:16.421: INFO: Waiting for pod downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39 to disappear
Dec 14 13:28:16.436: INFO: Pod downwardapi-volume-e19e1544-1256-4bf9-b9c3-57f66ed2fa39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:16.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8014" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":183,"skipped":2949,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 13:28:16.655: INFO: Waiting up to 5m0s for pod "pod-d7973770-65e2-4a93-bd60-376676680958" in namespace "emptydir-9940" to be "Succeeded or Failed"
Dec 14 13:28:16.661: INFO: Pod "pod-d7973770-65e2-4a93-bd60-376676680958": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30998ms
Dec 14 13:28:18.669: INFO: Pod "pod-d7973770-65e2-4a93-bd60-376676680958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013538169s
Dec 14 13:28:20.677: INFO: Pod "pod-d7973770-65e2-4a93-bd60-376676680958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02174417s
STEP: Saw pod success
Dec 14 13:28:20.677: INFO: Pod "pod-d7973770-65e2-4a93-bd60-376676680958" satisfied condition "Succeeded or Failed"
Dec 14 13:28:20.684: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-d7973770-65e2-4a93-bd60-376676680958 container test-container: <nil>
STEP: delete the pod
Dec 14 13:28:20.713: INFO: Waiting for pod pod-d7973770-65e2-4a93-bd60-376676680958 to disappear
Dec 14 13:28:20.716: INFO: Pod pod-d7973770-65e2-4a93-bd60-376676680958 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:20.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9940" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":184,"skipped":2949,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:20.734: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:28:20.957: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:28:22.970: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:28:24.963: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:26.970: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:29.015: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:30.972: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:32.969: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:34.964: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:36.966: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:38.969: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:40.964: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = false)
Dec 14 13:28:42.969: INFO: The status of Pod test-webserver-2b299823-86e3-4847-a532-cb92ae94221d is Running (Ready = true)
Dec 14 13:28:42.972: INFO: Container started at 2021-12-14 13:28:23 +0000 UTC, pod became ready at 2021-12-14 13:28:40 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:42.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9168" for this suite.

• [SLOW TEST:22.253 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":185,"skipped":2964,"failed":0}
S
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:42.986: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 13:28:43.211: INFO: The status of Pod pod-update-f32df154-ffce-444c-b664-df224b8ab901 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:28:45.222: INFO: The status of Pod pod-update-f32df154-ffce-444c-b664-df224b8ab901 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:28:47.218: INFO: The status of Pod pod-update-f32df154-ffce-444c-b664-df224b8ab901 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 13:28:47.755: INFO: Successfully updated pod "pod-update-f32df154-ffce-444c-b664-df224b8ab901"
STEP: verifying the updated pod is in kubernetes
Dec 14 13:28:47.764: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:47.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-928" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":186,"skipped":2965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:47.778: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Dec 14 13:28:48.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 create -f -'
Dec 14 13:28:48.868: INFO: stderr: ""
Dec 14 13:28:48.868: INFO: stdout: "pod/pause created\n"
Dec 14 13:28:48.868: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 14 13:28:48.868: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9697" to be "running and ready"
Dec 14 13:28:48.877: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.55507ms
Dec 14 13:28:50.882: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014174844s
Dec 14 13:28:52.891: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.02279768s
Dec 14 13:28:52.891: INFO: Pod "pause" satisfied condition "running and ready"
Dec 14 13:28:52.891: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 14 13:28:52.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 label pods pause testing-label=testing-label-value'
Dec 14 13:28:53.021: INFO: stderr: ""
Dec 14 13:28:53.021: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 14 13:28:53.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 get pod pause -L testing-label'
Dec 14 13:28:53.137: INFO: stderr: ""
Dec 14 13:28:53.137: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 14 13:28:53.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 label pods pause testing-label-'
Dec 14 13:28:53.261: INFO: stderr: ""
Dec 14 13:28:53.261: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 14 13:28:53.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 get pod pause -L testing-label'
Dec 14 13:28:53.365: INFO: stderr: ""
Dec 14 13:28:53.365: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Dec 14 13:28:53.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 delete --grace-period=0 --force -f -'
Dec 14 13:28:53.533: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 13:28:53.533: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 14 13:28:53.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 get rc,svc -l name=pause --no-headers'
Dec 14 13:28:53.661: INFO: stderr: "No resources found in kubectl-9697 namespace.\n"
Dec 14 13:28:53.661: INFO: stdout: ""
Dec 14 13:28:53.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-9697 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 13:28:53.770: INFO: stderr: ""
Dec 14 13:28:53.770: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:28:53.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9697" for this suite.

• [SLOW TEST:6.008 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":187,"skipped":2990,"failed":0}
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:28:53.786: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:29:10.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9088" for this suite.

• [SLOW TEST:16.770 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":188,"skipped":2990,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:29:10.555: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec 14 13:29:10.852: INFO: The status of Pod annotationupdate4ae00f2f-2008-4307-a084-50406c33c953 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:29:12.862: INFO: The status of Pod annotationupdate4ae00f2f-2008-4307-a084-50406c33c953 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:29:14.860: INFO: The status of Pod annotationupdate4ae00f2f-2008-4307-a084-50406c33c953 is Running (Ready = true)
Dec 14 13:29:15.409: INFO: Successfully updated pod "annotationupdate4ae00f2f-2008-4307-a084-50406c33c953"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:29:17.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9383" for this suite.

• [SLOW TEST:6.906 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":189,"skipped":2991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:29:17.462: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec 14 13:29:17.646: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Dec 14 13:29:18.147: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Dec 14 13:29:20.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:22.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:24.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:26.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:28.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:30.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:32.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:34.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085358, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 13:29:36.384: INFO: Waited 144.305298ms for the sample-apiserver to be ready to handle requests.
I1214 13:29:37.500054      20 request.go:668] Waited for 1.049429609s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/security.internal.openshift.io/v1?timeout=32s
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Dec 14 13:29:38.772: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:29:39.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1466" for this suite.

• [SLOW TEST:22.151 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":190,"skipped":3032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:29:39.613: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec 14 13:29:40.146: INFO: Waiting up to 5m0s for pod "security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986" in namespace "security-context-8007" to be "Succeeded or Failed"
Dec 14 13:29:40.151: INFO: Pod "security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430074ms
Dec 14 13:29:42.163: INFO: Pod "security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016390404s
Dec 14 13:29:44.174: INFO: Pod "security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027462442s
Dec 14 13:29:46.181: INFO: Pod "security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034920902s
STEP: Saw pod success
Dec 14 13:29:46.181: INFO: Pod "security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986" satisfied condition "Succeeded or Failed"
Dec 14 13:29:46.186: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986 container test-container: <nil>
STEP: delete the pod
Dec 14 13:29:46.211: INFO: Waiting for pod security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986 to disappear
Dec 14 13:29:46.215: INFO: Pod security-context-5c21d61d-f794-4ce7-a341-57b8d02fb986 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:29:46.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8007" for this suite.

• [SLOW TEST:6.616 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":191,"skipped":3055,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:29:46.229: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6480
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 13:29:46.404: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 13:29:46.518: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:29:48.530: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:29:50.524: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:52.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:54.525: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:56.529: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:29:58.645: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:30:00.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:30:02.523: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:30:04.527: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:30:06.525: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:30:08.529: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 13:30:08.537: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 13:30:08.546: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 13:30:12.618: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 13:30:12.618: INFO: Going to poll 10.129.2.59 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 13:30:12.622: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6480 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:30:12.622: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:30:13.727: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 13:30:13.727: INFO: Going to poll 10.131.0.104 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 13:30:13.734: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.104 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6480 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:30:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:30:14.843: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 14 13:30:14.843: INFO: Going to poll 10.128.2.153 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 13:30:14.849: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6480 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:30:14.849: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:30:15.967: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:30:15.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6480" for this suite.

• [SLOW TEST:29.764 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":192,"skipped":3071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:30:15.994: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Dec 14 13:30:16.194: INFO: Waiting up to 5m0s for pod "test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5" in namespace "svcaccounts-4326" to be "Succeeded or Failed"
Dec 14 13:30:16.199: INFO: Pod "test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190698ms
Dec 14 13:30:18.205: INFO: Pod "test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011011242s
Dec 14 13:30:20.218: INFO: Pod "test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023721961s
STEP: Saw pod success
Dec 14 13:30:20.218: INFO: Pod "test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5" satisfied condition "Succeeded or Failed"
Dec 14 13:30:20.221: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:30:20.248: INFO: Waiting for pod test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5 to disappear
Dec 14 13:30:20.253: INFO: Pod test-pod-71c89418-f039-46c3-aee6-188a89ccbdd5 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:30:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4326" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":193,"skipped":3099,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:30:20.265: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:30:21.062: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 13:30:23.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085421, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085421, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085421, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085421, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:30:26.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:30:26.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7066" for this suite.
STEP: Destroying namespace "webhook-7066-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.260 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":194,"skipped":3123,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:30:26.526: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:30:26.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3" in namespace "downward-api-2410" to be "Succeeded or Failed"
Dec 14 13:30:26.833: INFO: Pod "downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.853814ms
Dec 14 13:30:28.852: INFO: Pod "downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029630232s
Dec 14 13:30:30.861: INFO: Pod "downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038515696s
STEP: Saw pod success
Dec 14 13:30:30.861: INFO: Pod "downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3" satisfied condition "Succeeded or Failed"
Dec 14 13:30:30.864: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3 container client-container: <nil>
STEP: delete the pod
Dec 14 13:30:30.885: INFO: Waiting for pod downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3 to disappear
Dec 14 13:30:30.889: INFO: Pod downwardapi-volume-2b42da14-7f2d-4387-8aef-0faa82b70cd3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:30:30.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2410" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":195,"skipped":3130,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:30:30.914: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:30:31.130: INFO: Creating pod...
Dec 14 13:30:31.190: INFO: Pod Quantity: 1 Status: Pending
Dec 14 13:30:32.196: INFO: Pod Quantity: 1 Status: Pending
Dec 14 13:30:33.196: INFO: Pod Quantity: 1 Status: Pending
Dec 14 13:30:34.198: INFO: Pod Quantity: 1 Status: Pending
Dec 14 13:30:35.199: INFO: Pod Status: Running
Dec 14 13:30:35.199: INFO: Creating service...
Dec 14 13:30:35.216: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/pods/agnhost/proxy/some/path/with/DELETE
Dec 14 13:30:35.225: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 13:30:35.225: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/pods/agnhost/proxy/some/path/with/GET
Dec 14 13:30:35.230: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 13:30:35.230: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/pods/agnhost/proxy/some/path/with/HEAD
Dec 14 13:30:35.244: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 13:30:35.244: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/pods/agnhost/proxy/some/path/with/OPTIONS
Dec 14 13:30:35.255: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 13:30:35.255: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/pods/agnhost/proxy/some/path/with/PATCH
Dec 14 13:30:35.260: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 13:30:35.260: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/pods/agnhost/proxy/some/path/with/POST
Dec 14 13:30:35.265: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 13:30:35.265: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/pods/agnhost/proxy/some/path/with/PUT
Dec 14 13:30:35.271: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 13:30:35.271: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/services/test-service/proxy/some/path/with/DELETE
Dec 14 13:30:35.286: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 13:30:35.286: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/services/test-service/proxy/some/path/with/GET
Dec 14 13:30:35.294: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 13:30:35.294: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/services/test-service/proxy/some/path/with/HEAD
Dec 14 13:30:35.300: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 13:30:35.300: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/services/test-service/proxy/some/path/with/OPTIONS
Dec 14 13:30:35.308: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 13:30:35.308: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/services/test-service/proxy/some/path/with/PATCH
Dec 14 13:30:35.316: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 13:30:35.316: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/services/test-service/proxy/some/path/with/POST
Dec 14 13:30:35.324: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 13:30:35.324: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2503/services/test-service/proxy/some/path/with/PUT
Dec 14 13:30:35.332: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:30:35.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2503" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":196,"skipped":3131,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:30:35.350: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:30:36.373: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 13:30:38.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085436, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085436, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085436, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085436, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:30:41.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:30:41.429: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2722-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:30:44.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8368" for this suite.
STEP: Destroying namespace "webhook-8368-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.550 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":197,"skipped":3137,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:30:44.900: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:30:45.754: INFO: Waiting up to 5m0s for pod "downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0" in namespace "projected-4125" to be "Succeeded or Failed"
Dec 14 13:30:45.776: INFO: Pod "downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.201704ms
Dec 14 13:30:47.790: INFO: Pod "downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036336492s
Dec 14 13:30:49.799: INFO: Pod "downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045265726s
STEP: Saw pod success
Dec 14 13:30:49.799: INFO: Pod "downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0" satisfied condition "Succeeded or Failed"
Dec 14 13:30:49.803: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0 container client-container: <nil>
STEP: delete the pod
Dec 14 13:30:49.831: INFO: Waiting for pod downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0 to disappear
Dec 14 13:30:49.837: INFO: Pod downwardapi-volume-341615bd-c974-4567-ae7a-7578a68944d0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:30:49.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4125" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":198,"skipped":3143,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:30:49.853: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:31:03.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3708" for this suite.

• [SLOW TEST:13.455 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":199,"skipped":3161,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:31:03.309: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 13:31:03.702: INFO: Waiting up to 5m0s for pod "pod-c25ef7d3-078f-4729-825d-b453abb7278f" in namespace "emptydir-4564" to be "Succeeded or Failed"
Dec 14 13:31:03.706: INFO: Pod "pod-c25ef7d3-078f-4729-825d-b453abb7278f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374706ms
Dec 14 13:31:05.713: INFO: Pod "pod-c25ef7d3-078f-4729-825d-b453abb7278f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011493883s
Dec 14 13:31:07.724: INFO: Pod "pod-c25ef7d3-078f-4729-825d-b453abb7278f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022352267s
STEP: Saw pod success
Dec 14 13:31:07.724: INFO: Pod "pod-c25ef7d3-078f-4729-825d-b453abb7278f" satisfied condition "Succeeded or Failed"
Dec 14 13:31:07.728: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-c25ef7d3-078f-4729-825d-b453abb7278f container test-container: <nil>
STEP: delete the pod
Dec 14 13:31:07.760: INFO: Waiting for pod pod-c25ef7d3-078f-4729-825d-b453abb7278f to disappear
Dec 14 13:31:07.767: INFO: Pod pod-c25ef7d3-078f-4729-825d-b453abb7278f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:31:07.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4564" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":200,"skipped":3167,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:31:07.780: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:31:07.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be" in namespace "projected-8184" to be "Succeeded or Failed"
Dec 14 13:31:07.995: INFO: Pod "downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be": Phase="Pending", Reason="", readiness=false. Elapsed: 22.779772ms
Dec 14 13:31:10.004: INFO: Pod "downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03165597s
Dec 14 13:31:12.016: INFO: Pod "downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043531098s
STEP: Saw pod success
Dec 14 13:31:12.016: INFO: Pod "downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be" satisfied condition "Succeeded or Failed"
Dec 14 13:31:12.019: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be container client-container: <nil>
STEP: delete the pod
Dec 14 13:31:12.056: INFO: Waiting for pod downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be to disappear
Dec 14 13:31:12.059: INFO: Pod downwardapi-volume-b00ccca6-589b-422b-a91a-ddf1eddb59be no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:31:12.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8184" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":201,"skipped":3169,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:31:12.072: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:31:12.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8904" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":202,"skipped":3173,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:31:12.402: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:31:12.560: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-9299b397-2178-4cac-a1f0-a9066f7ba102
STEP: Creating secret with name s-test-opt-upd-3d1ec88e-f1f3-41a1-a91b-7138d45440fe
STEP: Creating the pod
Dec 14 13:31:12.623: INFO: The status of Pod pod-projected-secrets-321d572a-2730-4946-b127-959a0b7e9fb1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:31:14.629: INFO: The status of Pod pod-projected-secrets-321d572a-2730-4946-b127-959a0b7e9fb1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:31:16.639: INFO: The status of Pod pod-projected-secrets-321d572a-2730-4946-b127-959a0b7e9fb1 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-9299b397-2178-4cac-a1f0-a9066f7ba102
STEP: Updating secret s-test-opt-upd-3d1ec88e-f1f3-41a1-a91b-7138d45440fe
STEP: Creating secret with name s-test-opt-create-5a4cad90-a844-4505-b0b3-674ab12d5244
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:31:20.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-705" for this suite.

• [SLOW TEST:8.365 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":203,"skipped":3188,"failed":0}
SS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:31:20.768: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W1214 13:31:21.012308      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:33:01.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9354" for this suite.

• [SLOW TEST:100.298 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":204,"skipped":3190,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:33:01.066: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7918
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7918
I1214 13:33:01.367089      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7918, replica count: 2
I1214 13:33:04.418070      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:33:07.418845      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:33:07.418: INFO: Creating new exec pod
Dec 14 13:33:12.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 13:33:13.132: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:13.132: INFO: stdout: "externalname-service-jk59d"
Dec 14 13:33:13.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.97.231 80'
Dec 14 13:33:13.334: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.97.231 80\nConnection to 172.30.97.231 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:13.334: INFO: stdout: ""
Dec 14 13:33:14.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.97.231 80'
Dec 14 13:33:14.555: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.97.231 80\nConnection to 172.30.97.231 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:14.556: INFO: stdout: ""
Dec 14 13:33:15.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.97.231 80'
Dec 14 13:33:15.541: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.97.231 80\nConnection to 172.30.97.231 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:15.541: INFO: stdout: ""
Dec 14 13:33:16.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.97.231 80'
Dec 14 13:33:16.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.97.231 80\nConnection to 172.30.97.231 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:16.567: INFO: stdout: ""
Dec 14 13:33:17.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.97.231 80'
Dec 14 13:33:17.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.97.231 80\nConnection to 172.30.97.231 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:17.552: INFO: stdout: ""
Dec 14 13:33:18.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.97.231 80'
Dec 14 13:33:18.560: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.97.231 80\nConnection to 172.30.97.231 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:18.560: INFO: stdout: ""
Dec 14 13:33:19.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.97.231 80'
Dec 14 13:33:19.566: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 172.30.97.231 80\nConnection to 172.30.97.231 80 port [tcp/http] succeeded!\n"
Dec 14 13:33:19.566: INFO: stdout: "externalname-service-jk59d"
Dec 14 13:33:19.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.132.177 30970'
Dec 14 13:33:19.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.132.177 30970\nConnection to 10.0.132.177 30970 port [tcp/*] succeeded!\n"
Dec 14 13:33:19.868: INFO: stdout: ""
Dec 14 13:33:20.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.132.177 30970'
Dec 14 13:33:21.094: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.0.132.177 30970\nConnection to 10.0.132.177 30970 port [tcp/*] succeeded!\n"
Dec 14 13:33:21.094: INFO: stdout: "externalname-service-jk59d"
Dec 14 13:33:21.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.169.135 30970'
Dec 14 13:33:21.325: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.169.135 30970\nConnection to 10.0.169.135 30970 port [tcp/*] succeeded!\n"
Dec 14 13:33:21.325: INFO: stdout: ""
Dec 14 13:33:22.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-7918 exec execpod6rgc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.169.135 30970'
Dec 14 13:33:22.540: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.169.135 30970\nConnection to 10.0.169.135 30970 port [tcp/*] succeeded!\n"
Dec 14 13:33:22.540: INFO: stdout: "externalname-service-jk59d"
Dec 14 13:33:22.540: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:33:22.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7918" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:21.560 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":205,"skipped":3190,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:33:22.626: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
W1214 13:33:22.976630      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: getting
STEP: listing
STEP: watching
Dec 14 13:33:23.014: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 14 13:33:23.019: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 13:33:23.054: INFO: waiting for watch events with expected annotations
Dec 14 13:33:23.054: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:33:23.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8930" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":206,"skipped":3209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:33:23.153: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 14 13:33:23.400: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 13:34:23.669: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Dec 14 13:34:23.754: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 14 13:34:23.788: INFO: Created pod: pod1-sched-preemption-medium-priority
Dec 14 13:34:23.826: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:34:58.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9719" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:95.029 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":207,"skipped":3243,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:34:58.182: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-b1ce3798-a00a-4b26-9394-885f0f977e1c
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:34:58.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3502" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":208,"skipped":3256,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:34:58.518: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-0a2ae83a-5608-4e42-b077-77994d2bbacd
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:34:58.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3414" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":209,"skipped":3275,"failed":0}
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:34:58.751: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:34:59.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7600" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":210,"skipped":3278,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:34:59.081: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 13:34:59.396: INFO: Waiting up to 5m0s for pod "pod-4b773e05-b020-4f92-bb70-0be475699499" in namespace "emptydir-1228" to be "Succeeded or Failed"
Dec 14 13:34:59.405: INFO: Pod "pod-4b773e05-b020-4f92-bb70-0be475699499": Phase="Pending", Reason="", readiness=false. Elapsed: 8.933302ms
Dec 14 13:35:01.412: INFO: Pod "pod-4b773e05-b020-4f92-bb70-0be475699499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01676287s
Dec 14 13:35:03.434: INFO: Pod "pod-4b773e05-b020-4f92-bb70-0be475699499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037937394s
STEP: Saw pod success
Dec 14 13:35:03.434: INFO: Pod "pod-4b773e05-b020-4f92-bb70-0be475699499" satisfied condition "Succeeded or Failed"
Dec 14 13:35:03.438: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-4b773e05-b020-4f92-bb70-0be475699499 container test-container: <nil>
STEP: delete the pod
Dec 14 13:35:03.476: INFO: Waiting for pod pod-4b773e05-b020-4f92-bb70-0be475699499 to disappear
Dec 14 13:35:03.481: INFO: Pod pod-4b773e05-b020-4f92-bb70-0be475699499 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:35:03.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1228" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":211,"skipped":3281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:35:03.495: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:35:03.740: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ff0e6f22-f960-4a9e-a36e-428f1e3e8078" in namespace "security-context-test-4136" to be "Succeeded or Failed"
Dec 14 13:35:03.749: INFO: Pod "busybox-user-65534-ff0e6f22-f960-4a9e-a36e-428f1e3e8078": Phase="Pending", Reason="", readiness=false. Elapsed: 8.814094ms
Dec 14 13:35:05.755: INFO: Pod "busybox-user-65534-ff0e6f22-f960-4a9e-a36e-428f1e3e8078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014597628s
Dec 14 13:35:07.764: INFO: Pod "busybox-user-65534-ff0e6f22-f960-4a9e-a36e-428f1e3e8078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023827656s
Dec 14 13:35:07.764: INFO: Pod "busybox-user-65534-ff0e6f22-f960-4a9e-a36e-428f1e3e8078" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:35:07.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4136" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":212,"skipped":3311,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:35:07.783: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:35:14.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7886" for this suite.

• [SLOW TEST:6.285 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":213,"skipped":3317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:35:14.068: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec 14 13:35:14.277: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:35:16.284: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:35:18.289: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:35:20.285: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec 14 13:35:20.317: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:35:22.322: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:35:24.324: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec 14 13:35:24.337: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:24.342: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:26.343: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:26.349: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:28.348: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:28.362: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:30.342: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:30.352: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:32.342: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:32.353: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:34.342: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:34.352: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:36.342: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:36.350: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:38.343: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:38.356: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 13:35:40.343: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 13:35:40.354: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:35:40.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2663" for this suite.

• [SLOW TEST:26.310 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":214,"skipped":3353,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:35:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:35:41.363: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 13:35:43.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085741, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085741, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085741, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085741, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:35:46.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:35:56.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4242" for this suite.
STEP: Destroying namespace "webhook-4242-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":215,"skipped":3357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:35:56.779: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:35:57.127: INFO: Create a RollingUpdate DaemonSet
Dec 14 13:35:57.136: INFO: Check that daemon pods launch on every node of the cluster
Dec 14 13:35:57.146: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:35:57.146: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:35:57.146: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:35:57.149: INFO: Number of nodes with available pods: 0
Dec 14 13:35:57.149: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:35:58.163: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:35:58.163: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:35:58.163: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:35:58.173: INFO: Number of nodes with available pods: 0
Dec 14 13:35:58.173: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:35:59.177: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:35:59.178: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:35:59.178: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:35:59.184: INFO: Number of nodes with available pods: 0
Dec 14 13:35:59.184: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:36:00.161: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:00.161: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:00.161: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:00.166: INFO: Number of nodes with available pods: 1
Dec 14 13:36:00.166: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:36:01.161: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:01.161: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:01.161: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:01.165: INFO: Number of nodes with available pods: 3
Dec 14 13:36:01.166: INFO: Number of running nodes: 3, number of available pods: 3
Dec 14 13:36:01.166: INFO: Update the DaemonSet to trigger a rollout
Dec 14 13:36:01.239: INFO: Updating DaemonSet daemon-set
Dec 14 13:36:09.261: INFO: Roll back the DaemonSet before rollout is complete
Dec 14 13:36:09.275: INFO: Updating DaemonSet daemon-set
Dec 14 13:36:09.275: INFO: Make sure DaemonSet rollback is complete
Dec 14 13:36:09.279: INFO: Wrong image for pod: daemon-set-8g69f. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Dec 14 13:36:09.279: INFO: Pod daemon-set-8g69f is not available
Dec 14 13:36:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:09.285: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:10.298: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:10.298: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:10.298: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:11.299: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:11.299: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:11.299: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:12.299: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:12.299: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:12.299: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:13.305: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:13.305: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:13.305: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:14.305: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:14.305: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:14.305: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:15.304: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:15.304: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:15.304: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:16.296: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:16.296: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:16.297: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:17.306: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:17.306: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:17.306: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:18.298: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:18.298: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:18.298: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:36:19.293: INFO: Pod daemon-set-qzkmj is not available
Dec 14 13:36:19.301: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:19.301: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:36:19.301: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2796, will wait for the garbage collector to delete the pods
Dec 14 13:36:19.376: INFO: Deleting DaemonSet.extensions daemon-set took: 10.142651ms
Dec 14 13:36:19.477: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.917234ms
Dec 14 13:36:28.098: INFO: Number of nodes with available pods: 0
Dec 14 13:36:28.098: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 13:36:28.105: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"102042"},"items":null}

Dec 14 13:36:28.116: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"102042"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:36:28.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2796" for this suite.

• [SLOW TEST:31.403 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":216,"skipped":3386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:36:28.182: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 13:36:32.582: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:36:32.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-131" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":217,"skipped":3417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:36:32.624: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Dec 14 13:36:32.864: INFO: Waiting up to 5m0s for pod "client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7" in namespace "containers-2774" to be "Succeeded or Failed"
Dec 14 13:36:32.869: INFO: Pod "client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.706858ms
Dec 14 13:36:34.875: INFO: Pod "client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011446297s
Dec 14 13:36:36.884: INFO: Pod "client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020076844s
STEP: Saw pod success
Dec 14 13:36:36.884: INFO: Pod "client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7" satisfied condition "Succeeded or Failed"
Dec 14 13:36:36.887: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:36:36.913: INFO: Waiting for pod client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7 to disappear
Dec 14 13:36:36.916: INFO: Pod client-containers-e75f56bf-b828-4ce2-8036-64d7bc1685e7 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:36:36.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2774" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":218,"skipped":3467,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:36:36.933: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:36:37.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e" in namespace "downward-api-9240" to be "Succeeded or Failed"
Dec 14 13:36:37.161: INFO: Pod "downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.94325ms
Dec 14 13:36:39.177: INFO: Pod "downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026583855s
Dec 14 13:36:41.184: INFO: Pod "downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032808204s
STEP: Saw pod success
Dec 14 13:36:41.184: INFO: Pod "downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e" satisfied condition "Succeeded or Failed"
Dec 14 13:36:41.188: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e container client-container: <nil>
STEP: delete the pod
Dec 14 13:36:41.217: INFO: Waiting for pod downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e to disappear
Dec 14 13:36:41.226: INFO: Pod downwardapi-volume-2402ceb2-a3c0-4fb4-8ad3-c40c796d800e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:36:41.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9240" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":219,"skipped":3482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:36:41.241: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 13:36:41.519: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6" in namespace "downward-api-7691" to be "Succeeded or Failed"
Dec 14 13:36:41.526: INFO: Pod "downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.325467ms
Dec 14 13:36:43.544: INFO: Pod "downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024921891s
Dec 14 13:36:45.556: INFO: Pod "downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036845105s
STEP: Saw pod success
Dec 14 13:36:45.556: INFO: Pod "downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6" satisfied condition "Succeeded or Failed"
Dec 14 13:36:45.560: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6 container client-container: <nil>
STEP: delete the pod
Dec 14 13:36:45.590: INFO: Waiting for pod downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6 to disappear
Dec 14 13:36:45.594: INFO: Pod downwardapi-volume-a02ef96a-6e56-41d1-a56a-43cc3d97b3a6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:36:45.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7691" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":220,"skipped":3515,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:36:45.608: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:36:46.685: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 13:36:48.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085806, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085806, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085806, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775085806, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:36:51.749: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:36:51.768: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9677-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:36:54.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1532" for this suite.
STEP: Destroying namespace "webhook-1532-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.511 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":221,"skipped":3519,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:36:55.119: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:36:56.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1628" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":222,"skipped":3538,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:36:56.273: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-e2076175-12cf-4f09-b7cd-76ad64a675bf
STEP: Creating a pod to test consume secrets
Dec 14 13:36:56.818: INFO: Waiting up to 5m0s for pod "pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2" in namespace "secrets-7309" to be "Succeeded or Failed"
Dec 14 13:36:56.834: INFO: Pod "pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.095565ms
Dec 14 13:36:58.927: INFO: Pod "pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109013264s
Dec 14 13:37:00.942: INFO: Pod "pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.124507851s
STEP: Saw pod success
Dec 14 13:37:00.942: INFO: Pod "pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2" satisfied condition "Succeeded or Failed"
Dec 14 13:37:00.948: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:37:00.995: INFO: Waiting for pod pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2 to disappear
Dec 14 13:37:01.002: INFO: Pod pod-secrets-b1c84170-c5ec-4ea0-b6de-3f3daf3627b2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:37:01.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7309" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":223,"skipped":3544,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:37:01.018: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Dec 14 13:37:01.311: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:37:55.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9662" for this suite.

• [SLOW TEST:54.241 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":224,"skipped":3552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:37:55.259: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-fb1c331c-15b1-4ccc-a36b-0817e4071d3b
STEP: Creating a pod to test consume secrets
Dec 14 13:37:55.510: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5" in namespace "projected-3048" to be "Succeeded or Failed"
Dec 14 13:37:55.517: INFO: Pod "pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.656568ms
Dec 14 13:37:57.522: INFO: Pod "pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011877162s
Dec 14 13:37:59.531: INFO: Pod "pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021055254s
STEP: Saw pod success
Dec 14 13:37:59.531: INFO: Pod "pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5" satisfied condition "Succeeded or Failed"
Dec 14 13:37:59.535: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:37:59.560: INFO: Waiting for pod pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5 to disappear
Dec 14 13:37:59.564: INFO: Pod pod-projected-secrets-91624787-d231-417f-bb7e-61b1dbcd9cd5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:37:59.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3048" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":225,"skipped":3585,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:37:59.579: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-6069ba6d-c483-490a-b85b-0820edf39ca8
STEP: Creating a pod to test consume configMaps
Dec 14 13:37:59.810: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3" in namespace "projected-62" to be "Succeeded or Failed"
Dec 14 13:37:59.815: INFO: Pod "pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.296183ms
Dec 14 13:38:01.826: INFO: Pod "pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015808565s
Dec 14 13:38:03.839: INFO: Pod "pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028779619s
STEP: Saw pod success
Dec 14 13:38:03.839: INFO: Pod "pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3" satisfied condition "Succeeded or Failed"
Dec 14 13:38:03.843: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:38:03.879: INFO: Waiting for pod pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3 to disappear
Dec 14 13:38:03.893: INFO: Pod pod-projected-configmaps-9407e1ca-ab99-46b3-8d46-a0b9f9390df3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:38:03.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-62" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":226,"skipped":3589,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:38:03.926: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 13:38:04.126: INFO: Waiting up to 5m0s for pod "pod-bf83db2c-b3a4-4c63-a456-82b00a207f95" in namespace "emptydir-5078" to be "Succeeded or Failed"
Dec 14 13:38:04.133: INFO: Pod "pod-bf83db2c-b3a4-4c63-a456-82b00a207f95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.268255ms
Dec 14 13:38:06.140: INFO: Pod "pod-bf83db2c-b3a4-4c63-a456-82b00a207f95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013964898s
Dec 14 13:38:08.149: INFO: Pod "pod-bf83db2c-b3a4-4c63-a456-82b00a207f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023217639s
STEP: Saw pod success
Dec 14 13:38:08.150: INFO: Pod "pod-bf83db2c-b3a4-4c63-a456-82b00a207f95" satisfied condition "Succeeded or Failed"
Dec 14 13:38:08.153: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-bf83db2c-b3a4-4c63-a456-82b00a207f95 container test-container: <nil>
STEP: delete the pod
Dec 14 13:38:08.183: INFO: Waiting for pod pod-bf83db2c-b3a4-4c63-a456-82b00a207f95 to disappear
Dec 14 13:38:08.187: INFO: Pod pod-bf83db2c-b3a4-4c63-a456-82b00a207f95 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:38:08.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5078" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":227,"skipped":3592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:38:08.202: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec 14 13:38:08.494: INFO: The status of Pod labelsupdatebf01dcfe-3843-4c56-aac5-4be190eeca76 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:38:10.504: INFO: The status of Pod labelsupdatebf01dcfe-3843-4c56-aac5-4be190eeca76 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:38:12.504: INFO: The status of Pod labelsupdatebf01dcfe-3843-4c56-aac5-4be190eeca76 is Running (Ready = true)
Dec 14 13:38:13.043: INFO: Successfully updated pod "labelsupdatebf01dcfe-3843-4c56-aac5-4be190eeca76"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:38:15.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6263" for this suite.

• [SLOW TEST:6.878 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":228,"skipped":3626,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:38:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-3046
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-3046
Dec 14 13:38:15.305: INFO: Found 0 stateful pods, waiting for 1
Dec 14 13:38:25.326: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Dec 14 13:38:25.369: INFO: Deleting all statefulset in ns statefulset-3046
Dec 14 13:38:25.372: INFO: Scaling statefulset ss to 0
Dec 14 13:38:45.413: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 13:38:45.419: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:38:45.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3046" for this suite.

• [SLOW TEST:30.389 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":229,"skipped":3646,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:38:45.469: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-e1edaca9-5a5e-4d99-86eb-692ae30d38da in namespace container-probe-3268
Dec 14 13:38:49.668: INFO: Started pod liveness-e1edaca9-5a5e-4d99-86eb-692ae30d38da in namespace container-probe-3268
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:38:49.675: INFO: Initial restart count of pod liveness-e1edaca9-5a5e-4d99-86eb-692ae30d38da is 0
Dec 14 13:39:07.910: INFO: Restart count of pod container-probe-3268/liveness-e1edaca9-5a5e-4d99-86eb-692ae30d38da is now 1 (18.235215554s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:39:07.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3268" for this suite.

• [SLOW TEST:22.474 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":230,"skipped":3655,"failed":0}
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:39:07.943: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Dec 14 13:39:08.167: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:39:13.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4224" for this suite.

• [SLOW TEST:5.845 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":231,"skipped":3655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:39:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 13:39:18.040: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:39:18.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2030" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":232,"skipped":3679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:39:18.080: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec 14 13:39:18.317: INFO: Waiting up to 5m0s for pod "security-context-23b4feaf-404a-41fb-b49c-59c97156991c" in namespace "security-context-120" to be "Succeeded or Failed"
Dec 14 13:39:18.323: INFO: Pod "security-context-23b4feaf-404a-41fb-b49c-59c97156991c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.964587ms
Dec 14 13:39:20.329: INFO: Pod "security-context-23b4feaf-404a-41fb-b49c-59c97156991c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011667996s
Dec 14 13:39:22.345: INFO: Pod "security-context-23b4feaf-404a-41fb-b49c-59c97156991c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027417969s
STEP: Saw pod success
Dec 14 13:39:22.345: INFO: Pod "security-context-23b4feaf-404a-41fb-b49c-59c97156991c" satisfied condition "Succeeded or Failed"
Dec 14 13:39:22.348: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod security-context-23b4feaf-404a-41fb-b49c-59c97156991c container test-container: <nil>
STEP: delete the pod
Dec 14 13:39:22.378: INFO: Waiting for pod security-context-23b4feaf-404a-41fb-b49c-59c97156991c to disappear
Dec 14 13:39:22.389: INFO: Pod security-context-23b4feaf-404a-41fb-b49c-59c97156991c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:39:22.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-120" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":233,"skipped":3701,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:39:22.405: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Dec 14 13:39:23.143: INFO: created pod pod-service-account-defaultsa
Dec 14 13:39:23.143: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 14 13:39:23.180: INFO: created pod pod-service-account-mountsa
Dec 14 13:39:23.180: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 14 13:39:23.226: INFO: created pod pod-service-account-nomountsa
Dec 14 13:39:23.226: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 14 13:39:23.302: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 14 13:39:23.302: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 14 13:39:23.351: INFO: created pod pod-service-account-mountsa-mountspec
Dec 14 13:39:23.351: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 14 13:39:23.442: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 14 13:39:23.442: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 14 13:39:23.471: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 14 13:39:23.471: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 14 13:39:23.509: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 14 13:39:23.509: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 14 13:39:23.528: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 14 13:39:23.528: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:39:23.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6223" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":234,"skipped":3702,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:39:23.567: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 14 13:39:23.779: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 13:40:24.105: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:40:24.112: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Dec 14 13:40:28.816: INFO: found a healthy node: ip-10-0-169-135.ec2.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:40:45.435: INFO: pods created so far: [1 1 1]
Dec 14 13:40:45.435: INFO: length of pods created so far: 3
Dec 14 13:41:03.467: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:10.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7869" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:10.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3701" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:107.059 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":235,"skipped":3709,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:10.626: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:41:10.853: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:11.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-147" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":236,"skipped":3718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:11.915: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 14 13:41:12.224: INFO: Waiting up to 5m0s for pod "pod-4f3ebce0-28b3-4b6a-9602-408d58894c22" in namespace "emptydir-1403" to be "Succeeded or Failed"
Dec 14 13:41:12.228: INFO: Pod "pod-4f3ebce0-28b3-4b6a-9602-408d58894c22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413199ms
Dec 14 13:41:14.250: INFO: Pod "pod-4f3ebce0-28b3-4b6a-9602-408d58894c22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026027633s
Dec 14 13:41:16.256: INFO: Pod "pod-4f3ebce0-28b3-4b6a-9602-408d58894c22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032300452s
STEP: Saw pod success
Dec 14 13:41:16.256: INFO: Pod "pod-4f3ebce0-28b3-4b6a-9602-408d58894c22" satisfied condition "Succeeded or Failed"
Dec 14 13:41:16.261: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-4f3ebce0-28b3-4b6a-9602-408d58894c22 container test-container: <nil>
STEP: delete the pod
Dec 14 13:41:16.310: INFO: Waiting for pod pod-4f3ebce0-28b3-4b6a-9602-408d58894c22 to disappear
Dec 14 13:41:16.324: INFO: Pod pod-4f3ebce0-28b3-4b6a-9602-408d58894c22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:16.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1403" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":237,"skipped":3756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:16.340: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Dec 14 13:41:16.624: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:41:18.632: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:41:20.633: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Dec 14 13:41:20.663: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:41:22.675: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:41:24.681: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 13:41:24.713: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:24.717: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:41:26.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:26.727: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:41:28.724: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:28.731: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:41:30.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:30.727: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:41:32.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:32.726: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:41:34.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:34.724: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:41:36.717: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:36.726: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 13:41:38.718: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 13:41:38.728: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:38.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3246" for this suite.

• [SLOW TEST:22.406 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":238,"skipped":3787,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:38.746: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Dec 14 13:41:38.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6220 create -f -'
Dec 14 13:41:39.740: INFO: stderr: ""
Dec 14 13:41:39.740: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec 14 13:41:39.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6220 diff -f -'
Dec 14 13:41:40.254: INFO: rc: 1
Dec 14 13:41:40.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6220 delete -f -'
Dec 14 13:41:40.366: INFO: stderr: ""
Dec 14 13:41:40.366: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:40.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6220" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":239,"skipped":3798,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:40.384: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 14 13:41:40.559: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 13:41:40.580: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 13:41:40.588: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-177.ec2.internal before test
Dec 14 13:41:40.642: INFO: aws-ebs-csi-driver-node-2t5xl from openshift-cluster-csi-drivers started at 2021-12-14 11:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 13:41:40.642: INFO: tuned-ckp8j from openshift-cluster-node-tuning-operator started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container tuned ready: true, restart count 0
Dec 14 13:41:40.642: INFO: dns-default-xq8ng from openshift-dns started at 2021-12-14 11:52:45 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container dns ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: node-resolver-5tjmp from openshift-dns started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 13:41:40.642: INFO: image-registry-669979f5d8-zxpb9 from openshift-image-registry started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container registry ready: true, restart count 0
Dec 14 13:41:40.642: INFO: node-ca-gb64k from openshift-image-registry started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 13:41:40.642: INFO: ingress-canary-d5qjx from openshift-ingress-canary started at 2021-12-14 11:52:45 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 13:41:40.642: INFO: router-default-7fbf56f75b-4q5fx from openshift-ingress started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container router ready: true, restart count 0
Dec 14 13:41:40.642: INFO: machine-config-daemon-98m5c from openshift-machine-config-operator started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (5 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: node-exporter-f89s7 from openshift-monitoring started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 13:41:40.642: INFO: prometheus-adapter-74475585f4-cfsjn from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 13:41:40.642: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (7 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 13:41:40.642: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 13:41:40.642: INFO: thanos-querier-74cd47ff6f-jszlz from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (5 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 13:41:40.642: INFO: multus-7jp7s from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 13:41:40.642: INFO: multus-additional-cni-plugins-6kws7 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 13:41:40.642: INFO: network-metrics-daemon-8sqs4 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 13:41:40.642: INFO: network-check-target-hpkw5 from openshift-network-diagnostics started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 13:41:40.642: INFO: sdn-7t2qj from openshift-sdn started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container sdn ready: true, restart count 0
Dec 14 13:41:40.642: INFO: sonobuoy from sonobuoy started at 2021-12-14 12:26:18 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 13:41:40.642: INFO: sonobuoy-e2e-job-1d421b73e370443b from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container e2e ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:41:40.642: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-hgvlc from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.642: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 13:41:40.642: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-150-199.ec2.internal before test
Dec 14 13:41:40.693: INFO: aws-ebs-csi-driver-node-6djjl from openshift-cluster-csi-drivers started at 2021-12-14 11:50:28 +0000 UTC (3 container statuses recorded)
Dec 14 13:41:40.693: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 13:41:40.693: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 13:41:40.693: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 13:41:40.693: INFO: tuned-btrkv from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.693: INFO: 	Container tuned ready: true, restart count 0
Dec 14 13:41:40.693: INFO: dns-default-hk5vj from openshift-dns started at 2021-12-14 11:51:18 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.693: INFO: 	Container dns ready: true, restart count 0
Dec 14 13:41:40.693: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.693: INFO: node-resolver-zjfkl from openshift-dns started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.693: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 13:41:40.693: INFO: image-registry-669979f5d8-66fds from openshift-image-registry started at 2021-12-14 11:51:20 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container registry ready: true, restart count 0
Dec 14 13:41:40.694: INFO: node-ca-x5d9t from openshift-image-registry started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 13:41:40.694: INFO: ingress-canary-vth2d from openshift-ingress-canary started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 13:41:40.694: INFO: router-default-7fbf56f75b-ncphj from openshift-ingress started at 2021-12-14 11:51:19 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container router ready: true, restart count 0
Dec 14 13:41:40.694: INFO: machine-config-daemon-t7xdp from openshift-machine-config-operator started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: certified-operators-nt7k6 from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:41:40.694: INFO: community-operators-g4pdk from openshift-marketplace started at 2021-12-14 11:51:21 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:41:40.694: INFO: redhat-marketplace-qmxvd from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:41:40.694: INFO: redhat-operators-r2c5v from openshift-marketplace started at 2021-12-14 12:56:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:41:40.694: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (5 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-12-14 11:51:57 +0000 UTC (5 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: grafana-d5d47878d-mtw5p from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container grafana ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: kube-state-metrics-78d545c868-nnjp9 from openshift-monitoring started at 2021-12-14 11:51:22 +0000 UTC (3 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 14 13:41:40.694: INFO: node-exporter-2xvt4 from openshift-monitoring started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 13:41:40.694: INFO: openshift-state-metrics-85bc7c8d6c-jws2g from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 14 13:41:40.694: INFO: prometheus-adapter-74475585f4-skfcj from openshift-monitoring started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 13:41:40.694: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-12-14 11:52:02 +0000 UTC (7 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 13:41:40.694: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 13:41:40.694: INFO: telemeter-client-f96747c6d-sj25j from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container reload ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 14 13:41:40.694: INFO: thanos-querier-74cd47ff6f-wdf79 from openshift-monitoring started at 2021-12-14 11:52:00 +0000 UTC (5 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 13:41:40.694: INFO: multus-6drw6 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 13:41:40.694: INFO: multus-additional-cni-plugins-p2nx7 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 13:41:40.694: INFO: network-metrics-daemon-dvsjd from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 13:41:40.694: INFO: network-check-source-579795dc4b-k549q from openshift-network-diagnostics started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container check-endpoints ready: true, restart count 0
Dec 14 13:41:40.694: INFO: network-check-target-lrvdq from openshift-network-diagnostics started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 13:41:40.694: INFO: sdn-mwhj9 from openshift-sdn started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container sdn ready: true, restart count 0
Dec 14 13:41:40.694: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-2mpvh from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.694: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 13:41:40.694: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-169-135.ec2.internal before test
Dec 14 13:41:40.751: INFO: pod-handle-http-request from container-lifecycle-hook-3246 started at 2021-12-14 13:41:16 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container agnhost-container ready: true, restart count 0
Dec 14 13:41:40.751: INFO: httpd-deployment-8584777d8-48k7p from kubectl-6220 started at 2021-12-14 13:41:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container httpd ready: false, restart count 0
Dec 14 13:41:40.751: INFO: aws-ebs-csi-driver-node-mzk2q from openshift-cluster-csi-drivers started at 2021-12-14 11:50:39 +0000 UTC (3 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 13:41:40.751: INFO: tuned-fqln7 from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container tuned ready: true, restart count 0
Dec 14 13:41:40.751: INFO: dns-default-bwmjx from openshift-dns started at 2021-12-14 13:04:38 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container dns ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.751: INFO: node-resolver-tz2bj from openshift-dns started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 13:41:40.751: INFO: node-ca-plgtj from openshift-image-registry started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 13:41:40.751: INFO: ingress-canary-vw794 from openshift-ingress-canary started at 2021-12-14 13:04:38 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 13:41:40.751: INFO: machine-config-daemon-rwqcs from openshift-machine-config-operator started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:41:40.751: INFO: node-exporter-rvqjd from openshift-monitoring started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 13:41:40.751: INFO: multus-additional-cni-plugins-kjrl5 from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 13:41:40.751: INFO: multus-rmr4w from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 13:41:40.751: INFO: network-metrics-daemon-g4ppk from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 13:41:40.751: INFO: network-check-target-br6mv from openshift-network-diagnostics started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 13:41:40.751: INFO: sdn-jbrq7 from openshift-sdn started at 2021-12-14 11:50:40 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container sdn ready: true, restart count 0
Dec 14 13:41:40.751: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-7qgl9 from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:41:40.751: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:41:40.751: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-1186.16c0a2d5149dca0e], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16c0a2d5307556ed], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) were unschedulable.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:41.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1186" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":240,"skipped":3798,"failed":0}
SSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:41.907: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 14 13:41:46.150: INFO: &Pod{ObjectMeta:{send-events-86521f14-ea9b-4cd2-9a33-8b50f056e868  events-1703  7db26745-b114-4b31-abe1-a1d27e7d6778 107350 0 2021-12-14 13:41:42 +0000 UTC <nil> <nil> map[name:foo time:95840289] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.205"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.205"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-12-14 13:41:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:41:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:41:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhhjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhhjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:41:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:41:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:41:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:41:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.205,StartTime:2021-12-14 13:41:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:41:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://16db094c33ffc133eeb5778eea70781623e9fb0bdc95f0c635ec1c4ee80d18eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 14 13:41:48.164: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 14 13:41:50.172: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:50.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1703" for this suite.

• [SLOW TEST:8.313 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":241,"skipped":3801,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:50.220: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:50.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4426" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":242,"skipped":3814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:50.502: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-95ac2179-d35b-46ad-96c3-8d2bc0d1d074
STEP: Creating a pod to test consume configMaps
Dec 14 13:41:50.708: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec" in namespace "projected-4133" to be "Succeeded or Failed"
Dec 14 13:41:50.714: INFO: Pod "pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.212051ms
Dec 14 13:41:52.721: INFO: Pod "pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012928379s
Dec 14 13:41:54.733: INFO: Pod "pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024306441s
STEP: Saw pod success
Dec 14 13:41:54.733: INFO: Pod "pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec" satisfied condition "Succeeded or Failed"
Dec 14 13:41:54.736: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:41:54.767: INFO: Waiting for pod pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec to disappear
Dec 14 13:41:54.770: INFO: Pod pod-projected-configmaps-faa563a8-17d0-4a70-bc2e-b441c426dcec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:54.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4133" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":243,"skipped":3842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:54.785: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Dec 14 13:41:55.036: INFO: Waiting up to 5m0s for pod "client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e" in namespace "containers-6962" to be "Succeeded or Failed"
Dec 14 13:41:55.041: INFO: Pod "client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.089823ms
Dec 14 13:41:57.054: INFO: Pod "client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01728653s
Dec 14 13:41:59.135: INFO: Pod "client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098646177s
STEP: Saw pod success
Dec 14 13:41:59.135: INFO: Pod "client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e" satisfied condition "Succeeded or Failed"
Dec 14 13:41:59.259: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:41:59.392: INFO: Waiting for pod client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e to disappear
Dec 14 13:41:59.448: INFO: Pod client-containers-c13dda22-9009-443e-a8a5-0b8416fbcb9e no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:41:59.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6962" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":244,"skipped":3876,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:41:59.471: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-ffeea416-ce89-4775-bb6b-a539416ba13f in namespace container-probe-9943
Dec 14 13:42:03.796: INFO: Started pod liveness-ffeea416-ce89-4775-bb6b-a539416ba13f in namespace container-probe-9943
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 13:42:03.801: INFO: Initial restart count of pod liveness-ffeea416-ce89-4775-bb6b-a539416ba13f is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:46:05.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9943" for this suite.

• [SLOW TEST:245.768 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":245,"skipped":3903,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:46:05.239: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:46:22.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5768" for this suite.

• [SLOW TEST:17.367 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":246,"skipped":3912,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:46:22.606: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5437
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5437
I1214 13:46:22.854650      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5437, replica count: 2
I1214 13:46:25.905973      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:46:28.909: INFO: Creating new exec pod
I1214 13:46:28.908975      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:46:34.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-5437 exec execpodnbgh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 13:46:34.643: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 13:46:34.643: INFO: stdout: ""
Dec 14 13:46:35.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-5437 exec execpodnbgh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 13:46:35.872: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 13:46:35.872: INFO: stdout: ""
Dec 14 13:46:36.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-5437 exec execpodnbgh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 13:46:36.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 13:46:36.884: INFO: stdout: "externalname-service-k7t4d"
Dec 14 13:46:36.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-5437 exec execpodnbgh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.245.30 80'
Dec 14 13:46:37.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.245.30 80\nConnection to 172.30.245.30 80 port [tcp/http] succeeded!\n"
Dec 14 13:46:37.089: INFO: stdout: "externalname-service-k7t4d"
Dec 14 13:46:37.089: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:46:37.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5437" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:14.531 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":247,"skipped":3924,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:46:37.138: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:46:37.976: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 13:46:39.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086398, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086398, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086398, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086397, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:46:43.020: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:46:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7167" for this suite.
STEP: Destroying namespace "webhook-7167-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.124 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":248,"skipped":3937,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:46:43.262: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W1214 13:46:44.723690      20 metrics_grabber.go:102] Can't find any pods in namespace kube-system to grab metrics from
W1214 13:46:44.723720      20 metrics_grabber.go:138] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 13:46:44.723: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:46:44.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8469" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":249,"skipped":3940,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:46:44.738: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 14 13:46:45.208: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 13:46:45.220: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 13:46:45.228: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-177.ec2.internal before test
Dec 14 13:46:45.309: INFO: aws-ebs-csi-driver-node-2t5xl from openshift-cluster-csi-drivers started at 2021-12-14 11:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 13:46:45.310: INFO: tuned-ckp8j from openshift-cluster-node-tuning-operator started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container tuned ready: true, restart count 0
Dec 14 13:46:45.310: INFO: dns-default-xq8ng from openshift-dns started at 2021-12-14 11:52:45 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container dns ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: node-resolver-5tjmp from openshift-dns started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 13:46:45.310: INFO: image-registry-669979f5d8-zxpb9 from openshift-image-registry started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container registry ready: true, restart count 0
Dec 14 13:46:45.310: INFO: node-ca-gb64k from openshift-image-registry started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 13:46:45.310: INFO: ingress-canary-d5qjx from openshift-ingress-canary started at 2021-12-14 11:52:45 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 13:46:45.310: INFO: router-default-7fbf56f75b-4q5fx from openshift-ingress started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container router ready: true, restart count 0
Dec 14 13:46:45.310: INFO: machine-config-daemon-98m5c from openshift-machine-config-operator started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (5 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: node-exporter-f89s7 from openshift-monitoring started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 13:46:45.310: INFO: prometheus-adapter-74475585f4-cfsjn from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 13:46:45.310: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (7 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 13:46:45.310: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 13:46:45.310: INFO: thanos-querier-74cd47ff6f-jszlz from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (5 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 13:46:45.310: INFO: multus-7jp7s from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 13:46:45.310: INFO: multus-additional-cni-plugins-6kws7 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 13:46:45.310: INFO: network-metrics-daemon-8sqs4 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 13:46:45.310: INFO: network-check-target-hpkw5 from openshift-network-diagnostics started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 13:46:45.310: INFO: sdn-7t2qj from openshift-sdn started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container sdn ready: true, restart count 0
Dec 14 13:46:45.310: INFO: sonobuoy from sonobuoy started at 2021-12-14 12:26:18 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 13:46:45.310: INFO: sonobuoy-e2e-job-1d421b73e370443b from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container e2e ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:46:45.310: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-hgvlc from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.310: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 13:46:45.310: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-150-199.ec2.internal before test
Dec 14 13:46:45.364: INFO: aws-ebs-csi-driver-node-6djjl from openshift-cluster-csi-drivers started at 2021-12-14 11:50:28 +0000 UTC (3 container statuses recorded)
Dec 14 13:46:45.364: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 13:46:45.364: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 13:46:45.364: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 13:46:45.364: INFO: tuned-btrkv from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.364: INFO: 	Container tuned ready: true, restart count 0
Dec 14 13:46:45.364: INFO: dns-default-hk5vj from openshift-dns started at 2021-12-14 11:51:18 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.364: INFO: 	Container dns ready: true, restart count 0
Dec 14 13:46:45.364: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.364: INFO: node-resolver-zjfkl from openshift-dns started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.364: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 13:46:45.364: INFO: image-registry-669979f5d8-66fds from openshift-image-registry started at 2021-12-14 11:51:20 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.364: INFO: 	Container registry ready: true, restart count 0
Dec 14 13:46:45.365: INFO: node-ca-x5d9t from openshift-image-registry started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 13:46:45.365: INFO: ingress-canary-vth2d from openshift-ingress-canary started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 13:46:45.365: INFO: router-default-7fbf56f75b-ncphj from openshift-ingress started at 2021-12-14 11:51:19 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container router ready: true, restart count 0
Dec 14 13:46:45.365: INFO: machine-config-daemon-t7xdp from openshift-machine-config-operator started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: certified-operators-nt7k6 from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:46:45.365: INFO: community-operators-g4pdk from openshift-marketplace started at 2021-12-14 11:51:21 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:46:45.365: INFO: redhat-marketplace-qmxvd from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:46:45.365: INFO: redhat-operators-r2c5v from openshift-marketplace started at 2021-12-14 12:56:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 13:46:45.365: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (5 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-12-14 11:51:57 +0000 UTC (5 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: grafana-d5d47878d-mtw5p from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container grafana ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: kube-state-metrics-78d545c868-nnjp9 from openshift-monitoring started at 2021-12-14 11:51:22 +0000 UTC (3 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 14 13:46:45.365: INFO: node-exporter-2xvt4 from openshift-monitoring started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 13:46:45.365: INFO: openshift-state-metrics-85bc7c8d6c-jws2g from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 14 13:46:45.365: INFO: prometheus-adapter-74475585f4-skfcj from openshift-monitoring started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 13:46:45.365: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-12-14 11:52:02 +0000 UTC (7 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 13:46:45.365: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 13:46:45.365: INFO: telemeter-client-f96747c6d-sj25j from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container reload ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 14 13:46:45.365: INFO: thanos-querier-74cd47ff6f-wdf79 from openshift-monitoring started at 2021-12-14 11:52:00 +0000 UTC (5 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 13:46:45.365: INFO: multus-6drw6 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 13:46:45.365: INFO: multus-additional-cni-plugins-p2nx7 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 13:46:45.365: INFO: network-metrics-daemon-dvsjd from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 13:46:45.365: INFO: network-check-source-579795dc4b-k549q from openshift-network-diagnostics started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container check-endpoints ready: true, restart count 0
Dec 14 13:46:45.365: INFO: network-check-target-lrvdq from openshift-network-diagnostics started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 13:46:45.365: INFO: sdn-mwhj9 from openshift-sdn started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container sdn ready: true, restart count 0
Dec 14 13:46:45.365: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-2mpvh from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.365: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 13:46:45.365: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-169-135.ec2.internal before test
Dec 14 13:46:45.406: INFO: aws-ebs-csi-driver-node-mzk2q from openshift-cluster-csi-drivers started at 2021-12-14 11:50:39 +0000 UTC (3 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 13:46:45.406: INFO: tuned-fqln7 from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container tuned ready: true, restart count 0
Dec 14 13:46:45.406: INFO: dns-default-bwmjx from openshift-dns started at 2021-12-14 13:04:38 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container dns ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.406: INFO: node-resolver-tz2bj from openshift-dns started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 13:46:45.406: INFO: node-ca-plgtj from openshift-image-registry started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 13:46:45.406: INFO: ingress-canary-vw794 from openshift-ingress-canary started at 2021-12-14 13:04:38 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 13:46:45.406: INFO: machine-config-daemon-rwqcs from openshift-machine-config-operator started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 13:46:45.406: INFO: node-exporter-rvqjd from openshift-monitoring started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 13:46:45.406: INFO: multus-additional-cni-plugins-kjrl5 from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 13:46:45.406: INFO: multus-rmr4w from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 13:46:45.406: INFO: network-metrics-daemon-g4ppk from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 13:46:45.406: INFO: network-check-target-br6mv from openshift-network-diagnostics started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 13:46:45.406: INFO: sdn-jbrq7 from openshift-sdn started at 2021-12-14 11:50:40 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container sdn ready: true, restart count 0
Dec 14 13:46:45.406: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-7qgl9 from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 13:46:45.406: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 13:46:45.406: INFO: webhook-to-be-mutated from webhook-7167 started at 2021-12-14 13:46:43 +0000 UTC (1 container statuses recorded)
Dec 14 13:46:45.406: INFO: 	Container example ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-47c808ab-81dc-4669-8cda-f6adb0400e8e 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.169.135 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-47c808ab-81dc-4669-8cda-f6adb0400e8e off the node ip-10-0-169-135.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-47c808ab-81dc-4669-8cda-f6adb0400e8e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:51:53.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7154" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.894 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":250,"skipped":3941,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:51:53.631: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:51:53.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7737" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":251,"skipped":3942,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:51:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Dec 14 13:51:54.309: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 13:51:59.330: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:51:59.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6424" for this suite.

• [SLOW TEST:5.705 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":252,"skipped":3962,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:51:59.696: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 13:52:01.227: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 13:52:03.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086721, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086721, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086721, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775086721, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 13:52:06.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:52:06.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5263" for this suite.
STEP: Destroying namespace "webhook-5263-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.825 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":253,"skipped":3968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:52:06.522: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:52:10.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9159" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":254,"skipped":3997,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:52:10.944: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec 14 13:52:11.173: INFO: The status of Pod labelsupdate4a0094c6-d827-479e-ac54-378d383a6271 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:52:13.183: INFO: The status of Pod labelsupdate4a0094c6-d827-479e-ac54-378d383a6271 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:52:15.181: INFO: The status of Pod labelsupdate4a0094c6-d827-479e-ac54-378d383a6271 is Running (Ready = true)
Dec 14 13:52:15.717: INFO: Successfully updated pod "labelsupdate4a0094c6-d827-479e-ac54-378d383a6271"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:52:17.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1464" for this suite.

• [SLOW TEST:6.810 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":255,"skipped":4004,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:52:17.754: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 13:52:18.006: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:18.006: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:18.006: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:52:18.011: INFO: Number of nodes with available pods: 0
Dec 14 13:52:18.011: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:52:19.022: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:19.023: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:19.023: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:52:19.027: INFO: Number of nodes with available pods: 0
Dec 14 13:52:19.027: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:52:20.022: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:20.022: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:20.022: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:52:20.027: INFO: Number of nodes with available pods: 0
Dec 14 13:52:20.027: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:52:21.022: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:21.022: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:21.022: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:52:21.026: INFO: Number of nodes with available pods: 0
Dec 14 13:52:21.026: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 13:52:22.025: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:22.025: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:22.025: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:52:22.030: INFO: Number of nodes with available pods: 2
Dec 14 13:52:22.030: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:52:23.021: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:23.021: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:23.021: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:52:23.025: INFO: Number of nodes with available pods: 3
Dec 14 13:52:23.025: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 14 13:52:23.053: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:23.053: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 13:52:23.053: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 13:52:23.058: INFO: Number of nodes with available pods: 3
Dec 14 13:52:23.058: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6442, will wait for the garbage collector to delete the pods
Dec 14 13:52:24.146: INFO: Deleting DaemonSet.extensions daemon-set took: 20.196103ms
Dec 14 13:52:24.247: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.775082ms
Dec 14 13:52:38.055: INFO: Number of nodes with available pods: 0
Dec 14 13:52:38.055: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 13:52:38.060: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"114765"},"items":null}

Dec 14 13:52:38.064: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"114765"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:52:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6442" for this suite.

• [SLOW TEST:20.344 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":256,"skipped":4009,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:52:38.098: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6131
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 13:52:38.322: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 13:52:38.436: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:52:40.446: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:52:42.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:44.443: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:46.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:48.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:50.445: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:52.444: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:54.444: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:56.444: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:52:58.452: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 13:52:58.466: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 13:52:58.474: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 13:53:02.527: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 13:53:02.527: INFO: Breadth first check of 10.129.2.65 on host 10.0.132.177...
Dec 14 13:53:02.530: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.222:9080/dial?request=hostname&protocol=http&host=10.129.2.65&port=8080&tries=1'] Namespace:pod-network-test-6131 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:53:02.530: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:53:02.637: INFO: Waiting for responses: map[]
Dec 14 13:53:02.637: INFO: reached 10.129.2.65 after 0/1 tries
Dec 14 13:53:02.637: INFO: Breadth first check of 10.131.0.121 on host 10.0.150.199...
Dec 14 13:53:02.642: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.222:9080/dial?request=hostname&protocol=http&host=10.131.0.121&port=8080&tries=1'] Namespace:pod-network-test-6131 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:53:02.642: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:53:02.743: INFO: Waiting for responses: map[]
Dec 14 13:53:02.743: INFO: reached 10.131.0.121 after 0/1 tries
Dec 14 13:53:02.743: INFO: Breadth first check of 10.128.2.221 on host 10.0.169.135...
Dec 14 13:53:02.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.2.222:9080/dial?request=hostname&protocol=http&host=10.128.2.221&port=8080&tries=1'] Namespace:pod-network-test-6131 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:53:02.748: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:53:02.848: INFO: Waiting for responses: map[]
Dec 14 13:53:02.848: INFO: reached 10.128.2.221 after 0/1 tries
Dec 14 13:53:02.848: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:53:02.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6131" for this suite.

• [SLOW TEST:24.766 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":257,"skipped":4012,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:53:02.865: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-535e3bcf-1145-479c-ac64-6e8efa87a90d
STEP: Creating a pod to test consume secrets
Dec 14 13:53:03.093: INFO: Waiting up to 5m0s for pod "pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa" in namespace "secrets-8799" to be "Succeeded or Failed"
Dec 14 13:53:03.096: INFO: Pod "pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498208ms
Dec 14 13:53:05.104: INFO: Pod "pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010684084s
Dec 14 13:53:07.110: INFO: Pod "pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017318706s
STEP: Saw pod success
Dec 14 13:53:07.110: INFO: Pod "pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa" satisfied condition "Succeeded or Failed"
Dec 14 13:53:07.115: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:53:07.140: INFO: Waiting for pod pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa to disappear
Dec 14 13:53:07.146: INFO: Pod pod-secrets-cdf9c912-b4c1-4cb7-96a6-c5a8abdb0baa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:53:07.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8799" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":258,"skipped":4022,"failed":0}
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:53:07.160: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Dec 14 13:53:07.351: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 13:54:07.597: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:54:07.609: INFO: Starting informer...
STEP: Starting pod...
Dec 14 13:54:07.846: INFO: Pod is running on ip-10-0-169-135.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 14 13:54:07.873: INFO: Pod wasn't evicted. Proceeding
Dec 14 13:54:07.873: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 14 13:55:22.901: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:55:22.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6891" for this suite.

• [SLOW TEST:135.768 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":259,"skipped":4025,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:55:22.929: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:55:23.135: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 14 13:55:23.150: INFO: Number of nodes with available pods: 0
Dec 14 13:55:23.150: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 14 13:55:23.191: INFO: Number of nodes with available pods: 0
Dec 14 13:55:23.191: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:24.200: INFO: Number of nodes with available pods: 0
Dec 14 13:55:24.200: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:25.198: INFO: Number of nodes with available pods: 0
Dec 14 13:55:25.198: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:26.198: INFO: Number of nodes with available pods: 1
Dec 14 13:55:26.198: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 14 13:55:26.222: INFO: Number of nodes with available pods: 1
Dec 14 13:55:26.222: INFO: Number of running nodes: 0, number of available pods: 1
Dec 14 13:55:27.230: INFO: Number of nodes with available pods: 0
Dec 14 13:55:27.230: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 14 13:55:27.248: INFO: Number of nodes with available pods: 0
Dec 14 13:55:27.249: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:28.282: INFO: Number of nodes with available pods: 0
Dec 14 13:55:28.282: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:29.317: INFO: Number of nodes with available pods: 0
Dec 14 13:55:29.317: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:30.257: INFO: Number of nodes with available pods: 0
Dec 14 13:55:30.257: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:31.257: INFO: Number of nodes with available pods: 0
Dec 14 13:55:31.257: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:32.256: INFO: Number of nodes with available pods: 0
Dec 14 13:55:32.256: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:33.261: INFO: Number of nodes with available pods: 0
Dec 14 13:55:33.261: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:34.256: INFO: Number of nodes with available pods: 0
Dec 14 13:55:34.256: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:35.258: INFO: Number of nodes with available pods: 0
Dec 14 13:55:35.258: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:36.254: INFO: Number of nodes with available pods: 0
Dec 14 13:55:36.254: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:37.258: INFO: Number of nodes with available pods: 0
Dec 14 13:55:37.258: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:38.256: INFO: Number of nodes with available pods: 0
Dec 14 13:55:38.256: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:39.255: INFO: Number of nodes with available pods: 0
Dec 14 13:55:39.255: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:40.258: INFO: Number of nodes with available pods: 0
Dec 14 13:55:40.258: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 13:55:41.255: INFO: Number of nodes with available pods: 1
Dec 14 13:55:41.255: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5071, will wait for the garbage collector to delete the pods
Dec 14 13:55:41.334: INFO: Deleting DaemonSet.extensions daemon-set took: 8.57194ms
Dec 14 13:55:41.435: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.538307ms
Dec 14 13:55:44.746: INFO: Number of nodes with available pods: 0
Dec 14 13:55:44.746: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 13:55:44.752: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"117004"},"items":null}

Dec 14 13:55:44.755: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"117004"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:55:44.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5071" for this suite.

• [SLOW TEST:21.873 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":260,"skipped":4026,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:55:44.802: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Dec 14 13:55:45.208: INFO: observed Pod pod-test in namespace pods-6847 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec 14 13:55:45.216: INFO: observed Pod pod-test in namespace pods-6847 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC  }]
Dec 14 13:55:45.248: INFO: observed Pod pod-test in namespace pods-6847 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC  }]
Dec 14 13:55:47.317: INFO: observed Pod pod-test in namespace pods-6847 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC  }]
Dec 14 13:55:48.180: INFO: Found Pod pod-test in namespace pods-6847 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-12-14 13:55:45 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Dec 14 13:55:48.206: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Dec 14 13:55:48.242: INFO: observed event type ADDED
Dec 14 13:55:48.242: INFO: observed event type MODIFIED
Dec 14 13:55:48.242: INFO: observed event type MODIFIED
Dec 14 13:55:48.242: INFO: observed event type MODIFIED
Dec 14 13:55:48.242: INFO: observed event type MODIFIED
Dec 14 13:55:48.242: INFO: observed event type MODIFIED
Dec 14 13:55:48.242: INFO: observed event type MODIFIED
Dec 14 13:55:48.242: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:55:48.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6847" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":261,"skipped":4044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:55:48.261: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:55:55.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8054" for this suite.
STEP: Destroying namespace "nsdeletetest-6828" for this suite.
Dec 14 13:55:55.899: INFO: Namespace nsdeletetest-6828 was already deleted
STEP: Destroying namespace "nsdeletetest-7235" for this suite.

• [SLOW TEST:7.660 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":262,"skipped":4148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:55:55.922: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Dec 14 13:55:56.099: INFO: created test-podtemplate-1
Dec 14 13:55:56.108: INFO: created test-podtemplate-2
Dec 14 13:55:56.116: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec 14 13:55:56.121: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec 14 13:55:56.161: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:55:56.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3432" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":263,"skipped":4180,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:55:56.183: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-6a98a1b4-7129-4ab1-ada1-6b5d1712f638
STEP: Creating a pod to test consume secrets
Dec 14 13:55:56.422: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55" in namespace "projected-833" to be "Succeeded or Failed"
Dec 14 13:55:56.427: INFO: Pod "pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55": Phase="Pending", Reason="", readiness=false. Elapsed: 5.013816ms
Dec 14 13:55:58.492: INFO: Pod "pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069531038s
Dec 14 13:56:00.501: INFO: Pod "pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078871133s
STEP: Saw pod success
Dec 14 13:56:00.501: INFO: Pod "pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55" satisfied condition "Succeeded or Failed"
Dec 14 13:56:00.506: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 13:56:00.535: INFO: Waiting for pod pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55 to disappear
Dec 14 13:56:00.541: INFO: Pod pod-projected-secrets-0ed7ac87-20c1-4184-bf42-660560b0bc55 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:56:00.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-833" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":264,"skipped":4188,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:56:00.556: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 13:56:00.781: INFO: Creating deployment "webserver-deployment"
Dec 14 13:56:00.794: INFO: Waiting for observed generation 1
Dec 14 13:56:02.806: INFO: Waiting for all required pods to come up
Dec 14 13:56:02.811: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 14 13:56:08.836: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 14 13:56:08.849: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 14 13:56:08.925: INFO: Updating deployment webserver-deployment
Dec 14 13:56:08.925: INFO: Waiting for observed generation 2
Dec 14 13:56:10.949: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 14 13:56:10.953: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 14 13:56:10.957: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 13:56:10.971: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 14 13:56:10.971: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 14 13:56:10.975: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 13:56:10.994: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 14 13:56:10.994: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 14 13:56:11.008: INFO: Updating deployment webserver-deployment
Dec 14 13:56:11.009: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 14 13:56:11.023: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 14 13:56:11.029: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Dec 14 13:56:13.056: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9502  f3d417a2-bdaa-42e2-ad89-f636a89793dd 118032 3 2021-12-14 13:56:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-12-14 13:56:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f1328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-12-14 13:56:11 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-12-14 13:56:11 +0000 UTC,LastTransitionTime:2021-12-14 13:56:00 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 14 13:56:13.062: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9502  dce0f066-b09b-443d-bce4-b7d62bc2dd7a 118024 3 2021-12-14 13:56:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f3d417a2-bdaa-42e2-ad89-f636a89793dd 0xc0045f1727 0xc0045f1728}] []  [{kube-controller-manager Update apps/v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3d417a2-bdaa-42e2-ad89-f636a89793dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f17a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:56:13.062: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 14 13:56:13.062: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-9502  fae2cd84-118c-4d06-9274-a382e5ab21e6 118007 3 2021-12-14 13:56:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f3d417a2-bdaa-42e2-ad89-f636a89793dd 0xc0045f1807 0xc0045f1808}] []  [{kube-controller-manager Update apps/v1 2021-12-14 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3d417a2-bdaa-42e2-ad89-f636a89793dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f1878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 14 13:56:13.084: INFO: Pod "webserver-deployment-795d758f88-5z6dv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5z6dv webserver-deployment-795d758f88- deployment-9502  358c3c54-60f6-476d-bffa-29f220062d34 118014 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0045f1d47 0xc0045f1d48}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gn9wx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gn9wx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.084: INFO: Pod "webserver-deployment-795d758f88-984dn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-984dn webserver-deployment-795d758f88- deployment-9502  16bb3f4d-5c37-4f4e-ab14-95098a014392 117970 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0045f1f67 0xc0045f1f68}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58tn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58tn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.085: INFO: Pod "webserver-deployment-795d758f88-98dkc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-98dkc webserver-deployment-795d758f88- deployment-9502  518f924c-c91c-40e3-9816-bc0a98f6d8b9 118039 0 2021-12-14 13:56:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.234"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.234"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bc187 0xc0044bc188}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wggtr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wggtr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.085: INFO: Pod "webserver-deployment-795d758f88-c58vs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c58vs webserver-deployment-795d758f88- deployment-9502  0e7d2681-e005-435e-9c3a-2bd21ce22b2a 118033 0 2021-12-14 13:56:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.131"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.131"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bc3b7 0xc0044bc3b8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hj6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hj6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.085: INFO: Pod "webserver-deployment-795d758f88-dpzlb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dpzlb webserver-deployment-795d758f88- deployment-9502  950f34da-4d90-4554-ba8a-7f2546093ace 117922 0 2021-12-14 13:56:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.68"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.68"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bc627 0xc0044bc628}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-12-14 13:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hf47s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf47s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.098: INFO: Pod "webserver-deployment-795d758f88-dxd9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dxd9d webserver-deployment-795d758f88- deployment-9502  5f98548c-88ce-4393-a79e-c48aa64e506f 118027 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bca07 0xc0044bca08}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2cvp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2cvp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.104: INFO: Pod "webserver-deployment-795d758f88-grkwt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-grkwt webserver-deployment-795d758f88- deployment-9502  9dca4384-00be-40f0-8b0a-feb618ccf1f2 117986 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bcd97 0xc0044bcd98}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6529v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6529v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.104: INFO: Pod "webserver-deployment-795d758f88-nh9wn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nh9wn webserver-deployment-795d758f88- deployment-9502  f6d9d785-2cb7-4bf3-b0ad-abc05ed17ebc 118043 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bd197 0xc0044bd198}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85jq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85jq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.106: INFO: Pod "webserver-deployment-795d758f88-ptszh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ptszh webserver-deployment-795d758f88- deployment-9502  4d0be931-061a-4b33-82d8-98d8d89526d8 117934 0 2021-12-14 13:56:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.132"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.132"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bd547 0xc0044bd548}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dmznz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dmznz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.107: INFO: Pod "webserver-deployment-795d758f88-qnzrk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qnzrk webserver-deployment-795d758f88- deployment-9502  6f358cc5-4b9d-40a1-adca-29fc23cc52f5 118023 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bd8f7 0xc0044bd8f8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhcwz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhcwz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.107: INFO: Pod "webserver-deployment-795d758f88-tfqwn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tfqwn webserver-deployment-795d758f88- deployment-9502  154cb805-8ca5-49bc-92bd-fd79a66ac66d 118003 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc0044bdc37 0xc0044bdc38}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gnhph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gnhph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.107: INFO: Pod "webserver-deployment-795d758f88-xcmt2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xcmt2 webserver-deployment-795d758f88- deployment-9502  3414d01e-d577-4cb6-8b00-8e9fbd00c742 118037 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc000ac2447 0xc000ac2448}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vsfxh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vsfxh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.108: INFO: Pod "webserver-deployment-795d758f88-z5f4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z5f4x webserver-deployment-795d758f88- deployment-9502  9940c175-dc14-4f05-9688-c7f35d210383 117920 0 2021-12-14 13:56:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.233"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.233"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 dce0f066-b09b-443d-bce4-b7d62bc2dd7a 0xc000ac26d7 0xc000ac26d8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dce0f066-b09b-443d-bce4-b7d62bc2dd7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {multus Update v1 2021-12-14 13:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gpdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gpdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.108: INFO: Pod "webserver-deployment-847dcfb7fb-2ncxr" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2ncxr webserver-deployment-847dcfb7fb- deployment-9502  59ef568d-e388-4760-89a7-adc0f5bd3703 117753 0 2021-12-14 13:56:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.232"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.232"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc000ac2cf7 0xc000ac2cf8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgqkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgqkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.232,StartTime:2021-12-14 13:56:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://fba45823f763035085bd779e763206ee7bb2768642b0343c7c3bd3fd71a81cd3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.108: INFO: Pod "webserver-deployment-847dcfb7fb-49vm8" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-49vm8 webserver-deployment-847dcfb7fb- deployment-9502  038f7ce2-5497-44bc-803f-db6f066c803d 117747 0 2021-12-14 13:56:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.67"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.67"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc000ac31f7 0xc000ac31f8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8hdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8hdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:10.129.2.67,StartTime:2021-12-14 13:56:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://c600e1fc92fe9a16e2596cdccb04900946e1b13fd04c2db3b7632d84562735dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.109: INFO: Pod "webserver-deployment-847dcfb7fb-6fq5l" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6fq5l webserver-deployment-847dcfb7fb- deployment-9502  63a0fba3-c6e5-41ef-845e-93c01f872c7e 118022 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc000ac3437 0xc000ac3438}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2l59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2l59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.109: INFO: Pod "webserver-deployment-847dcfb7fb-6psrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6psrp webserver-deployment-847dcfb7fb- deployment-9502  d64da1c3-9065-4c01-9c1a-718cadc21b7e 118038 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc000ac37f7 0xc000ac37f8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvrnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvrnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.109: INFO: Pod "webserver-deployment-847dcfb7fb-bk2v9" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-bk2v9 webserver-deployment-847dcfb7fb- deployment-9502  101c0650-9435-4731-8226-676f2877ed00 118021 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc000ac39f7 0xc000ac39f8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4zfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4zfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.110: INFO: Pod "webserver-deployment-847dcfb7fb-bpq86" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-bpq86 webserver-deployment-847dcfb7fb- deployment-9502  d1635dd8-1000-4d69-9179-ed3375b995b4 117758 0 2021-12-14 13:56:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.230"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.230"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc000ac3bf7 0xc000ac3bf8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kvjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kvjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.230,StartTime:2021-12-14 13:56:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://b378c02dfbed664d3cd16f4ef0400e7dc1cdf642d1c6285e730a8348189f4376,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.110: INFO: Pod "webserver-deployment-847dcfb7fb-cfr5p" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cfr5p webserver-deployment-847dcfb7fb- deployment-9502  47e81a9a-1c66-45c2-9d2c-8d5e238cb3fe 118018 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc000ac3e47 0xc000ac3e48}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8h9cq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8h9cq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.110: INFO: Pod "webserver-deployment-847dcfb7fb-cv68x" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cv68x webserver-deployment-847dcfb7fb- deployment-9502  92bc243f-af74-4c35-ac84-5e43588b32f2 118047 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbc067 0xc003fbc068}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhq2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhq2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.111: INFO: Pod "webserver-deployment-847dcfb7fb-flgxq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-flgxq webserver-deployment-847dcfb7fb- deployment-9502  a3fcdc78-840b-47ff-b8f4-c85ff4a5a6d9 117791 0 2021-12-14 13:56:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.129"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.129"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbc267 0xc003fbc268}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzfs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzfs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:10.131.0.129,StartTime:2021-12-14 13:56:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://9ccec8c0ebf41ee2fba906ac7ef5dd8412a1dd5f3fd290eb905bf7d8446ee4f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.111: INFO: Pod "webserver-deployment-847dcfb7fb-hkk2h" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hkk2h webserver-deployment-847dcfb7fb- deployment-9502  ff60cb8a-b4e0-49c5-9b0f-8a76191a446a 117949 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbc497 0xc003fbc498}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxcjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxcjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.111: INFO: Pod "webserver-deployment-847dcfb7fb-j7j96" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-j7j96 webserver-deployment-847dcfb7fb- deployment-9502  ba92e050-4948-46fa-84fc-61a9a6a69a83 117999 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbc697 0xc003fbc698}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z7bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z7bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.111: INFO: Pod "webserver-deployment-847dcfb7fb-jjz6p" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jjz6p webserver-deployment-847dcfb7fb- deployment-9502  6f6b9bca-b60c-41e7-90c9-efd59fe92e0a 118004 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbc897 0xc003fbc898}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6t5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6t5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.112: INFO: Pod "webserver-deployment-847dcfb7fb-kp5bg" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kp5bg webserver-deployment-847dcfb7fb- deployment-9502  02a86833-34d4-4cb9-8db1-15a8b3497be3 117695 0 2021-12-14 13:56:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.229"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.229"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbca97 0xc003fbca98}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4p42,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4p42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.229,StartTime:2021-12-14 13:56:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://9dbdf85baefda27f790936d4fafa38770c033b48003e19a539c07173a145bb9c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.112: INFO: Pod "webserver-deployment-847dcfb7fb-kz2d5" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kz2d5 webserver-deployment-847dcfb7fb- deployment-9502  5dd6f409-eec6-4e3d-96bf-fb69e53d5257 118044 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbccd7 0xc003fbccd8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ln4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ln4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.113: INFO: Pod "webserver-deployment-847dcfb7fb-lwczj" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lwczj webserver-deployment-847dcfb7fb- deployment-9502  e1831d20-7e94-4416-94fb-a3df397859dc 117716 0 2021-12-14 13:56:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.66"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.66"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbced7 0xc003fbced8}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ltxqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ltxqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:10.129.2.66,StartTime:2021-12-14 13:56:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://8c5db3d7cf36823ba7aa54ad4797025762ea23c5d95f52432a97b5afec724792,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.113: INFO: Pod "webserver-deployment-847dcfb7fb-m99k9" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-m99k9 webserver-deployment-847dcfb7fb- deployment-9502  37250886-eb7b-4810-9dae-f133f166f394 117952 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbd107 0xc003fbd108}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bvjbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bvjbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.113: INFO: Pod "webserver-deployment-847dcfb7fb-w8zw4" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-w8zw4 webserver-deployment-847dcfb7fb- deployment-9502  4a4d68f8-0862-419f-ba01-1e6318dade5a 117755 0 2021-12-14 13:56:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.231"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.231"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbd307 0xc003fbd308}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r2mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r2mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.2.231,StartTime:2021-12-14 13:56:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://98d40b0271d682b265a69fdf6da5cdd2cdb9659158b933eb3ff71fef2a1e3af9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.113: INFO: Pod "webserver-deployment-847dcfb7fb-wb4bn" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wb4bn webserver-deployment-847dcfb7fb- deployment-9502  39274707-0040-4d61-b519-c6a0ce430aa6 118053 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbd547 0xc003fbd548}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97plt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97plt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.114: INFO: Pod "webserver-deployment-847dcfb7fb-x27gm" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-x27gm webserver-deployment-847dcfb7fb- deployment-9502  43f241eb-8707-499c-a2ff-390bac3abbb5 117798 0 2021-12-14 13:56:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.130"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.130"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbd747 0xc003fbd748}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 13:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 13:56:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsk2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsk2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-150-199.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.150.199,PodIP:10.131.0.130,StartTime:2021-12-14 13:56:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 13:56:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://2be052ec7e7fb946143f038c8f3bf4c0576cf7be1c49c7e6a55479289d1040fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 13:56:13.114: INFO: Pod "webserver-deployment-847dcfb7fb-z5v6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-z5v6k webserver-deployment-847dcfb7fb- deployment-9502  b61cb07b-6bcb-40f3-9a1a-086eae3498e1 118028 0 2021-12-14 13:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb fae2cd84-118c-4d06-9274-a382e5ab21e6 0xc003fbd977 0xc003fbd978}] []  [{kube-controller-manager Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fae2cd84-118c-4d06-9274-a382e5ab21e6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-12-14 13:56:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4rlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4rlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-177.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c59,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6phbt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 13:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.177,PodIP:,StartTime:2021-12-14 13:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:56:13.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9502" for this suite.

• [SLOW TEST:12.584 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":265,"skipped":4204,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:56:13.141: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-8hptf in namespace proxy-83
I1214 13:56:13.514206      20 runners.go:190] Created replication controller with name: proxy-service-8hptf, namespace: proxy-83, replica count: 1
I1214 13:56:14.573138      20 runners.go:190] proxy-service-8hptf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:56:15.599949      20 runners.go:190] proxy-service-8hptf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:56:16.600198      20 runners.go:190] proxy-service-8hptf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:56:17.601263      20 runners.go:190] proxy-service-8hptf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 13:56:18.602169      20 runners.go:190] proxy-service-8hptf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:56:18.607: INFO: setup took 5.296978862s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 14 13:56:18.617: INFO: (0) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 9.751035ms)
Dec 14 13:56:18.619: INFO: (0) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 11.150051ms)
Dec 14 13:56:18.619: INFO: (0) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 11.245377ms)
Dec 14 13:56:18.619: INFO: (0) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 11.456488ms)
Dec 14 13:56:18.620: INFO: (0) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 12.504339ms)
Dec 14 13:56:18.624: INFO: (0) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 15.667163ms)
Dec 14 13:56:18.624: INFO: (0) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 15.774322ms)
Dec 14 13:56:18.627: INFO: (0) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 18.512917ms)
Dec 14 13:56:18.628: INFO: (0) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 19.65557ms)
Dec 14 13:56:18.629: INFO: (0) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 21.626036ms)
Dec 14 13:56:18.629: INFO: (0) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 21.539337ms)
Dec 14 13:56:18.629: INFO: (0) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 21.403888ms)
Dec 14 13:56:18.629: INFO: (0) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 21.343028ms)
Dec 14 13:56:18.629: INFO: (0) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 21.611918ms)
Dec 14 13:56:18.629: INFO: (0) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 21.419773ms)
Dec 14 13:56:18.634: INFO: (0) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 25.999304ms)
Dec 14 13:56:18.642: INFO: (1) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 7.419077ms)
Dec 14 13:56:18.642: INFO: (1) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 7.784319ms)
Dec 14 13:56:18.643: INFO: (1) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 8.954954ms)
Dec 14 13:56:18.643: INFO: (1) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 8.376951ms)
Dec 14 13:56:18.644: INFO: (1) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 8.909068ms)
Dec 14 13:56:18.645: INFO: (1) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 10.049189ms)
Dec 14 13:56:18.645: INFO: (1) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 9.928202ms)
Dec 14 13:56:18.645: INFO: (1) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 11.078703ms)
Dec 14 13:56:18.646: INFO: (1) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 11.654981ms)
Dec 14 13:56:18.646: INFO: (1) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 12.342311ms)
Dec 14 13:56:18.647: INFO: (1) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 12.306927ms)
Dec 14 13:56:18.648: INFO: (1) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 13.609481ms)
Dec 14 13:56:18.648: INFO: (1) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 12.869867ms)
Dec 14 13:56:18.648: INFO: (1) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 13.093172ms)
Dec 14 13:56:18.648: INFO: (1) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 14.191446ms)
Dec 14 13:56:18.650: INFO: (1) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 15.530884ms)
Dec 14 13:56:18.663: INFO: (2) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 13.150676ms)
Dec 14 13:56:18.664: INFO: (2) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 12.836876ms)
Dec 14 13:56:18.664: INFO: (2) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 13.903274ms)
Dec 14 13:56:18.664: INFO: (2) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 14.131074ms)
Dec 14 13:56:18.665: INFO: (2) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 13.776651ms)
Dec 14 13:56:18.665: INFO: (2) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 14.324231ms)
Dec 14 13:56:18.665: INFO: (2) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 14.367296ms)
Dec 14 13:56:18.666: INFO: (2) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 15.683049ms)
Dec 14 13:56:18.667: INFO: (2) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 16.053095ms)
Dec 14 13:56:18.668: INFO: (2) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 17.656349ms)
Dec 14 13:56:18.669: INFO: (2) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 18.385329ms)
Dec 14 13:56:18.669: INFO: (2) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 18.421363ms)
Dec 14 13:56:18.669: INFO: (2) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 18.615395ms)
Dec 14 13:56:18.670: INFO: (2) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 19.393152ms)
Dec 14 13:56:18.670: INFO: (2) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 19.991412ms)
Dec 14 13:56:18.671: INFO: (2) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 20.599836ms)
Dec 14 13:56:18.679: INFO: (3) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 6.583758ms)
Dec 14 13:56:18.681: INFO: (3) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 9.36094ms)
Dec 14 13:56:18.681: INFO: (3) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 9.452074ms)
Dec 14 13:56:18.682: INFO: (3) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 10.349202ms)
Dec 14 13:56:18.683: INFO: (3) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 11.033151ms)
Dec 14 13:56:18.683: INFO: (3) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 10.804307ms)
Dec 14 13:56:18.683: INFO: (3) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 11.362338ms)
Dec 14 13:56:18.684: INFO: (3) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 11.596765ms)
Dec 14 13:56:18.684: INFO: (3) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 12.033028ms)
Dec 14 13:56:18.684: INFO: (3) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 11.795919ms)
Dec 14 13:56:18.685: INFO: (3) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 12.709651ms)
Dec 14 13:56:18.685: INFO: (3) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 13.298253ms)
Dec 14 13:56:18.686: INFO: (3) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 14.881071ms)
Dec 14 13:56:18.686: INFO: (3) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 14.614672ms)
Dec 14 13:56:18.687: INFO: (3) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 15.543909ms)
Dec 14 13:56:18.689: INFO: (3) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 16.908668ms)
Dec 14 13:56:18.694: INFO: (4) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 5.122752ms)
Dec 14 13:56:18.695: INFO: (4) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 6.318847ms)
Dec 14 13:56:18.696: INFO: (4) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 7.341418ms)
Dec 14 13:56:18.698: INFO: (4) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 8.562402ms)
Dec 14 13:56:18.699: INFO: (4) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 9.216144ms)
Dec 14 13:56:18.699: INFO: (4) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 9.430787ms)
Dec 14 13:56:18.703: INFO: (4) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 13.674996ms)
Dec 14 13:56:18.709: INFO: (4) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 20.142162ms)
Dec 14 13:56:18.709: INFO: (4) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 20.229684ms)
Dec 14 13:56:18.709: INFO: (4) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 19.789458ms)
Dec 14 13:56:18.710: INFO: (4) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 19.961735ms)
Dec 14 13:56:18.710: INFO: (4) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 20.21404ms)
Dec 14 13:56:18.710: INFO: (4) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 20.569681ms)
Dec 14 13:56:18.711: INFO: (4) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 21.224082ms)
Dec 14 13:56:18.714: INFO: (4) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 24.891518ms)
Dec 14 13:56:18.715: INFO: (4) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 25.209288ms)
Dec 14 13:56:18.723: INFO: (5) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 7.941268ms)
Dec 14 13:56:18.723: INFO: (5) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 8.527268ms)
Dec 14 13:56:18.727: INFO: (5) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 10.843886ms)
Dec 14 13:56:18.727: INFO: (5) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 11.874066ms)
Dec 14 13:56:18.727: INFO: (5) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 12.272536ms)
Dec 14 13:56:18.731: INFO: (5) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 14.731926ms)
Dec 14 13:56:18.731: INFO: (5) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 15.319539ms)
Dec 14 13:56:18.731: INFO: (5) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 15.083008ms)
Dec 14 13:56:18.732: INFO: (5) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 15.390127ms)
Dec 14 13:56:18.732: INFO: (5) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 15.72589ms)
Dec 14 13:56:18.732: INFO: (5) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 16.37944ms)
Dec 14 13:56:18.732: INFO: (5) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 16.860604ms)
Dec 14 13:56:18.733: INFO: (5) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 16.694472ms)
Dec 14 13:56:18.734: INFO: (5) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 17.311629ms)
Dec 14 13:56:18.734: INFO: (5) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 18.473103ms)
Dec 14 13:56:18.736: INFO: (5) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 19.322405ms)
Dec 14 13:56:18.745: INFO: (6) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 9.325705ms)
Dec 14 13:56:18.745: INFO: (6) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 9.771731ms)
Dec 14 13:56:18.746: INFO: (6) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 10.1025ms)
Dec 14 13:56:18.746: INFO: (6) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 10.309646ms)
Dec 14 13:56:18.746: INFO: (6) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 10.167503ms)
Dec 14 13:56:18.747: INFO: (6) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 10.646184ms)
Dec 14 13:56:18.747: INFO: (6) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 11.113512ms)
Dec 14 13:56:18.747: INFO: (6) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 10.787077ms)
Dec 14 13:56:18.748: INFO: (6) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 11.933307ms)
Dec 14 13:56:18.748: INFO: (6) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 12.12528ms)
Dec 14 13:56:18.748: INFO: (6) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 12.538884ms)
Dec 14 13:56:18.749: INFO: (6) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 12.842445ms)
Dec 14 13:56:18.749: INFO: (6) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 12.606702ms)
Dec 14 13:56:18.749: INFO: (6) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 13.078434ms)
Dec 14 13:56:18.750: INFO: (6) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 13.404234ms)
Dec 14 13:56:18.751: INFO: (6) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 14.774051ms)
Dec 14 13:56:18.759: INFO: (7) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 7.986726ms)
Dec 14 13:56:18.760: INFO: (7) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 9.152695ms)
Dec 14 13:56:18.760: INFO: (7) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 9.081256ms)
Dec 14 13:56:18.760: INFO: (7) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 8.999468ms)
Dec 14 13:56:18.760: INFO: (7) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 8.749224ms)
Dec 14 13:56:18.761: INFO: (7) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 9.185458ms)
Dec 14 13:56:18.761: INFO: (7) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 9.086634ms)
Dec 14 13:56:18.761: INFO: (7) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 9.709003ms)
Dec 14 13:56:18.761: INFO: (7) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 9.655976ms)
Dec 14 13:56:18.762: INFO: (7) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 9.985549ms)
Dec 14 13:56:18.763: INFO: (7) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 11.604396ms)
Dec 14 13:56:18.765: INFO: (7) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 13.47571ms)
Dec 14 13:56:18.767: INFO: (7) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 15.121007ms)
Dec 14 13:56:18.771: INFO: (7) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 19.077301ms)
Dec 14 13:56:18.778: INFO: (7) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 26.384667ms)
Dec 14 13:56:18.778: INFO: (7) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 26.959257ms)
Dec 14 13:56:18.785: INFO: (8) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 7.224903ms)
Dec 14 13:56:18.788: INFO: (8) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 9.033397ms)
Dec 14 13:56:18.788: INFO: (8) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 9.155096ms)
Dec 14 13:56:18.788: INFO: (8) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 8.898736ms)
Dec 14 13:56:18.788: INFO: (8) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 9.377944ms)
Dec 14 13:56:18.790: INFO: (8) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 11.239875ms)
Dec 14 13:56:18.790: INFO: (8) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 12.066426ms)
Dec 14 13:56:18.790: INFO: (8) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 11.424129ms)
Dec 14 13:56:18.791: INFO: (8) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 12.799245ms)
Dec 14 13:56:18.792: INFO: (8) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 12.462799ms)
Dec 14 13:56:18.792: INFO: (8) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 13.611214ms)
Dec 14 13:56:18.800: INFO: (8) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 21.998434ms)
Dec 14 13:56:18.801: INFO: (8) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 21.774432ms)
Dec 14 13:56:18.801: INFO: (8) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 22.255546ms)
Dec 14 13:56:18.802: INFO: (8) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 23.205564ms)
Dec 14 13:56:18.804: INFO: (8) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 25.750507ms)
Dec 14 13:56:18.811: INFO: (9) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 6.308004ms)
Dec 14 13:56:18.812: INFO: (9) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 7.598171ms)
Dec 14 13:56:18.813: INFO: (9) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 8.148865ms)
Dec 14 13:56:18.813: INFO: (9) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 7.984294ms)
Dec 14 13:56:18.814: INFO: (9) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 8.33018ms)
Dec 14 13:56:18.814: INFO: (9) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 8.778805ms)
Dec 14 13:56:18.814: INFO: (9) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 9.513661ms)
Dec 14 13:56:18.815: INFO: (9) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 9.922007ms)
Dec 14 13:56:18.815: INFO: (9) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 9.613174ms)
Dec 14 13:56:18.816: INFO: (9) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 11.297405ms)
Dec 14 13:56:18.817: INFO: (9) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 11.378824ms)
Dec 14 13:56:18.817: INFO: (9) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 12.346212ms)
Dec 14 13:56:18.818: INFO: (9) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 13.032421ms)
Dec 14 13:56:18.819: INFO: (9) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 13.321518ms)
Dec 14 13:56:18.820: INFO: (9) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 14.596259ms)
Dec 14 13:56:18.820: INFO: (9) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 14.967911ms)
Dec 14 13:56:18.828: INFO: (10) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 7.514456ms)
Dec 14 13:56:18.829: INFO: (10) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 8.759186ms)
Dec 14 13:56:18.830: INFO: (10) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 8.791204ms)
Dec 14 13:56:18.833: INFO: (10) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 11.725761ms)
Dec 14 13:56:18.833: INFO: (10) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 11.658055ms)
Dec 14 13:56:18.833: INFO: (10) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 11.294994ms)
Dec 14 13:56:18.833: INFO: (10) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 11.292997ms)
Dec 14 13:56:18.833: INFO: (10) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 11.557615ms)
Dec 14 13:56:18.833: INFO: (10) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 12.386205ms)
Dec 14 13:56:18.835: INFO: (10) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 13.169583ms)
Dec 14 13:56:18.835: INFO: (10) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 14.922974ms)
Dec 14 13:56:18.836: INFO: (10) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 15.116424ms)
Dec 14 13:56:18.841: INFO: (10) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 19.021578ms)
Dec 14 13:56:18.841: INFO: (10) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 19.778791ms)
Dec 14 13:56:18.841: INFO: (10) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 19.505673ms)
Dec 14 13:56:18.852: INFO: (10) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 30.493121ms)
Dec 14 13:56:18.858: INFO: (11) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 5.881625ms)
Dec 14 13:56:18.859: INFO: (11) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 6.490268ms)
Dec 14 13:56:18.861: INFO: (11) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 8.224061ms)
Dec 14 13:56:18.862: INFO: (11) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 9.945114ms)
Dec 14 13:56:18.862: INFO: (11) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 10.163262ms)
Dec 14 13:56:18.863: INFO: (11) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 10.801789ms)
Dec 14 13:56:18.864: INFO: (11) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 9.717971ms)
Dec 14 13:56:18.864: INFO: (11) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 9.878699ms)
Dec 14 13:56:18.866: INFO: (11) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 13.101673ms)
Dec 14 13:56:18.866: INFO: (11) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 13.435152ms)
Dec 14 13:56:18.866: INFO: (11) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 12.814804ms)
Dec 14 13:56:18.867: INFO: (11) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 14.187954ms)
Dec 14 13:56:18.868: INFO: (11) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 14.491321ms)
Dec 14 13:56:18.869: INFO: (11) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 15.330029ms)
Dec 14 13:56:18.869: INFO: (11) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 16.112939ms)
Dec 14 13:56:18.879: INFO: (11) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 24.958572ms)
Dec 14 13:56:18.899: INFO: (12) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 19.873689ms)
Dec 14 13:56:18.899: INFO: (12) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 20.024674ms)
Dec 14 13:56:18.899: INFO: (12) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 20.612831ms)
Dec 14 13:56:18.901: INFO: (12) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 22.102095ms)
Dec 14 13:56:18.901: INFO: (12) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 22.392364ms)
Dec 14 13:56:18.901: INFO: (12) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 22.337196ms)
Dec 14 13:56:18.910: INFO: (12) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 12.200379ms)
Dec 14 13:56:18.910: INFO: (12) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 11.882351ms)
Dec 14 13:56:18.910: INFO: (12) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 12.085219ms)
Dec 14 13:56:18.910: INFO: (12) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 12.194744ms)
Dec 14 13:56:18.910: INFO: (12) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 12.284402ms)
Dec 14 13:56:18.910: INFO: (12) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 12.138832ms)
Dec 14 13:56:18.918: INFO: (12) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 20.470184ms)
Dec 14 13:56:18.918: INFO: (12) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 20.483211ms)
Dec 14 13:56:18.918: INFO: (12) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 20.414885ms)
Dec 14 13:56:18.918: INFO: (12) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 20.409348ms)
Dec 14 13:56:18.932: INFO: (13) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 13.044124ms)
Dec 14 13:56:18.932: INFO: (13) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 12.984165ms)
Dec 14 13:56:18.933: INFO: (13) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 13.734002ms)
Dec 14 13:56:18.937: INFO: (13) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 18.189748ms)
Dec 14 13:56:18.937: INFO: (13) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 15.66249ms)
Dec 14 13:56:18.941: INFO: (13) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 19.752455ms)
Dec 14 13:56:18.949: INFO: (13) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 28.109962ms)
Dec 14 13:56:18.951: INFO: (13) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 31.739579ms)
Dec 14 13:56:18.951: INFO: (13) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 31.88889ms)
Dec 14 13:56:18.953: INFO: (13) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 33.627541ms)
Dec 14 13:56:18.960: INFO: (13) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 38.864404ms)
Dec 14 13:56:18.962: INFO: (13) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 30.712071ms)
Dec 14 13:56:18.965: INFO: (13) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 33.733192ms)
Dec 14 13:56:18.971: INFO: (13) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 40.131067ms)
Dec 14 13:56:18.978: INFO: (13) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 46.198902ms)
Dec 14 13:56:18.985: INFO: (13) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 53.510528ms)
Dec 14 13:56:18.995: INFO: (14) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 9.333127ms)
Dec 14 13:56:18.997: INFO: (14) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 10.851836ms)
Dec 14 13:56:18.998: INFO: (14) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 12.23006ms)
Dec 14 13:56:18.998: INFO: (14) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 11.894287ms)
Dec 14 13:56:18.999: INFO: (14) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 13.437719ms)
Dec 14 13:56:19.000: INFO: (14) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 13.661129ms)
Dec 14 13:56:19.000: INFO: (14) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 15.235787ms)
Dec 14 13:56:19.000: INFO: (14) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 14.888902ms)
Dec 14 13:56:19.000: INFO: (14) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 14.757357ms)
Dec 14 13:56:19.001: INFO: (14) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 15.619462ms)
Dec 14 13:56:19.002: INFO: (14) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 15.423821ms)
Dec 14 13:56:19.004: INFO: (14) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 18.024284ms)
Dec 14 13:56:19.004: INFO: (14) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 18.603976ms)
Dec 14 13:56:19.004: INFO: (14) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 18.000867ms)
Dec 14 13:56:19.004: INFO: (14) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 18.476714ms)
Dec 14 13:56:19.005: INFO: (14) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 18.605137ms)
Dec 14 13:56:19.022: INFO: (15) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 16.122387ms)
Dec 14 13:56:19.022: INFO: (15) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 16.379912ms)
Dec 14 13:56:19.022: INFO: (15) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 16.496074ms)
Dec 14 13:56:19.022: INFO: (15) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 16.645306ms)
Dec 14 13:56:19.022: INFO: (15) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 16.447725ms)
Dec 14 13:56:19.023: INFO: (15) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 17.15746ms)
Dec 14 13:56:19.026: INFO: (15) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 19.612218ms)
Dec 14 13:56:19.026: INFO: (15) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 19.640837ms)
Dec 14 13:56:19.026: INFO: (15) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 20.277698ms)
Dec 14 13:56:19.026: INFO: (15) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 20.491992ms)
Dec 14 13:56:19.026: INFO: (15) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 20.010329ms)
Dec 14 13:56:19.026: INFO: (15) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 20.663805ms)
Dec 14 13:56:19.027: INFO: (15) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 21.64121ms)
Dec 14 13:56:19.027: INFO: (15) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 21.772323ms)
Dec 14 13:56:19.030: INFO: (15) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 23.685202ms)
Dec 14 13:56:19.031: INFO: (15) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 24.827519ms)
Dec 14 13:56:19.044: INFO: (16) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 13.203446ms)
Dec 14 13:56:19.054: INFO: (16) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 22.904885ms)
Dec 14 13:56:19.056: INFO: (16) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 24.237489ms)
Dec 14 13:56:19.056: INFO: (16) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 23.981634ms)
Dec 14 13:56:19.056: INFO: (16) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 24.151008ms)
Dec 14 13:56:19.056: INFO: (16) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 24.122083ms)
Dec 14 13:56:19.056: INFO: (16) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 24.769762ms)
Dec 14 13:56:19.056: INFO: (16) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 24.672782ms)
Dec 14 13:56:19.056: INFO: (16) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 25.155195ms)
Dec 14 13:56:19.064: INFO: (16) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 32.41375ms)
Dec 14 13:56:19.064: INFO: (16) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 31.931871ms)
Dec 14 13:56:19.073: INFO: (16) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 41.491841ms)
Dec 14 13:56:19.073: INFO: (16) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 42.588871ms)
Dec 14 13:56:19.074: INFO: (16) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 41.636569ms)
Dec 14 13:56:19.074: INFO: (16) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 42.124049ms)
Dec 14 13:56:19.075: INFO: (16) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 43.503323ms)
Dec 14 13:56:19.091: INFO: (17) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 16.214426ms)
Dec 14 13:56:19.092: INFO: (17) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 16.402875ms)
Dec 14 13:56:19.092: INFO: (17) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 16.577807ms)
Dec 14 13:56:19.093: INFO: (17) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 17.061071ms)
Dec 14 13:56:19.093: INFO: (17) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 16.900679ms)
Dec 14 13:56:19.094: INFO: (17) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 18.472401ms)
Dec 14 13:56:19.094: INFO: (17) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 18.276777ms)
Dec 14 13:56:19.094: INFO: (17) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 18.565907ms)
Dec 14 13:56:19.097: INFO: (17) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 21.058528ms)
Dec 14 13:56:19.098: INFO: (17) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 23.19976ms)
Dec 14 13:56:19.098: INFO: (17) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 22.245696ms)
Dec 14 13:56:19.099: INFO: (17) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 23.621806ms)
Dec 14 13:56:19.109: INFO: (17) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 32.848549ms)
Dec 14 13:56:19.109: INFO: (17) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 33.864535ms)
Dec 14 13:56:19.109: INFO: (17) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 33.817008ms)
Dec 14 13:56:19.118: INFO: (17) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 41.685545ms)
Dec 14 13:56:19.134: INFO: (18) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 15.93025ms)
Dec 14 13:56:19.149: INFO: (18) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 30.995616ms)
Dec 14 13:56:19.149: INFO: (18) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 31.556338ms)
Dec 14 13:56:19.153: INFO: (18) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 34.98753ms)
Dec 14 13:56:19.154: INFO: (18) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 35.034227ms)
Dec 14 13:56:19.154: INFO: (18) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 35.336105ms)
Dec 14 13:56:19.156: INFO: (18) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 38.212883ms)
Dec 14 13:56:19.157: INFO: (18) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 38.184143ms)
Dec 14 13:56:19.159: INFO: (18) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 41.01436ms)
Dec 14 13:56:19.159: INFO: (18) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 41.35441ms)
Dec 14 13:56:19.160: INFO: (18) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 42.047172ms)
Dec 14 13:56:19.160: INFO: (18) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 41.679034ms)
Dec 14 13:56:19.164: INFO: (18) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 45.928441ms)
Dec 14 13:56:19.165: INFO: (18) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 46.41309ms)
Dec 14 13:56:19.165: INFO: (18) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 47.184806ms)
Dec 14 13:56:19.193: INFO: (18) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 74.235372ms)
Dec 14 13:56:19.210: INFO: (19) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk/proxy/rewriteme">test</a> (200; 16.796932ms)
Dec 14 13:56:19.211: INFO: (19) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 18.254171ms)
Dec 14 13:56:19.224: INFO: (19) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 30.874515ms)
Dec 14 13:56:19.226: INFO: (19) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:462/proxy/: tls qux (200; 33.376735ms)
Dec 14 13:56:19.226: INFO: (19) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">te... (200; 32.995185ms)
Dec 14 13:56:19.226: INFO: (19) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:460/proxy/: tls baz (200; 33.694475ms)
Dec 14 13:56:19.228: INFO: (19) /api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/proxy-service-8hptf-hxzhk:1080/proxy/rewriteme">test</a... (200; 34.141146ms)
Dec 14 13:56:19.228: INFO: (19) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:162/proxy/: bar (200; 34.839927ms)
Dec 14 13:56:19.232: INFO: (19) /api/v1/namespaces/proxy-83/pods/http:proxy-service-8hptf-hxzhk:160/proxy/: foo (200; 39.716115ms)
Dec 14 13:56:19.239: INFO: (19) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname2/proxy/: bar (200; 45.660808ms)
Dec 14 13:56:19.239: INFO: (19) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname1/proxy/: foo (200; 46.113543ms)
Dec 14 13:56:19.239: INFO: (19) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname2/proxy/: tls qux (200; 46.744038ms)
Dec 14 13:56:19.248: INFO: (19) /api/v1/namespaces/proxy-83/services/proxy-service-8hptf:portname1/proxy/: foo (200; 55.603403ms)
Dec 14 13:56:19.248: INFO: (19) /api/v1/namespaces/proxy-83/services/http:proxy-service-8hptf:portname2/proxy/: bar (200; 55.076077ms)
Dec 14 13:56:19.248: INFO: (19) /api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/: <a href="/api/v1/namespaces/proxy-83/pods/https:proxy-service-8hptf-hxzhk:443/proxy/tlsrewriteme"... (200; 55.726187ms)
Dec 14 13:56:19.249: INFO: (19) /api/v1/namespaces/proxy-83/services/https:proxy-service-8hptf:tlsportname1/proxy/: tls baz (200; 55.679436ms)
STEP: deleting ReplicationController proxy-service-8hptf in namespace proxy-83, will wait for the garbage collector to delete the pods
Dec 14 13:56:19.329: INFO: Deleting ReplicationController proxy-service-8hptf took: 15.596562ms
Dec 14 13:56:19.429: INFO: Terminating ReplicationController proxy-service-8hptf pods took: 100.286229ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:56:28.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-83" for this suite.

• [SLOW TEST:15.426 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":266,"skipped":4223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:56:28.568: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 14 13:56:34.895: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:56:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1214 13:56:34.895654      20 metrics_grabber.go:102] Can't find any pods in namespace kube-system to grab metrics from
W1214 13:56:34.895685      20 metrics_grabber.go:138] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-6630" for this suite.

• [SLOW TEST:6.349 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":267,"skipped":4246,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:56:34.918: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-50bdd30a-85ba-496b-90df-733584d93a25
STEP: Creating a pod to test consume configMaps
Dec 14 13:56:35.191: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6" in namespace "projected-2119" to be "Succeeded or Failed"
Dec 14 13:56:35.204: INFO: Pod "pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.412968ms
Dec 14 13:56:37.215: INFO: Pod "pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023528199s
Dec 14 13:56:39.221: INFO: Pod "pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029672509s
STEP: Saw pod success
Dec 14 13:56:39.221: INFO: Pod "pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6" satisfied condition "Succeeded or Failed"
Dec 14 13:56:39.225: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 13:56:39.250: INFO: Waiting for pod pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6 to disappear
Dec 14 13:56:39.254: INFO: Pod pod-projected-configmaps-17f58841-2a92-4faf-9120-ec5a7d65d0f6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:56:39.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2119" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":268,"skipped":4251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:56:39.269: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:56:56.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5495" for this suite.

• [SLOW TEST:16.789 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":269,"skipped":4274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:56:56.058: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-7442
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7442
STEP: Deleting pre-stop pod
Dec 14 13:57:09.389: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:57:09.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7442" for this suite.

• [SLOW TEST:13.363 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":270,"skipped":4297,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:57:09.421: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Dec 14 13:57:09.717: INFO: Found Service test-service-nqgws in namespace services-2265 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec 14 13:57:09.717: INFO: Service test-service-nqgws created
STEP: Getting /status
Dec 14 13:57:09.722: INFO: Service test-service-nqgws has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Dec 14 13:57:09.735: INFO: observed Service test-service-nqgws in namespace services-2265 with annotations: map[] & LoadBalancer: {[]}
Dec 14 13:57:09.735: INFO: Found Service test-service-nqgws in namespace services-2265 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec 14 13:57:09.735: INFO: Service test-service-nqgws has service status patched
STEP: updating the ServiceStatus
Dec 14 13:57:09.752: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Dec 14 13:57:09.758: INFO: Observed Service test-service-nqgws in namespace services-2265 with annotations: map[] & Conditions: {[]}
Dec 14 13:57:09.758: INFO: Observed event: &Service{ObjectMeta:{test-service-nqgws  services-2265  60cf98ff-f6e0-4603-b240-be3ce28a01a8 119522 0 2021-12-14 13:57:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-12-14 13:57:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.78.153,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.78.153],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec 14 13:57:09.758: INFO: Found Service test-service-nqgws in namespace services-2265 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 13:57:09.760: INFO: Service test-service-nqgws has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Dec 14 13:57:09.785: INFO: observed Service test-service-nqgws in namespace services-2265 with labels: map[test-service-static:true]
Dec 14 13:57:09.786: INFO: observed Service test-service-nqgws in namespace services-2265 with labels: map[test-service-static:true]
Dec 14 13:57:09.786: INFO: observed Service test-service-nqgws in namespace services-2265 with labels: map[test-service-static:true]
Dec 14 13:57:09.798: INFO: Found Service test-service-nqgws in namespace services-2265 with labels: map[test-service:patched test-service-static:true]
Dec 14 13:57:09.798: INFO: Service test-service-nqgws patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Dec 14 13:57:09.826: INFO: Observed event: ADDED
Dec 14 13:57:09.826: INFO: Observed event: MODIFIED
Dec 14 13:57:09.826: INFO: Observed event: MODIFIED
Dec 14 13:57:09.826: INFO: Observed event: MODIFIED
Dec 14 13:57:09.827: INFO: Found Service test-service-nqgws in namespace services-2265 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec 14 13:57:09.827: INFO: Service test-service-nqgws deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:57:09.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2265" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":271,"skipped":4304,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:57:09.877: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2762
STEP: creating service affinity-nodeport-transition in namespace services-2762
STEP: creating replication controller affinity-nodeport-transition in namespace services-2762
I1214 13:57:10.199429      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-2762, replica count: 3
I1214 13:57:13.250591      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:57:16.250862      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 13:57:16.276: INFO: Creating new exec pod
Dec 14 13:57:21.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2762 exec execpod-affinity67gc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Dec 14 13:57:21.919: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 14 13:57:21.919: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:57:21.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2762 exec execpod-affinity67gc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.72.3 80'
Dec 14 13:57:22.132: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.72.3 80\nConnection to 172.30.72.3 80 port [tcp/http] succeeded!\n"
Dec 14 13:57:22.132: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:57:22.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2762 exec execpod-affinity67gc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.169.135 30463'
Dec 14 13:57:22.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.169.135 30463\nConnection to 10.0.169.135 30463 port [tcp/*] succeeded!\n"
Dec 14 13:57:22.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:57:22.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2762 exec execpod-affinity67gc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.150.199 30463'
Dec 14 13:57:22.575: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.150.199 30463\nConnection to 10.0.150.199 30463 port [tcp/*] succeeded!\n"
Dec 14 13:57:22.575: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 13:57:22.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2762 exec execpod-affinity67gc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.177:30463/ ; done'
Dec 14 13:57:23.135: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n"
Dec 14 13:57:23.135: INFO: stdout: "\naffinity-nodeport-transition-k9qwz\naffinity-nodeport-transition-k9qwz\naffinity-nodeport-transition-rvkfs\naffinity-nodeport-transition-rvkfs\naffinity-nodeport-transition-k9qwz\naffinity-nodeport-transition-rvkfs\naffinity-nodeport-transition-k9qwz\naffinity-nodeport-transition-k9qwz\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-rvkfs\naffinity-nodeport-transition-rvkfs\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-rvkfs\naffinity-nodeport-transition-rvkfs\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-k9qwz"
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-k9qwz
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-k9qwz
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-rvkfs
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-rvkfs
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-k9qwz
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-rvkfs
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-k9qwz
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-k9qwz
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-rvkfs
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-rvkfs
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-rvkfs
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-rvkfs
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.135: INFO: Received response from host: affinity-nodeport-transition-k9qwz
Dec 14 13:57:23.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2762 exec execpod-affinity67gc4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.177:30463/ ; done'
Dec 14 13:57:23.614: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30463/\n"
Dec 14 13:57:23.614: INFO: stdout: "\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5\naffinity-nodeport-transition-5w2g5"
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Received response from host: affinity-nodeport-transition-5w2g5
Dec 14 13:57:23.614: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2762, will wait for the garbage collector to delete the pods
Dec 14 13:57:23.702: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.754456ms
Dec 14 13:57:23.802: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.230332ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:57:38.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2762" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:28.797 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":272,"skipped":4317,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:57:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 14 13:57:38.942: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 13:58:39.353: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Dec 14 13:58:39.412: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 14 13:58:39.447: INFO: Created pod: pod1-sched-preemption-medium-priority
Dec 14 13:58:39.493: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:59:05.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8346" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:86.995 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":273,"skipped":4335,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:59:05.670: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:59:05.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2604" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":274,"skipped":4347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:59:05.944: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-916/secret-test-e9d19886-d3e9-4918-a0d1-c880206459e5
STEP: Creating a pod to test consume secrets
Dec 14 13:59:06.593: INFO: Waiting up to 5m0s for pod "pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326" in namespace "secrets-916" to be "Succeeded or Failed"
Dec 14 13:59:06.597: INFO: Pod "pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31446ms
Dec 14 13:59:08.609: INFO: Pod "pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015752139s
Dec 14 13:59:10.616: INFO: Pod "pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02284849s
STEP: Saw pod success
Dec 14 13:59:10.616: INFO: Pod "pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326" satisfied condition "Succeeded or Failed"
Dec 14 13:59:10.619: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326 container env-test: <nil>
STEP: delete the pod
Dec 14 13:59:10.666: INFO: Waiting for pod pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326 to disappear
Dec 14 13:59:10.677: INFO: Pod pod-configmaps-8517294d-53af-42b9-a28c-d9b2255ba326 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:59:10.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-916" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":275,"skipped":4399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:59:10.697: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Dec 14 13:59:11.034: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Dec 14 13:59:13.054: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Dec 14 13:59:15.079: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:59:17.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3234" for this suite.

• [SLOW TEST:6.410 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":276,"skipped":4449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:59:17.108: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8733
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 13:59:17.294: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 13:59:17.452: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:59:19.469: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:59:21.462: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:23.461: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:25.464: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:27.462: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:29.543: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:31.463: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:33.461: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:35.462: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 13:59:37.462: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 13:59:37.470: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 13:59:37.478: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 13:59:39.485: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 13:59:43.552: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 13:59:43.552: INFO: Going to poll 10.129.2.81 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Dec 14 13:59:43.555: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.81:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:59:43.555: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:59:44.435: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 13:59:44.435: INFO: Going to poll 10.131.0.147 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Dec 14 13:59:44.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.0.147:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:59:44.441: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:59:45.337: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 14 13:59:45.337: INFO: Going to poll 10.128.2.253 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Dec 14 13:59:45.346: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.253:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 13:59:45.346: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 13:59:45.901: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 13:59:45.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8733" for this suite.

• [SLOW TEST:28.815 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":277,"skipped":4474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 13:59:45.924: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6611
Dec 14 13:59:46.155: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:59:48.165: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 14 13:59:50.165: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 14 13:59:50.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 13:59:50.704: INFO: rc: 7
Dec 14 13:59:50.719: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 13:59:50.725: INFO: Pod kube-proxy-mode-detector no longer exists
Dec 14 13:59:50.725: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-6611
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6611
I1214 13:59:50.752083      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6611, replica count: 3
I1214 13:59:53.806640      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:59:56.806909      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 13:59:59.807214      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 14:00:02.807501      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 14:00:05.808058      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 14:00:08.808843      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 14:00:08.830: INFO: Creating new exec pod
Dec 14 14:00:13.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec execpod-affinityqvpxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Dec 14 14:00:14.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 14:00:14.289: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:00:14.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec execpod-affinityqvpxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.34.210 80'
Dec 14 14:00:14.537: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.34.210 80\nConnection to 172.30.34.210 80 port [tcp/http] succeeded!\n"
Dec 14 14:00:14.537: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:00:14.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec execpod-affinityqvpxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.132.177 30931'
Dec 14 14:00:14.825: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.132.177 30931\nConnection to 10.0.132.177 30931 port [tcp/*] succeeded!\n"
Dec 14 14:00:14.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:00:14.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec execpod-affinityqvpxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.150.199 30931'
Dec 14 14:00:15.087: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.150.199 30931\nConnection to 10.0.150.199 30931 port [tcp/*] succeeded!\n"
Dec 14 14:00:15.087: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:00:15.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec execpod-affinityqvpxp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.177:30931/ ; done'
Dec 14 14:00:15.395: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n"
Dec 14 14:00:15.395: INFO: stdout: "\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62\naffinity-nodeport-timeout-6fx62"
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.395: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.396: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.396: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.396: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.396: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.396: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.396: INFO: Received response from host: affinity-nodeport-timeout-6fx62
Dec 14 14:00:15.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec execpod-affinityqvpxp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.132.177:30931/'
Dec 14 14:00:15.667: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n"
Dec 14 14:00:15.667: INFO: stdout: "affinity-nodeport-timeout-6fx62"
Dec 14 14:00:35.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-6611 exec execpod-affinityqvpxp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.132.177:30931/'
Dec 14 14:00:35.929: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.132.177:30931/\n"
Dec 14 14:00:35.929: INFO: stdout: "affinity-nodeport-timeout-zbz2q"
Dec 14 14:00:35.929: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6611, will wait for the garbage collector to delete the pods
Dec 14 14:00:36.021: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.699055ms
Dec 14 14:00:36.123: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 102.120904ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:00:48.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6611" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:62.684 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":278,"skipped":4517,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:00:48.608: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-4699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[]
Dec 14 14:00:48.994: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 14:00:50.006: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4699
Dec 14 14:00:50.040: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:00:52.050: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:00:54.048: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod1:[80]]
Dec 14 14:00:54.070: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-4699
Dec 14 14:00:54.093: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:00:56.108: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:00:58.119: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:01:00.106: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 14 14:01:00.128: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-4699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[pod2:[80]]
Dec 14 14:01:01.173: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-4699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4699 to expose endpoints map[]
Dec 14 14:01:01.216: INFO: successfully validated that service endpoint-test2 in namespace services-4699 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:01:01.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4699" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:12.661 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":279,"skipped":4524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:01:01.271: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 14:01:01.639: INFO: created pod
Dec 14 14:01:01.639: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4508" to be "Succeeded or Failed"
Dec 14 14:01:01.670: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 31.273491ms
Dec 14 14:01:03.682: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042388542s
Dec 14 14:01:05.689: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049468769s
STEP: Saw pod success
Dec 14 14:01:05.689: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec 14 14:01:35.689: INFO: polling logs
Dec 14 14:01:35.704: INFO: Pod logs: 
2021/12/14 14:01:03 OK: Got token
2021/12/14 14:01:03 validating with in-cluster discovery
2021/12/14 14:01:03 OK: got issuer https://kubernetes.default.svc
2021/12/14 14:01:03 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4508:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1639491061, NotBefore:1639490461, IssuedAt:1639490461, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4508", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"90ff425a-517b-4d12-8916-1ca75131da0a"}}}
2021/12/14 14:01:03 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
2021/12/14 14:01:04 OK: Validated signature on JWT
2021/12/14 14:01:04 OK: Got valid claims from token!
2021/12/14 14:01:04 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4508:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1639491061, NotBefore:1639490461, IssuedAt:1639490461, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4508", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"90ff425a-517b-4d12-8916-1ca75131da0a"}}}

Dec 14 14:01:35.704: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:01:35.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4508" for this suite.

• [SLOW TEST:34.465 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":280,"skipped":4594,"failed":0}
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:01:35.736: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec 14 14:01:39.988: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7339 PodName:var-expansion-93445a06-e0df-4967-9108-732868197202 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:01:39.988: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: test for file in mounted path
Dec 14 14:01:40.089: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7339 PodName:var-expansion-93445a06-e0df-4967-9108-732868197202 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:01:40.089: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: updating the annotation value
Dec 14 14:01:40.733: INFO: Successfully updated pod "var-expansion-93445a06-e0df-4967-9108-732868197202"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec 14 14:01:40.741: INFO: Deleting pod "var-expansion-93445a06-e0df-4967-9108-732868197202" in namespace "var-expansion-7339"
Dec 14 14:01:40.753: INFO: Wait up to 5m0s for pod "var-expansion-93445a06-e0df-4967-9108-732868197202" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:02:18.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7339" for this suite.

• [SLOW TEST:43.042 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":281,"skipped":4594,"failed":0}
S
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:02:18.778: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 14:02:19.623: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 14 14:02:19.624: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 14 14:02:19.624: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.624: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 14 14:02:19.624: INFO: Checking APIGroup: apps
Dec 14 14:02:19.626: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 14 14:02:19.626: INFO: Versions found [{apps/v1 v1}]
Dec 14 14:02:19.626: INFO: apps/v1 matches apps/v1
Dec 14 14:02:19.626: INFO: Checking APIGroup: events.k8s.io
Dec 14 14:02:19.627: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 14 14:02:19.627: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.627: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 14 14:02:19.627: INFO: Checking APIGroup: authentication.k8s.io
Dec 14 14:02:19.629: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 14 14:02:19.629: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.629: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 14 14:02:19.629: INFO: Checking APIGroup: authorization.k8s.io
Dec 14 14:02:19.630: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 14 14:02:19.630: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.630: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 14 14:02:19.630: INFO: Checking APIGroup: autoscaling
Dec 14 14:02:19.631: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Dec 14 14:02:19.631: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec 14 14:02:19.631: INFO: autoscaling/v1 matches autoscaling/v1
Dec 14 14:02:19.631: INFO: Checking APIGroup: batch
Dec 14 14:02:19.633: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 14 14:02:19.633: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Dec 14 14:02:19.633: INFO: batch/v1 matches batch/v1
Dec 14 14:02:19.633: INFO: Checking APIGroup: certificates.k8s.io
Dec 14 14:02:19.634: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 14 14:02:19.634: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.634: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 14 14:02:19.634: INFO: Checking APIGroup: networking.k8s.io
Dec 14 14:02:19.635: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 14 14:02:19.635: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.635: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 14 14:02:19.635: INFO: Checking APIGroup: extensions
Dec 14 14:02:19.637: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Dec 14 14:02:19.637: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Dec 14 14:02:19.637: INFO: extensions/v1beta1 matches extensions/v1beta1
Dec 14 14:02:19.637: INFO: Checking APIGroup: policy
Dec 14 14:02:19.638: INFO: PreferredVersion.GroupVersion: policy/v1
Dec 14 14:02:19.638: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Dec 14 14:02:19.638: INFO: policy/v1 matches policy/v1
Dec 14 14:02:19.638: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 14 14:02:19.639: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 14 14:02:19.639: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.639: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 14 14:02:19.639: INFO: Checking APIGroup: storage.k8s.io
Dec 14 14:02:19.641: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 14 14:02:19.641: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.641: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 14 14:02:19.641: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 14 14:02:19.642: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 14 14:02:19.642: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.642: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 14 14:02:19.642: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 14 14:02:19.643: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 14 14:02:19.643: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.643: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 14 14:02:19.643: INFO: Checking APIGroup: scheduling.k8s.io
Dec 14 14:02:19.645: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 14 14:02:19.645: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.645: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 14 14:02:19.645: INFO: Checking APIGroup: coordination.k8s.io
Dec 14 14:02:19.646: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 14 14:02:19.646: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.646: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 14 14:02:19.646: INFO: Checking APIGroup: node.k8s.io
Dec 14 14:02:19.647: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 14 14:02:19.647: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.647: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 14 14:02:19.647: INFO: Checking APIGroup: discovery.k8s.io
Dec 14 14:02:19.649: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec 14 14:02:19.649: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.649: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec 14 14:02:19.649: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 14 14:02:19.650: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Dec 14 14:02:19.650: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.650: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Dec 14 14:02:19.650: INFO: Checking APIGroup: apps.openshift.io
Dec 14 14:02:19.651: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Dec 14 14:02:19.651: INFO: Versions found [{apps.openshift.io/v1 v1}]
Dec 14 14:02:19.651: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Dec 14 14:02:19.651: INFO: Checking APIGroup: authorization.openshift.io
Dec 14 14:02:19.653: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Dec 14 14:02:19.653: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Dec 14 14:02:19.653: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Dec 14 14:02:19.653: INFO: Checking APIGroup: build.openshift.io
Dec 14 14:02:19.654: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Dec 14 14:02:19.654: INFO: Versions found [{build.openshift.io/v1 v1}]
Dec 14 14:02:19.654: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Dec 14 14:02:19.654: INFO: Checking APIGroup: image.openshift.io
Dec 14 14:02:19.655: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Dec 14 14:02:19.655: INFO: Versions found [{image.openshift.io/v1 v1}]
Dec 14 14:02:19.655: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Dec 14 14:02:19.655: INFO: Checking APIGroup: oauth.openshift.io
Dec 14 14:02:19.657: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Dec 14 14:02:19.657: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Dec 14 14:02:19.657: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Dec 14 14:02:19.657: INFO: Checking APIGroup: project.openshift.io
Dec 14 14:02:19.658: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Dec 14 14:02:19.658: INFO: Versions found [{project.openshift.io/v1 v1}]
Dec 14 14:02:19.658: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Dec 14 14:02:19.658: INFO: Checking APIGroup: quota.openshift.io
Dec 14 14:02:19.659: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Dec 14 14:02:19.659: INFO: Versions found [{quota.openshift.io/v1 v1}]
Dec 14 14:02:19.659: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Dec 14 14:02:19.659: INFO: Checking APIGroup: route.openshift.io
Dec 14 14:02:19.661: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Dec 14 14:02:19.661: INFO: Versions found [{route.openshift.io/v1 v1}]
Dec 14 14:02:19.661: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Dec 14 14:02:19.661: INFO: Checking APIGroup: security.openshift.io
Dec 14 14:02:19.662: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Dec 14 14:02:19.662: INFO: Versions found [{security.openshift.io/v1 v1}]
Dec 14 14:02:19.662: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Dec 14 14:02:19.662: INFO: Checking APIGroup: template.openshift.io
Dec 14 14:02:19.663: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Dec 14 14:02:19.663: INFO: Versions found [{template.openshift.io/v1 v1}]
Dec 14 14:02:19.663: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Dec 14 14:02:19.663: INFO: Checking APIGroup: user.openshift.io
Dec 14 14:02:19.664: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Dec 14 14:02:19.665: INFO: Versions found [{user.openshift.io/v1 v1}]
Dec 14 14:02:19.665: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Dec 14 14:02:19.665: INFO: Checking APIGroup: packages.operators.coreos.com
Dec 14 14:02:19.666: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Dec 14 14:02:19.666: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Dec 14 14:02:19.666: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Dec 14 14:02:19.666: INFO: Checking APIGroup: config.openshift.io
Dec 14 14:02:19.667: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Dec 14 14:02:19.667: INFO: Versions found [{config.openshift.io/v1 v1}]
Dec 14 14:02:19.667: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Dec 14 14:02:19.667: INFO: Checking APIGroup: operator.openshift.io
Dec 14 14:02:19.668: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Dec 14 14:02:19.668: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Dec 14 14:02:19.668: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Dec 14 14:02:19.668: INFO: Checking APIGroup: apiserver.openshift.io
Dec 14 14:02:19.670: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Dec 14 14:02:19.670: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Dec 14 14:02:19.670: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Dec 14 14:02:19.670: INFO: Checking APIGroup: autoscaling.openshift.io
Dec 14 14:02:19.671: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
Dec 14 14:02:19.671: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
Dec 14 14:02:19.671: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
Dec 14 14:02:19.671: INFO: Checking APIGroup: cloudcredential.openshift.io
Dec 14 14:02:19.672: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Dec 14 14:02:19.672: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Dec 14 14:02:19.672: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Dec 14 14:02:19.672: INFO: Checking APIGroup: console.openshift.io
Dec 14 14:02:19.673: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Dec 14 14:02:19.673: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Dec 14 14:02:19.673: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Dec 14 14:02:19.673: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Dec 14 14:02:19.675: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Dec 14 14:02:19.675: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Dec 14 14:02:19.675: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Dec 14 14:02:19.675: INFO: Checking APIGroup: ingress.operator.openshift.io
Dec 14 14:02:19.676: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Dec 14 14:02:19.676: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Dec 14 14:02:19.676: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Dec 14 14:02:19.676: INFO: Checking APIGroup: k8s.cni.cncf.io
Dec 14 14:02:19.677: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Dec 14 14:02:19.677: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Dec 14 14:02:19.677: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Dec 14 14:02:19.677: INFO: Checking APIGroup: machineconfiguration.openshift.io
Dec 14 14:02:19.678: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Dec 14 14:02:19.678: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Dec 14 14:02:19.678: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Dec 14 14:02:19.678: INFO: Checking APIGroup: monitoring.coreos.com
Dec 14 14:02:19.680: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Dec 14 14:02:19.680: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Dec 14 14:02:19.680: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Dec 14 14:02:19.680: INFO: Checking APIGroup: network.openshift.io
Dec 14 14:02:19.681: INFO: PreferredVersion.GroupVersion: network.openshift.io/v1
Dec 14 14:02:19.681: INFO: Versions found [{network.openshift.io/v1 v1}]
Dec 14 14:02:19.681: INFO: network.openshift.io/v1 matches network.openshift.io/v1
Dec 14 14:02:19.681: INFO: Checking APIGroup: network.operator.openshift.io
Dec 14 14:02:19.682: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Dec 14 14:02:19.682: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Dec 14 14:02:19.682: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Dec 14 14:02:19.682: INFO: Checking APIGroup: operators.coreos.com
Dec 14 14:02:19.684: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Dec 14 14:02:19.684: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Dec 14 14:02:19.684: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Dec 14 14:02:19.684: INFO: Checking APIGroup: samples.operator.openshift.io
Dec 14 14:02:19.685: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Dec 14 14:02:19.685: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Dec 14 14:02:19.685: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Dec 14 14:02:19.685: INFO: Checking APIGroup: security.internal.openshift.io
Dec 14 14:02:19.686: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Dec 14 14:02:19.686: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Dec 14 14:02:19.686: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Dec 14 14:02:19.686: INFO: Checking APIGroup: snapshot.storage.k8s.io
Dec 14 14:02:19.688: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Dec 14 14:02:19.688: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:19.688: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Dec 14 14:02:19.688: INFO: Checking APIGroup: tuned.openshift.io
Dec 14 14:02:19.689: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Dec 14 14:02:19.689: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Dec 14 14:02:19.689: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Dec 14 14:02:19.689: INFO: Checking APIGroup: controlplane.operator.openshift.io
Dec 14 14:02:19.721: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Dec 14 14:02:19.721: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Dec 14 14:02:19.721: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Dec 14 14:02:19.721: INFO: Checking APIGroup: metal3.io
Dec 14 14:02:19.771: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Dec 14 14:02:19.771: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Dec 14 14:02:19.771: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Dec 14 14:02:19.771: INFO: Checking APIGroup: migration.k8s.io
Dec 14 14:02:19.821: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Dec 14 14:02:19.821: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Dec 14 14:02:19.821: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Dec 14 14:02:19.821: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Dec 14 14:02:19.871: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Dec 14 14:02:19.871: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Dec 14 14:02:19.871: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Dec 14 14:02:19.871: INFO: Checking APIGroup: helm.openshift.io
Dec 14 14:02:19.921: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Dec 14 14:02:19.921: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Dec 14 14:02:19.921: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Dec 14 14:02:19.921: INFO: Checking APIGroup: machine.openshift.io
Dec 14 14:02:19.971: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1beta1
Dec 14 14:02:19.971: INFO: Versions found [{machine.openshift.io/v1beta1 v1beta1}]
Dec 14 14:02:19.971: INFO: machine.openshift.io/v1beta1 matches machine.openshift.io/v1beta1
Dec 14 14:02:19.971: INFO: Checking APIGroup: metrics.k8s.io
Dec 14 14:02:20.021: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Dec 14 14:02:20.021: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Dec 14 14:02:20.021: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:02:20.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9897" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":282,"skipped":4595,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:02:20.130: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Dec 14 14:02:20.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 create -f -'
Dec 14 14:02:20.908: INFO: stderr: ""
Dec 14 14:02:20.908: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 14:02:20.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 14:02:21.014: INFO: stderr: ""
Dec 14 14:02:21.014: INFO: stdout: "update-demo-nautilus-8zxs9 update-demo-nautilus-gk9rk "
Dec 14 14:02:21.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-8zxs9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:21.145: INFO: stderr: ""
Dec 14 14:02:21.145: INFO: stdout: ""
Dec 14 14:02:21.145: INFO: update-demo-nautilus-8zxs9 is created but not running
Dec 14 14:02:26.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 14:02:26.251: INFO: stderr: ""
Dec 14 14:02:26.251: INFO: stdout: "update-demo-nautilus-8zxs9 update-demo-nautilus-gk9rk "
Dec 14 14:02:26.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-8zxs9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:26.370: INFO: stderr: ""
Dec 14 14:02:26.371: INFO: stdout: ""
Dec 14 14:02:26.371: INFO: update-demo-nautilus-8zxs9 is created but not running
Dec 14 14:02:31.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 14:02:31.500: INFO: stderr: ""
Dec 14 14:02:31.500: INFO: stdout: "update-demo-nautilus-8zxs9 update-demo-nautilus-gk9rk "
Dec 14 14:02:31.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-8zxs9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:31.614: INFO: stderr: ""
Dec 14 14:02:31.614: INFO: stdout: "true"
Dec 14 14:02:31.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-8zxs9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 14:02:31.752: INFO: stderr: ""
Dec 14 14:02:31.752: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 14:02:31.752: INFO: validating pod update-demo-nautilus-8zxs9
Dec 14 14:02:31.762: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 14:02:31.762: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 14:02:31.762: INFO: update-demo-nautilus-8zxs9 is verified up and running
Dec 14 14:02:31.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:31.878: INFO: stderr: ""
Dec 14 14:02:31.878: INFO: stdout: "true"
Dec 14 14:02:31.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 14:02:32.067: INFO: stderr: ""
Dec 14 14:02:32.067: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 14:02:32.067: INFO: validating pod update-demo-nautilus-gk9rk
Dec 14 14:02:32.075: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 14:02:32.075: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 14:02:32.075: INFO: update-demo-nautilus-gk9rk is verified up and running
STEP: scaling down the replication controller
Dec 14 14:02:32.080: INFO: scanned /root for discovery docs: <nil>
Dec 14 14:02:32.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 14 14:02:33.233: INFO: stderr: ""
Dec 14 14:02:33.233: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 14:02:33.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 14:02:33.477: INFO: stderr: ""
Dec 14 14:02:33.477: INFO: stdout: "update-demo-nautilus-8zxs9 update-demo-nautilus-gk9rk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 14 14:02:38.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 14:02:38.777: INFO: stderr: ""
Dec 14 14:02:38.777: INFO: stdout: "update-demo-nautilus-gk9rk "
Dec 14 14:02:38.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:38.907: INFO: stderr: ""
Dec 14 14:02:38.907: INFO: stdout: "true"
Dec 14 14:02:38.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 14:02:39.034: INFO: stderr: ""
Dec 14 14:02:39.034: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 14:02:39.034: INFO: validating pod update-demo-nautilus-gk9rk
Dec 14 14:02:39.042: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 14:02:39.042: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 14:02:39.042: INFO: update-demo-nautilus-gk9rk is verified up and running
STEP: scaling up the replication controller
Dec 14 14:02:39.047: INFO: scanned /root for discovery docs: <nil>
Dec 14 14:02:39.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 14 14:02:40.194: INFO: stderr: ""
Dec 14 14:02:40.194: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 14:02:40.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 14:02:40.302: INFO: stderr: ""
Dec 14 14:02:40.302: INFO: stdout: "update-demo-nautilus-gk9rk update-demo-nautilus-mbn8b "
Dec 14 14:02:40.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:40.404: INFO: stderr: ""
Dec 14 14:02:40.404: INFO: stdout: "true"
Dec 14 14:02:40.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 14:02:40.519: INFO: stderr: ""
Dec 14 14:02:40.520: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 14:02:40.520: INFO: validating pod update-demo-nautilus-gk9rk
Dec 14 14:02:40.527: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 14:02:40.528: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 14:02:40.528: INFO: update-demo-nautilus-gk9rk is verified up and running
Dec 14 14:02:40.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-mbn8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:40.643: INFO: stderr: ""
Dec 14 14:02:40.643: INFO: stdout: ""
Dec 14 14:02:40.643: INFO: update-demo-nautilus-mbn8b is created but not running
Dec 14 14:02:45.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 14:02:45.764: INFO: stderr: ""
Dec 14 14:02:45.764: INFO: stdout: "update-demo-nautilus-gk9rk update-demo-nautilus-mbn8b "
Dec 14 14:02:45.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:45.889: INFO: stderr: ""
Dec 14 14:02:45.889: INFO: stdout: "true"
Dec 14 14:02:45.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-gk9rk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 14:02:46.001: INFO: stderr: ""
Dec 14 14:02:46.001: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 14:02:46.001: INFO: validating pod update-demo-nautilus-gk9rk
Dec 14 14:02:46.007: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 14:02:46.007: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 14:02:46.007: INFO: update-demo-nautilus-gk9rk is verified up and running
Dec 14 14:02:46.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-mbn8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 14:02:46.107: INFO: stderr: ""
Dec 14 14:02:46.107: INFO: stdout: "true"
Dec 14 14:02:46.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods update-demo-nautilus-mbn8b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 14:02:46.220: INFO: stderr: ""
Dec 14 14:02:46.220: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Dec 14 14:02:46.220: INFO: validating pod update-demo-nautilus-mbn8b
Dec 14 14:02:46.228: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 14:02:46.229: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 14:02:46.229: INFO: update-demo-nautilus-mbn8b is verified up and running
STEP: using delete to clean up resources
Dec 14 14:02:46.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 delete --grace-period=0 --force -f -'
Dec 14 14:02:46.327: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 14:02:46.327: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 14:02:46.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get rc,svc -l name=update-demo --no-headers'
Dec 14 14:02:46.442: INFO: stderr: "No resources found in kubectl-5352 namespace.\n"
Dec 14 14:02:46.442: INFO: stdout: ""
Dec 14 14:02:46.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5352 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 14:02:46.577: INFO: stderr: ""
Dec 14 14:02:46.577: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:02:46.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5352" for this suite.

• [SLOW TEST:26.477 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":283,"skipped":4601,"failed":0}
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:02:46.607: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:02:46.836: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-7286
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:02:49.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5876" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:02:49.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7286" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":284,"skipped":4602,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:02:49.884: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec 14 14:03:30.130: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 14:03:30.130: INFO: Deleting pod "simpletest.rc-57988" in namespace "gc-4097"
W1214 14:03:30.129946      20 metrics_grabber.go:102] Can't find any pods in namespace kube-system to grab metrics from
W1214 14:03:30.129974      20 metrics_grabber.go:138] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 14:03:30.145: INFO: Deleting pod "simpletest.rc-5jdrg" in namespace "gc-4097"
Dec 14 14:03:30.162: INFO: Deleting pod "simpletest.rc-6jk96" in namespace "gc-4097"
Dec 14 14:03:30.178: INFO: Deleting pod "simpletest.rc-6nk2s" in namespace "gc-4097"
Dec 14 14:03:30.211: INFO: Deleting pod "simpletest.rc-f7prq" in namespace "gc-4097"
Dec 14 14:03:30.228: INFO: Deleting pod "simpletest.rc-kbvg8" in namespace "gc-4097"
Dec 14 14:03:30.251: INFO: Deleting pod "simpletest.rc-l9jgw" in namespace "gc-4097"
Dec 14 14:03:30.376: INFO: Deleting pod "simpletest.rc-sz2kn" in namespace "gc-4097"
Dec 14 14:03:30.474: INFO: Deleting pod "simpletest.rc-wgznz" in namespace "gc-4097"
Dec 14 14:03:30.493: INFO: Deleting pod "simpletest.rc-z7kbp" in namespace "gc-4097"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:03:30.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4097" for this suite.

• [SLOW TEST:40.653 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":285,"skipped":4602,"failed":0}
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:03:30.537: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec 14 14:03:30.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-8577 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Dec 14 14:03:31.275: INFO: stderr: ""
Dec 14 14:03:31.275: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec 14 14:03:31.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-8577 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Dec 14 14:03:32.016: INFO: stderr: ""
Dec 14 14:03:32.016: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec 14 14:03:32.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-8577 delete pods e2e-test-httpd-pod'
Dec 14 14:03:33.799: INFO: stderr: ""
Dec 14 14:03:33.799: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:03:33.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8577" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":286,"skipped":4602,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:03:33.816: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-a89887e0-07ca-46fd-b918-96127d47414d
STEP: Creating a pod to test consume secrets
Dec 14 14:03:34.051: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199" in namespace "projected-3" to be "Succeeded or Failed"
Dec 14 14:03:34.058: INFO: Pod "pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199": Phase="Pending", Reason="", readiness=false. Elapsed: 6.911497ms
Dec 14 14:03:36.068: INFO: Pod "pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016552364s
Dec 14 14:03:38.077: INFO: Pod "pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02593153s
STEP: Saw pod success
Dec 14 14:03:38.077: INFO: Pod "pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199" satisfied condition "Succeeded or Failed"
Dec 14 14:03:38.082: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 14:03:38.116: INFO: Waiting for pod pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199 to disappear
Dec 14 14:03:38.121: INFO: Pod pod-projected-secrets-62813e32-7ca7-4b01-ac02-ab424cb7c199 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:03:38.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":287,"skipped":4615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:03:38.139: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:03:38.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-941" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":288,"skipped":4652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:03:38.706: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-4523
STEP: creating replication controller nodeport-test in namespace services-4523
I1214 14:03:38.988413      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4523, replica count: 2
I1214 14:03:42.039970      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 14:03:45.040887      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 14:03:45.040: INFO: Creating new exec pod
Dec 14 14:03:50.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-4523 exec execpodm4r5j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec 14 14:03:50.306: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 14 14:03:50.306: INFO: stdout: "nodeport-test-cslsx"
Dec 14 14:03:50.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-4523 exec execpodm4r5j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.74.218 80'
Dec 14 14:03:50.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.74.218 80\nConnection to 172.30.74.218 80 port [tcp/http] succeeded!\n"
Dec 14 14:03:50.511: INFO: stdout: "nodeport-test-28vfk"
Dec 14 14:03:50.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-4523 exec execpodm4r5j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.132.177 31244'
Dec 14 14:03:50.725: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.132.177 31244\nConnection to 10.0.132.177 31244 port [tcp/*] succeeded!\n"
Dec 14 14:03:50.725: INFO: stdout: "nodeport-test-28vfk"
Dec 14 14:03:50.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-4523 exec execpodm4r5j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.169.135 31244'
Dec 14 14:03:50.961: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.169.135 31244\nConnection to 10.0.169.135 31244 port [tcp/*] succeeded!\n"
Dec 14 14:03:50.961: INFO: stdout: "nodeport-test-cslsx"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:03:50.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4523" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:12.273 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":289,"skipped":4684,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:03:50.979: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Dec 14 14:05:51.750: INFO: Successfully updated pod "var-expansion-f8e97d61-c0fa-48a9-aa14-8ac987bfb3d6"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec 14 14:05:53.764: INFO: Deleting pod "var-expansion-f8e97d61-c0fa-48a9-aa14-8ac987bfb3d6" in namespace "var-expansion-2541"
Dec 14 14:05:53.776: INFO: Wait up to 5m0s for pod "var-expansion-f8e97d61-c0fa-48a9-aa14-8ac987bfb3d6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:06:29.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2541" for this suite.

• [SLOW TEST:158.831 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":290,"skipped":4704,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:06:29.810: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 14:06:29.985: INFO: Creating simple deployment test-new-deployment
Dec 14 14:06:30.020: INFO: deployment "test-new-deployment" doesn't have the required revision set
Dec 14 14:06:32.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087590, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087590, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087590, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087590, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Dec 14 14:06:34.076: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6290  4c4a1ba3-ae22-41ad-abe4-1632ed181083 127039 3 2021-12-14 14:06:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-12-14 14:06:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-12-14 14:06:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f0448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-12-14 14:06:33 +0000 UTC,LastTransitionTime:2021-12-14 14:06:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-12-14 14:06:33 +0000 UTC,LastTransitionTime:2021-12-14 14:06:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 14:06:34.080: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-6290  dc91daa2-073a-4a00-8b22-0013e5471ed7 127040 2 2021-12-14 14:06:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4c4a1ba3-ae22-41ad-abe4-1632ed181083 0xc0045f0857 0xc0045f0858}] []  [{kube-controller-manager Update apps/v1 2021-12-14 14:06:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c4a1ba3-ae22-41ad-abe4-1632ed181083\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f08c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 14:06:34.083: INFO: Pod "test-new-deployment-847dcfb7fb-p7hgp" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-p7hgp test-new-deployment-847dcfb7fb- deployment-6290  03db1bf6-af20-4e42-88d9-eb1c3ac78475 127030 0 2021-12-14 14:06:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.16"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.3.16"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb dc91daa2-073a-4a00-8b22-0013e5471ed7 0xc0045f0c77 0xc0045f0c78}] []  [{kube-controller-manager Update v1 2021-12-14 14:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc91daa2-073a-4a00-8b22-0013e5471ed7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {multus Update v1 2021-12-14 14:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-12-14 14:06:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.3.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqwms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqwms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-169-135.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c62,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ftj82,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 14:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 14:06:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 14:06:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-12-14 14:06:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.169.135,PodIP:10.128.3.16,StartTime:2021-12-14 14:06:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-12-14 14:06:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://84fd9b8d6d413bd8ea598c58ef0c74d1de38380c1147d5d4f4964e9e162ae25c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.3.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:06:34.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6290" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":291,"skipped":4707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:06:34.104: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6116.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6116.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6116.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6116.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6116.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6116.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 14:06:38.521: INFO: DNS probes using dns-6116/dns-test-11678391-6b5f-4855-bef2-112e7f137485 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:06:38.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6116" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":292,"skipped":4738,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:06:38.613: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Dec 14 14:06:38.802: INFO: Waiting up to 5m0s for pod "var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130" in namespace "var-expansion-1869" to be "Succeeded or Failed"
Dec 14 14:06:38.812: INFO: Pod "var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130": Phase="Pending", Reason="", readiness=false. Elapsed: 10.545362ms
Dec 14 14:06:40.820: INFO: Pod "var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018523017s
Dec 14 14:06:42.836: INFO: Pod "var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034074398s
STEP: Saw pod success
Dec 14 14:06:42.836: INFO: Pod "var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130" satisfied condition "Succeeded or Failed"
Dec 14 14:06:42.841: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130 container dapi-container: <nil>
STEP: delete the pod
Dec 14 14:06:42.872: INFO: Waiting for pod var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130 to disappear
Dec 14 14:06:42.877: INFO: Pod var-expansion-cb8ed09a-8650-4f6a-88b3-5b78e36b0130 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:06:42.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1869" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":293,"skipped":4745,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:06:42.890: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Dec 14 14:06:43.092: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:06:45.100: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:06:47.181: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:06:48.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3565" for this suite.

• [SLOW TEST:5.403 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":294,"skipped":4751,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:06:48.293: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:07:16.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7727" for this suite.

• [SLOW TEST:28.297 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":295,"skipped":4753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:07:16.592: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:07:27.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7469" for this suite.

• [SLOW TEST:11.389 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":296,"skipped":4780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:07:27.985: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Dec 14 14:07:28.313: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 14:07:28.377: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 14:07:28.958: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-177.ec2.internal before test
Dec 14 14:07:29.320: INFO: aws-ebs-csi-driver-node-2t5xl from openshift-cluster-csi-drivers started at 2021-12-14 11:51:55 +0000 UTC (3 container statuses recorded)
Dec 14 14:07:29.368: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 14:07:29.368: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 14:07:29.368: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 14:07:29.368: INFO: tuned-ckp8j from openshift-cluster-node-tuning-operator started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.368: INFO: 	Container tuned ready: true, restart count 0
Dec 14 14:07:29.368: INFO: dns-default-xq8ng from openshift-dns started at 2021-12-14 11:52:45 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.368: INFO: 	Container dns ready: true, restart count 0
Dec 14 14:07:29.368: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.368: INFO: node-resolver-5tjmp from openshift-dns started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.368: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 14:07:29.368: INFO: image-registry-669979f5d8-zxpb9 from openshift-image-registry started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.368: INFO: 	Container registry ready: true, restart count 0
Dec 14 14:07:29.368: INFO: node-ca-gb64k from openshift-image-registry started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 14:07:29.369: INFO: ingress-canary-d5qjx from openshift-ingress-canary started at 2021-12-14 11:52:45 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 14:07:29.369: INFO: router-default-7fbf56f75b-4q5fx from openshift-ingress started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container router ready: true, restart count 0
Dec 14 14:07:29.369: INFO: machine-config-daemon-98m5c from openshift-machine-config-operator started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (5 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: node-exporter-f89s7 from openshift-monitoring started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 14:07:29.369: INFO: prometheus-adapter-74475585f4-cfsjn from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 14:07:29.369: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (7 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 14:07:29.369: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 14:07:29.369: INFO: thanos-querier-74cd47ff6f-jszlz from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (5 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 14:07:29.369: INFO: multus-7jp7s from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 14:07:29.369: INFO: multus-additional-cni-plugins-6kws7 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 14:07:29.369: INFO: network-metrics-daemon-8sqs4 from openshift-multus started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 14:07:29.369: INFO: network-check-target-hpkw5 from openshift-network-diagnostics started at 2021-12-14 11:51:55 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 14:07:29.369: INFO: sdn-7t2qj from openshift-sdn started at 2021-12-14 11:51:55 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container sdn ready: true, restart count 0
Dec 14 14:07:29.369: INFO: sonobuoy from sonobuoy started at 2021-12-14 12:26:18 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 14:07:29.369: INFO: sonobuoy-e2e-job-1d421b73e370443b from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container e2e ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 14:07:29.369: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-hgvlc from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.369: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 14:07:29.369: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-150-199.ec2.internal before test
Dec 14 14:07:29.700: INFO: aws-ebs-csi-driver-node-6djjl from openshift-cluster-csi-drivers started at 2021-12-14 11:50:28 +0000 UTC (3 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 14:07:29.700: INFO: tuned-btrkv from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container tuned ready: true, restart count 0
Dec 14 14:07:29.700: INFO: dns-default-hk5vj from openshift-dns started at 2021-12-14 11:51:18 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container dns ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: node-resolver-zjfkl from openshift-dns started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 14:07:29.700: INFO: image-registry-669979f5d8-66fds from openshift-image-registry started at 2021-12-14 11:51:20 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container registry ready: true, restart count 0
Dec 14 14:07:29.700: INFO: node-ca-x5d9t from openshift-image-registry started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 14:07:29.700: INFO: ingress-canary-vth2d from openshift-ingress-canary started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 14:07:29.700: INFO: router-default-7fbf56f75b-ncphj from openshift-ingress started at 2021-12-14 11:51:19 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container router ready: true, restart count 0
Dec 14 14:07:29.700: INFO: machine-config-daemon-t7xdp from openshift-machine-config-operator started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: certified-operators-nt7k6 from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 14:07:29.700: INFO: community-operators-g4pdk from openshift-marketplace started at 2021-12-14 11:51:21 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 14:07:29.700: INFO: redhat-marketplace-qmxvd from openshift-marketplace started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 14:07:29.700: INFO: redhat-operators-fr7pq from openshift-marketplace started at 2021-12-14 13:53:09 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container registry-server ready: true, restart count 0
Dec 14 14:07:29.700: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-12-14 13:04:18 +0000 UTC (5 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-12-14 11:51:57 +0000 UTC (5 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container alertmanager ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: grafana-d5d47878d-mtw5p from openshift-monitoring started at 2021-12-14 13:04:05 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container grafana ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container grafana-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: kube-state-metrics-78d545c868-nnjp9 from openshift-monitoring started at 2021-12-14 11:51:22 +0000 UTC (3 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec 14 14:07:29.700: INFO: node-exporter-2xvt4 from openshift-monitoring started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 14:07:29.700: INFO: openshift-state-metrics-85bc7c8d6c-jws2g from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Dec 14 14:07:29.700: INFO: prometheus-adapter-74475585f4-skfcj from openshift-monitoring started at 2021-12-14 11:51:18 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container prometheus-adapter ready: true, restart count 0
Dec 14 14:07:29.700: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-12-14 11:52:02 +0000 UTC (7 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container config-reloader ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container prometheus ready: true, restart count 1
Dec 14 14:07:29.700: INFO: 	Container prometheus-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container thanos-sidecar ready: true, restart count 0
Dec 14 14:07:29.700: INFO: telemeter-client-f96747c6d-sj25j from openshift-monitoring started at 2021-12-14 11:51:21 +0000 UTC (3 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container reload ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container telemeter-client ready: true, restart count 0
Dec 14 14:07:29.700: INFO: thanos-querier-74cd47ff6f-wdf79 from openshift-monitoring started at 2021-12-14 11:52:00 +0000 UTC (5 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container prom-label-proxy ready: true, restart count 0
Dec 14 14:07:29.700: INFO: 	Container thanos-query ready: true, restart count 0
Dec 14 14:07:29.700: INFO: multus-6drw6 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.700: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 14:07:29.700: INFO: multus-additional-cni-plugins-p2nx7 from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.701: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 14:07:29.701: INFO: network-metrics-daemon-dvsjd from openshift-multus started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.701: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.701: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 14:07:29.701: INFO: network-check-source-579795dc4b-k549q from openshift-network-diagnostics started at 2021-12-14 11:51:22 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.701: INFO: 	Container check-endpoints ready: true, restart count 0
Dec 14 14:07:29.701: INFO: network-check-target-lrvdq from openshift-network-diagnostics started at 2021-12-14 11:50:28 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.701: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 14:07:29.701: INFO: sdn-mwhj9 from openshift-sdn started at 2021-12-14 11:50:28 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.701: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.701: INFO: 	Container sdn ready: true, restart count 0
Dec 14 14:07:29.701: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-2mpvh from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 14:07:29.701: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 14:07:29.701: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-169-135.ec2.internal before test
Dec 14 14:07:29.737: INFO: aws-ebs-csi-driver-node-mzk2q from openshift-cluster-csi-drivers started at 2021-12-14 11:50:39 +0000 UTC (3 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container csi-driver ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Dec 14 14:07:29.737: INFO: tuned-fqln7 from openshift-cluster-node-tuning-operator started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container tuned ready: true, restart count 0
Dec 14 14:07:29.737: INFO: dns-default-zwq99 from openshift-dns started at 2021-12-14 13:54:38 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container dns ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.737: INFO: node-resolver-tz2bj from openshift-dns started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container dns-node-resolver ready: true, restart count 0
Dec 14 14:07:29.737: INFO: node-ca-plgtj from openshift-image-registry started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container node-ca ready: true, restart count 0
Dec 14 14:07:29.737: INFO: ingress-canary-2scwt from openshift-ingress-canary started at 2021-12-14 13:54:18 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Dec 14 14:07:29.737: INFO: machine-config-daemon-rwqcs from openshift-machine-config-operator started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container machine-config-daemon ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container oauth-proxy ready: true, restart count 0
Dec 14 14:07:29.737: INFO: node-exporter-rvqjd from openshift-monitoring started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container node-exporter ready: true, restart count 0
Dec 14 14:07:29.737: INFO: multus-additional-cni-plugins-kjrl5 from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Dec 14 14:07:29.737: INFO: multus-rmr4w from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container kube-multus ready: true, restart count 0
Dec 14 14:07:29.737: INFO: network-metrics-daemon-g4ppk from openshift-multus started at 2021-12-14 11:50:39 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Dec 14 14:07:29.737: INFO: network-check-target-br6mv from openshift-network-diagnostics started at 2021-12-14 11:50:39 +0000 UTC (1 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container network-check-target-container ready: true, restart count 0
Dec 14 14:07:29.737: INFO: sdn-jbrq7 from openshift-sdn started at 2021-12-14 11:50:40 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container sdn ready: true, restart count 0
Dec 14 14:07:29.737: INFO: sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-7qgl9 from sonobuoy started at 2021-12-14 12:26:22 +0000 UTC (2 container statuses recorded)
Dec 14 14:07:29.737: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 14:07:29.737: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node ip-10-0-132-177.ec2.internal
STEP: verifying the node has the label node ip-10-0-150-199.ec2.internal
STEP: verifying the node has the label node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod aws-ebs-csi-driver-node-2t5xl requesting resource cpu=30m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod aws-ebs-csi-driver-node-6djjl requesting resource cpu=30m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod aws-ebs-csi-driver-node-mzk2q requesting resource cpu=30m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod tuned-btrkv requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod tuned-ckp8j requesting resource cpu=10m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod tuned-fqln7 requesting resource cpu=10m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod dns-default-hk5vj requesting resource cpu=60m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod dns-default-xq8ng requesting resource cpu=60m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod dns-default-zwq99 requesting resource cpu=60m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-resolver-5tjmp requesting resource cpu=5m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-resolver-tz2bj requesting resource cpu=5m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-resolver-zjfkl requesting resource cpu=5m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod image-registry-669979f5d8-66fds requesting resource cpu=100m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod image-registry-669979f5d8-zxpb9 requesting resource cpu=100m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-ca-gb64k requesting resource cpu=10m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-ca-plgtj requesting resource cpu=10m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-ca-x5d9t requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod ingress-canary-2scwt requesting resource cpu=10m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod ingress-canary-d5qjx requesting resource cpu=10m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod ingress-canary-vth2d requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod router-default-7fbf56f75b-4q5fx requesting resource cpu=100m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod router-default-7fbf56f75b-ncphj requesting resource cpu=100m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod machine-config-daemon-98m5c requesting resource cpu=40m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod machine-config-daemon-rwqcs requesting resource cpu=40m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod machine-config-daemon-t7xdp requesting resource cpu=40m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod certified-operators-nt7k6 requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod community-operators-g4pdk requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod redhat-marketplace-qmxvd requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod redhat-operators-fr7pq requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod grafana-d5d47878d-mtw5p requesting resource cpu=5m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod kube-state-metrics-78d545c868-nnjp9 requesting resource cpu=4m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-exporter-2xvt4 requesting resource cpu=9m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-exporter-f89s7 requesting resource cpu=9m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod node-exporter-rvqjd requesting resource cpu=9m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod openshift-state-metrics-85bc7c8d6c-jws2g requesting resource cpu=3m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod prometheus-adapter-74475585f4-cfsjn requesting resource cpu=1m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod prometheus-adapter-74475585f4-skfcj requesting resource cpu=1m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod telemeter-client-f96747c6d-sj25j requesting resource cpu=3m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod thanos-querier-74cd47ff6f-jszlz requesting resource cpu=14m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod thanos-querier-74cd47ff6f-wdf79 requesting resource cpu=14m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod multus-6drw6 requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod multus-7jp7s requesting resource cpu=10m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod multus-additional-cni-plugins-6kws7 requesting resource cpu=10m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod multus-additional-cni-plugins-kjrl5 requesting resource cpu=10m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod multus-additional-cni-plugins-p2nx7 requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod multus-rmr4w requesting resource cpu=10m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod network-metrics-daemon-8sqs4 requesting resource cpu=20m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod network-metrics-daemon-dvsjd requesting resource cpu=20m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod network-metrics-daemon-g4ppk requesting resource cpu=20m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod network-check-source-579795dc4b-k549q requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod network-check-target-br6mv requesting resource cpu=10m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod network-check-target-hpkw5 requesting resource cpu=10m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod network-check-target-lrvdq requesting resource cpu=10m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sdn-7t2qj requesting resource cpu=110m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sdn-jbrq7 requesting resource cpu=110m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sdn-mwhj9 requesting resource cpu=110m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sonobuoy-e2e-job-1d421b73e370443b requesting resource cpu=0m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-2mpvh requesting resource cpu=0m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-7qgl9 requesting resource cpu=0m on Node ip-10-0-169-135.ec2.internal
Dec 14 14:07:30.170: INFO: Pod sonobuoy-systemd-logs-daemon-set-92e5f85479044ebd-hgvlc requesting resource cpu=0m on Node ip-10-0-132-177.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Dec 14 14:07:30.170: INFO: Creating a pod which consumes cpu=606m on Node ip-10-0-132-177.ec2.internal
Dec 14 14:07:30.189: INFO: Creating a pod which consumes cpu=555m on Node ip-10-0-150-199.ec2.internal
Dec 14 14:07:30.217: INFO: Creating a pod which consumes cpu=816m on Node ip-10-0-169-135.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a.16c0a43deab70917], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a to ip-10-0-132-177.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a.16c0a43e6c204040], Reason = [AddedInterface], Message = [Add eth0 [10.129.2.85/23] from openshift-sdn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a.16c0a43e7a876705], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a.16c0a43e860e2d7b], Reason = [Created], Message = [Created container filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a.16c0a43e87c9942e], Reason = [Started], Message = [Started container filler-pod-0318ee51-3f8e-4521-bca1-e605362a922a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43.16c0a43dec462532], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43 to ip-10-0-150-199.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43.16c0a43e6bd65c9d], Reason = [AddedInterface], Message = [Add eth0 [10.131.0.159/23] from openshift-sdn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43.16c0a43e7bd971ff], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43.16c0a43e8a4a5bf3], Reason = [Created], Message = [Created container filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43.16c0a43e8c281f41], Reason = [Started], Message = [Started container filler-pod-3b281f6a-a172-4643-88c7-27d2118c0b43]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906.16c0a43ded829c03], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6613/filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906 to ip-10-0-169-135.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906.16c0a43e5f793f63], Reason = [AddedInterface], Message = [Add eth0 [10.128.3.20/23] from openshift-sdn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906.16c0a43e70ecb449], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906.16c0a43e7be93c42], Reason = [Created], Message = [Created container filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906.16c0a43e7dcc68fc], Reason = [Started], Message = [Started container filler-pod-9bd1a462-c489-416b-a090-8ff8fc8dc906]
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-6613.16c0a43d69ac9aa7], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16c0a43ee0224856], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) were unschedulable.]
STEP: removing the label node off the node ip-10-0-132-177.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-150-199.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-169-135.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:07:35.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6613" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:7.432 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":297,"skipped":4841,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:07:35.418: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-d24g
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 14:07:35.746: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d24g" in namespace "subpath-9445" to be "Succeeded or Failed"
Dec 14 14:07:35.762: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Pending", Reason="", readiness=false. Elapsed: 16.353173ms
Dec 14 14:07:37.770: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023454063s
Dec 14 14:07:39.779: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032356794s
Dec 14 14:07:41.785: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 6.039183161s
Dec 14 14:07:43.793: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 8.046555499s
Dec 14 14:07:45.807: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 10.060583816s
Dec 14 14:07:47.816: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 12.069540551s
Dec 14 14:07:49.828: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 14.082281057s
Dec 14 14:07:51.838: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 16.091493633s
Dec 14 14:07:53.847: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 18.10041165s
Dec 14 14:07:55.852: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 20.10564407s
Dec 14 14:07:57.872: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 22.12557717s
Dec 14 14:07:59.890: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Running", Reason="", readiness=true. Elapsed: 24.143574869s
Dec 14 14:08:01.900: INFO: Pod "pod-subpath-test-configmap-d24g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.153412215s
STEP: Saw pod success
Dec 14 14:08:01.900: INFO: Pod "pod-subpath-test-configmap-d24g" satisfied condition "Succeeded or Failed"
Dec 14 14:08:01.904: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-subpath-test-configmap-d24g container test-container-subpath-configmap-d24g: <nil>
STEP: delete the pod
Dec 14 14:08:01.946: INFO: Waiting for pod pod-subpath-test-configmap-d24g to disappear
Dec 14 14:08:01.949: INFO: Pod pod-subpath-test-configmap-d24g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d24g
Dec 14 14:08:01.949: INFO: Deleting pod "pod-subpath-test-configmap-d24g" in namespace "subpath-9445"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:08:01.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9445" for this suite.

• [SLOW TEST:26.548 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":298,"skipped":4860,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:08:01.966: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 14 14:08:02.147: INFO: Waiting up to 5m0s for pod "pod-9822f989-46d7-4c99-be50-bc3ec6311e65" in namespace "emptydir-2648" to be "Succeeded or Failed"
Dec 14 14:08:02.158: INFO: Pod "pod-9822f989-46d7-4c99-be50-bc3ec6311e65": Phase="Pending", Reason="", readiness=false. Elapsed: 10.910478ms
Dec 14 14:08:04.163: INFO: Pod "pod-9822f989-46d7-4c99-be50-bc3ec6311e65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015925218s
Dec 14 14:08:06.170: INFO: Pod "pod-9822f989-46d7-4c99-be50-bc3ec6311e65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022466909s
STEP: Saw pod success
Dec 14 14:08:06.170: INFO: Pod "pod-9822f989-46d7-4c99-be50-bc3ec6311e65" satisfied condition "Succeeded or Failed"
Dec 14 14:08:06.175: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-9822f989-46d7-4c99-be50-bc3ec6311e65 container test-container: <nil>
STEP: delete the pod
Dec 14 14:08:06.206: INFO: Waiting for pod pod-9822f989-46d7-4c99-be50-bc3ec6311e65 to disappear
Dec 14 14:08:06.210: INFO: Pod pod-9822f989-46d7-4c99-be50-bc3ec6311e65 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:08:06.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2648" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":299,"skipped":4881,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:08:06.223: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Dec 14 14:08:06.436: INFO: Waiting up to 5m0s for pod "var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3" in namespace "var-expansion-4029" to be "Succeeded or Failed"
Dec 14 14:08:06.440: INFO: Pod "var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.681013ms
Dec 14 14:08:08.451: INFO: Pod "var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014254899s
Dec 14 14:08:10.458: INFO: Pod "var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0214582s
STEP: Saw pod success
Dec 14 14:08:10.458: INFO: Pod "var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3" satisfied condition "Succeeded or Failed"
Dec 14 14:08:10.462: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3 container dapi-container: <nil>
STEP: delete the pod
Dec 14 14:08:10.507: INFO: Waiting for pod var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3 to disappear
Dec 14 14:08:10.517: INFO: Pod var-expansion-aae67c0b-f4b2-445b-8133-183089591ef3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:08:10.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4029" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":300,"skipped":4886,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:08:10.532: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2058
STEP: creating service affinity-nodeport in namespace services-2058
STEP: creating replication controller affinity-nodeport in namespace services-2058
I1214 14:08:10.771468      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2058, replica count: 3
I1214 14:08:13.822547      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 14:08:16.823662      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 14:08:16.838: INFO: Creating new exec pod
Dec 14 14:08:21.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2058 exec execpod-affinitytf7tz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Dec 14 14:08:22.530: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 14 14:08:22.530: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:08:22.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2058 exec execpod-affinitytf7tz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.68.243 80'
Dec 14 14:08:22.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.68.243 80\nConnection to 172.30.68.243 80 port [tcp/http] succeeded!\n"
Dec 14 14:08:22.793: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:08:22.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2058 exec execpod-affinitytf7tz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.150.199 31425'
Dec 14 14:08:23.010: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.150.199 31425\nConnection to 10.0.150.199 31425 port [tcp/*] succeeded!\n"
Dec 14 14:08:23.010: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:08:23.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2058 exec execpod-affinitytf7tz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.132.177 31425'
Dec 14 14:08:23.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.132.177 31425\nConnection to 10.0.132.177 31425 port [tcp/*] succeeded!\n"
Dec 14 14:08:23.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 14:08:23.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=services-2058 exec execpod-affinitytf7tz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.177:31425/ ; done'
Dec 14 14:08:23.538: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.177:31425/\n"
Dec 14 14:08:23.538: INFO: stdout: "\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7\naffinity-nodeport-ztbt7"
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.538: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Received response from host: affinity-nodeport-ztbt7
Dec 14 14:08:23.539: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2058, will wait for the garbage collector to delete the pods
Dec 14 14:08:23.638: INFO: Deleting ReplicationController affinity-nodeport took: 7.361588ms
Dec 14 14:08:23.739: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.633747ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:08:38.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2058" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:28.071 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":301,"skipped":4887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:08:38.605: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec 14 14:08:38.959: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:08:39.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1303" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":302,"skipped":4914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:08:39.033: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 14:08:39.296: INFO: Waiting up to 5m0s for pod "pod-9867c603-65e0-47a2-b919-b0c8e10c594a" in namespace "emptydir-4572" to be "Succeeded or Failed"
Dec 14 14:08:39.300: INFO: Pod "pod-9867c603-65e0-47a2-b919-b0c8e10c594a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272358ms
Dec 14 14:08:41.307: INFO: Pod "pod-9867c603-65e0-47a2-b919-b0c8e10c594a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0106874s
Dec 14 14:08:43.317: INFO: Pod "pod-9867c603-65e0-47a2-b919-b0c8e10c594a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021196026s
STEP: Saw pod success
Dec 14 14:08:43.317: INFO: Pod "pod-9867c603-65e0-47a2-b919-b0c8e10c594a" satisfied condition "Succeeded or Failed"
Dec 14 14:08:43.323: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-9867c603-65e0-47a2-b919-b0c8e10c594a container test-container: <nil>
STEP: delete the pod
Dec 14 14:08:43.361: INFO: Waiting for pod pod-9867c603-65e0-47a2-b919-b0c8e10c594a to disappear
Dec 14 14:08:43.367: INFO: Pod pod-9867c603-65e0-47a2-b919-b0c8e10c594a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:08:43.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4572" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":303,"skipped":4948,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:08:43.386: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 14:08:44.038: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 14:08:46.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087724, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087724, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087724, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775087724, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 14:08:49.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 14:08:49.089: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2474-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:08:52.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4111" for this suite.
STEP: Destroying namespace "webhook-4111-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.273 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":304,"skipped":4951,"failed":0}
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:08:52.659: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 14:08:53.673: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-b12cac6a-b852-40aa-aef9-fe54c175e60e
STEP: Creating configMap with name cm-test-opt-upd-27df3b28-2bfa-422c-96e1-980ffbe91a90
STEP: Creating the pod
Dec 14 14:08:53.752: INFO: The status of Pod pod-projected-configmaps-5f1f4e8c-87aa-4359-bc76-6f764f6c7cef is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:08:55.764: INFO: The status of Pod pod-projected-configmaps-5f1f4e8c-87aa-4359-bc76-6f764f6c7cef is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:08:57.826: INFO: The status of Pod pod-projected-configmaps-5f1f4e8c-87aa-4359-bc76-6f764f6c7cef is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-b12cac6a-b852-40aa-aef9-fe54c175e60e
STEP: Updating configmap cm-test-opt-upd-27df3b28-2bfa-422c-96e1-980ffbe91a90
STEP: Creating configMap with name cm-test-opt-create-b27a88e6-6344-4d0e-af32-8874f39de991
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:09:00.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5379" for this suite.

• [SLOW TEST:7.722 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":305,"skipped":4951,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:09:00.381: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:00.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7812" for this suite.

• [SLOW TEST:60.255 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":306,"skipped":4956,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:00.636: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 14:10:00.869: INFO: Waiting up to 5m0s for pod "pod-401eeb1f-996a-46a5-9909-b4cf4779e796" in namespace "emptydir-7182" to be "Succeeded or Failed"
Dec 14 14:10:00.878: INFO: Pod "pod-401eeb1f-996a-46a5-9909-b4cf4779e796": Phase="Pending", Reason="", readiness=false. Elapsed: 8.93825ms
Dec 14 14:10:02.890: INFO: Pod "pod-401eeb1f-996a-46a5-9909-b4cf4779e796": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021424951s
Dec 14 14:10:04.896: INFO: Pod "pod-401eeb1f-996a-46a5-9909-b4cf4779e796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027671419s
STEP: Saw pod success
Dec 14 14:10:04.896: INFO: Pod "pod-401eeb1f-996a-46a5-9909-b4cf4779e796" satisfied condition "Succeeded or Failed"
Dec 14 14:10:04.901: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-401eeb1f-996a-46a5-9909-b4cf4779e796 container test-container: <nil>
STEP: delete the pod
Dec 14 14:10:04.931: INFO: Waiting for pod pod-401eeb1f-996a-46a5-9909-b4cf4779e796 to disappear
Dec 14 14:10:04.936: INFO: Pod pod-401eeb1f-996a-46a5-9909-b4cf4779e796 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:04.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7182" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":307,"skipped":4958,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:04.949: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Dec 14 14:10:05.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5658 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Dec 14 14:10:05.291: INFO: stderr: ""
Dec 14 14:10:05.291: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 14 14:10:10.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5658 get pod e2e-test-httpd-pod -o json'
Dec 14 14:10:10.444: INFO: stderr: ""
Dec 14 14:10:10.444: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.3.31\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.3.31\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-12-14T14:10:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5658\",\n        \"resourceVersion\": \"130680\",\n        \"uid\": \"27071a73-ca11-4497-9bf0-0e8597261635\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-rcxm6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-78jbw\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-169-135.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c63,c37\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-rcxm6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-14T14:10:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-14T14:10:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-14T14:10:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-12-14T14:10:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://9cc2e561d3d8b2f5e837e40ef0e7fbcf7a39c6d88fb8aef45197e9f3a377383d\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-12-14T14:10:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.169.135\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.3.31\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.128.3.31\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-12-14T14:10:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 14 14:10:10.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5658 replace -f -'
Dec 14 14:10:11.332: INFO: stderr: ""
Dec 14 14:10:11.332: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Dec 14 14:10:11.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-5658 delete pods e2e-test-httpd-pod'
Dec 14 14:10:18.513: INFO: stderr: ""
Dec 14 14:10:18.513: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:18.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5658" for this suite.

• [SLOW TEST:13.580 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":308,"skipped":4962,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:18.529: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-266ee3ae-fe47-48ab-a769-d8d0ab7f41d3
STEP: Creating a pod to test consume configMaps
Dec 14 14:10:18.738: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9" in namespace "projected-8596" to be "Succeeded or Failed"
Dec 14 14:10:18.741: INFO: Pod "pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.667239ms
Dec 14 14:10:20.749: INFO: Pod "pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011159503s
Dec 14 14:10:22.758: INFO: Pod "pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019893418s
STEP: Saw pod success
Dec 14 14:10:22.758: INFO: Pod "pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9" satisfied condition "Succeeded or Failed"
Dec 14 14:10:22.762: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 14:10:22.789: INFO: Waiting for pod pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9 to disappear
Dec 14 14:10:22.793: INFO: Pod pod-projected-configmaps-c28d353d-e566-4af3-b15b-babf128588d9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:22.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8596" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":309,"skipped":4979,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:22.810: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 14 14:10:23.020: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2718  416eb605-3e71-485e-ba18-6a2b5ef516bc 130945 0 2021-12-14 14:10:22 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-12-14 14:10:22 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g567z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g567z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c47,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-bj7w7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 14:10:23.026: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:10:25.032: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:10:27.033: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 14 14:10:27.033: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2718 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:10:27.033: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Verifying customized DNS server is configured on pod...
Dec 14 14:10:27.170: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2718 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:10:27.170: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:10:27.290: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2718" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":310,"skipped":4979,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:27.332: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-84328e7d-eef8-425f-a131-daec904559c8
STEP: Creating a pod to test consume secrets
Dec 14 14:10:27.912: INFO: Waiting up to 5m0s for pod "pod-secrets-96737db0-a830-460c-8d0f-da788286adbb" in namespace "secrets-8904" to be "Succeeded or Failed"
Dec 14 14:10:27.923: INFO: Pod "pod-secrets-96737db0-a830-460c-8d0f-da788286adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.318592ms
Dec 14 14:10:29.931: INFO: Pod "pod-secrets-96737db0-a830-460c-8d0f-da788286adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019104434s
Dec 14 14:10:31.937: INFO: Pod "pod-secrets-96737db0-a830-460c-8d0f-da788286adbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024839768s
STEP: Saw pod success
Dec 14 14:10:31.937: INFO: Pod "pod-secrets-96737db0-a830-460c-8d0f-da788286adbb" satisfied condition "Succeeded or Failed"
Dec 14 14:10:31.941: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-secrets-96737db0-a830-460c-8d0f-da788286adbb container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 14:10:31.967: INFO: Waiting for pod pod-secrets-96737db0-a830-460c-8d0f-da788286adbb to disappear
Dec 14 14:10:31.972: INFO: Pod pod-secrets-96737db0-a830-460c-8d0f-da788286adbb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:31.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8904" for this suite.
STEP: Destroying namespace "secret-namespace-5895" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":311,"skipped":4983,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:31.997: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7463.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7463.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7463.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7463.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7463.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7463.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 14:10:36.237: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local from pod dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3: the server could not find the requested resource (get pods dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3)
Dec 14 14:10:36.246: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7463.svc.cluster.local from pod dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3: the server could not find the requested resource (get pods dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3)
Dec 14 14:10:36.264: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local from pod dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3: the server could not find the requested resource (get pods dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3)
Dec 14 14:10:36.268: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local from pod dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3: the server could not find the requested resource (get pods dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3)
Dec 14 14:10:36.273: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7463.svc.cluster.local from pod dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3: the server could not find the requested resource (get pods dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3)
Dec 14 14:10:36.277: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7463.svc.cluster.local from pod dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3: the server could not find the requested resource (get pods dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3)
Dec 14 14:10:36.286: INFO: Lookups using dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7463.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7463.svc.cluster.local jessie_udp@dns-test-service-2.dns-7463.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7463.svc.cluster.local]

Dec 14 14:10:41.351: INFO: DNS probes using dns-7463/dns-test-ddfcfbaf-e110-4972-9ab9-c2971c196ee3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:41.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7463" for this suite.

• [SLOW TEST:9.418 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":312,"skipped":4992,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:41.416: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Dec 14 14:10:43.742: INFO: running pods: 0 < 3
Dec 14 14:10:45.747: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:10:47.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3845" for this suite.

• [SLOW TEST:6.361 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":313,"skipped":5007,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:10:47.777: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W1214 14:10:47.954829      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:15:47.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5600" for this suite.

• [SLOW TEST:300.237 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":314,"skipped":5015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:15:48.014: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:15:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6298" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":315,"skipped":5038,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:15:52.302: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Dec 14 14:15:52.440: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-317132523 --namespace=kubectl-6543 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:15:52.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6543" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":316,"skipped":5041,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:15:52.549: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Dec 14 14:15:52.771: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:15:54.782: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:15:56.777: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:15:58.798: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Dec 14 14:15:58.852: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:16:00.861: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:16:02.860: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 14 14:16:02.865: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:02.865: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:02.970: INFO: Exec stderr: ""
Dec 14 14:16:02.970: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:02.970: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:03.069: INFO: Exec stderr: ""
Dec 14 14:16:03.069: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:03.069: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:03.164: INFO: Exec stderr: ""
Dec 14 14:16:03.164: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:03.164: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:03.274: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 14 14:16:03.274: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:03.274: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:03.379: INFO: Exec stderr: ""
Dec 14 14:16:03.379: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:03.379: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:03.487: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 14 14:16:03.487: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:03.487: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:03.676: INFO: Exec stderr: ""
Dec 14 14:16:03.676: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:03.676: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:03.873: INFO: Exec stderr: ""
Dec 14 14:16:03.873: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:03.873: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:04.060: INFO: Exec stderr: ""
Dec 14 14:16:04.060: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8136 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 14:16:04.060: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
Dec 14 14:16:04.231: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:16:04.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8136" for this suite.

• [SLOW TEST:11.703 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":317,"skipped":5053,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:16:04.252: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 14 14:16:14.588: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1214 14:16:14.588118      20 metrics_grabber.go:102] Can't find any pods in namespace kube-system to grab metrics from
W1214 14:16:14.588145      20 metrics_grabber.go:138] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Dec 14 14:16:14.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-776nv" in namespace "gc-831"
Dec 14 14:16:14.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b4j9" in namespace "gc-831"
Dec 14 14:16:14.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5g7c" in namespace "gc-831"
Dec 14 14:16:14.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-hx6br" in namespace "gc-831"
Dec 14 14:16:14.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcs4r" in namespace "gc-831"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:16:14.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-831" for this suite.

• [SLOW TEST:10.463 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":318,"skipped":5062,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:16:14.715: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:16:19.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-545" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":319,"skipped":5079,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:16:19.029: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Dec 14 14:16:19.275: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:16:21.286: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:16:23.283: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 14 14:16:24.314: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:16:25.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6529" for this suite.

• [SLOW TEST:6.344 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":320,"skipped":5081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:16:25.373: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:16:51.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6945" for this suite.

• [SLOW TEST:25.774 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":321,"skipped":5120,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:16:51.147: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 14:16:52.522: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 14:16:54.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088212, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088212, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088212, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088212, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 14:16:57.562: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:17:10.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2422" for this suite.
STEP: Destroying namespace "webhook-2422-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.052 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":322,"skipped":5137,"failed":0}
SSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:17:10.199: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec 14 14:17:10.457: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec 14 14:17:10.480: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 14:17:10.480: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec 14 14:17:10.570: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 14:17:10.570: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec 14 14:17:10.607: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 14 14:17:10.607: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec 14 14:17:17.680: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:17:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7143" for this suite.

• [SLOW TEST:7.521 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":323,"skipped":5142,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:17:17.721: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-045ae0ab-3308-4667-8bef-3998e28ff587 in namespace container-probe-8343
Dec 14 14:17:21.915: INFO: Started pod busybox-045ae0ab-3308-4667-8bef-3998e28ff587 in namespace container-probe-8343
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 14:17:21.920: INFO: Initial restart count of pod busybox-045ae0ab-3308-4667-8bef-3998e28ff587 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:21:23.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8343" for this suite.

• [SLOW TEST:245.713 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":324,"skipped":5147,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:21:23.434: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:21:34.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9603" for this suite.

• [SLOW TEST:11.466 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":325,"skipped":5157,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:21:34.900: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 14:21:35.451: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 14 14:21:37.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 14:21:39.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63775088495, loc:(*time.Location)(0x9e1f020)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 14:21:42.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:21:42.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1173" for this suite.
STEP: Destroying namespace "webhook-1173-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.965 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":326,"skipped":5160,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:21:42.865: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 14:21:43.188: INFO: The status of Pod busybox-host-aliasesd8fc152d-15f4-44bd-88d3-f9849ffe8eaf is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:21:45.195: INFO: The status of Pod busybox-host-aliasesd8fc152d-15f4-44bd-88d3-f9849ffe8eaf is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:21:47.197: INFO: The status of Pod busybox-host-aliasesd8fc152d-15f4-44bd-88d3-f9849ffe8eaf is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:21:47.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9074" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":327,"skipped":5164,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:21:47.236: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2448.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2448.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 14:21:51.557: INFO: DNS probes using dns-2448/dns-test-8be6c8f8-b8fa-4318-8531-89af23553394 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:21:51.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2448" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":328,"skipped":5165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:21:51.592: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Dec 14 14:21:51.831: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 14:21:51.845: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:51.845: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:51.845: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:51.848: INFO: Number of nodes with available pods: 0
Dec 14 14:21:51.848: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 14:21:52.857: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:52.857: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:52.857: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:52.863: INFO: Number of nodes with available pods: 0
Dec 14 14:21:52.863: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 14:21:53.875: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:53.875: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:53.875: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:53.882: INFO: Number of nodes with available pods: 0
Dec 14 14:21:53.882: INFO: Node ip-10-0-132-177.ec2.internal is running more than one daemon pod
Dec 14 14:21:54.858: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:54.858: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:54.858: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:54.863: INFO: Number of nodes with available pods: 2
Dec 14 14:21:54.864: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:21:55.858: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:55.858: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:55.858: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:55.862: INFO: Number of nodes with available pods: 2
Dec 14 14:21:55.862: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:21:56.856: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:56.857: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:56.857: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:56.861: INFO: Number of nodes with available pods: 2
Dec 14 14:21:56.861: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:21:57.860: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:57.860: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:57.860: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:57.869: INFO: Number of nodes with available pods: 2
Dec 14 14:21:57.869: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:21:58.951: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:58.951: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:58.952: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:58.958: INFO: Number of nodes with available pods: 2
Dec 14 14:21:58.958: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:21:59.930: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:59.930: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:21:59.930: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:21:59.936: INFO: Number of nodes with available pods: 2
Dec 14 14:21:59.936: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:22:00.856: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:00.856: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:00.856: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:00.861: INFO: Number of nodes with available pods: 2
Dec 14 14:22:00.861: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:22:01.859: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:01.859: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:01.859: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:01.866: INFO: Number of nodes with available pods: 2
Dec 14 14:22:01.866: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:22:02.859: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:02.859: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:02.859: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:02.865: INFO: Number of nodes with available pods: 2
Dec 14 14:22:02.865: INFO: Node ip-10-0-150-199.ec2.internal is running more than one daemon pod
Dec 14 14:22:03.858: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:03.859: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:03.859: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:03.863: INFO: Number of nodes with available pods: 3
Dec 14 14:22:03.863: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 14 14:22:03.904: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:03.904: INFO: Wrong image for pod: daemon-set-r4wkz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:03.904: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:03.911: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:03.911: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:03.911: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:04.918: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:04.918: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:04.925: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:04.925: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:04.925: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:05.922: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:05.922: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:05.932: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:05.932: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:05.932: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:06.920: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:06.920: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:06.927: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:06.927: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:06.927: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:07.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:07.919: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:07.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:07.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:07.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:08.920: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:08.920: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:08.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:08.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:08.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:09.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:09.919: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:09.925: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:09.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:09.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:10.920: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:10.920: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:10.927: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:10.927: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:10.927: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:11.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:11.919: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:11.925: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:11.925: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:11.925: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:12.920: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:12.920: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:12.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:12.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:12.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:13.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:13.919: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:13.927: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:13.927: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:13.928: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:14.917: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:14.917: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:14.923: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:14.923: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:14.923: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:15.918: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:15.918: INFO: Pod daemon-set-rgc48 is not available
Dec 14 14:22:15.918: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:15.925: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:15.925: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:15.925: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:16.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:16.919: INFO: Pod daemon-set-rgc48 is not available
Dec 14 14:22:16.919: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:16.924: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:16.924: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:16.924: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:17.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:17.919: INFO: Pod daemon-set-rgc48 is not available
Dec 14 14:22:17.919: INFO: Wrong image for pod: daemon-set-w9rv7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:17.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:17.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:17.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:18.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:18.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:18.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:18.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:19.917: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:19.923: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:19.924: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:19.924: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:20.921: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:20.921: INFO: Pod daemon-set-n2wnd is not available
Dec 14 14:22:20.928: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:20.928: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:20.928: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:21.920: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:21.920: INFO: Pod daemon-set-n2wnd is not available
Dec 14 14:22:21.928: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:21.928: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:21.928: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:22.919: INFO: Wrong image for pod: daemon-set-bsnx2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Dec 14 14:22:22.919: INFO: Pod daemon-set-n2wnd is not available
Dec 14 14:22:22.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:22.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:22.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:23.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:23.927: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:23.927: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:24.928: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:24.928: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:24.928: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:25.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:25.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:25.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:26.926: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:26.926: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:26.926: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:28.039: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:28.039: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:28.039: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:28.920: INFO: Pod daemon-set-qqwsr is not available
Dec 14 14:22:28.953: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:28.953: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:28.953: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 14 14:22:28.962: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:28.978: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:28.978: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:28.991: INFO: Number of nodes with available pods: 2
Dec 14 14:22:28.991: INFO: Node ip-10-0-169-135.ec2.internal is running more than one daemon pod
Dec 14 14:22:30.004: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:30.004: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:30.004: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:30.008: INFO: Number of nodes with available pods: 2
Dec 14 14:22:30.008: INFO: Node ip-10-0-169-135.ec2.internal is running more than one daemon pod
Dec 14 14:22:30.999: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:30.999: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:30.999: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:31.004: INFO: Number of nodes with available pods: 2
Dec 14 14:22:31.004: INFO: Node ip-10-0-169-135.ec2.internal is running more than one daemon pod
Dec 14 14:22:32.005: INFO: DaemonSet pods can't tolerate node ip-10-0-139-73.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:32.005: INFO: DaemonSet pods can't tolerate node ip-10-0-159-148.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:11 +0000 UTC}], skip checking this node
Dec 14 14:22:32.005: INFO: DaemonSet pods can't tolerate node ip-10-0-164-40.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2021-12-14 12:18:12 +0000 UTC}], skip checking this node
Dec 14 14:22:32.009: INFO: Number of nodes with available pods: 3
Dec 14 14:22:32.009: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3371, will wait for the garbage collector to delete the pods
Dec 14 14:22:32.099: INFO: Deleting DaemonSet.extensions daemon-set took: 11.112651ms
Dec 14 14:22:32.200: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.887383ms
Dec 14 14:22:48.012: INFO: Number of nodes with available pods: 0
Dec 14 14:22:48.012: INFO: Number of running nodes: 0, number of available pods: 0
Dec 14 14:22:48.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"140019"},"items":null}

Dec 14 14:22:48.019: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"140019"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:22:48.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3371" for this suite.

• [SLOW TEST:56.471 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":329,"skipped":5220,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:22:48.063: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 14 14:22:48.244: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:22:58.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1262" for this suite.

• [SLOW TEST:10.865 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":330,"skipped":5235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:22:58.928: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 14:22:59.161: INFO: Waiting up to 5m0s for pod "pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e" in namespace "emptydir-4358" to be "Succeeded or Failed"
Dec 14 14:22:59.199: INFO: Pod "pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 37.455708ms
Dec 14 14:23:01.210: INFO: Pod "pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048602709s
Dec 14 14:23:03.221: INFO: Pod "pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059376891s
Dec 14 14:23:05.228: INFO: Pod "pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066253733s
STEP: Saw pod success
Dec 14 14:23:05.228: INFO: Pod "pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e" satisfied condition "Succeeded or Failed"
Dec 14 14:23:05.232: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e container test-container: <nil>
STEP: delete the pod
Dec 14 14:23:05.264: INFO: Waiting for pod pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e to disappear
Dec 14 14:23:05.271: INFO: Pod pod-fc2b90a3-a18d-4c2d-8d31-d3e782b0bf3e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:23:05.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4358" for this suite.

• [SLOW TEST:6.358 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":331,"skipped":5260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:23:05.287: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Dec 14 14:23:05.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f" in namespace "downward-api-8362" to be "Succeeded or Failed"
Dec 14 14:23:05.540: INFO: Pod "downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.270465ms
Dec 14 14:23:07.547: INFO: Pod "downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024991166s
Dec 14 14:23:09.568: INFO: Pod "downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045708883s
STEP: Saw pod success
Dec 14 14:23:09.568: INFO: Pod "downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f" satisfied condition "Succeeded or Failed"
Dec 14 14:23:09.572: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f container client-container: <nil>
STEP: delete the pod
Dec 14 14:23:09.622: INFO: Waiting for pod downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f to disappear
Dec 14 14:23:09.626: INFO: Pod downwardapi-volume-f095f6a3-80b2-45fa-be49-b0fdee19208f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:23:09.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8362" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":332,"skipped":5283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:23:09.640: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-f07e69b8-65a4-4723-8939-89d01ca90836
STEP: Creating a pod to test consume secrets
Dec 14 14:23:09.846: INFO: Waiting up to 5m0s for pod "pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3" in namespace "secrets-5793" to be "Succeeded or Failed"
Dec 14 14:23:09.850: INFO: Pod "pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.289123ms
Dec 14 14:23:11.863: INFO: Pod "pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017098463s
Dec 14 14:23:13.874: INFO: Pod "pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028279206s
Dec 14 14:23:15.884: INFO: Pod "pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037397702s
Dec 14 14:23:17.892: INFO: Pod "pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045701151s
STEP: Saw pod success
Dec 14 14:23:17.892: INFO: Pod "pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3" satisfied condition "Succeeded or Failed"
Dec 14 14:23:17.907: INFO: Trying to get logs from node ip-10-0-169-135.ec2.internal pod pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 14:23:17.939: INFO: Waiting for pod pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3 to disappear
Dec 14 14:23:17.944: INFO: Pod pod-secrets-8e7540a0-6dfc-4f8a-b730-122af3efdcd3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:23:17.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5793" for this suite.

• [SLOW TEST:8.323 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":333,"skipped":5308,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:23:17.963: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-729
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Dec 14 14:23:18.144: INFO: Found 0 stateful pods, waiting for 3
Dec 14 14:23:28.319: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 14:23:28.319: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 14:23:28.319: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec 14 14:23:38.160: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 14:23:38.160: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 14:23:38.160: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Dec 14 14:23:38.206: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 14 14:23:48.255: INFO: Updating stateful set ss2
Dec 14 14:23:48.267: INFO: Waiting for Pod statefulset-729/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Dec 14 14:23:58.398: INFO: Found 1 stateful pods, waiting for 3
Dec 14 14:24:08.413: INFO: Found 2 stateful pods, waiting for 3
Dec 14 14:24:18.406: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 14:24:18.406: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 14:24:18.406: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 14 14:24:18.440: INFO: Updating stateful set ss2
Dec 14 14:24:18.449: INFO: Waiting for Pod statefulset-729/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Dec 14 14:24:28.498: INFO: Updating stateful set ss2
Dec 14 14:24:28.512: INFO: Waiting for StatefulSet statefulset-729/ss2 to complete update
Dec 14 14:24:28.512: INFO: Waiting for Pod statefulset-729/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Dec 14 14:24:48.525: INFO: Deleting all statefulset in ns statefulset-729
Dec 14 14:24:48.529: INFO: Scaling statefulset ss2 to 0
Dec 14 14:25:18.560: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 14:25:18.565: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:25:18.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-729" for this suite.

• [SLOW TEST:120.636 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":334,"skipped":5314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:25:18.599: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Dec 14 14:25:18.894: INFO: The status of Pod annotationupdatee19cbcf2-7683-4c45-b0b2-6bb4f1363941 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:25:20.900: INFO: The status of Pod annotationupdatee19cbcf2-7683-4c45-b0b2-6bb4f1363941 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 14:25:22.899: INFO: The status of Pod annotationupdatee19cbcf2-7683-4c45-b0b2-6bb4f1363941 is Running (Ready = true)
Dec 14 14:25:23.448: INFO: Successfully updated pod "annotationupdatee19cbcf2-7683-4c45-b0b2-6bb4f1363941"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:25:27.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-272" for this suite.

• [SLOW TEST:8.982 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":335,"skipped":5355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:25:27.582: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:25:28.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8526" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":336,"skipped":5385,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:25:28.425: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:25:28.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2209" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":337,"skipped":5397,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:25:29.028: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 14 14:25:33.859: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-690 pod-service-account-74f1e7b2-f50b-4266-9f03-0433a68ad93c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 14 14:25:34.466: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-690 pod-service-account-74f1e7b2-f50b-4266-9f03-0433a68ad93c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 14 14:25:34.718: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-690 pod-service-account-74f1e7b2-f50b-4266-9f03-0433a68ad93c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:25:34.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-690" for this suite.

• [SLOW TEST:5.930 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":338,"skipped":5401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Dec 14 14:25:34.958: INFO: >>> kubeConfig: /tmp/kubeconfig-317132523
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Dec 14 14:25:35.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8714" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":339,"skipped":5428,"failed":0}
SSSDec 14 14:25:35.321: INFO: Running AfterSuite actions on all nodes
Dec 14 14:25:35.321: INFO: Running AfterSuite actions on node 1
Dec 14 14:25:35.321: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5431,"failed":0}

Ran 339 of 5770 Specs in 7148.542 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5431 Skipped
PASS

Ginkgo ran 1 suite in 1h59m10.813272552s
Test Suite Passed
