I0119 22:27:14.942166      22 e2e.go:132] Starting e2e run "51a0ad83-197b-44da-b76e-3062d64f5602" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1642631234 - Will randomize all specs
Will run 346 of 7042 specs

Jan 19 22:27:17.196: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:27:17.198: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 19 22:27:17.213: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 19 22:27:17.251: INFO: The status of Pod minio-setup-hldwk is Succeeded, skipping waiting
Jan 19 22:27:17.251: INFO: 24 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 19 22:27:17.251: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jan 19 22:27:17.251: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 19 22:27:17.255: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 19 22:27:17.255: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 19 22:27:17.255: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'velero-restic' (0 seconds elapsed)
Jan 19 22:27:17.255: INFO: e2e test version: v1.23.1
Jan 19 22:27:17.256: INFO: kube-apiserver version: v1.23.1
Jan 19 22:27:17.256: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:27:17.258: INFO: Cluster IP family: ipv4
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:17.258: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
Jan 19 22:27:17.295: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
W0119 22:27:17.295366      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:27:18.755: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 22:27:20.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 27, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:27:23.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:23.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8427" for this suite.
STEP: Destroying namespace "webhook-8427-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.606 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":1,"skipped":2,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:23.865: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 19 22:27:25.101: INFO: starting watch
STEP: patching
STEP: updating
Jan 19 22:27:25.114: INFO: waiting for watch events with expected annotations
Jan 19 22:27:25.114: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:25.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8136" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":2,"skipped":10,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:25.175: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 22:27:25.211: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78" in namespace "projected-8408" to be "Succeeded or Failed"
Jan 19 22:27:25.213: INFO: Pod "downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78": Phase="Pending", Reason="", readiness=false. Elapsed: 1.811339ms
Jan 19 22:27:27.217: INFO: Pod "downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005981032s
Jan 19 22:27:29.223: INFO: Pod "downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011503146s
STEP: Saw pod success
Jan 19 22:27:29.223: INFO: Pod "downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78" satisfied condition "Succeeded or Failed"
Jan 19 22:27:29.224: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78 container client-container: <nil>
STEP: delete the pod
Jan 19 22:27:29.248: INFO: Waiting for pod downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78 to disappear
Jan 19 22:27:29.256: INFO: Pod downwardapi-volume-dcce492a-1766-48ac-a9ee-43ad46b05b78 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:29.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8408" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":3,"skipped":27,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:29.262: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:27:29.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 22:27:31.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 27, 29, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 27, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 27, 29, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 27, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:27:34.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 19 22:27:38.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=webhook-4480 attach --namespace=webhook-4480 to-be-attached-pod -i -c=container1'
Jan 19 22:27:39.607: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:39.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4480" for this suite.
STEP: Destroying namespace "webhook-4480-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.396 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":4,"skipped":28,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:39.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:39.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4322" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":5,"skipped":40,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:39.764: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-b0abdc0b-7809-4adb-942e-296cf5eb2787
STEP: Creating a pod to test consume configMaps
Jan 19 22:27:39.838: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25" in namespace "projected-6098" to be "Succeeded or Failed"
Jan 19 22:27:39.850: INFO: Pod "pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25": Phase="Pending", Reason="", readiness=false. Elapsed: 11.861386ms
Jan 19 22:27:41.855: INFO: Pod "pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016237136s
Jan 19 22:27:43.861: INFO: Pod "pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022133463s
STEP: Saw pod success
Jan 19 22:27:43.861: INFO: Pod "pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25" satisfied condition "Succeeded or Failed"
Jan 19 22:27:43.862: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 22:27:43.885: INFO: Waiting for pod pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25 to disappear
Jan 19 22:27:43.887: INFO: Pod pod-projected-configmaps-0c200967-da66-4961-a112-677badaa5c25 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:43.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6098" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":69,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:43.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 19 22:27:43.932: INFO: Waiting up to 5m0s for pod "pod-53719bc7-b71e-4229-a9f1-a76c23a6186c" in namespace "emptydir-7615" to be "Succeeded or Failed"
Jan 19 22:27:43.934: INFO: Pod "pod-53719bc7-b71e-4229-a9f1-a76c23a6186c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.915115ms
Jan 19 22:27:45.937: INFO: Pod "pod-53719bc7-b71e-4229-a9f1-a76c23a6186c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005053791s
Jan 19 22:27:47.943: INFO: Pod "pod-53719bc7-b71e-4229-a9f1-a76c23a6186c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010629415s
STEP: Saw pod success
Jan 19 22:27:47.943: INFO: Pod "pod-53719bc7-b71e-4229-a9f1-a76c23a6186c" satisfied condition "Succeeded or Failed"
Jan 19 22:27:47.945: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-53719bc7-b71e-4229-a9f1-a76c23a6186c container test-container: <nil>
STEP: delete the pod
Jan 19 22:27:47.957: INFO: Waiting for pod pod-53719bc7-b71e-4229-a9f1-a76c23a6186c to disappear
Jan 19 22:27:47.965: INFO: Pod pod-53719bc7-b71e-4229-a9f1-a76c23a6186c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:47.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7615" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":7,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:47.971: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:27:48.017: INFO: The status of Pod pod-secrets-752a77f2-3e8f-43bb-8e0a-c407bc557bf7 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:27:50.021: INFO: The status of Pod pod-secrets-752a77f2-3e8f-43bb-8e0a-c407bc557bf7 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:27:52.021: INFO: The status of Pod pod-secrets-752a77f2-3e8f-43bb-8e0a-c407bc557bf7 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:52.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-486" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":8,"skipped":133,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jan 19 22:27:52.101: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 22:27:57.110: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:57.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-551" for this suite.

• [SLOW TEST:5.103 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":9,"skipped":143,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:57.159: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 19 22:27:57.216: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 19 22:27:57.219: INFO: starting watch
STEP: patching
STEP: updating
Jan 19 22:27:57.236: INFO: waiting for watch events with expected annotations
Jan 19 22:27:57.236: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:27:57.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4843" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":10,"skipped":152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:27:57.275: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:27:58.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 22:28:00.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 27, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 27, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 27, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 27, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:28:03.383: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:28:03.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2313" for this suite.
STEP: Destroying namespace "webhook-2313-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.197 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":11,"skipped":181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:28:03.473: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-7c1cb778-43e3-4b1d-bcae-20a510b3bdf5
STEP: Creating a pod to test consume secrets
Jan 19 22:28:03.532: INFO: Waiting up to 5m0s for pod "pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293" in namespace "secrets-4050" to be "Succeeded or Failed"
Jan 19 22:28:03.534: INFO: Pod "pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293": Phase="Pending", Reason="", readiness=false. Elapsed: 1.887656ms
Jan 19 22:28:05.538: INFO: Pod "pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006027275s
Jan 19 22:28:07.542: INFO: Pod "pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009421631s
STEP: Saw pod success
Jan 19 22:28:07.542: INFO: Pod "pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293" satisfied condition "Succeeded or Failed"
Jan 19 22:28:07.543: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 22:28:07.570: INFO: Waiting for pod pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293 to disappear
Jan 19 22:28:07.572: INFO: Pod pod-secrets-e4dd2404-7710-46cc-b5e3-632c6d8b6293 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:28:07.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4050" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:28:07.578: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-cca46aae-aacd-468d-8c42-9b605c99608a
STEP: Creating a pod to test consume secrets
Jan 19 22:28:07.619: INFO: Waiting up to 5m0s for pod "pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48" in namespace "secrets-5617" to be "Succeeded or Failed"
Jan 19 22:28:07.622: INFO: Pod "pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.959068ms
Jan 19 22:28:09.625: INFO: Pod "pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006456634s
Jan 19 22:28:11.630: INFO: Pod "pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01128979s
STEP: Saw pod success
Jan 19 22:28:11.630: INFO: Pod "pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48" satisfied condition "Succeeded or Failed"
Jan 19 22:28:11.632: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 22:28:11.652: INFO: Waiting for pod pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48 to disappear
Jan 19 22:28:11.654: INFO: Pod pod-secrets-d7f8b7e3-c7c0-42f6-bf43-80a976493b48 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:28:11.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5617" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":283,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:28:11.661: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-d1d38687-fe72-4a21-9905-6343e96fdc4d
STEP: Creating a pod to test consume configMaps
Jan 19 22:28:11.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2" in namespace "configmap-5300" to be "Succeeded or Failed"
Jan 19 22:28:11.704: INFO: Pod "pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.888486ms
Jan 19 22:28:13.706: INFO: Pod "pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004487088s
Jan 19 22:28:15.712: INFO: Pod "pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010169161s
STEP: Saw pod success
Jan 19 22:28:15.712: INFO: Pod "pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2" satisfied condition "Succeeded or Failed"
Jan 19 22:28:15.714: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 22:28:15.736: INFO: Waiting for pod pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2 to disappear
Jan 19 22:28:15.738: INFO: Pod pod-configmaps-d3e023c1-7566-406f-947b-c591ea0e39f2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:28:15.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5300" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:28:15.745: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 19 22:28:15.770: INFO: PodSpec: initContainers in spec.initContainers
Jan 19 22:29:01.018: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-aa87008d-fc07-4e13-885b-989ba5352198", GenerateName:"", Namespace:"init-container-441", SelfLink:"", UID:"666030c1-f8d2-42b3-92b7-6fc97839cff0", ResourceVersion:"59556", Generation:0, CreationTimestamp:time.Date(2022, time.January, 19, 22, 28, 15, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"770761642"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"f2c8d8e634544dff5b29a2e9cec91a2b0aa8e683f0b5912d63d7eff45e1ac4ff", "cni.projectcalico.org/podIP":"172.16.146.43/32", "cni.projectcalico.org/podIPs":"172.16.146.43/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.January, 19, 22, 28, 15, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003d15860), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.January, 19, 22, 28, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003d15890), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.January, 19, 22, 28, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003d158c0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hg6z6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003af7240), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hg6z6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hg6z6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hg6z6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0053040d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-1-153.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00256efc0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005304150)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005304170)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005304178), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00530417c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001207980), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 19, 22, 28, 15, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 19, 22, 28, 15, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 19, 22, 28, 15, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.January, 19, 22, 28, 15, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.1.153", PodIP:"172.16.146.43", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.146.43"}}, StartTime:time.Date(2022, time.January, 19, 22, 28, 15, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00256f0a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00256f110)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"docker://7596b34107fb35a53510a7de6a4dba7d309e81102e44cf7341c220a1e52dd472", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003af72c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003af72a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc0053041ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:29:01.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-441" for this suite.

• [SLOW TEST:45.280 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":15,"skipped":345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:29:01.026: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Jan 19 22:29:01.068: INFO: Creating simple deployment test-deployment-jrbwz
Jan 19 22:29:01.075: INFO: new replicaset for deployment "test-deployment-jrbwz" is yet to be created
Jan 19 22:29:03.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 29, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 29, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 29, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-jrbwz-764bc7c4b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Jan 19 22:29:05.093: INFO: Deployment test-deployment-jrbwz has Conditions: [{Available True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrbwz-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Jan 19 22:29:05.099: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 29, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 29, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 29, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 29, 1, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jrbwz-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jan 19 22:29:05.100: INFO: Observed &Deployment event: ADDED
Jan 19 22:29:05.101: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrbwz-764bc7c4b7"}
Jan 19 22:29:05.101: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.101: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrbwz-764bc7c4b7"}
Jan 19 22:29:05.101: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 22:29:05.101: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.101: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 22:29:05.101: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jrbwz-764bc7c4b7" is progressing.}
Jan 19 22:29:05.101: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.101: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 22:29:05.101: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrbwz-764bc7c4b7" has successfully progressed.}
Jan 19 22:29:05.101: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.102: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 22:29:05.102: INFO: Observed Deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrbwz-764bc7c4b7" has successfully progressed.}
Jan 19 22:29:05.102: INFO: Found Deployment test-deployment-jrbwz in namespace deployment-9756 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 22:29:05.102: INFO: Deployment test-deployment-jrbwz has an updated status
STEP: patching the Statefulset Status
Jan 19 22:29:05.102: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 22:29:05.109: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jan 19 22:29:05.110: INFO: Observed &Deployment event: ADDED
Jan 19 22:29:05.110: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrbwz-764bc7c4b7"}
Jan 19 22:29:05.111: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jrbwz-764bc7c4b7"}
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 22:29:05.111: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:01 +0000 UTC 2022-01-19 22:29:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jrbwz-764bc7c4b7" is progressing.}
Jan 19 22:29:05.111: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrbwz-764bc7c4b7" has successfully progressed.}
Jan 19 22:29:05.111: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-01-19 22:29:03 +0000 UTC 2022-01-19 22:29:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jrbwz-764bc7c4b7" has successfully progressed.}
Jan 19 22:29:05.111: INFO: Observed deployment test-deployment-jrbwz in namespace deployment-9756 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 22:29:05.111: INFO: Observed &Deployment event: MODIFIED
Jan 19 22:29:05.111: INFO: Found deployment test-deployment-jrbwz in namespace deployment-9756 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 19 22:29:05.111: INFO: Deployment test-deployment-jrbwz has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 19 22:29:05.114: INFO: Deployment "test-deployment-jrbwz":
&Deployment{ObjectMeta:{test-deployment-jrbwz  deployment-9756  2a660993-9cce-4cd7-9ca0-55a991d8c97d 59599 1 2022-01-19 22:29:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-01-19 22:29:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 22:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-01-19 22:29:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d727f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 22:29:05.119: INFO: New ReplicaSet "test-deployment-jrbwz-764bc7c4b7" of Deployment "test-deployment-jrbwz":
&ReplicaSet{ObjectMeta:{test-deployment-jrbwz-764bc7c4b7  deployment-9756  1490521c-1e71-4491-91e3-b911ca47d631 59589 1 2022-01-19 22:29:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jrbwz 2a660993-9cce-4cd7-9ca0-55a991d8c97d 0xc003d72b97 0xc003d72b98}] []  [{kube-controller-manager Update apps/v1 2022-01-19 22:29:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a660993-9cce-4cd7-9ca0-55a991d8c97d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 22:29:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d72c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 22:29:05.121: INFO: Pod "test-deployment-jrbwz-764bc7c4b7-qd5df" is available:
&Pod{ObjectMeta:{test-deployment-jrbwz-764bc7c4b7-qd5df test-deployment-jrbwz-764bc7c4b7- deployment-9756  a5b86cc2-1dc6-4d52-92ab-632ee479bdb4 59588 0 2022-01-19 22:29:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/containerID:0ca24b25d1ab46aa3b2310bfb2eec107936c5a54337258a6d3aaf8654741fcb7 cni.projectcalico.org/podIP:172.16.146.42/32 cni.projectcalico.org/podIPs:172.16.146.42/32] [{apps/v1 ReplicaSet test-deployment-jrbwz-764bc7c4b7 1490521c-1e71-4491-91e3-b911ca47d631 0xc003f3ebf0 0xc003f3ebf1}] []  [{kube-controller-manager Update v1 2022-01-19 22:29:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1490521c-1e71-4491-91e3-b911ca47d631\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 22:29:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 22:29:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgstp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgstp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:29:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:29:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:29:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:29:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:172.16.146.42,StartTime:2022-01-19 22:29:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 22:29:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://933c5275ab4774de70e3fc6ad4d7c7b25a8f8067cc87194f42e6d57b080a8068,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.146.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:29:05.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9756" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":16,"skipped":435,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:29:05.130: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 19 22:29:05.168: INFO: Waiting up to 5m0s for pod "pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d" in namespace "emptydir-5636" to be "Succeeded or Failed"
Jan 19 22:29:05.170: INFO: Pod "pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.754561ms
Jan 19 22:29:07.172: INFO: Pod "pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004366243s
Jan 19 22:29:09.179: INFO: Pod "pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010986173s
STEP: Saw pod success
Jan 19 22:29:09.179: INFO: Pod "pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d" satisfied condition "Succeeded or Failed"
Jan 19 22:29:09.181: INFO: Trying to get logs from node ip-10-0-1-62.eu-central-1.compute.internal pod pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d container test-container: <nil>
STEP: delete the pod
Jan 19 22:29:09.206: INFO: Waiting for pod pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d to disappear
Jan 19 22:29:09.225: INFO: Pod pod-2f06c46b-a443-46a3-8478-5c6eab0f8c7d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:29:09.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5636" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":17,"skipped":450,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:29:09.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 19 22:29:09.272: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 22:30:09.331: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jan 19 22:30:09.349: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 19 22:30:09.356: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 19 22:30:09.375: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 19 22:30:09.385: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 19 22:30:09.405: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 19 22:30:09.414: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 19 22:30:09.429: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 19 22:30:09.435: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:30:25.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9680" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.306 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":18,"skipped":469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:30:25.538: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-pfkf
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 22:30:25.580: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pfkf" in namespace "subpath-5458" to be "Succeeded or Failed"
Jan 19 22:30:25.583: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.755157ms
Jan 19 22:30:27.587: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007719295s
Jan 19 22:30:29.592: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 4.012752374s
Jan 19 22:30:31.595: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015730245s
Jan 19 22:30:33.601: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 8.021099552s
Jan 19 22:30:35.605: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 10.025734757s
Jan 19 22:30:37.609: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 12.029378895s
Jan 19 22:30:39.613: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 14.033188267s
Jan 19 22:30:41.618: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 16.03844733s
Jan 19 22:30:43.623: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 18.043109008s
Jan 19 22:30:45.626: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 20.045906705s
Jan 19 22:30:47.631: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Running", Reason="", readiness=true. Elapsed: 22.051845756s
Jan 19 22:30:49.636: INFO: Pod "pod-subpath-test-configmap-pfkf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.056178266s
STEP: Saw pod success
Jan 19 22:30:49.636: INFO: Pod "pod-subpath-test-configmap-pfkf" satisfied condition "Succeeded or Failed"
Jan 19 22:30:49.638: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-subpath-test-configmap-pfkf container test-container-subpath-configmap-pfkf: <nil>
STEP: delete the pod
Jan 19 22:30:49.657: INFO: Waiting for pod pod-subpath-test-configmap-pfkf to disappear
Jan 19 22:30:49.664: INFO: Pod pod-subpath-test-configmap-pfkf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pfkf
Jan 19 22:30:49.664: INFO: Deleting pod "pod-subpath-test-configmap-pfkf" in namespace "subpath-5458"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:30:49.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5458" for this suite.

• [SLOW TEST:24.133 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":19,"skipped":506,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:30:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:30:49.710: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 22:30:54.716: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 19 22:30:54.723: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 19 22:30:54.738: INFO: observed ReplicaSet test-rs in namespace replicaset-4375 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 22:30:54.757: INFO: observed ReplicaSet test-rs in namespace replicaset-4375 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 22:30:54.789: INFO: observed ReplicaSet test-rs in namespace replicaset-4375 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 22:30:54.796: INFO: observed ReplicaSet test-rs in namespace replicaset-4375 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 22:30:56.829: INFO: observed ReplicaSet test-rs in namespace replicaset-4375 with ReadyReplicas 2, AvailableReplicas 2
Jan 19 22:30:56.952: INFO: observed Replicaset test-rs in namespace replicaset-4375 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:30:56.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4375" for this suite.

• [SLOW TEST:7.287 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":20,"skipped":514,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:30:56.959: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 19 22:30:57.000: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:31:02.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-569" for this suite.

• [SLOW TEST:5.276 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:31:02.236: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4764
STEP: creating service affinity-nodeport-transition in namespace services-4764
STEP: creating replication controller affinity-nodeport-transition in namespace services-4764
I0119 22:31:02.313340      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4764, replica count: 3
I0119 22:31:05.364794      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 22:31:05.372: INFO: Creating new exec pod
Jan 19 22:31:10.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4764 exec execpod-affinitywx47j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 19 22:31:10.558: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 19 22:31:10.558: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:31:10.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4764 exec execpod-affinitywx47j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.3.23 80'
Jan 19 22:31:10.721: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.3.23 80\nConnection to 10.104.3.23 80 port [tcp/http] succeeded!\n"
Jan 19 22:31:10.721: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:31:10.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4764 exec execpod-affinitywx47j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.62 30130'
Jan 19 22:31:10.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.62 30130\nConnection to 10.0.1.62 30130 port [tcp/*] succeeded!\n"
Jan 19 22:31:10.864: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:31:10.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4764 exec execpod-affinitywx47j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.103 30130'
Jan 19 22:31:11.014: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.103 30130\nConnection to 10.0.1.103 30130 port [tcp/*] succeeded!\n"
Jan 19 22:31:11.014: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:31:11.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4764 exec execpod-affinitywx47j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.103:30130/ ; done'
Jan 19 22:31:11.270: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n"
Jan 19 22:31:11.270: INFO: stdout: "\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-lx4vz\naffinity-nodeport-transition-lx4vz\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-lx4vz\naffinity-nodeport-transition-6fs6c\naffinity-nodeport-transition-6fs6c\naffinity-nodeport-transition-lx4vz\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-lx4vz\naffinity-nodeport-transition-6fs6c\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-lx4vz\naffinity-nodeport-transition-lx4vz\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-6fs6c"
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-lx4vz
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-lx4vz
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-lx4vz
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-6fs6c
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-6fs6c
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-lx4vz
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-lx4vz
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-6fs6c
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-lx4vz
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-lx4vz
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.270: INFO: Received response from host: affinity-nodeport-transition-6fs6c
Jan 19 22:31:11.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4764 exec execpod-affinitywx47j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.103:30130/ ; done'
Jan 19 22:31:11.514: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30130/\n"
Jan 19 22:31:11.514: INFO: stdout: "\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l\naffinity-nodeport-transition-gbt9l"
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Received response from host: affinity-nodeport-transition-gbt9l
Jan 19 22:31:11.514: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4764, will wait for the garbage collector to delete the pods
Jan 19 22:31:11.583: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.65474ms
Jan 19 22:31:11.684: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.939992ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:31:13.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4764" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.683 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":22,"skipped":557,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:31:13.919: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:31:13.977: INFO: Create a RollingUpdate DaemonSet
Jan 19 22:31:13.982: INFO: Check that daemon pods launch on every node of the cluster
Jan 19 22:31:13.985: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:13.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:31:13.987: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 22:31:14.990: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:14.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:31:14.993: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 22:31:15.992: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:15.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:31:15.994: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 22:31:16.991: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:16.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 22:31:16.993: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Jan 19 22:31:16.993: INFO: Update the DaemonSet to trigger a rollout
Jan 19 22:31:17.001: INFO: Updating DaemonSet daemon-set
Jan 19 22:31:20.012: INFO: Roll back the DaemonSet before rollout is complete
Jan 19 22:31:20.020: INFO: Updating DaemonSet daemon-set
Jan 19 22:31:20.020: INFO: Make sure DaemonSet rollback is complete
Jan 19 22:31:20.023: INFO: Wrong image for pod: daemon-set-87xjp. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 19 22:31:20.023: INFO: Pod daemon-set-87xjp is not available
Jan 19 22:31:20.026: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:21.032: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:22.032: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:23.034: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:31:24.032: INFO: Pod daemon-set-bfgs5 is not available
Jan 19 22:31:24.035: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9682, will wait for the garbage collector to delete the pods
Jan 19 22:31:24.097: INFO: Deleting DaemonSet.extensions daemon-set took: 4.641341ms
Jan 19 22:31:24.198: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.217758ms
Jan 19 22:31:26.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:31:26.600: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 22:31:26.602: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60737"},"items":null}

Jan 19 22:31:26.604: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60737"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:31:26.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9682" for this suite.

• [SLOW TEST:12.701 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":23,"skipped":561,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:31:26.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:33:00.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1294" for this suite.

• [SLOW TEST:94.071 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":24,"skipped":654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:33:00.695: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2600
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jan 19 22:33:00.822: INFO: Found 0 stateful pods, waiting for 3
Jan 19 22:33:10.827: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:33:10.827: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:33:10.827: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 19 22:33:10.850: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 19 22:33:20.879: INFO: Updating stateful set ss2
Jan 19 22:33:20.883: INFO: Waiting for Pod statefulset-2600/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jan 19 22:33:30.943: INFO: Found 2 stateful pods, waiting for 3
Jan 19 22:33:40.948: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:33:40.948: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:33:40.948: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 19 22:33:40.971: INFO: Updating stateful set ss2
Jan 19 22:33:40.975: INFO: Waiting for Pod statefulset-2600/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 19 22:33:51.003: INFO: Updating stateful set ss2
Jan 19 22:33:51.013: INFO: Waiting for StatefulSet statefulset-2600/ss2 to complete update
Jan 19 22:33:51.013: INFO: Waiting for Pod statefulset-2600/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 19 22:34:01.024: INFO: Deleting all statefulset in ns statefulset-2600
Jan 19 22:34:01.026: INFO: Scaling statefulset ss2 to 0
Jan 19 22:34:11.047: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:34:11.048: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:34:11.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2600" for this suite.

• [SLOW TEST:70.371 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":25,"skipped":693,"failed":0}
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:34:11.066: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 19 22:34:11.111: INFO: Waiting up to 5m0s for pod "downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d" in namespace "downward-api-2093" to be "Succeeded or Failed"
Jan 19 22:34:11.113: INFO: Pod "downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.880701ms
Jan 19 22:34:13.115: INFO: Pod "downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004186562s
Jan 19 22:34:15.120: INFO: Pod "downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008382571s
STEP: Saw pod success
Jan 19 22:34:15.120: INFO: Pod "downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d" satisfied condition "Succeeded or Failed"
Jan 19 22:34:15.121: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d container dapi-container: <nil>
STEP: delete the pod
Jan 19 22:34:15.156: INFO: Waiting for pod downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d to disappear
Jan 19 22:34:15.158: INFO: Pod downward-api-465e26ac-d34c-4cf5-bf93-f9446cbbe85d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:34:15.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2093" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:34:15.165: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:36:01.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3815" for this suite.

• [SLOW TEST:106.059 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":27,"skipped":778,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:36:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-47a72333-2db1-4b94-9174-5d5dffcdb349
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:36:05.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4486" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":779,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:36:05.339: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:36:05.367: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 19 22:36:10.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-3555 --namespace=crd-publish-openapi-3555 create -f -'
Jan 19 22:36:12.447: INFO: stderr: ""
Jan 19 22:36:12.447: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 22:36:12.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-3555 --namespace=crd-publish-openapi-3555 delete e2e-test-crd-publish-openapi-2966-crds test-cr'
Jan 19 22:36:12.503: INFO: stderr: ""
Jan 19 22:36:12.503: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 19 22:36:12.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-3555 --namespace=crd-publish-openapi-3555 apply -f -'
Jan 19 22:36:12.713: INFO: stderr: ""
Jan 19 22:36:12.713: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 22:36:12.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-3555 --namespace=crd-publish-openapi-3555 delete e2e-test-crd-publish-openapi-2966-crds test-cr'
Jan 19 22:36:12.770: INFO: stderr: ""
Jan 19 22:36:12.770: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 19 22:36:12.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-3555 explain e2e-test-crd-publish-openapi-2966-crds'
Jan 19 22:36:12.973: INFO: stderr: ""
Jan 19 22:36:12.973: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2966-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:36:18.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3555" for this suite.

• [SLOW TEST:12.753 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":29,"skipped":779,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:36:18.092: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-8gw8
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 22:36:18.137: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8gw8" in namespace "subpath-6606" to be "Succeeded or Failed"
Jan 19 22:36:18.139: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188969ms
Jan 19 22:36:20.141: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004564109s
Jan 19 22:36:22.144: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 4.007756638s
Jan 19 22:36:24.148: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 6.011078628s
Jan 19 22:36:26.151: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 8.01427173s
Jan 19 22:36:28.156: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 10.019823084s
Jan 19 22:36:30.162: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 12.02517741s
Jan 19 22:36:32.169: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 14.032553666s
Jan 19 22:36:34.174: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 16.037442056s
Jan 19 22:36:36.178: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 18.041134834s
Jan 19 22:36:38.182: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 20.045216647s
Jan 19 22:36:40.185: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Running", Reason="", readiness=true. Elapsed: 22.048083228s
Jan 19 22:36:42.192: INFO: Pod "pod-subpath-test-projected-8gw8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055773487s
STEP: Saw pod success
Jan 19 22:36:42.192: INFO: Pod "pod-subpath-test-projected-8gw8" satisfied condition "Succeeded or Failed"
Jan 19 22:36:42.194: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-subpath-test-projected-8gw8 container test-container-subpath-projected-8gw8: <nil>
STEP: delete the pod
Jan 19 22:36:42.215: INFO: Waiting for pod pod-subpath-test-projected-8gw8 to disappear
Jan 19 22:36:42.217: INFO: Pod pod-subpath-test-projected-8gw8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-8gw8
Jan 19 22:36:42.217: INFO: Deleting pod "pod-subpath-test-projected-8gw8" in namespace "subpath-6606"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:36:42.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6606" for this suite.

• [SLOW TEST:24.133 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":30,"skipped":779,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:36:42.225: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 22:36:42.287: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:36:42.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:36:42.289: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 22:36:43.292: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:36:43.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:36:43.295: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 22:36:44.293: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:36:44.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:36:44.295: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 22:36:45.293: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:36:45.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 22:36:45.296: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Jan 19 22:36:45.314: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62173"},"items":null}

Jan 19 22:36:45.317: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62173"},"items":[{"metadata":{"name":"daemon-set-7pn4t","generateName":"daemon-set-","namespace":"daemonsets-4067","uid":"844ab374-f112-45b2-904a-c5a29a101cdd","resourceVersion":"62164","creationTimestamp":"2022-01-19T22:36:42Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"171d9fa27b1d4b2bc7f57285289da4ac0bc12fe893993551199c1be3806cb339","cni.projectcalico.org/podIP":"172.16.146.52/32","cni.projectcalico.org/podIPs":"172.16.146.52/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3d5b5ae-21db-42b1-9391-e75db43cc9f9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3d5b5ae-21db-42b1-9391-e75db43cc9f9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hj9kc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hj9kc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-153.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-153.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"}],"hostIP":"10.0.1.153","podIP":"172.16.146.52","podIPs":[{"ip":"172.16.146.52"}],"startTime":"2022-01-19T22:36:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-01-19T22:36:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://75d88e616af681f116b0d94f8179925127f3a73b056b4510293ea43f05fd0f53","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-94mpr","generateName":"daemon-set-","namespace":"daemonsets-4067","uid":"27e4dda5-c303-40c8-92c7-720a3f307d08","resourceVersion":"62168","creationTimestamp":"2022-01-19T22:36:42Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e4c44c0d35622a15ab19500a7053df1762075424b7651031f9626bce7893641c","cni.projectcalico.org/podIP":"172.16.214.252/32","cni.projectcalico.org/podIPs":"172.16.214.252/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3d5b5ae-21db-42b1-9391-e75db43cc9f9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3d5b5ae-21db-42b1-9391-e75db43cc9f9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.214.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6q6bw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6q6bw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-103.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-103.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"}],"hostIP":"10.0.1.103","podIP":"172.16.214.252","podIPs":[{"ip":"172.16.214.252"}],"startTime":"2022-01-19T22:36:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-01-19T22:36:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://aa57ab362c1631a49d2586360166b3a7bf367de7b78aeb699a75941c2352cc70","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-j65fx","generateName":"daemon-set-","namespace":"daemonsets-4067","uid":"cd0f43a1-66db-4ee0-8d35-ee1518e209f7","resourceVersion":"62166","creationTimestamp":"2022-01-19T22:36:42Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"9975cd69b286aa7b8a5900822a5b4eaba1969ac47e66011dc7ed72934aacc993","cni.projectcalico.org/podIP":"172.16.32.174/32","cni.projectcalico.org/podIPs":"172.16.32.174/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3d5b5ae-21db-42b1-9391-e75db43cc9f9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3d5b5ae-21db-42b1-9391-e75db43cc9f9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.32.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bvkdh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bvkdh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-62.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-62.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"}],"hostIP":"10.0.1.62","podIP":"172.16.32.174","podIPs":[{"ip":"172.16.32.174"}],"startTime":"2022-01-19T22:36:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-01-19T22:36:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://8fdf36b1a160d2226941131ae36713895459e4f11fc239bfcedc2de8a7a7a6b5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lsbzr","generateName":"daemon-set-","namespace":"daemonsets-4067","uid":"eb46e74f-6803-434a-84d7-2c114fc10802","resourceVersion":"62170","creationTimestamp":"2022-01-19T22:36:42Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0ed3d1ac75ee1a8527fd2c01e02f1909401f4df9690ea5285fe195d18ba77e29","cni.projectcalico.org/podIP":"172.16.138.31/32","cni.projectcalico.org/podIPs":"172.16.138.31/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d3d5b5ae-21db-42b1-9391-e75db43cc9f9","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3d5b5ae-21db-42b1-9391-e75db43cc9f9\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-01-19T22:36:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.138.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-chv6d","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-chv6d","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-1-48.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-1-48.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-01-19T22:36:42Z"}],"hostIP":"10.0.1.48","podIP":"172.16.138.31","podIPs":[{"ip":"172.16.138.31"}],"startTime":"2022-01-19T22:36:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-01-19T22:36:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://a02d9bd4636f53f782f702e3df96d32c0a8b280ea20e050f7f2cea6bfb8194b8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:36:45.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4067" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":31,"skipped":800,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:36:45.355: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-422a0fba-7d82-42e7-ba72-aa71902cc6ce
STEP: Creating a pod to test consume configMaps
Jan 19 22:36:45.391: INFO: Waiting up to 5m0s for pod "pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb" in namespace "configmap-9269" to be "Succeeded or Failed"
Jan 19 22:36:45.393: INFO: Pod "pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161559ms
Jan 19 22:36:47.398: INFO: Pod "pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00714916s
Jan 19 22:36:49.402: INFO: Pod "pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010588027s
STEP: Saw pod success
Jan 19 22:36:49.402: INFO: Pod "pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb" satisfied condition "Succeeded or Failed"
Jan 19 22:36:49.404: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb container agnhost-container: <nil>
STEP: delete the pod
Jan 19 22:36:49.425: INFO: Waiting for pod pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb to disappear
Jan 19 22:36:49.427: INFO: Pod pod-configmaps-b0b1499d-73d4-4b18-b79f-b979176a18bb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:36:49.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9269" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":804,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:36:49.433: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 19 22:36:49.465: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 19 22:37:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:37:13.530: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:37:33.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9889" for this suite.

• [SLOW TEST:43.845 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":33,"skipped":839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:37:33.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-38bc0d9a-3196-4232-8853-34d9bf74c5dd
STEP: Creating a pod to test consume secrets
Jan 19 22:37:33.325: INFO: Waiting up to 5m0s for pod "pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5" in namespace "secrets-5977" to be "Succeeded or Failed"
Jan 19 22:37:33.330: INFO: Pod "pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.714354ms
Jan 19 22:37:35.333: INFO: Pod "pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008016696s
Jan 19 22:37:37.337: INFO: Pod "pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011715018s
STEP: Saw pod success
Jan 19 22:37:37.337: INFO: Pod "pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5" satisfied condition "Succeeded or Failed"
Jan 19 22:37:37.339: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 22:37:37.351: INFO: Waiting for pod pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5 to disappear
Jan 19 22:37:37.359: INFO: Pod pod-secrets-cd6e7940-da3e-45e2-9565-4316868c46f5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:37:37.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5977" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":868,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:37:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:37:43.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-75" for this suite.
STEP: Destroying namespace "nsdeletetest-3560" for this suite.
Jan 19 22:37:43.506: INFO: Namespace nsdeletetest-3560 was already deleted
STEP: Destroying namespace "nsdeletetest-6864" for this suite.

• [SLOW TEST:6.144 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":35,"skipped":872,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:37:43.509: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4131
STEP: creating service affinity-clusterip-transition in namespace services-4131
STEP: creating replication controller affinity-clusterip-transition in namespace services-4131
I0119 22:37:43.606106      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4131, replica count: 3
I0119 22:37:46.656930      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 22:37:46.660: INFO: Creating new exec pod
Jan 19 22:37:51.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4131 exec execpod-affinityg6m8v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 19 22:37:51.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 19 22:37:51.845: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:37:51.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4131 exec execpod-affinityg6m8v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.133.35 80'
Jan 19 22:37:51.998: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.133.35 80\nConnection to 10.96.133.35 80 port [tcp/http] succeeded!\n"
Jan 19 22:37:51.998: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:37:52.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4131 exec execpod-affinityg6m8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.133.35:80/ ; done'
Jan 19 22:37:52.236: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n"
Jan 19 22:37:52.236: INFO: stdout: "\naffinity-clusterip-transition-gp7ps\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-2xwgj\naffinity-clusterip-transition-gp7ps\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-2xwgj\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-2xwgj\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-2xwgj\naffinity-clusterip-transition-gp7ps\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-2xwgj\naffinity-clusterip-transition-2xwgj\naffinity-clusterip-transition-2xwgj\naffinity-clusterip-transition-2xwgj"
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-gp7ps
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-gp7ps
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-gp7ps
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.236: INFO: Received response from host: affinity-clusterip-transition-2xwgj
Jan 19 22:37:52.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4131 exec execpod-affinityg6m8v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.133.35:80/ ; done'
Jan 19 22:37:52.476: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.133.35:80/\n"
Jan 19 22:37:52.476: INFO: stdout: "\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22\naffinity-clusterip-transition-h5x22"
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Received response from host: affinity-clusterip-transition-h5x22
Jan 19 22:37:52.476: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4131, will wait for the garbage collector to delete the pods
Jan 19 22:37:52.540: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.913423ms
Jan 19 22:37:52.641: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.507413ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:37:55.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4131" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.865 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":36,"skipped":876,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:37:55.375: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:37:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6162" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":37,"skipped":893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:37:55.409: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 19 22:37:55.448: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 19 22:37:55.467: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:37:55.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7291" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":38,"skipped":916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:37:55.489: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:37:55.519: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 19 22:38:00.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 create -f -'
Jan 19 22:38:02.626: INFO: stderr: ""
Jan 19 22:38:02.626: INFO: stdout: "e2e-test-crd-publish-openapi-6209-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 22:38:02.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 delete e2e-test-crd-publish-openapi-6209-crds test-foo'
Jan 19 22:38:02.683: INFO: stderr: ""
Jan 19 22:38:02.683: INFO: stdout: "e2e-test-crd-publish-openapi-6209-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 19 22:38:02.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 apply -f -'
Jan 19 22:38:02.892: INFO: stderr: ""
Jan 19 22:38:02.892: INFO: stdout: "e2e-test-crd-publish-openapi-6209-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 22:38:02.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 delete e2e-test-crd-publish-openapi-6209-crds test-foo'
Jan 19 22:38:02.949: INFO: stderr: ""
Jan 19 22:38:02.949: INFO: stdout: "e2e-test-crd-publish-openapi-6209-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Jan 19 22:38:02.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 create -f -'
Jan 19 22:38:03.141: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 19 22:38:03.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 create -f -'
Jan 19 22:38:03.333: INFO: rc: 1
Jan 19 22:38:03.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 apply -f -'
Jan 19 22:38:03.533: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 19 22:38:03.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 create -f -'
Jan 19 22:38:03.721: INFO: rc: 1
Jan 19 22:38:03.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 --namespace=crd-publish-openapi-1409 apply -f -'
Jan 19 22:38:03.914: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 19 22:38:03.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 explain e2e-test-crd-publish-openapi-6209-crds'
Jan 19 22:38:04.136: INFO: stderr: ""
Jan 19 22:38:04.136: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6209-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 19 22:38:04.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 explain e2e-test-crd-publish-openapi-6209-crds.metadata'
Jan 19 22:38:04.362: INFO: stderr: ""
Jan 19 22:38:04.362: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6209-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 19 22:38:04.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 explain e2e-test-crd-publish-openapi-6209-crds.spec'
Jan 19 22:38:04.559: INFO: stderr: ""
Jan 19 22:38:04.559: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6209-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 19 22:38:04.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 explain e2e-test-crd-publish-openapi-6209-crds.spec.bars'
Jan 19 22:38:04.764: INFO: stderr: ""
Jan 19 22:38:04.764: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6209-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 19 22:38:04.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-1409 explain e2e-test-crd-publish-openapi-6209-crds.spec.bars2'
Jan 19 22:38:04.956: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:10.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1409" for this suite.

• [SLOW TEST:14.615 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":39,"skipped":941,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:10.104: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:38:10.131: INFO: Creating pod...
Jan 19 22:38:10.140: INFO: Pod Quantity: 1 Status: Pending
Jan 19 22:38:11.144: INFO: Pod Quantity: 1 Status: Pending
Jan 19 22:38:12.144: INFO: Pod Quantity: 1 Status: Pending
Jan 19 22:38:13.143: INFO: Pod Status: Running
Jan 19 22:38:13.143: INFO: Creating service...
Jan 19 22:38:13.151: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/pods/agnhost/proxy/some/path/with/DELETE
Jan 19 22:38:13.162: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 22:38:13.162: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/pods/agnhost/proxy/some/path/with/GET
Jan 19 22:38:13.169: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 19 22:38:13.169: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/pods/agnhost/proxy/some/path/with/HEAD
Jan 19 22:38:13.172: INFO: http.Client request:HEAD | StatusCode:200
Jan 19 22:38:13.172: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 19 22:38:13.174: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 22:38:13.174: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/pods/agnhost/proxy/some/path/with/PATCH
Jan 19 22:38:13.176: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 22:38:13.176: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/pods/agnhost/proxy/some/path/with/POST
Jan 19 22:38:13.182: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 22:38:13.182: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/pods/agnhost/proxy/some/path/with/PUT
Jan 19 22:38:13.184: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 19 22:38:13.184: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/services/test-service/proxy/some/path/with/DELETE
Jan 19 22:38:13.187: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 22:38:13.187: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/services/test-service/proxy/some/path/with/GET
Jan 19 22:38:13.190: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 19 22:38:13.190: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/services/test-service/proxy/some/path/with/HEAD
Jan 19 22:38:13.193: INFO: http.Client request:HEAD | StatusCode:200
Jan 19 22:38:13.193: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/services/test-service/proxy/some/path/with/OPTIONS
Jan 19 22:38:13.197: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 22:38:13.197: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/services/test-service/proxy/some/path/with/PATCH
Jan 19 22:38:13.199: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 22:38:13.199: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/services/test-service/proxy/some/path/with/POST
Jan 19 22:38:13.202: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 22:38:13.202: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4350/services/test-service/proxy/some/path/with/PUT
Jan 19 22:38:13.205: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:13.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4350" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":40,"skipped":945,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:13.212: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:38:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:19.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4564" for this suite.

• [SLOW TEST:6.496 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":41,"skipped":956,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:19.708: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 19 22:38:19.773: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:38:21.778: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:38:23.776: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 19 22:38:23.786: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:38:25.788: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:38:27.792: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 19 22:38:27.799: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 22:38:27.801: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 22:38:29.802: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 22:38:29.806: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:29.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8335" for this suite.

• [SLOW TEST:10.120 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":958,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:29.829: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 19 22:38:29.868: INFO: The status of Pod pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:38:31.875: INFO: The status of Pod pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:38:33.872: INFO: The status of Pod pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 19 22:38:34.391: INFO: Successfully updated pod "pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1"
Jan 19 22:38:34.391: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1" in namespace "pods-4873" to be "terminated due to deadline exceeded"
Jan 19 22:38:34.393: INFO: Pod "pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1": Phase="Running", Reason="", readiness=true. Elapsed: 2.332973ms
Jan 19 22:38:36.398: INFO: Pod "pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.007285167s
Jan 19 22:38:36.398: INFO: Pod "pod-update-activedeadlineseconds-98a19e7f-94d5-479b-8008-042a80565ef1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:36.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4873" for this suite.

• [SLOW TEST:6.575 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":960,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:36.404: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jan 19 22:38:36.442: INFO: Waiting up to 5m0s for pod "client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0" in namespace "containers-9473" to be "Succeeded or Failed"
Jan 19 22:38:36.444: INFO: Pod "client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.813565ms
Jan 19 22:38:38.449: INFO: Pod "client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00619034s
Jan 19 22:38:40.454: INFO: Pod "client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011108305s
STEP: Saw pod success
Jan 19 22:38:40.454: INFO: Pod "client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0" satisfied condition "Succeeded or Failed"
Jan 19 22:38:40.455: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 22:38:40.477: INFO: Waiting for pod client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0 to disappear
Jan 19 22:38:40.479: INFO: Pod client-containers-f120b074-a6ec-4d38-8c7e-492c3e01c4e0 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:40.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9473" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":968,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:40.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:56.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3380" for this suite.

• [SLOW TEST:16.106 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":45,"skipped":974,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:56.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:38:56.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7220" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":46,"skipped":995,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:38:56.662: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:39:11.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2575" for this suite.
STEP: Destroying namespace "nsdeletetest-5284" for this suite.
Jan 19 22:39:11.863: INFO: Namespace nsdeletetest-5284 was already deleted
STEP: Destroying namespace "nsdeletetest-1622" for this suite.

• [SLOW TEST:15.205 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":47,"skipped":1002,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:39:11.867: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-8bb6603a-1f35-4ccc-b2d1-4858baba4183
STEP: Creating a pod to test consume configMaps
Jan 19 22:39:11.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154" in namespace "configmap-8032" to be "Succeeded or Failed"
Jan 19 22:39:11.915: INFO: Pod "pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.381868ms
Jan 19 22:39:13.920: INFO: Pod "pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00666811s
Jan 19 22:39:15.926: INFO: Pod "pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012589276s
STEP: Saw pod success
Jan 19 22:39:15.926: INFO: Pod "pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154" satisfied condition "Succeeded or Failed"
Jan 19 22:39:15.927: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 22:39:15.954: INFO: Waiting for pod pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154 to disappear
Jan 19 22:39:15.956: INFO: Pod pod-configmaps-d361b73a-dfed-454d-b915-8d480942b154 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:39:15.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8032" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":1019,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:39:15.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:39:15.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:39:19.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6102" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":49,"skipped":1032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:39:19.115: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:39:19.484: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 22:39:21.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 39, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 39, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 39, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 39, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:39:24.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:39:24.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7786" for this suite.
STEP: Destroying namespace "webhook-7786-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.722 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":50,"skipped":1087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:39:24.837: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:39:24.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7819" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":51,"skipped":1113,"failed":0}
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:39:24.899: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jan 19 22:39:24.932: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 22:40:24.983: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:40:24.985: INFO: Starting informer...
STEP: Starting pods...
Jan 19 22:40:25.201: INFO: Pod1 is running on ip-10-0-1-153.eu-central-1.compute.internal. Tainting Node
Jan 19 22:40:29.416: INFO: Pod2 is running on ip-10-0-1-153.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 19 22:40:36.647: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 19 22:40:55.884: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:40:55.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7503" for this suite.

• [SLOW TEST:91.023 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":52,"skipped":1120,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:40:55.922: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2607
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2607
Jan 19 22:40:55.998: INFO: Found 0 stateful pods, waiting for 1
Jan 19 22:41:06.002: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jan 19 22:41:06.016: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jan 19 22:41:06.021: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jan 19 22:41:06.022: INFO: Observed &StatefulSet event: ADDED
Jan 19 22:41:06.022: INFO: Found Statefulset ss in namespace statefulset-2607 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 22:41:06.022: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jan 19 22:41:06.022: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 22:41:06.026: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jan 19 22:41:06.028: INFO: Observed &StatefulSet event: ADDED
Jan 19 22:41:06.028: INFO: Observed Statefulset ss in namespace statefulset-2607 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 22:41:06.028: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 19 22:41:06.028: INFO: Deleting all statefulset in ns statefulset-2607
Jan 19 22:41:06.034: INFO: Scaling statefulset ss to 0
Jan 19 22:41:16.050: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:41:16.052: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:41:16.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2607" for this suite.

• [SLOW TEST:20.148 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":53,"skipped":1120,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:41:16.071: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 19 22:41:16.111: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:41:18.115: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:41:20.114: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 19 22:41:21.128: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:41:22.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5530" for this suite.

• [SLOW TEST:6.079 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":54,"skipped":1129,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:41:22.150: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-4114/configmap-test-6de928f0-0ca7-40be-beea-d6c6111276b3
STEP: Creating a pod to test consume configMaps
Jan 19 22:41:22.197: INFO: Waiting up to 5m0s for pod "pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536" in namespace "configmap-4114" to be "Succeeded or Failed"
Jan 19 22:41:22.200: INFO: Pod "pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536": Phase="Pending", Reason="", readiness=false. Elapsed: 1.874325ms
Jan 19 22:41:24.206: INFO: Pod "pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007758826s
Jan 19 22:41:26.209: INFO: Pod "pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011549722s
STEP: Saw pod success
Jan 19 22:41:26.209: INFO: Pod "pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536" satisfied condition "Succeeded or Failed"
Jan 19 22:41:26.211: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536 container env-test: <nil>
STEP: delete the pod
Jan 19 22:41:26.238: INFO: Waiting for pod pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536 to disappear
Jan 19 22:41:26.240: INFO: Pod pod-configmaps-f73495a6-ab50-4564-9b40-359a7c56a536 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:41:26.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4114" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":1148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:41:26.246: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:41:26.333: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"64d748be-dd1e-4d2b-ad66-6f46c8348c73", Controller:(*bool)(0xc002a3e266), BlockOwnerDeletion:(*bool)(0xc002a3e267)}}
Jan 19 22:41:26.342: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"69357574-8041-463d-a19b-c7e3095ca9f7", Controller:(*bool)(0xc002e1c246), BlockOwnerDeletion:(*bool)(0xc002e1c247)}}
Jan 19 22:41:26.347: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"616c47a7-d608-4976-9756-8185e3414ccd", Controller:(*bool)(0xc002ba7186), BlockOwnerDeletion:(*bool)(0xc002ba7187)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:41:31.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7404" for this suite.

• [SLOW TEST:5.120 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":56,"skipped":1176,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:41:31.367: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:41:31.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4117" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":57,"skipped":1185,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:41:31.478: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 19 22:41:36.032: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6252 pod-service-account-f0c305d8-f6f4-4a12-963d-4968ecd6cae1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 19 22:41:36.180: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6252 pod-service-account-f0c305d8-f6f4-4a12-963d-4968ecd6cae1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 19 22:41:36.332: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6252 pod-service-account-f0c305d8-f6f4-4a12-963d-4968ecd6cae1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:41:36.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6252" for this suite.

• [SLOW TEST:5.025 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":58,"skipped":1204,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:41:36.503: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9063.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 22:41:40.574: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.576: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.578: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.581: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.583: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.585: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.587: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.589: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:40.589: INFO: Lookups using dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local]

Jan 19 22:41:45.592: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.594: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.596: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.598: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.600: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.602: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.605: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.607: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:45.607: INFO: Lookups using dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local]

Jan 19 22:41:50.593: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.596: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.598: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.600: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.602: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.604: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.606: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.608: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:50.608: INFO: Lookups using dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local]

Jan 19 22:41:55.592: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.595: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.597: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.599: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.601: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.603: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.606: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.607: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:41:55.608: INFO: Lookups using dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local]

Jan 19 22:42:00.593: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.595: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.597: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.599: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.601: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.603: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.606: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.608: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:00.608: INFO: Lookups using dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local]

Jan 19 22:42:05.593: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.595: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.597: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.599: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.601: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.604: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.606: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.608: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local from pod dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40: the server could not find the requested resource (get pods dns-test-29575878-e26d-48a8-a685-7e1742fc4d40)
Jan 19 22:42:05.608: INFO: Lookups using dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9063.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9063.svc.cluster.local jessie_udp@dns-test-service-2.dns-9063.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9063.svc.cluster.local]

Jan 19 22:42:10.607: INFO: DNS probes using dns-9063/dns-test-29575878-e26d-48a8-a685-7e1742fc4d40 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:42:10.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9063" for this suite.

• [SLOW TEST:34.154 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":59,"skipped":1206,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:42:10.657: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1537
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 19 22:42:10.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-3450 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jan 19 22:42:10.759: INFO: stderr: ""
Jan 19 22:42:10.759: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
Jan 19 22:42:10.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-3450 delete pods e2e-test-httpd-pod'
Jan 19 22:42:13.946: INFO: stderr: ""
Jan 19 22:42:13.946: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:42:13.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3450" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":60,"skipped":1214,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:42:13.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-8060/secret-test-07a638e5-6882-4cc9-b02e-d38d08da82f7
STEP: Creating a pod to test consume secrets
Jan 19 22:42:13.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072" in namespace "secrets-8060" to be "Succeeded or Failed"
Jan 19 22:42:13.998: INFO: Pod "pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.425966ms
Jan 19 22:42:16.002: INFO: Pod "pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005559944s
Jan 19 22:42:18.004: INFO: Pod "pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008414271s
STEP: Saw pod success
Jan 19 22:42:18.004: INFO: Pod "pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072" satisfied condition "Succeeded or Failed"
Jan 19 22:42:18.006: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072 container env-test: <nil>
STEP: delete the pod
Jan 19 22:42:18.017: INFO: Waiting for pod pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072 to disappear
Jan 19 22:42:18.025: INFO: Pod pod-configmaps-0ff1fa97-67a3-400d-8a61-e8c09c97c072 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:42:18.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8060" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1220,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:42:18.031: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:42:34.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1240" for this suite.

• [SLOW TEST:16.111 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":62,"skipped":1223,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:42:34.143: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 22:42:34.181: INFO: Waiting up to 5m0s for pod "downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add" in namespace "downward-api-6891" to be "Succeeded or Failed"
Jan 19 22:42:34.183: INFO: Pod "downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041414ms
Jan 19 22:42:36.186: INFO: Pod "downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005142631s
Jan 19 22:42:38.191: INFO: Pod "downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010421574s
STEP: Saw pod success
Jan 19 22:42:38.191: INFO: Pod "downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add" satisfied condition "Succeeded or Failed"
Jan 19 22:42:38.193: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add container client-container: <nil>
STEP: delete the pod
Jan 19 22:42:38.214: INFO: Waiting for pod downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add to disappear
Jan 19 22:42:38.216: INFO: Pod downwardapi-volume-357b2e2c-610c-453c-a6f5-e86d4f275add no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:42:38.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6891" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1239,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:42:38.223: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5372.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5372.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5372.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5372.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 220.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.220_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5372.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5372.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5372.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5372.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5372.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 220.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.153.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.153.220_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 22:42:42.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.296: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.298: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.300: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.311: INFO: Unable to read jessie_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.314: INFO: Unable to read jessie_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.316: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.318: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:42.328: INFO: Lookups using dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b failed for: [wheezy_udp@dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_udp@dns-test-service.dns-5372.svc.cluster.local jessie_tcp@dns-test-service.dns-5372.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local]

Jan 19 22:42:47.332: INFO: Unable to read wheezy_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.334: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.336: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.349: INFO: Unable to read jessie_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.353: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.355: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:47.363: INFO: Lookups using dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b failed for: [wheezy_udp@dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_udp@dns-test-service.dns-5372.svc.cluster.local jessie_tcp@dns-test-service.dns-5372.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local]

Jan 19 22:42:52.333: INFO: Unable to read wheezy_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.335: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.337: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.349: INFO: Unable to read jessie_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.353: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.355: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:52.364: INFO: Lookups using dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b failed for: [wheezy_udp@dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_udp@dns-test-service.dns-5372.svc.cluster.local jessie_tcp@dns-test-service.dns-5372.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local]

Jan 19 22:42:57.333: INFO: Unable to read wheezy_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.335: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.337: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.350: INFO: Unable to read jessie_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.356: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:42:57.365: INFO: Lookups using dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b failed for: [wheezy_udp@dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_udp@dns-test-service.dns-5372.svc.cluster.local jessie_tcp@dns-test-service.dns-5372.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local]

Jan 19 22:43:02.333: INFO: Unable to read wheezy_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.336: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.338: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.340: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.350: INFO: Unable to read jessie_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.356: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:02.364: INFO: Lookups using dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b failed for: [wheezy_udp@dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_udp@dns-test-service.dns-5372.svc.cluster.local jessie_tcp@dns-test-service.dns-5372.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local]

Jan 19 22:43:07.332: INFO: Unable to read wheezy_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.334: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.336: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.338: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.349: INFO: Unable to read jessie_udp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.353: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.355: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local from pod dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b: the server could not find the requested resource (get pods dns-test-d21764e4-47ee-4bab-abf0-17d47543981b)
Jan 19 22:43:07.364: INFO: Lookups using dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b failed for: [wheezy_udp@dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@dns-test-service.dns-5372.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_udp@dns-test-service.dns-5372.svc.cluster.local jessie_tcp@dns-test-service.dns-5372.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5372.svc.cluster.local]

Jan 19 22:43:12.364: INFO: DNS probes using dns-5372/dns-test-d21764e4-47ee-4bab-abf0-17d47543981b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:43:12.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5372" for this suite.

• [SLOW TEST:34.204 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":64,"skipped":1292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:43:12.428: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 22:43:15.478: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:43:15.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-703" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":65,"skipped":1377,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:43:15.498: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 19 22:45:16.067: INFO: Successfully updated pod "var-expansion-fe8a2343-2629-4cbe-94c3-56700dd1e713"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 19 22:45:18.073: INFO: Deleting pod "var-expansion-fe8a2343-2629-4cbe-94c3-56700dd1e713" in namespace "var-expansion-8515"
Jan 19 22:45:18.077: INFO: Wait up to 5m0s for pod "var-expansion-fe8a2343-2629-4cbe-94c3-56700dd1e713" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:45:50.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8515" for this suite.

• [SLOW TEST:154.593 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":66,"skipped":1382,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:45:50.091: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 19 22:45:50.134: INFO: Waiting up to 5m0s for pod "security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44" in namespace "security-context-7120" to be "Succeeded or Failed"
Jan 19 22:45:50.136: INFO: Pod "security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44": Phase="Pending", Reason="", readiness=false. Elapsed: 1.938562ms
Jan 19 22:45:52.140: INFO: Pod "security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005880562s
Jan 19 22:45:54.145: INFO: Pod "security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010884303s
STEP: Saw pod success
Jan 19 22:45:54.145: INFO: Pod "security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44" satisfied condition "Succeeded or Failed"
Jan 19 22:45:54.147: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44 container test-container: <nil>
STEP: delete the pod
Jan 19 22:45:54.188: INFO: Waiting for pod security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44 to disappear
Jan 19 22:45:54.190: INFO: Pod security-context-82c3a22f-936f-4ce7-bebb-b650d1bb8d44 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:45:54.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7120" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":67,"skipped":1387,"failed":0}
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:45:54.199: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:45:54.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6426" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":68,"skipped":1392,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:45:54.237: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 19 22:45:58.288: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1639 PodName:pod-sharedvolume-cd835082-2ff1-437c-b622-d47bc18d72c3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 22:45:58.288: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:45:58.288: INFO: ExecWithOptions: Clientset creation
Jan 19 22:45:58.288: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1639/pods/pod-sharedvolume-cd835082-2ff1-437c-b622-d47bc18d72c3/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 22:45:58.388: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:45:58.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1639" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":69,"skipped":1393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:45:58.396: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:45:58.441: INFO: The status of Pod busybox-readonly-fs84feac47-7bfe-4d8a-894c-43e8333f0c00 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:46:00.443: INFO: The status of Pod busybox-readonly-fs84feac47-7bfe-4d8a-894c-43e8333f0c00 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:46:02.450: INFO: The status of Pod busybox-readonly-fs84feac47-7bfe-4d8a-894c-43e8333f0c00 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:46:02.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2584" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:46:02.465: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-832
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-832
STEP: Waiting until pod test-pod will start running in namespace statefulset-832
STEP: Creating statefulset with conflicting port in namespace statefulset-832
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-832
Jan 19 22:46:06.544: INFO: Observed stateful pod in namespace: statefulset-832, name: ss-0, uid: e0811bc8-1c3b-40b0-a9c3-c163ef08df66, status phase: Pending. Waiting for statefulset controller to delete.
Jan 19 22:46:06.563: INFO: Observed stateful pod in namespace: statefulset-832, name: ss-0, uid: e0811bc8-1c3b-40b0-a9c3-c163ef08df66, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 22:46:06.577: INFO: Observed stateful pod in namespace: statefulset-832, name: ss-0, uid: e0811bc8-1c3b-40b0-a9c3-c163ef08df66, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 22:46:06.578: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-832
STEP: Removing pod with conflicting port in namespace statefulset-832
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-832 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 19 22:46:10.611: INFO: Deleting all statefulset in ns statefulset-832
Jan 19 22:46:10.613: INFO: Scaling statefulset ss to 0
Jan 19 22:46:20.626: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:46:20.628: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:46:20.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-832" for this suite.

• [SLOW TEST:18.181 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":71,"skipped":1462,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:46:20.646: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 19 22:46:20.680: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 22:46:20.687: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 22:46:20.689: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-103.eu-central-1.compute.internal before test
Jan 19 22:46:20.698: INFO: cert-manager-774679ff99-fg5j9 from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container cert-manager ready: true, restart count 0
Jan 19 22:46:20.698: INFO: gatekeeper-audit-59fcc9f644-7fmkr from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container manager ready: true, restart count 0
Jan 19 22:46:20.698: INFO: gatekeeper-controller-manager-86677cdd6d-59r5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container manager ready: true, restart count 0
Jan 19 22:46:20.698: INFO: calico-node-rpdnw from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 22:46:20.698: INFO: kube-proxy-l9kpk from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 22:46:20.698: INFO: minio-0 from kube-system started at 2022-01-19 22:19:21 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container minio ready: true, restart count 0
Jan 19 22:46:20.698: INFO: velero-restic-dr6p6 from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container restic ready: true, restart count 0
Jan 19 22:46:20.698: INFO: elasticsearch-0 from logging started at 2022-01-19 22:19:23 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container elasticsearch ready: true, restart count 0
Jan 19 22:46:20.698: INFO: 	Container exporter ready: true, restart count 0
Jan 19 22:46:20.698: INFO: fluentbit-zwj8m from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 22:46:20.698: INFO: alertmanager-main-2 from monitoring started at 2022-01-19 22:40:34 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 22:46:20.698: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:46:20.698: INFO: goldpinger-96ctd from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 22:46:20.698: INFO: kube-proxy-metrics-wbqwc from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 22:46:20.698: INFO: node-exporter-tqbmk from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:46:20.698: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 22:46:20.698: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csw5s from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:46:20.698: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:46:20.698: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-153.eu-central-1.compute.internal before test
Jan 19 22:46:20.707: INFO: calico-node-c7xgd from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 22:46:20.707: INFO: kube-proxy-9p6s2 from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 22:46:20.707: INFO: velero-restic-7pr54 from kube-system started at 2022-01-19 22:40:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container restic ready: true, restart count 0
Jan 19 22:46:20.707: INFO: busybox-readonly-fs84feac47-7bfe-4d8a-894c-43e8333f0c00 from kubelet-test-2584 started at 2022-01-19 22:45:58 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container busybox-readonly-fs84feac47-7bfe-4d8a-894c-43e8333f0c00 ready: true, restart count 0
Jan 19 22:46:20.707: INFO: fluentbit-nz7dc from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 22:46:20.707: INFO: fluentd-1 from logging started at 2022-01-19 22:40:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container fluentd ready: true, restart count 0
Jan 19 22:46:20.707: INFO: goldpinger-9n9xc from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 22:46:20.707: INFO: kube-proxy-metrics-nx74g from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 22:46:20.707: INFO: node-exporter-ptzzd from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:46:20.707: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 22:46:20.707: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-nbt7p from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.707: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:46:20.707: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:46:20.707: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-48.eu-central-1.compute.internal before test
Jan 19 22:46:20.717: INFO: cert-manager-cainjector-66fcc559bf-wgm9t from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container cainjector ready: true, restart count 0
Jan 19 22:46:20.717: INFO: gatekeeper-controller-manager-86677cdd6d-fps5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container manager ready: true, restart count 0
Jan 19 22:46:20.717: INFO: gatekeeper-policy-manager-7644bb5475-cwmpf from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container gatekeeper-policy-manager ready: true, restart count 0
Jan 19 22:46:20.717: INFO: nginx-ingress-controller-6bcf978d9-w2wlr from ingress-nginx started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 22:46:20.717: INFO: calico-node-c8np5 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 22:46:20.717: INFO: kube-proxy-prc9k from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 22:46:20.717: INFO: velero-restic-nkgcd from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container restic ready: true, restart count 0
Jan 19 22:46:20.717: INFO: cerebro-6f88ff7888-plncf from logging started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container cerebro ready: true, restart count 0
Jan 19 22:46:20.717: INFO: fluentbit-wt6cx from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 22:46:20.717: INFO: fluentd-0 from logging started at 2022-01-19 22:19:52 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container fluentd ready: true, restart count 1
Jan 19 22:46:20.717: INFO: alertmanager-main-0 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 22:46:20.717: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:46:20.717: INFO: goldpinger-g9xm2 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 22:46:20.717: INFO: kube-proxy-metrics-z42ff from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 22:46:20.717: INFO: node-exporter-w54w5 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:46:20.717: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 22:46:20.717: INFO: prometheus-k8s-0 from monitoring started at 2022-01-19 22:21:16 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:46:20.717: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 22:46:20.717: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-qdwj2 from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:46:20.717: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:46:20.717: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-62.eu-central-1.compute.internal before test
Jan 19 22:46:20.729: INFO: cert-manager-webhook-77ffc44fc6-8fxlk from cert-manager started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container webhook ready: true, restart count 0
Jan 19 22:46:20.729: INFO: gatekeeper-controller-manager-86677cdd6d-mnx56 from gatekeeper-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container manager ready: true, restart count 0
Jan 19 22:46:20.729: INFO: calico-kube-controllers-59c967558b-nwcpg from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 22:46:20.729: INFO: calico-node-jbfb7 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 22:46:20.729: INFO: coredns-64897985d-cftqm from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container coredns ready: true, restart count 0
Jan 19 22:46:20.729: INFO: coredns-64897985d-l9c4t from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container coredns ready: true, restart count 0
Jan 19 22:46:20.729: INFO: kube-proxy-7shjp from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 22:46:20.729: INFO: metrics-server-6f5f78d676-mnlwp from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 22:46:20.729: INFO: velero-6f84f7465-tgzks from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container velero ready: true, restart count 0
Jan 19 22:46:20.729: INFO: velero-restic-6xdjj from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container restic ready: true, restart count 0
Jan 19 22:46:20.729: INFO: local-path-provisioner-5b94755fb4-8zcbz from local-path-storage started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jan 19 22:46:20.729: INFO: fluentbit-sj8hh from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 22:46:20.729: INFO: fluentd-2 from logging started at 2022-01-19 22:20:56 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container fluentd ready: true, restart count 0
Jan 19 22:46:20.729: INFO: kibana-8565fbd49b-pm9nr from logging started at 2022-01-19 22:40:29 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container kibana ready: true, restart count 0
Jan 19 22:46:20.729: INFO: 	Container kibana-index-patterns ready: true, restart count 0
Jan 19 22:46:20.729: INFO: alertmanager-main-1 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 22:46:20.729: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:46:20.729: INFO: goldpinger-ml8vn from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 22:46:20.729: INFO: grafana-7c96db944d-cq4n6 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container dashboard-sidecar ready: true, restart count 0
Jan 19 22:46:20.729: INFO: 	Container grafana ready: true, restart count 0
Jan 19 22:46:20.729: INFO: kube-proxy-metrics-lvbg9 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 22:46:20.729: INFO: kube-state-metrics-7456544d4b-wjgk8 from monitoring started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 22:46:20.729: INFO: node-exporter-x46g7 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:46:20.729: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 22:46:20.729: INFO: prometheus-operator-7db7f75fdb-624bl from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 22:46:20.729: INFO: sonobuoy from sonobuoy started at 2022-01-19 22:27:04 +0000 UTC (1 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 22:46:20.729: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csbhg from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 22:46:20.729: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:46:20.729: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-200aa906-113f-43aa-923d-4c5ecf63048b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-200aa906-113f-43aa-923d-4c5ecf63048b off the node ip-10-0-1-153.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-200aa906-113f-43aa-923d-4c5ecf63048b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:46:28.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5621" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.170 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":72,"skipped":1470,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:46:28.817: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:46:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2646" for this suite.

• [SLOW TEST:8.045 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":73,"skipped":1484,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:46:36.863: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jan 19 22:46:36.901: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 19 22:46:36.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 create -f -'
Jan 19 22:46:38.382: INFO: stderr: ""
Jan 19 22:46:38.382: INFO: stdout: "service/agnhost-replica created\n"
Jan 19 22:46:38.382: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 19 22:46:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 create -f -'
Jan 19 22:46:38.595: INFO: stderr: ""
Jan 19 22:46:38.595: INFO: stdout: "service/agnhost-primary created\n"
Jan 19 22:46:38.595: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 19 22:46:38.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 create -f -'
Jan 19 22:46:38.822: INFO: stderr: ""
Jan 19 22:46:38.822: INFO: stdout: "service/frontend created\n"
Jan 19 22:46:38.822: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 19 22:46:38.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 create -f -'
Jan 19 22:46:39.025: INFO: stderr: ""
Jan 19 22:46:39.025: INFO: stdout: "deployment.apps/frontend created\n"
Jan 19 22:46:39.025: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 22:46:39.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 create -f -'
Jan 19 22:46:39.250: INFO: stderr: ""
Jan 19 22:46:39.250: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 19 22:46:39.251: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 22:46:39.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 create -f -'
Jan 19 22:46:39.471: INFO: stderr: ""
Jan 19 22:46:39.471: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 19 22:46:39.471: INFO: Waiting for all frontend pods to be Running.
Jan 19 22:46:44.523: INFO: Waiting for frontend to serve content.
Jan 19 22:46:44.533: INFO: Trying to add a new entry to the guestbook.
Jan 19 22:46:44.540: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 19 22:46:44.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 delete --grace-period=0 --force -f -'
Jan 19 22:46:44.607: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 22:46:44.607: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 22:46:44.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 delete --grace-period=0 --force -f -'
Jan 19 22:46:44.672: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 22:46:44.672: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 22:46:44.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 delete --grace-period=0 --force -f -'
Jan 19 22:46:44.779: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 22:46:44.779: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 22:46:44.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 delete --grace-period=0 --force -f -'
Jan 19 22:46:44.835: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 22:46:44.835: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 22:46:44.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 delete --grace-period=0 --force -f -'
Jan 19 22:46:44.893: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 22:46:44.893: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 22:46:44.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-236 delete --grace-period=0 --force -f -'
Jan 19 22:46:44.949: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 22:46:44.949: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:46:44.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-236" for this suite.

• [SLOW TEST:8.102 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":74,"skipped":1488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:46:44.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-8428af86-4a1d-4730-9b26-1dc13fa58370
STEP: Creating a pod to test consume secrets
Jan 19 22:46:45.009: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259" in namespace "projected-7866" to be "Succeeded or Failed"
Jan 19 22:46:45.014: INFO: Pod "pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259": Phase="Pending", Reason="", readiness=false. Elapsed: 5.22205ms
Jan 19 22:46:47.017: INFO: Pod "pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008028532s
Jan 19 22:46:49.020: INFO: Pod "pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011498731s
STEP: Saw pod success
Jan 19 22:46:49.020: INFO: Pod "pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259" satisfied condition "Succeeded or Failed"
Jan 19 22:46:49.022: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 22:46:49.043: INFO: Waiting for pod pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259 to disappear
Jan 19 22:46:49.045: INFO: Pod pod-projected-secrets-20b29d50-73c1-41a8-9414-ee464b65c259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:46:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7866" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1515,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:46:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3565, will wait for the garbage collector to delete the pods
Jan 19 22:46:53.145: INFO: Deleting Job.batch foo took: 3.063412ms
Jan 19 22:46:53.246: INFO: Terminating Job.batch foo pods took: 101.040816ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:47:25.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3565" for this suite.

• [SLOW TEST:36.406 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":76,"skipped":1533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:47:25.458: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1621
Jan 19 22:47:25.511: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:47:27.515: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:47:29.518: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 19 22:47:29.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1621 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 19 22:47:29.669: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 19 22:47:29.669: INFO: stdout: "iptables"
Jan 19 22:47:29.669: INFO: proxyMode: iptables
Jan 19 22:47:29.677: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 19 22:47:29.679: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1621
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1621
I0119 22:47:29.714783      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1621, replica count: 3
I0119 22:47:32.765898      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 22:47:32.772: INFO: Creating new exec pod
Jan 19 22:47:37.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1621 exec execpod-affinity9vm5q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 19 22:47:37.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 19 22:47:37.944: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:47:37.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1621 exec execpod-affinity9vm5q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.14.42 80'
Jan 19 22:47:38.090: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.14.42 80\nConnection to 10.106.14.42 80 port [tcp/http] succeeded!\n"
Jan 19 22:47:38.090: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:47:38.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1621 exec execpod-affinity9vm5q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.14.42:80/ ; done'
Jan 19 22:47:38.302: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n"
Jan 19 22:47:38.302: INFO: stdout: "\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf\naffinity-clusterip-timeout-vvmzf"
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Received response from host: affinity-clusterip-timeout-vvmzf
Jan 19 22:47:38.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1621 exec execpod-affinity9vm5q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.14.42:80/'
Jan 19 22:47:38.478: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n"
Jan 19 22:47:38.478: INFO: stdout: "affinity-clusterip-timeout-vvmzf"
Jan 19 22:47:58.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1621 exec execpod-affinity9vm5q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.106.14.42:80/'
Jan 19 22:47:58.636: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.106.14.42:80/\n"
Jan 19 22:47:58.636: INFO: stdout: "affinity-clusterip-timeout-6zb9s"
Jan 19 22:47:58.636: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1621, will wait for the garbage collector to delete the pods
Jan 19 22:47:58.706: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 3.23942ms
Jan 19 22:47:58.806: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.12696ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:48:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1621" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:36.070 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":77,"skipped":1558,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:48:01.529: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 19 22:48:01.564: INFO: Waiting up to 5m0s for pod "pod-640e1c3e-a7a3-449b-bdde-695463687634" in namespace "emptydir-9485" to be "Succeeded or Failed"
Jan 19 22:48:01.566: INFO: Pod "pod-640e1c3e-a7a3-449b-bdde-695463687634": Phase="Pending", Reason="", readiness=false. Elapsed: 1.861251ms
Jan 19 22:48:03.570: INFO: Pod "pod-640e1c3e-a7a3-449b-bdde-695463687634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005999367s
Jan 19 22:48:05.574: INFO: Pod "pod-640e1c3e-a7a3-449b-bdde-695463687634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009241665s
STEP: Saw pod success
Jan 19 22:48:05.574: INFO: Pod "pod-640e1c3e-a7a3-449b-bdde-695463687634" satisfied condition "Succeeded or Failed"
Jan 19 22:48:05.576: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-640e1c3e-a7a3-449b-bdde-695463687634 container test-container: <nil>
STEP: delete the pod
Jan 19 22:48:05.587: INFO: Waiting for pod pod-640e1c3e-a7a3-449b-bdde-695463687634 to disappear
Jan 19 22:48:05.596: INFO: Pod pod-640e1c3e-a7a3-449b-bdde-695463687634 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:48:05.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9485" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":78,"skipped":1575,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:48:05.602: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6363
STEP: creating service affinity-clusterip in namespace services-6363
STEP: creating replication controller affinity-clusterip in namespace services-6363
I0119 22:48:05.649029      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6363, replica count: 3
I0119 22:48:08.700822      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 22:48:08.705: INFO: Creating new exec pod
Jan 19 22:48:13.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-6363 exec execpod-affinityh9c4s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 19 22:48:14.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 19 22:48:14.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:48:14.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-6363 exec execpod-affinityh9c4s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.214.255 80'
Jan 19 22:48:14.936: INFO: stderr: "+ + echonc -v -t -w 2 10.102.214.255 80\n hostName\nConnection to 10.102.214.255 80 port [tcp/http] succeeded!\n"
Jan 19 22:48:14.936: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:48:14.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-6363 exec execpod-affinityh9c4s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.214.255:80/ ; done'
Jan 19 22:48:15.144: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.214.255:80/\n"
Jan 19 22:48:15.144: INFO: stdout: "\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x\naffinity-clusterip-4ml9x"
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.144: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.145: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.145: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.145: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.145: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.145: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.145: INFO: Received response from host: affinity-clusterip-4ml9x
Jan 19 22:48:15.145: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6363, will wait for the garbage collector to delete the pods
Jan 19 22:48:15.208: INFO: Deleting ReplicationController affinity-clusterip took: 3.139322ms
Jan 19 22:48:15.308: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.493365ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:48:17.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6363" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.832 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":79,"skipped":1585,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:48:17.434: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d8306ca6-8a4d-4463-ad6b-37dd542b481e
STEP: Creating the pod
Jan 19 22:48:17.483: INFO: The status of Pod pod-projected-configmaps-73d6340f-de0e-4d2b-a0c5-7610e044fd47 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:48:19.488: INFO: The status of Pod pod-projected-configmaps-73d6340f-de0e-4d2b-a0c5-7610e044fd47 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:48:21.486: INFO: The status of Pod pod-projected-configmaps-73d6340f-de0e-4d2b-a0c5-7610e044fd47 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-d8306ca6-8a4d-4463-ad6b-37dd542b481e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:49:23.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5519" for this suite.

• [SLOW TEST:66.314 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":80,"skipped":1591,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:49:23.749: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8535 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8535;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8535 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8535;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8535.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8535.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8535.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8535.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8535.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8535.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8535.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8535.svc;check="$$(dig +notcp +noall +answer +search 192.42.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.42.192_udp@PTR;check="$$(dig +tcp +noall +answer +search 192.42.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.42.192_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8535 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8535;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8535 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8535;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8535.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8535.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8535.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8535.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8535.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8535.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8535.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8535.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8535.svc;check="$$(dig +notcp +noall +answer +search 192.42.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.42.192_udp@PTR;check="$$(dig +tcp +noall +answer +search 192.42.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.42.192_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 22:49:27.846: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.848: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.851: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.853: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.855: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.857: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.872: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.874: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.876: INFO: Unable to read jessie_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.878: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.880: INFO: Unable to read jessie_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.882: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.884: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.887: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:27.895: INFO: Lookups using dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8535 wheezy_tcp@dns-test-service.dns-8535 wheezy_udp@dns-test-service.dns-8535.svc wheezy_tcp@dns-test-service.dns-8535.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8535 jessie_tcp@dns-test-service.dns-8535 jessie_udp@dns-test-service.dns-8535.svc jessie_tcp@dns-test-service.dns-8535.svc jessie_udp@_http._tcp.dns-test-service.dns-8535.svc jessie_tcp@_http._tcp.dns-test-service.dns-8535.svc]

Jan 19 22:49:32.900: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.902: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.904: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.906: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.908: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.910: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.924: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.927: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.929: INFO: Unable to read jessie_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.931: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.933: INFO: Unable to read jessie_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.935: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:32.947: INFO: Lookups using dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8535 wheezy_tcp@dns-test-service.dns-8535 wheezy_udp@dns-test-service.dns-8535.svc wheezy_tcp@dns-test-service.dns-8535.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8535 jessie_tcp@dns-test-service.dns-8535 jessie_udp@dns-test-service.dns-8535.svc jessie_tcp@dns-test-service.dns-8535.svc]

Jan 19 22:49:37.899: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.901: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.903: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.906: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.908: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.910: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.925: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.927: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.929: INFO: Unable to read jessie_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.931: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.933: INFO: Unable to read jessie_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:37.948: INFO: Lookups using dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8535 wheezy_tcp@dns-test-service.dns-8535 wheezy_udp@dns-test-service.dns-8535.svc wheezy_tcp@dns-test-service.dns-8535.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8535 jessie_tcp@dns-test-service.dns-8535 jessie_udp@dns-test-service.dns-8535.svc jessie_tcp@dns-test-service.dns-8535.svc]

Jan 19 22:49:42.899: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.901: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.903: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.906: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.908: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.910: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.925: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.928: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.930: INFO: Unable to read jessie_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.931: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.933: INFO: Unable to read jessie_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.935: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:42.948: INFO: Lookups using dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8535 wheezy_tcp@dns-test-service.dns-8535 wheezy_udp@dns-test-service.dns-8535.svc wheezy_tcp@dns-test-service.dns-8535.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8535 jessie_tcp@dns-test-service.dns-8535 jessie_udp@dns-test-service.dns-8535.svc jessie_tcp@dns-test-service.dns-8535.svc]

Jan 19 22:49:47.900: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.902: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.904: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.906: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.909: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.911: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.926: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.928: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.930: INFO: Unable to read jessie_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.932: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.934: INFO: Unable to read jessie_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.936: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:47.948: INFO: Lookups using dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8535 wheezy_tcp@dns-test-service.dns-8535 wheezy_udp@dns-test-service.dns-8535.svc wheezy_tcp@dns-test-service.dns-8535.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8535 jessie_tcp@dns-test-service.dns-8535 jessie_udp@dns-test-service.dns-8535.svc jessie_tcp@dns-test-service.dns-8535.svc]

Jan 19 22:49:52.898: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.900: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.902: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.904: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.908: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.923: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.925: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.926: INFO: Unable to read jessie_udp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.929: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535 from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.931: INFO: Unable to read jessie_udp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.933: INFO: Unable to read jessie_tcp@dns-test-service.dns-8535.svc from pod dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209: the server could not find the requested resource (get pods dns-test-b889641c-bc9c-4985-90f9-053d31f17209)
Jan 19 22:49:52.945: INFO: Lookups using dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8535 wheezy_tcp@dns-test-service.dns-8535 wheezy_udp@dns-test-service.dns-8535.svc wheezy_tcp@dns-test-service.dns-8535.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8535 jessie_tcp@dns-test-service.dns-8535 jessie_udp@dns-test-service.dns-8535.svc jessie_tcp@dns-test-service.dns-8535.svc]

Jan 19 22:49:57.953: INFO: DNS probes using dns-8535/dns-test-b889641c-bc9c-4985-90f9-053d31f17209 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:49:58.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8535" for this suite.

• [SLOW TEST:34.283 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":81,"skipped":1599,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:49:58.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:49:58.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5204" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":82,"skipped":1611,"failed":0}
SSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:49:58.092: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:50:02.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2649" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:50:02.158: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 19 22:50:02.196: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:02.196: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:02.210: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:02.210: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:02.223: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:02.223: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:02.267: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:02.267: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 22:50:04.318: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 19 22:50:04.318: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 19 22:50:04.359: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 19 22:50:04.368: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 0
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.370: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.380: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.380: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.400: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.400: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:04.410: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:04.410: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:04.427: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:04.428: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:06.423: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:06.424: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:06.453: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
STEP: listing Deployments
Jan 19 22:50:06.457: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 19 22:50:06.466: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 19 22:50:06.471: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:06.476: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:06.497: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:06.517: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:06.526: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:08.710: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:08.765: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:08.785: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:08.803: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:08.812: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 22:50:10.880: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 19 22:50:10.923: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:10.923: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:10.923: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 1
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 2
Jan 19 22:50:10.924: INFO: observed Deployment test-deployment in namespace deployment-2124 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 19 22:50:10.929: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
Jan 19 22:50:10.930: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 19 22:50:10.933: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 19 22:50:10.937: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-2124  1d90574f-2be0-4e70-a2b3-deb451b86faf 67521 4 2022-01-19 22:50:04 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 81531072-8205-479c-aee2-b27b65b662b3 0xc003ee22e7 0xc003ee22e8}] []  [{kube-controller-manager Update apps/v1 2022-01-19 22:50:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81531072-8205-479c-aee2-b27b65b662b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 22:50:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee2520 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 19 22:50:10.939: INFO: pod: "test-deployment-5ddd8b47d8-hl9wg":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-hl9wg test-deployment-5ddd8b47d8- deployment-2124  e0eb8c3b-5ee9-46e1-98fe-aa46bab03748 67516 0 2022-01-19 22:50:04 +0000 UTC 2022-01-19 22:50:11 +0000 UTC 0xc003ee3108 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:56391f15956b7d884262fc37533e54c3f7790e83491704d860df4ba4ceda0d65 cni.projectcalico.org/podIP:172.16.146.53/32 cni.projectcalico.org/podIPs:172.16.146.53/32] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 1d90574f-2be0-4e70-a2b3-deb451b86faf 0xc003ee3157 0xc003ee3158}] []  [{kube-controller-manager Update v1 2022-01-19 22:50:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d90574f-2be0-4e70-a2b3-deb451b86faf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 22:50:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 22:50:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8f7xd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8f7xd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:172.16.146.53,StartTime:2022-01-19 22:50:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 22:50:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:docker://1d208e9c86877e60190b45d60001193fe7714c02e72e56ecf5829d3cd0b1b790,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.146.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 22:50:10.940: INFO: pod: "test-deployment-5ddd8b47d8-qz5js":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-qz5js test-deployment-5ddd8b47d8- deployment-2124  0f78c564-6101-4dd5-9359-f99de2b99414 67485 0 2022-01-19 22:50:06 +0000 UTC 2022-01-19 22:50:09 +0000 UTC 0xc003ee3640 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:cf251c708b567cb845782b02221bc89f439b7dc8bb78a75bd2b73bca307ca80d cni.projectcalico.org/podIP:172.16.138.28/32 cni.projectcalico.org/podIPs:172.16.138.28/32] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 1d90574f-2be0-4e70-a2b3-deb451b86faf 0xc003ee3697 0xc003ee3698}] []  [{kube-controller-manager Update v1 2022-01-19 22:50:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d90574f-2be0-4e70-a2b3-deb451b86faf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 22:50:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 22:50:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.138.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hbvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hbvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:172.16.138.28,StartTime:2022-01-19 22:50:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 22:50:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:docker://45eaf832271ab36d364391e4fb32c499ca6c6d9226c76d9b59d60dc497f30d0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.138.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 22:50:10.940: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-2124  71dcc295-f106-4427-88fe-8b28550666f7 67399 3 2022-01-19 22:50:02 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 81531072-8205-479c-aee2-b27b65b662b3 0xc003ee25d7 0xc003ee25d8}] []  [{kube-controller-manager Update apps/v1 2022-01-19 22:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81531072-8205-479c-aee2-b27b65b662b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 22:50:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee2700 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 19 22:50:10.943: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-2124  e5e8dabd-11c7-48c7-ba01-201d4e1c2320 67513 2 2022-01-19 22:50:06 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 81531072-8205-479c-aee2-b27b65b662b3 0xc003ee2797 0xc003ee2798}] []  [{kube-controller-manager Update apps/v1 2022-01-19 22:50:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81531072-8205-479c-aee2-b27b65b662b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 22:50:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ee29a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 19 22:50:10.945: INFO: pod: "test-deployment-854fdc678-dxwj6":
&Pod{ObjectMeta:{test-deployment-854fdc678-dxwj6 test-deployment-854fdc678- deployment-2124  ca8437a2-75c4-4e05-9de6-ceab3b216deb 67512 0 2022-01-19 22:50:08 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:c41b1299f221027e5eb394533f22bbc2987612417146af96a38763a0c35b9265 cni.projectcalico.org/podIP:172.16.138.42/32 cni.projectcalico.org/podIPs:172.16.138.42/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 e5e8dabd-11c7-48c7-ba01-201d4e1c2320 0xc002fbe657 0xc002fbe658}] []  [{kube-controller-manager Update v1 2022-01-19 22:50:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5e8dabd-11c7-48c7-ba01-201d4e1c2320\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-01-19 22:50:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.138.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-01-19 22:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jp49v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jp49v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:172.16.138.42,StartTime:2022-01-19 22:50:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 22:50:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://63bcbe0ae101a6e21405542c9d3785d7078abe78ef0e59bb5aaf75bbcb721368,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.138.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 22:50:10.946: INFO: pod: "test-deployment-854fdc678-ptdbw":
&Pod{ObjectMeta:{test-deployment-854fdc678-ptdbw test-deployment-854fdc678- deployment-2124  b894dcd1-b354-49b5-a9d0-e59a4c938ed4 67468 0 2022-01-19 22:50:06 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:c77183623150dc0f359229531e4adcdc1cdbd78f97a15fac75e4c605183d00d3 cni.projectcalico.org/podIP:172.16.146.57/32 cni.projectcalico.org/podIPs:172.16.146.57/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 e5e8dabd-11c7-48c7-ba01-201d4e1c2320 0xc002fbe887 0xc002fbe888}] []  [{kube-controller-manager Update v1 2022-01-19 22:50:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5e8dabd-11c7-48c7-ba01-201d4e1c2320\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 22:50:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 22:50:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cszfz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cszfz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 22:50:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:172.16.146.57,StartTime:2022-01-19 22:50:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 22:50:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://d32a7a2dc85ef1efc98b62043514be476547162252ccc8bbaa58f012e1b0d98c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.146.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:50:10.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2124" for this suite.

• [SLOW TEST:8.795 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":84,"skipped":1668,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:50:10.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jan 19 22:50:10.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-3406 create -f -'
Jan 19 22:50:11.223: INFO: stderr: ""
Jan 19 22:50:11.223: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 19 22:50:12.226: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 22:50:12.226: INFO: Found 0 / 1
Jan 19 22:50:13.226: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 22:50:13.226: INFO: Found 0 / 1
Jan 19 22:50:14.228: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 22:50:14.228: INFO: Found 1 / 1
Jan 19 22:50:14.228: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 19 22:50:14.230: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 22:50:14.230: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 22:50:14.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-3406 patch pod agnhost-primary-9v2zj -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 19 22:50:14.295: INFO: stderr: ""
Jan 19 22:50:14.295: INFO: stdout: "pod/agnhost-primary-9v2zj patched\n"
STEP: checking annotations
Jan 19 22:50:14.297: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 22:50:14.297: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:50:14.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3406" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":85,"skipped":1674,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:50:14.304: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 22:50:14.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4" in namespace "projected-3000" to be "Succeeded or Failed"
Jan 19 22:50:14.370: INFO: Pod "downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034006ms
Jan 19 22:50:16.374: INFO: Pod "downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015996457s
Jan 19 22:50:18.377: INFO: Pod "downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019434507s
STEP: Saw pod success
Jan 19 22:50:18.377: INFO: Pod "downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4" satisfied condition "Succeeded or Failed"
Jan 19 22:50:18.379: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4 container client-container: <nil>
STEP: delete the pod
Jan 19 22:50:18.389: INFO: Waiting for pod downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4 to disappear
Jan 19 22:50:18.397: INFO: Pod downwardapi-volume-0ddce8e7-ca08-41c6-be13-b0b36f11fef4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:50:18.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3000" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1694,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:50:18.404: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:50:18.441: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:50:20.444: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:50:22.444: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:24.444: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:26.446: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:28.447: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:30.447: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:32.445: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:34.446: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:36.445: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:38.445: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = false)
Jan 19 22:50:40.447: INFO: The status of Pod test-webserver-0e71ae75-a8cd-426c-b9a4-3780bd7ded1a is Running (Ready = true)
Jan 19 22:50:40.449: INFO: Container started at 2022-01-19 22:50:20 +0000 UTC, pod became ready at 2022-01-19 22:50:38 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:50:40.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7142" for this suite.

• [SLOW TEST:22.051 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:50:40.456: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:50:40.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 22:50:42.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 50, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 50, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 50, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:50:45.940: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:50:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7603-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:50:49.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5586" for this suite.
STEP: Destroying namespace "webhook-5586-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.717 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":88,"skipped":1752,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:50:49.173: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:50:49.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2011" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":89,"skipped":1766,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:50:49.242: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1258
Jan 19 22:50:49.287: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:50:51.291: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:50:53.293: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 19 22:50:53.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 19 22:50:53.453: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 19 22:50:53.453: INFO: stdout: "iptables"
Jan 19 22:50:53.453: INFO: proxyMode: iptables
Jan 19 22:50:53.459: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 19 22:50:53.461: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1258
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1258
I0119 22:50:53.489871      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1258, replica count: 3
I0119 22:50:56.540730      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 22:50:56.548: INFO: Creating new exec pod
Jan 19 22:51:01.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 19 22:51:01.720: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 19 22:51:01.720: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:51:01.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.178.64 80'
Jan 19 22:51:01.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.178.64 80\nConnection to 10.106.178.64 80 port [tcp/http] succeeded!\n"
Jan 19 22:51:01.884: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:51:01.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.153 30658'
Jan 19 22:51:02.040: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.153 30658\nConnection to 10.0.1.153 30658 port [tcp/*] succeeded!\n"
Jan 19 22:51:02.040: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:51:02.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.103 30658'
Jan 19 22:51:02.192: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.103 30658\nConnection to 10.0.1.103 30658 port [tcp/*] succeeded!\n"
Jan 19 22:51:02.192: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:51:02.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.103:30658/ ; done'
Jan 19 22:51:02.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n"
Jan 19 22:51:02.404: INFO: stdout: "\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r\naffinity-nodeport-timeout-kmn5r"
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Received response from host: affinity-nodeport-timeout-kmn5r
Jan 19 22:51:02.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.103:30658/'
Jan 19 22:51:02.559: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n"
Jan 19 22:51:02.559: INFO: stdout: "affinity-nodeport-timeout-kmn5r"
Jan 19 22:51:22.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.103:30658/'
Jan 19 22:51:22.714: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n"
Jan 19 22:51:22.714: INFO: stdout: "affinity-nodeport-timeout-kmn5r"
Jan 19 22:51:42.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-1258 exec execpod-affinityjvvw4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.103:30658/'
Jan 19 22:51:42.874: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.103:30658/\n"
Jan 19 22:51:42.874: INFO: stdout: "affinity-nodeport-timeout-px9zt"
Jan 19 22:51:42.874: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1258, will wait for the garbage collector to delete the pods
Jan 19 22:51:42.943: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.021941ms
Jan 19 22:51:43.044: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.71391ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:51:45.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1258" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:56.644 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":90,"skipped":1777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:51:45.887: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:51:45.928: INFO: The status of Pod busybox-scheduling-11611ab2-ebc2-44c4-8fec-12acf4705854 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:51:47.933: INFO: The status of Pod busybox-scheduling-11611ab2-ebc2-44c4-8fec-12acf4705854 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:51:49.931: INFO: The status of Pod busybox-scheduling-11611ab2-ebc2-44c4-8fec-12acf4705854 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:51:49.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4727" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1814,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:51:49.956: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:51:50.395: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 22:51:52.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 51, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 51, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 51, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 51, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:51:55.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:52:07.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1673" for this suite.
STEP: Destroying namespace "webhook-1673-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.599 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":92,"skipped":1819,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:52:07.555: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jan 19 22:52:07.600: INFO: Waiting up to 5m0s for pod "test-pod-d0e7b162-2622-411b-848d-f1ab93394c57" in namespace "svcaccounts-721" to be "Succeeded or Failed"
Jan 19 22:52:07.606: INFO: Pod "test-pod-d0e7b162-2622-411b-848d-f1ab93394c57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.260676ms
Jan 19 22:52:09.610: INFO: Pod "test-pod-d0e7b162-2622-411b-848d-f1ab93394c57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009817549s
Jan 19 22:52:11.616: INFO: Pod "test-pod-d0e7b162-2622-411b-848d-f1ab93394c57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015875213s
STEP: Saw pod success
Jan 19 22:52:11.616: INFO: Pod "test-pod-d0e7b162-2622-411b-848d-f1ab93394c57" satisfied condition "Succeeded or Failed"
Jan 19 22:52:11.618: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod test-pod-d0e7b162-2622-411b-848d-f1ab93394c57 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 22:52:11.635: INFO: Waiting for pod test-pod-d0e7b162-2622-411b-848d-f1ab93394c57 to disappear
Jan 19 22:52:11.639: INFO: Pod test-pod-d0e7b162-2622-411b-848d-f1ab93394c57 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:52:11.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-721" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":93,"skipped":1820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:52:11.647: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 19 22:52:13.689: INFO: running pods: 0 < 1
Jan 19 22:52:15.695: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:52:17.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2761" for this suite.

• [SLOW TEST:6.082 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":94,"skipped":1846,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:52:17.729: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:52:17.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5027" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1848,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:52:17.807: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1331
STEP: creating the pod
Jan 19 22:52:17.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 create -f -'
Jan 19 22:52:19.146: INFO: stderr: ""
Jan 19 22:52:19.146: INFO: stdout: "pod/pause created\n"
Jan 19 22:52:19.146: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 19 22:52:19.146: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1651" to be "running and ready"
Jan 19 22:52:19.148: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.234671ms
Jan 19 22:52:21.150: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004343546s
Jan 19 22:52:23.163: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.016571364s
Jan 19 22:52:23.163: INFO: Pod "pause" satisfied condition "running and ready"
Jan 19 22:52:23.163: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 19 22:52:23.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 label pods pause testing-label=testing-label-value'
Jan 19 22:52:23.238: INFO: stderr: ""
Jan 19 22:52:23.238: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 19 22:52:23.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 get pod pause -L testing-label'
Jan 19 22:52:23.291: INFO: stderr: ""
Jan 19 22:52:23.291: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 19 22:52:23.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 label pods pause testing-label-'
Jan 19 22:52:23.353: INFO: stderr: ""
Jan 19 22:52:23.353: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 19 22:52:23.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 get pod pause -L testing-label'
Jan 19 22:52:23.403: INFO: stderr: ""
Jan 19 22:52:23.403: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1337
STEP: using delete to clean up resources
Jan 19 22:52:23.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 delete --grace-period=0 --force -f -'
Jan 19 22:52:23.462: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 22:52:23.462: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 19 22:52:23.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 get rc,svc -l name=pause --no-headers'
Jan 19 22:52:23.524: INFO: stderr: "No resources found in kubectl-1651 namespace.\n"
Jan 19 22:52:23.524: INFO: stdout: ""
Jan 19 22:52:23.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1651 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 22:52:23.577: INFO: stderr: ""
Jan 19 22:52:23.577: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:52:23.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1651" for this suite.

• [SLOW TEST:5.777 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1329
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":96,"skipped":1848,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:52:23.584: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-5a48f515-9835-48f4-aae3-53398c18b413
STEP: Creating a pod to test consume configMaps
Jan 19 22:52:23.626: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6017287-caed-4ee5-971c-647724941976" in namespace "configmap-6090" to be "Succeeded or Failed"
Jan 19 22:52:23.638: INFO: Pod "pod-configmaps-b6017287-caed-4ee5-971c-647724941976": Phase="Pending", Reason="", readiness=false. Elapsed: 10.965874ms
Jan 19 22:52:25.642: INFO: Pod "pod-configmaps-b6017287-caed-4ee5-971c-647724941976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015613269s
Jan 19 22:52:27.646: INFO: Pod "pod-configmaps-b6017287-caed-4ee5-971c-647724941976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018902145s
STEP: Saw pod success
Jan 19 22:52:27.646: INFO: Pod "pod-configmaps-b6017287-caed-4ee5-971c-647724941976" satisfied condition "Succeeded or Failed"
Jan 19 22:52:27.648: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-b6017287-caed-4ee5-971c-647724941976 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 22:52:27.666: INFO: Waiting for pod pod-configmaps-b6017287-caed-4ee5-971c-647724941976 to disappear
Jan 19 22:52:27.674: INFO: Pod pod-configmaps-b6017287-caed-4ee5-971c-647724941976 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:52:27.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6090" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1849,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:52:27.681: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jan 19 22:52:28.788: INFO: The status of Pod kube-controller-manager-ip-10-0-0-80.eu-central-1.compute.internal is Running (Ready = true)
Jan 19 22:52:29.080: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:52:29.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9036" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":98,"skipped":1860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:52:29.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-1098
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-1098
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1098
Jan 19 22:52:29.139: INFO: Found 0 stateful pods, waiting for 1
Jan 19 22:52:39.146: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 19 22:52:39.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-1098 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:52:39.313: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:52:39.313: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:52:39.313: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:52:39.315: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 22:52:49.321: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:52:49.321: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:52:49.343: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 22:52:49.343: INFO: ss-0  ip-10-0-1-153.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:29 +0000 UTC  }]
Jan 19 22:52:49.343: INFO: 
Jan 19 22:52:49.343: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 19 22:52:50.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997826829s
Jan 19 22:52:51.351: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993187375s
Jan 19 22:52:52.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98920596s
Jan 19 22:52:53.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984649365s
Jan 19 22:52:54.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980130672s
Jan 19 22:52:55.370: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974995655s
Jan 19 22:52:56.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970118356s
Jan 19 22:52:57.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966627309s
Jan 19 22:52:58.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.841024ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1098
Jan 19 22:52:59.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-1098 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 22:52:59.536: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 22:52:59.536: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 22:52:59.536: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 22:52:59.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-1098 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 22:52:59.704: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 22:52:59.704: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 22:52:59.704: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 22:52:59.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-1098 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 22:52:59.864: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 22:52:59.864: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 22:52:59.864: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 22:52:59.867: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 19 22:53:09.871: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:53:09.871: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:53:09.871: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 19 22:53:09.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-1098 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:53:10.025: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:53:10.025: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:53:10.025: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:53:10.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-1098 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:53:10.193: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:53:10.193: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:53:10.193: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:53:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-1098 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:53:10.359: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:53:10.359: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:53:10.359: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:53:10.359: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:53:10.363: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 19 22:53:20.368: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:53:20.368: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:53:20.368: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:53:20.378: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 22:53:20.378: INFO: ss-0  ip-10-0-1-153.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:29 +0000 UTC  }]
Jan 19 22:53:20.378: INFO: ss-1  ip-10-0-1-48.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:49 +0000 UTC  }]
Jan 19 22:53:20.378: INFO: ss-2  ip-10-0-1-62.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:49 +0000 UTC  }]
Jan 19 22:53:20.378: INFO: 
Jan 19 22:53:20.378: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 19 22:53:21.389: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 22:53:21.389: INFO: ss-0  ip-10-0-1-153.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:29 +0000 UTC  }]
Jan 19 22:53:21.389: INFO: ss-1  ip-10-0-1-48.eu-central-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:53:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 22:52:49 +0000 UTC  }]
Jan 19 22:53:21.389: INFO: 
Jan 19 22:53:21.389: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 19 22:53:22.391: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986449252s
Jan 19 22:53:23.394: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984323987s
Jan 19 22:53:24.398: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98145434s
Jan 19 22:53:25.403: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.977131925s
Jan 19 22:53:26.407: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971402228s
Jan 19 22:53:27.409: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968353957s
Jan 19 22:53:28.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.96612497s
Jan 19 22:53:29.417: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.620497ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1098
Jan 19 22:53:30.422: INFO: Scaling statefulset ss to 0
Jan 19 22:53:30.433: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 19 22:53:30.434: INFO: Deleting all statefulset in ns statefulset-1098
Jan 19 22:53:30.436: INFO: Scaling statefulset ss to 0
Jan 19 22:53:30.444: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:53:30.446: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:53:30.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1098" for this suite.

• [SLOW TEST:61.370 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":99,"skipped":1901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:53:30.466: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:53:30.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1494" for this suite.
STEP: Destroying namespace "nspatchtest-5af169ed-58db-4b4b-8464-21029a951971-2924" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":100,"skipped":1951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:53:30.547: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 19 22:53:35.097: INFO: Successfully updated pod "adopt-release-2kfcw"
STEP: Checking that the Job readopts the Pod
Jan 19 22:53:35.097: INFO: Waiting up to 15m0s for pod "adopt-release-2kfcw" in namespace "job-3005" to be "adopted"
Jan 19 22:53:35.099: INFO: Pod "adopt-release-2kfcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.62392ms
Jan 19 22:53:37.105: INFO: Pod "adopt-release-2kfcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.007983874s
Jan 19 22:53:37.105: INFO: Pod "adopt-release-2kfcw" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 19 22:53:37.616: INFO: Successfully updated pod "adopt-release-2kfcw"
STEP: Checking that the Job releases the Pod
Jan 19 22:53:37.616: INFO: Waiting up to 15m0s for pod "adopt-release-2kfcw" in namespace "job-3005" to be "released"
Jan 19 22:53:37.619: INFO: Pod "adopt-release-2kfcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.920102ms
Jan 19 22:53:39.629: INFO: Pod "adopt-release-2kfcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.012960413s
Jan 19 22:53:39.629: INFO: Pod "adopt-release-2kfcw" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:53:39.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3005" for this suite.

• [SLOW TEST:9.089 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":101,"skipped":2117,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:53:39.636: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:53:56.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8965" for this suite.

• [SLOW TEST:17.072 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":102,"skipped":2120,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:53:56.710: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:53:57.215: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 19 22:53:59.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 53, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 53, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 53, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 53, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:54:02.239: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 19 22:54:02.255: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:54:02.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9190" for this suite.
STEP: Destroying namespace "webhook-9190-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.602 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":103,"skipped":2125,"failed":0}
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:54:02.312: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2868
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2868
STEP: creating replication controller externalsvc in namespace services-2868
I0119 22:54:02.393147      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2868, replica count: 2
I0119 22:54:05.444351      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 19 22:54:05.469: INFO: Creating new exec pod
Jan 19 22:54:09.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2868 exec execpodqfc95 -- /bin/sh -x -c nslookup nodeport-service.services-2868.svc.cluster.local'
Jan 19 22:54:09.650: INFO: stderr: "+ nslookup nodeport-service.services-2868.svc.cluster.local\n"
Jan 19 22:54:09.650: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2868.svc.cluster.local\tcanonical name = externalsvc.services-2868.svc.cluster.local.\nName:\texternalsvc.services-2868.svc.cluster.local\nAddress: 10.100.136.250\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2868, will wait for the garbage collector to delete the pods
Jan 19 22:54:09.708: INFO: Deleting ReplicationController externalsvc took: 4.398637ms
Jan 19 22:54:09.809: INFO: Terminating ReplicationController externalsvc pods took: 101.099125ms
Jan 19 22:54:12.320: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:54:12.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2868" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.030 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":104,"skipped":2125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:54:12.343: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:54:14.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4905" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":105,"skipped":2168,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:54:14.419: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-676
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 22:54:14.452: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 22:54:14.509: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:54:16.514: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:54:18.512: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:20.512: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:22.512: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:24.513: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:26.513: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:28.513: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:30.515: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:32.513: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 22:54:34.512: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 19 22:54:34.516: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 19 22:54:34.520: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 19 22:54:34.524: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jan 19 22:54:38.547: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan 19 22:54:38.547: INFO: Breadth first check of 172.16.214.217 on host 10.0.1.103...
Jan 19 22:54:38.549: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.26:9080/dial?request=hostname&protocol=udp&host=172.16.214.217&port=8081&tries=1'] Namespace:pod-network-test-676 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 22:54:38.549: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:54:38.549: INFO: ExecWithOptions: Clientset creation
Jan 19 22:54:38.549: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-676/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.26%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.214.217%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 22:54:38.646: INFO: Waiting for responses: map[]
Jan 19 22:54:38.646: INFO: reached 172.16.214.217 after 0/1 tries
Jan 19 22:54:38.646: INFO: Breadth first check of 172.16.146.17 on host 10.0.1.153...
Jan 19 22:54:38.648: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.26:9080/dial?request=hostname&protocol=udp&host=172.16.146.17&port=8081&tries=1'] Namespace:pod-network-test-676 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 22:54:38.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:54:38.649: INFO: ExecWithOptions: Clientset creation
Jan 19 22:54:38.649: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-676/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.26%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.146.17%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 22:54:38.741: INFO: Waiting for responses: map[]
Jan 19 22:54:38.741: INFO: reached 172.16.146.17 after 0/1 tries
Jan 19 22:54:38.741: INFO: Breadth first check of 172.16.138.33 on host 10.0.1.48...
Jan 19 22:54:38.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.26:9080/dial?request=hostname&protocol=udp&host=172.16.138.33&port=8081&tries=1'] Namespace:pod-network-test-676 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 22:54:38.743: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:54:38.743: INFO: ExecWithOptions: Clientset creation
Jan 19 22:54:38.743: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-676/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.26%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.138.33%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 22:54:38.836: INFO: Waiting for responses: map[]
Jan 19 22:54:38.836: INFO: reached 172.16.138.33 after 0/1 tries
Jan 19 22:54:38.836: INFO: Breadth first check of 172.16.32.145 on host 10.0.1.62...
Jan 19 22:54:38.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.26:9080/dial?request=hostname&protocol=udp&host=172.16.32.145&port=8081&tries=1'] Namespace:pod-network-test-676 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 22:54:38.838: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 22:54:38.839: INFO: ExecWithOptions: Clientset creation
Jan 19 22:54:38.839: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-676/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.26%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.32.145%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 22:54:38.939: INFO: Waiting for responses: map[]
Jan 19 22:54:38.939: INFO: reached 172.16.32.145 after 0/1 tries
Jan 19 22:54:38.939: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:54:38.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-676" for this suite.

• [SLOW TEST:24.527 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":106,"skipped":2169,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:54:38.947: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6288
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6288
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6288
Jan 19 22:54:38.997: INFO: Found 0 stateful pods, waiting for 1
Jan 19 22:54:49.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 19 22:54:49.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:54:49.152: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:54:49.152: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:54:49.152: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:54:49.154: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 22:54:59.161: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:54:59.161: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:54:59.173: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998466s
Jan 19 22:55:00.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997212009s
Jan 19 22:55:01.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992583701s
Jan 19 22:55:02.186: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989842294s
Jan 19 22:55:03.191: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984688949s
Jan 19 22:55:04.195: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979658776s
Jan 19 22:55:05.200: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975032723s
Jan 19 22:55:06.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970728397s
Jan 19 22:55:07.208: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967461815s
Jan 19 22:55:08.211: INFO: Verifying statefulset ss doesn't scale past 1 for another 962.833515ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6288
Jan 19 22:55:09.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 22:55:09.359: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 22:55:09.359: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 22:55:09.359: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 22:55:09.362: INFO: Found 1 stateful pods, waiting for 3
Jan 19 22:55:19.366: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:55:19.366: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 22:55:19.366: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 19 22:55:19.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:55:19.516: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:55:19.516: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:55:19.516: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:55:19.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:55:19.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:55:19.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:55:19.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:55:19.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 22:55:19.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 22:55:19.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 22:55:19.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 22:55:19.843: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:55:19.845: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 19 22:55:29.856: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:55:29.856: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:55:29.856: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 22:55:29.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998323s
Jan 19 22:55:30.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99713323s
Jan 19 22:55:31.875: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993606207s
Jan 19 22:55:32.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988656477s
Jan 19 22:55:33.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98354905s
Jan 19 22:55:34.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979672457s
Jan 19 22:55:35.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975909131s
Jan 19 22:55:36.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971504694s
Jan 19 22:55:37.899: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967538048s
Jan 19 22:55:38.902: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.571718ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6288
Jan 19 22:55:39.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 22:55:40.051: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 22:55:40.051: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 22:55:40.051: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 22:55:40.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 22:55:40.215: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 22:55:40.215: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 22:55:40.215: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 22:55:40.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-6288 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 22:55:40.363: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 22:55:40.363: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 22:55:40.363: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 22:55:40.363: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 19 22:55:50.378: INFO: Deleting all statefulset in ns statefulset-6288
Jan 19 22:55:50.380: INFO: Scaling statefulset ss to 0
Jan 19 22:55:50.390: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 22:55:50.391: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:55:50.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6288" for this suite.

• [SLOW TEST:71.460 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":107,"skipped":2181,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:55:50.408: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:55:50.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 22:55:52.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 55, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 55, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 55, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 55, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:55:55.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:55:55.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4542" for this suite.
STEP: Destroying namespace "webhook-4542-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.589 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":108,"skipped":2181,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:55:55.997: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 22:55:56.640: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 22:55:58.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 22, 55, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 55, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 22, 55, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 22, 55, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 22:56:01.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:56:01.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7136" for this suite.
STEP: Destroying namespace "webhook-7136-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.826 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":109,"skipped":2189,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:56:01.823: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:56:01.872: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:56:02.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7840" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":110,"skipped":2206,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:56:02.895: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4582
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4582
I0119 22:56:02.968202      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4582, replica count: 2
Jan 19 22:56:06.019: INFO: Creating new exec pod
I0119 22:56:06.019286      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 22:56:11.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4582 exec execpod4d7h6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 22:56:11.214: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 22:56:11.214: INFO: stdout: ""
Jan 19 22:56:12.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4582 exec execpod4d7h6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 22:56:12.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 22:56:12.364: INFO: stdout: "externalname-service-rr5jm"
Jan 19 22:56:12.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4582 exec execpod4d7h6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.129.202 80'
Jan 19 22:56:12.516: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.129.202 80\nConnection to 10.96.129.202 80 port [tcp/http] succeeded!\n"
Jan 19 22:56:12.516: INFO: stdout: "externalname-service-rr5jm"
Jan 19 22:56:12.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4582 exec execpod4d7h6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.48 32621'
Jan 19 22:56:12.660: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.48 32621\nConnection to 10.0.1.48 32621 port [tcp/*] succeeded!\n"
Jan 19 22:56:12.660: INFO: stdout: "externalname-service-rr5jm"
Jan 19 22:56:12.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4582 exec execpod4d7h6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.103 32621'
Jan 19 22:56:12.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.103 32621\nConnection to 10.0.1.103 32621 port [tcp/*] succeeded!\n"
Jan 19 22:56:12.824: INFO: stdout: "externalname-service-pr2tz"
Jan 19 22:56:12.824: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:56:12.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4582" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.959 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":111,"skipped":2212,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:56:12.854: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-7b9ff3e1-b4a6-4591-a1b5-a42289198e4f
STEP: Creating secret with name s-test-opt-upd-04a11666-8495-48e8-9e5d-f650e93152f8
STEP: Creating the pod
Jan 19 22:56:12.908: INFO: The status of Pod pod-secrets-1f19c4c8-d415-4795-aa22-15f61337d9dc is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:56:14.911: INFO: The status of Pod pod-secrets-1f19c4c8-d415-4795-aa22-15f61337d9dc is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:56:16.916: INFO: The status of Pod pod-secrets-1f19c4c8-d415-4795-aa22-15f61337d9dc is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-7b9ff3e1-b4a6-4591-a1b5-a42289198e4f
STEP: Updating secret s-test-opt-upd-04a11666-8495-48e8-9e5d-f650e93152f8
STEP: Creating secret with name s-test-opt-create-21c1ebb4-057a-44bd-8399-9139e1963f56
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:57:21.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3066" for this suite.

• [SLOW TEST:68.361 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":2219,"failed":0}
SSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:57:21.215: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jan 19 22:57:21.248: INFO: created test-podtemplate-1
Jan 19 22:57:21.252: INFO: created test-podtemplate-2
Jan 19 22:57:21.256: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 19 22:57:21.262: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 19 22:57:21.269: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:57:21.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9222" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":113,"skipped":2225,"failed":0}
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:57:21.276: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jan 19 22:57:21.301: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 22:58:21.351: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:58:21.354: INFO: Starting informer...
STEP: Starting pod...
Jan 19 22:58:21.565: INFO: Pod is running on ip-10-0-1-153.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 19 22:58:21.583: INFO: Pod wasn't evicted. Proceeding
Jan 19 22:58:21.584: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 19 22:59:36.646: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:59:36.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5396" for this suite.

• [SLOW TEST:135.378 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":114,"skipped":2229,"failed":0}
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:59:36.654: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7008.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7008.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7008.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7008.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 22:59:40.720: INFO: DNS probes using dns-7008/dns-test-089c771a-ac67-4e20-96e7-0bdf0051af02 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:59:40.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7008" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":115,"skipped":2229,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:59:40.738: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 22:59:40.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed" in namespace "downward-api-8840" to be "Succeeded or Failed"
Jan 19 22:59:40.779: INFO: Pod "downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.952093ms
Jan 19 22:59:42.782: INFO: Pod "downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005171914s
Jan 19 22:59:44.785: INFO: Pod "downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008444739s
STEP: Saw pod success
Jan 19 22:59:44.785: INFO: Pod "downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed" satisfied condition "Succeeded or Failed"
Jan 19 22:59:44.787: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed container client-container: <nil>
STEP: delete the pod
Jan 19 22:59:44.816: INFO: Waiting for pod downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed to disappear
Jan 19 22:59:44.824: INFO: Pod downwardapi-volume-fe2ffc8d-8064-4467-823d-88085730f4ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:59:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8840" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2230,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:59:44.833: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jan 19 22:59:44.874: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 19 22:59:49.877: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jan 19 22:59:49.879: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:59:49.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6788" for this suite.

• [SLOW TEST:5.071 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":117,"skipped":2230,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:59:49.904: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 22:59:49.952: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 19 22:59:49.962: INFO: The status of Pod pod-exec-websocket-ce493471-8356-4cbc-a7a6-732a66e56c04 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:59:51.965: INFO: The status of Pod pod-exec-websocket-ce493471-8356-4cbc-a7a6-732a66e56c04 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:59:53.966: INFO: The status of Pod pod-exec-websocket-ce493471-8356-4cbc-a7a6-732a66e56c04 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:59:54.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7292" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2232,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:59:54.074: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-74bd7a48-58cb-49b5-9b98-c0effda5f160
STEP: Creating a pod to test consume secrets
Jan 19 22:59:54.129: INFO: Waiting up to 5m0s for pod "pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51" in namespace "secrets-2586" to be "Succeeded or Failed"
Jan 19 22:59:54.132: INFO: Pod "pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205233ms
Jan 19 22:59:56.135: INFO: Pod "pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005345355s
Jan 19 22:59:58.139: INFO: Pod "pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009502645s
STEP: Saw pod success
Jan 19 22:59:58.139: INFO: Pod "pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51" satisfied condition "Succeeded or Failed"
Jan 19 22:59:58.141: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51 container secret-env-test: <nil>
STEP: delete the pod
Jan 19 22:59:58.161: INFO: Waiting for pod pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51 to disappear
Jan 19 22:59:58.163: INFO: Pod pod-secrets-398321f2-cc71-4eb2-860a-e09ae8315a51 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 22:59:58.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2586" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2239,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 22:59:58.170: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 22:59:58.226: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:59:58.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:59:58.228: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 22:59:59.233: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 22:59:59.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:59:59.235: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:00:00.232: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:00:00.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:00:00.235: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:00:01.234: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:00:01.236: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 23:00:01.236: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 19 23:00:01.251: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:00:01.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:00:01.259: INFO: Node ip-10-0-1-48.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:00:02.265: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:00:02.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:00:02.268: INFO: Node ip-10-0-1-48.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:00:03.264: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:00:03.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:00:03.266: INFO: Node ip-10-0-1-48.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:00:04.264: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:00:04.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 23:00:04.266: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2016, will wait for the garbage collector to delete the pods
Jan 19 23:00:04.326: INFO: Deleting DaemonSet.extensions daemon-set took: 3.449131ms
Jan 19 23:00:04.427: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.213276ms
Jan 19 23:00:06.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:00:06.930: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 23:00:06.932: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"71382"},"items":null}

Jan 19 23:00:06.933: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"71382"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:00:06.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2016" for this suite.

• [SLOW TEST:8.784 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":120,"skipped":2245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:00:06.954: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-5fa105c5-6a11-4ee1-ba9d-3063abb0aba1
STEP: Creating a pod to test consume configMaps
Jan 19 23:00:06.994: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280" in namespace "projected-8594" to be "Succeeded or Failed"
Jan 19 23:00:06.996: INFO: Pod "pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913205ms
Jan 19 23:00:09.001: INFO: Pod "pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006811861s
Jan 19 23:00:11.004: INFO: Pod "pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010224827s
STEP: Saw pod success
Jan 19 23:00:11.004: INFO: Pod "pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280" satisfied condition "Succeeded or Failed"
Jan 19 23:00:11.006: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:00:11.027: INFO: Waiting for pod pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280 to disappear
Jan 19 23:00:11.029: INFO: Pod pod-projected-configmaps-30ea9e03-1127-4e0c-9970-4eea994cc280 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:00:11.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8594" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2267,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:00:11.040: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4644
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 23:00:11.068: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 23:00:11.135: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:00:13.138: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:00:15.139: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:17.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:19.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:21.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:23.141: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:25.142: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:27.140: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:29.139: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:00:31.140: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 19 23:00:31.144: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 19 23:00:31.147: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 19 23:00:31.151: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jan 19 23:00:35.165: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan 19 23:00:35.165: INFO: Breadth first check of 172.16.214.218 on host 10.0.1.103...
Jan 19 23:00:35.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.43:9080/dial?request=hostname&protocol=http&host=172.16.214.218&port=8083&tries=1'] Namespace:pod-network-test-4644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:00:35.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:00:35.167: INFO: ExecWithOptions: Clientset creation
Jan 19 23:00:35.167: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.214.218%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:00:35.273: INFO: Waiting for responses: map[]
Jan 19 23:00:35.273: INFO: reached 172.16.214.218 after 0/1 tries
Jan 19 23:00:35.273: INFO: Breadth first check of 172.16.146.42 on host 10.0.1.153...
Jan 19 23:00:35.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.43:9080/dial?request=hostname&protocol=http&host=172.16.146.42&port=8083&tries=1'] Namespace:pod-network-test-4644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:00:35.276: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:00:35.276: INFO: ExecWithOptions: Clientset creation
Jan 19 23:00:35.276: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.146.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:00:35.383: INFO: Waiting for responses: map[]
Jan 19 23:00:35.383: INFO: reached 172.16.146.42 after 0/1 tries
Jan 19 23:00:35.383: INFO: Breadth first check of 172.16.138.29 on host 10.0.1.48...
Jan 19 23:00:35.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.43:9080/dial?request=hostname&protocol=http&host=172.16.138.29&port=8083&tries=1'] Namespace:pod-network-test-4644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:00:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:00:35.386: INFO: ExecWithOptions: Clientset creation
Jan 19 23:00:35.386: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.138.29%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:00:35.478: INFO: Waiting for responses: map[]
Jan 19 23:00:35.478: INFO: reached 172.16.138.29 after 0/1 tries
Jan 19 23:00:35.478: INFO: Breadth first check of 172.16.32.143 on host 10.0.1.62...
Jan 19 23:00:35.480: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.43:9080/dial?request=hostname&protocol=http&host=172.16.32.143&port=8083&tries=1'] Namespace:pod-network-test-4644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:00:35.480: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:00:35.481: INFO: ExecWithOptions: Clientset creation
Jan 19 23:00:35.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.146.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.32.143%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:00:35.591: INFO: Waiting for responses: map[]
Jan 19 23:00:35.591: INFO: reached 172.16.32.143 after 0/1 tries
Jan 19 23:00:35.591: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:00:35.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4644" for this suite.

• [SLOW TEST:24.557 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2267,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:00:35.598: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-3f6826f7-3877-4403-8708-02fba58af813
STEP: Creating a pod to test consume configMaps
Jan 19 23:00:35.647: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c" in namespace "projected-3636" to be "Succeeded or Failed"
Jan 19 23:00:35.649: INFO: Pod "pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246088ms
Jan 19 23:00:37.652: INFO: Pod "pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004948942s
Jan 19 23:00:39.658: INFO: Pod "pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011320367s
STEP: Saw pod success
Jan 19 23:00:39.658: INFO: Pod "pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c" satisfied condition "Succeeded or Failed"
Jan 19 23:00:39.660: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:00:39.681: INFO: Waiting for pod pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c to disappear
Jan 19 23:00:39.683: INFO: Pod pod-projected-configmaps-a807092d-88d0-4a8c-85b9-5ab59062c90c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:00:39.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3636" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2286,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:00:39.690: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:01:06.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9152" for this suite.

• [SLOW TEST:27.267 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":124,"skipped":2288,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:01:06.958: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:01:06.995: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a41cd612-4bcf-4e53-a44a-e7757ed73402" in namespace "security-context-test-2993" to be "Succeeded or Failed"
Jan 19 23:01:06.997: INFO: Pod "busybox-privileged-false-a41cd612-4bcf-4e53-a44a-e7757ed73402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222519ms
Jan 19 23:01:09.003: INFO: Pod "busybox-privileged-false-a41cd612-4bcf-4e53-a44a-e7757ed73402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00816275s
Jan 19 23:01:11.008: INFO: Pod "busybox-privileged-false-a41cd612-4bcf-4e53-a44a-e7757ed73402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013327593s
Jan 19 23:01:11.008: INFO: Pod "busybox-privileged-false-a41cd612-4bcf-4e53-a44a-e7757ed73402" satisfied condition "Succeeded or Failed"
Jan 19 23:01:11.014: INFO: Got logs for pod "busybox-privileged-false-a41cd612-4bcf-4e53-a44a-e7757ed73402": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:01:11.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2993" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2305,"failed":0}
SSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:01:11.021: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 19 23:01:11.058: INFO: Waiting up to 5m0s for pod "security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01" in namespace "security-context-5727" to be "Succeeded or Failed"
Jan 19 23:01:11.060: INFO: Pod "security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01": Phase="Pending", Reason="", readiness=false. Elapsed: 1.874769ms
Jan 19 23:01:13.063: INFO: Pod "security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004798423s
Jan 19 23:01:15.067: INFO: Pod "security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009057292s
STEP: Saw pod success
Jan 19 23:01:15.067: INFO: Pod "security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01" satisfied condition "Succeeded or Failed"
Jan 19 23:01:15.069: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01 container test-container: <nil>
STEP: delete the pod
Jan 19 23:01:15.089: INFO: Waiting for pod security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01 to disappear
Jan 19 23:01:15.091: INFO: Pod security-context-dc8230c3-3fa5-4fbe-a5e2-489f33aeba01 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:01:15.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5727" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":126,"skipped":2309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:01:15.097: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:01:43.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3150" for this suite.

• [SLOW TEST:28.078 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":127,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:01:43.175: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:01:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-314" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":128,"skipped":2417,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:01:46.030: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:06:46.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1015" for this suite.

• [SLOW TEST:300.059 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":129,"skipped":2418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:06:46.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jan 19 23:06:48.140: INFO: pods: 0 < 3
Jan 19 23:06:50.144: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 19 23:06:56.227: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:06:58.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2155" for this suite.

• [SLOW TEST:12.166 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":130,"skipped":2451,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:06:58.256: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:06:58.295: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 19 23:07:03.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 create -f -'
Jan 19 23:07:05.430: INFO: stderr: ""
Jan 19 23:07:05.430: INFO: stdout: "e2e-test-crd-publish-openapi-4955-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 23:07:05.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 delete e2e-test-crd-publish-openapi-4955-crds test-cr'
Jan 19 23:07:05.487: INFO: stderr: ""
Jan 19 23:07:05.487: INFO: stdout: "e2e-test-crd-publish-openapi-4955-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 19 23:07:05.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 apply -f -'
Jan 19 23:07:05.693: INFO: stderr: ""
Jan 19 23:07:05.693: INFO: stdout: "e2e-test-crd-publish-openapi-4955-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 23:07:05.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-5314 --namespace=crd-publish-openapi-5314 delete e2e-test-crd-publish-openapi-4955-crds test-cr'
Jan 19 23:07:05.754: INFO: stderr: ""
Jan 19 23:07:05.754: INFO: stdout: "e2e-test-crd-publish-openapi-4955-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 19 23:07:05.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-5314 explain e2e-test-crd-publish-openapi-4955-crds'
Jan 19 23:07:05.945: INFO: stderr: ""
Jan 19 23:07:05.945: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4955-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:11.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5314" for this suite.

• [SLOW TEST:12.827 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":131,"skipped":2459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:11.084: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-gz2qf in namespace proxy-129
I0119 23:07:11.139273      22 runners.go:193] Created replication controller with name: proxy-service-gz2qf, namespace: proxy-129, replica count: 1
I0119 23:07:12.190814      22 runners.go:193] proxy-service-gz2qf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 23:07:13.192029      22 runners.go:193] proxy-service-gz2qf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 23:07:14.192443      22 runners.go:193] proxy-service-gz2qf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 23:07:14.197: INFO: setup took 3.077558573s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 19 23:07:14.202: INFO: (0) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.573714ms)
Jan 19 23:07:14.202: INFO: (0) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.544701ms)
Jan 19 23:07:14.202: INFO: (0) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.577683ms)
Jan 19 23:07:14.202: INFO: (0) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.771025ms)
Jan 19 23:07:14.202: INFO: (0) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.77676ms)
Jan 19 23:07:14.203: INFO: (0) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.803512ms)
Jan 19 23:07:14.203: INFO: (0) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.893345ms)
Jan 19 23:07:14.204: INFO: (0) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.247362ms)
Jan 19 23:07:14.204: INFO: (0) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.398655ms)
Jan 19 23:07:14.204: INFO: (0) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.419303ms)
Jan 19 23:07:14.204: INFO: (0) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 6.537859ms)
Jan 19 23:07:14.210: INFO: (0) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 12.238697ms)
Jan 19 23:07:14.211: INFO: (0) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 13.196467ms)
Jan 19 23:07:14.211: INFO: (0) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 13.508724ms)
Jan 19 23:07:14.211: INFO: (0) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 13.340887ms)
Jan 19 23:07:14.211: INFO: (0) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 13.551373ms)
Jan 19 23:07:14.214: INFO: (1) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 2.549994ms)
Jan 19 23:07:14.215: INFO: (1) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 3.976615ms)
Jan 19 23:07:14.216: INFO: (1) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.917478ms)
Jan 19 23:07:14.216: INFO: (1) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.981867ms)
Jan 19 23:07:14.216: INFO: (1) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 5.020659ms)
Jan 19 23:07:14.216: INFO: (1) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.960406ms)
Jan 19 23:07:14.216: INFO: (1) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.100879ms)
Jan 19 23:07:14.216: INFO: (1) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 4.983039ms)
Jan 19 23:07:14.217: INFO: (1) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 5.474542ms)
Jan 19 23:07:14.217: INFO: (1) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 5.431416ms)
Jan 19 23:07:14.217: INFO: (1) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.884825ms)
Jan 19 23:07:14.218: INFO: (1) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.384134ms)
Jan 19 23:07:14.218: INFO: (1) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.346346ms)
Jan 19 23:07:14.218: INFO: (1) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.477675ms)
Jan 19 23:07:14.218: INFO: (1) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.403584ms)
Jan 19 23:07:14.218: INFO: (1) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.526789ms)
Jan 19 23:07:14.222: INFO: (2) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.487902ms)
Jan 19 23:07:14.223: INFO: (2) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.16158ms)
Jan 19 23:07:14.223: INFO: (2) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 5.439569ms)
Jan 19 23:07:14.223: INFO: (2) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.129986ms)
Jan 19 23:07:14.223: INFO: (2) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.430751ms)
Jan 19 23:07:14.223: INFO: (2) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.49533ms)
Jan 19 23:07:14.223: INFO: (2) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 5.285639ms)
Jan 19 23:07:14.223: INFO: (2) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.241591ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.785175ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 5.700439ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 6.173678ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 6.316208ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.473762ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 6.434797ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.457707ms)
Jan 19 23:07:14.224: INFO: (2) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 6.487529ms)
Jan 19 23:07:14.229: INFO: (3) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.480677ms)
Jan 19 23:07:14.230: INFO: (3) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.534439ms)
Jan 19 23:07:14.230: INFO: (3) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 5.574636ms)
Jan 19 23:07:14.230: INFO: (3) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.772748ms)
Jan 19 23:07:14.230: INFO: (3) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.679075ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 5.891845ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 5.859476ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.921072ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 6.00281ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.065651ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.17797ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 6.0987ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 6.153265ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.515353ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.552347ms)
Jan 19 23:07:14.231: INFO: (3) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.630121ms)
Jan 19 23:07:14.236: INFO: (4) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 3.96991ms)
Jan 19 23:07:14.236: INFO: (4) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.458682ms)
Jan 19 23:07:14.236: INFO: (4) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 5.040537ms)
Jan 19 23:07:14.237: INFO: (4) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 4.877054ms)
Jan 19 23:07:14.237: INFO: (4) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.636912ms)
Jan 19 23:07:14.237: INFO: (4) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 5.840577ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.723481ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.198875ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 5.914232ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.972242ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 5.929942ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.263852ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 6.624768ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.639062ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.786468ms)
Jan 19 23:07:14.238: INFO: (4) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.873789ms)
Jan 19 23:07:14.243: INFO: (5) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.496302ms)
Jan 19 23:07:14.243: INFO: (5) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 4.526906ms)
Jan 19 23:07:14.243: INFO: (5) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.717091ms)
Jan 19 23:07:14.243: INFO: (5) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 4.615777ms)
Jan 19 23:07:14.243: INFO: (5) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 4.61004ms)
Jan 19 23:07:14.243: INFO: (5) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.650376ms)
Jan 19 23:07:14.243: INFO: (5) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.759032ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.670901ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 5.813859ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.626304ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 5.821263ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 5.662883ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 5.788731ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.711049ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.677224ms)
Jan 19 23:07:14.244: INFO: (5) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.764201ms)
Jan 19 23:07:14.248: INFO: (6) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 3.463622ms)
Jan 19 23:07:14.248: INFO: (6) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 3.517294ms)
Jan 19 23:07:14.250: INFO: (6) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.228717ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.925116ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 5.948689ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.109462ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 6.010151ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 6.03079ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.075796ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.107341ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 5.966049ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.117431ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.209263ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 6.079561ms)
Jan 19 23:07:14.251: INFO: (6) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.238082ms)
Jan 19 23:07:14.252: INFO: (6) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 7.067284ms)
Jan 19 23:07:14.256: INFO: (7) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 4.422125ms)
Jan 19 23:07:14.256: INFO: (7) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.445464ms)
Jan 19 23:07:14.256: INFO: (7) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.593163ms)
Jan 19 23:07:14.256: INFO: (7) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 4.485827ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.602983ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 4.666166ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.768036ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 4.838024ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 4.860097ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.874671ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.168277ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 5.556429ms)
Jan 19 23:07:14.257: INFO: (7) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.600921ms)
Jan 19 23:07:14.258: INFO: (7) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.551596ms)
Jan 19 23:07:14.258: INFO: (7) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.740397ms)
Jan 19 23:07:14.258: INFO: (7) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 5.643373ms)
Jan 19 23:07:14.262: INFO: (8) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 3.690982ms)
Jan 19 23:07:14.262: INFO: (8) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 3.946325ms)
Jan 19 23:07:14.262: INFO: (8) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 3.687336ms)
Jan 19 23:07:14.263: INFO: (8) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.501389ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.431349ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 5.09274ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.783399ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 5.597258ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.658072ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.610028ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.827115ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 5.926634ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.177761ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.259299ms)
Jan 19 23:07:14.264: INFO: (8) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.387414ms)
Jan 19 23:07:14.265: INFO: (8) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.052729ms)
Jan 19 23:07:14.270: INFO: (9) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 3.659948ms)
Jan 19 23:07:14.270: INFO: (9) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 3.89126ms)
Jan 19 23:07:14.270: INFO: (9) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.814989ms)
Jan 19 23:07:14.270: INFO: (9) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 4.601297ms)
Jan 19 23:07:14.270: INFO: (9) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.313047ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 6.044222ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 6.183648ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.290143ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.536296ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 5.821002ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 6.06241ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 5.159024ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.176401ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.30723ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.000958ms)
Jan 19 23:07:14.271: INFO: (9) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 6.098509ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.553841ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 4.659506ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.511331ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 4.49282ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.59287ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 4.681896ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 4.580785ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 4.503196ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.661958ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 4.792769ms)
Jan 19 23:07:14.276: INFO: (10) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.785594ms)
Jan 19 23:07:14.277: INFO: (10) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 5.284783ms)
Jan 19 23:07:14.277: INFO: (10) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.562681ms)
Jan 19 23:07:14.277: INFO: (10) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.462248ms)
Jan 19 23:07:14.277: INFO: (10) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.530109ms)
Jan 19 23:07:14.277: INFO: (10) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.682025ms)
Jan 19 23:07:14.280: INFO: (11) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 3.224219ms)
Jan 19 23:07:14.283: INFO: (11) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.308738ms)
Jan 19 23:07:14.283: INFO: (11) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 5.327088ms)
Jan 19 23:07:14.283: INFO: (11) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 5.445949ms)
Jan 19 23:07:14.283: INFO: (11) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.827221ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.190401ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.479574ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 6.516399ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.626297ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 6.583219ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.628364ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.888913ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.99312ms)
Jan 19 23:07:14.284: INFO: (11) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.829087ms)
Jan 19 23:07:14.285: INFO: (11) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 7.279191ms)
Jan 19 23:07:14.285: INFO: (11) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 7.286116ms)
Jan 19 23:07:14.288: INFO: (12) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 2.802731ms)
Jan 19 23:07:14.290: INFO: (12) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.271285ms)
Jan 19 23:07:14.290: INFO: (12) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.445116ms)
Jan 19 23:07:14.291: INFO: (12) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.797199ms)
Jan 19 23:07:14.291: INFO: (12) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 5.865865ms)
Jan 19 23:07:14.291: INFO: (12) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.253482ms)
Jan 19 23:07:14.291: INFO: (12) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.033788ms)
Jan 19 23:07:14.291: INFO: (12) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 6.17562ms)
Jan 19 23:07:14.291: INFO: (12) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 6.286887ms)
Jan 19 23:07:14.291: INFO: (12) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.348566ms)
Jan 19 23:07:14.292: INFO: (12) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.74138ms)
Jan 19 23:07:14.292: INFO: (12) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.706937ms)
Jan 19 23:07:14.292: INFO: (12) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.642831ms)
Jan 19 23:07:14.292: INFO: (12) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.800572ms)
Jan 19 23:07:14.292: INFO: (12) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 6.832976ms)
Jan 19 23:07:14.292: INFO: (12) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 6.707201ms)
Jan 19 23:07:14.296: INFO: (13) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 3.690199ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 5.468184ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 5.556772ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.684323ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 5.804498ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.632564ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.969886ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.286159ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 6.379978ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.380746ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.520398ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.632225ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.345314ms)
Jan 19 23:07:14.298: INFO: (13) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.405841ms)
Jan 19 23:07:14.299: INFO: (13) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.459545ms)
Jan 19 23:07:14.300: INFO: (13) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 7.875946ms)
Jan 19 23:07:14.304: INFO: (14) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 3.515791ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.637358ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.655238ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.691462ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 4.642993ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 4.687278ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 4.682299ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 5.365462ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.184013ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.261868ms)
Jan 19 23:07:14.305: INFO: (14) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 5.452642ms)
Jan 19 23:07:14.306: INFO: (14) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 5.497161ms)
Jan 19 23:07:14.306: INFO: (14) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 5.600133ms)
Jan 19 23:07:14.306: INFO: (14) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.6997ms)
Jan 19 23:07:14.306: INFO: (14) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.622911ms)
Jan 19 23:07:14.306: INFO: (14) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 5.704215ms)
Jan 19 23:07:14.309: INFO: (15) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 2.867157ms)
Jan 19 23:07:14.309: INFO: (15) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 2.414841ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 5.614931ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 4.571376ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 4.379836ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.755965ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.188271ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 4.909443ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.036528ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 5.155175ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 5.698312ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 4.824096ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 4.767973ms)
Jan 19 23:07:14.312: INFO: (15) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.802935ms)
Jan 19 23:07:14.313: INFO: (15) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.34851ms)
Jan 19 23:07:14.313: INFO: (15) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.589932ms)
Jan 19 23:07:14.315: INFO: (16) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 2.331388ms)
Jan 19 23:07:14.315: INFO: (16) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 2.59581ms)
Jan 19 23:07:14.319: INFO: (16) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.460231ms)
Jan 19 23:07:14.319: INFO: (16) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 6.404954ms)
Jan 19 23:07:14.319: INFO: (16) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.440198ms)
Jan 19 23:07:14.319: INFO: (16) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.628865ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.556462ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.487156ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 6.545399ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 6.626524ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 6.616285ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 6.580973ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.803665ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.566856ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.677642ms)
Jan 19 23:07:14.320: INFO: (16) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.564791ms)
Jan 19 23:07:14.323: INFO: (17) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 3.425808ms)
Jan 19 23:07:14.324: INFO: (17) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 3.715076ms)
Jan 19 23:07:14.324: INFO: (17) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 3.866499ms)
Jan 19 23:07:14.324: INFO: (17) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 3.876153ms)
Jan 19 23:07:14.327: INFO: (17) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 6.568506ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 7.81289ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 7.780416ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 7.801508ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 7.930439ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 7.998214ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 7.954254ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 8.021169ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 8.003409ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 8.043251ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 8.100829ms)
Jan 19 23:07:14.328: INFO: (17) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 8.113108ms)
Jan 19 23:07:14.332: INFO: (18) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.140881ms)
Jan 19 23:07:14.332: INFO: (18) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 4.189299ms)
Jan 19 23:07:14.332: INFO: (18) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 4.148969ms)
Jan 19 23:07:14.333: INFO: (18) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 5.091722ms)
Jan 19 23:07:14.333: INFO: (18) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 5.333925ms)
Jan 19 23:07:14.333: INFO: (18) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.310612ms)
Jan 19 23:07:14.333: INFO: (18) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 5.159385ms)
Jan 19 23:07:14.333: INFO: (18) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 5.185136ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 5.928939ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 5.846326ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.123663ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.002398ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 6.106912ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.042125ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.185905ms)
Jan 19 23:07:14.334: INFO: (18) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.277644ms)
Jan 19 23:07:14.337: INFO: (19) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">t... (200; 2.092462ms)
Jan 19 23:07:14.337: INFO: (19) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:460/proxy/: tls baz (200; 2.492627ms)
Jan 19 23:07:14.339: INFO: (19) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 3.695015ms)
Jan 19 23:07:14.339: INFO: (19) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:462/proxy/: tls qux (200; 3.654124ms)
Jan 19 23:07:14.340: INFO: (19) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 3.934472ms)
Jan 19 23:07:14.340: INFO: (19) /api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/https:proxy-service-gz2qf-8zpsv:443/proxy/tlsrewriteme... (200; 3.784129ms)
Jan 19 23:07:14.340: INFO: (19) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv/proxy/rewriteme">test</a> (200; 4.11688ms)
Jan 19 23:07:14.340: INFO: (19) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:162/proxy/: bar (200; 4.378966ms)
Jan 19 23:07:14.341: INFO: (19) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname1/proxy/: foo (200; 6.156198ms)
Jan 19 23:07:14.341: INFO: (19) /api/v1/namespaces/proxy-129/services/http:proxy-service-gz2qf:portname2/proxy/: bar (200; 5.734085ms)
Jan 19 23:07:14.341: INFO: (19) /api/v1/namespaces/proxy-129/pods/http:proxy-service-gz2qf-8zpsv:160/proxy/: foo (200; 6.393224ms)
Jan 19 23:07:14.341: INFO: (19) /api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/: <a href="/api/v1/namespaces/proxy-129/pods/proxy-service-gz2qf-8zpsv:1080/proxy/rewriteme">test</... (200; 5.901557ms)
Jan 19 23:07:14.341: INFO: (19) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname1/proxy/: tls baz (200; 6.107212ms)
Jan 19 23:07:14.341: INFO: (19) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname2/proxy/: bar (200; 6.574344ms)
Jan 19 23:07:14.342: INFO: (19) /api/v1/namespaces/proxy-129/services/https:proxy-service-gz2qf:tlsportname2/proxy/: tls qux (200; 6.45004ms)
Jan 19 23:07:14.341: INFO: (19) /api/v1/namespaces/proxy-129/services/proxy-service-gz2qf:portname1/proxy/: foo (200; 6.458228ms)
STEP: deleting ReplicationController proxy-service-gz2qf in namespace proxy-129, will wait for the garbage collector to delete the pods
Jan 19 23:07:14.398: INFO: Deleting ReplicationController proxy-service-gz2qf took: 3.925328ms
Jan 19 23:07:14.499: INFO: Terminating ReplicationController proxy-service-gz2qf pods took: 100.737339ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:17.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-129" for this suite.

• [SLOW TEST:6.325 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":132,"skipped":2483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:17.409: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-7eb77153-adcc-4c9a-963c-691d0c3edff2
STEP: Creating secret with name secret-projected-all-test-volume-6a6f60bc-7a39-4f65-b80f-9b5b9b903056
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 19 23:07:17.451: INFO: Waiting up to 5m0s for pod "projected-volume-bfb12408-b587-4af0-abdc-87e82443c468" in namespace "projected-5940" to be "Succeeded or Failed"
Jan 19 23:07:17.454: INFO: Pod "projected-volume-bfb12408-b587-4af0-abdc-87e82443c468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434581ms
Jan 19 23:07:19.456: INFO: Pod "projected-volume-bfb12408-b587-4af0-abdc-87e82443c468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004851517s
Jan 19 23:07:21.459: INFO: Pod "projected-volume-bfb12408-b587-4af0-abdc-87e82443c468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007938057s
STEP: Saw pod success
Jan 19 23:07:21.459: INFO: Pod "projected-volume-bfb12408-b587-4af0-abdc-87e82443c468" satisfied condition "Succeeded or Failed"
Jan 19 23:07:21.461: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod projected-volume-bfb12408-b587-4af0-abdc-87e82443c468 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 19 23:07:21.487: INFO: Waiting for pod projected-volume-bfb12408-b587-4af0-abdc-87e82443c468 to disappear
Jan 19 23:07:21.495: INFO: Pod projected-volume-bfb12408-b587-4af0-abdc-87e82443c468 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:21.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5940" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2507,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:21.502: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:07:21.549: INFO: Endpoints addresses: [10.0.0.80] , ports: [6443]
Jan 19 23:07:21.549: INFO: EndpointSlices addresses: [10.0.0.80] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:21.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6880" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":134,"skipped":2514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:21.563: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:07:21.600: INFO: Creating deployment "test-recreate-deployment"
Jan 19 23:07:21.604: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 19 23:07:21.608: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 19 23:07:23.612: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 19 23:07:23.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 7, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 7, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 7, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 7, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d659f7dc9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:07:25.620: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 19 23:07:25.629: INFO: Updating deployment test-recreate-deployment
Jan 19 23:07:25.629: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 19 23:07:25.719: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6739  0ff103a9-4ae4-41f8-a496-b763727784eb 73206 2 2022-01-19 23:07:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-19 23:07:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:07:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a13a6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-01-19 23:07:25 +0000 UTC,LastTransitionTime:2022-01-19 23:07:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-01-19 23:07:25 +0000 UTC,LastTransitionTime:2022-01-19 23:07:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 19 23:07:25.739: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-6739  c972f777-2693-4c2a-b2b0-de096d3ec39d 73204 1 2022-01-19 23:07:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0ff103a9-4ae4-41f8-a496-b763727784eb 0xc009fe06e7 0xc009fe06e8}] []  [{kube-controller-manager Update apps/v1 2022-01-19 23:07:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ff103a9-4ae4-41f8-a496-b763727784eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:07:25 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009fe0788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:07:25.739: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 19 23:07:25.739: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-6739  e762d10a-e35c-44e4-a82f-9188574d2058 73195 2 2022-01-19 23:07:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0ff103a9-4ae4-41f8-a496-b763727784eb 0xc009fe07f7 0xc009fe07f8}] []  [{kube-controller-manager Update apps/v1 2022-01-19 23:07:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ff103a9-4ae4-41f8-a496-b763727784eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:07:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009fe08a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:07:25.741: INFO: Pod "test-recreate-deployment-5b99bd5487-rqvn5" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-rqvn5 test-recreate-deployment-5b99bd5487- deployment-6739  952fc725-5fe5-4945-b8fd-4fe055b5ace0 73207 0 2022-01-19 23:07:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 c972f777-2693-4c2a-b2b0-de096d3ec39d 0xc009fe0d17 0xc009fe0d18}] []  [{Go-http-client Update v1 2022-01-19 23:07:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:07:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c972f777-2693-4c2a-b2b0-de096d3ec39d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kqx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kqx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:07:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:07:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:07:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:07:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:,StartTime:2022-01-19 23:07:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:25.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6739" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":135,"skipped":2571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:25.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:07:26.403: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 19 23:07:28.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 7, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 7, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 7, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 7, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:07:31.430: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:07:31.435: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:34.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-544" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.841 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":136,"skipped":2595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:34.589: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:07:34.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550" in namespace "projected-7405" to be "Succeeded or Failed"
Jan 19 23:07:34.647: INFO: Pod "downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680059ms
Jan 19 23:07:36.650: INFO: Pod "downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074261s
Jan 19 23:07:38.655: INFO: Pod "downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011060489s
STEP: Saw pod success
Jan 19 23:07:38.655: INFO: Pod "downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550" satisfied condition "Succeeded or Failed"
Jan 19 23:07:38.657: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550 container client-container: <nil>
STEP: delete the pod
Jan 19 23:07:38.677: INFO: Waiting for pod downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550 to disappear
Jan 19 23:07:38.679: INFO: Pod downwardapi-volume-afe2552c-1500-4dff-82c1-42365da63550 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:38.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7405" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2617,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:38.685: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:38.721: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-7181
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:44.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-4871" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:44.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7181" for this suite.

• [SLOW TEST:6.160 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":138,"skipped":2624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:44.846: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-0db72a57-6d01-47e3-a1b9-6a371c6d80d5
STEP: Creating a pod to test consume secrets
Jan 19 23:07:44.883: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746" in namespace "projected-1395" to be "Succeeded or Failed"
Jan 19 23:07:44.887: INFO: Pod "pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774571ms
Jan 19 23:07:46.890: INFO: Pod "pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006942602s
Jan 19 23:07:48.896: INFO: Pod "pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013194594s
STEP: Saw pod success
Jan 19 23:07:48.896: INFO: Pod "pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746" satisfied condition "Succeeded or Failed"
Jan 19 23:07:48.898: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:07:48.910: INFO: Waiting for pod pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746 to disappear
Jan 19 23:07:48.918: INFO: Pod pod-projected-secrets-0fb1a930-ce84-4801-a102-2c5627ae3746 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:48.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1395" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":139,"skipped":2676,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:48.925: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:07:48.968: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7c966a08-f10e-48c5-9429-30e9fb9d48b7" in namespace "security-context-test-2183" to be "Succeeded or Failed"
Jan 19 23:07:48.970: INFO: Pod "busybox-user-65534-7c966a08-f10e-48c5-9429-30e9fb9d48b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.999433ms
Jan 19 23:07:50.973: INFO: Pod "busybox-user-65534-7c966a08-f10e-48c5-9429-30e9fb9d48b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004933122s
Jan 19 23:07:52.978: INFO: Pod "busybox-user-65534-7c966a08-f10e-48c5-9429-30e9fb9d48b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009508933s
Jan 19 23:07:52.978: INFO: Pod "busybox-user-65534-7c966a08-f10e-48c5-9429-30e9fb9d48b7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:52.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2183" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2691,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:52.985: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:07:53.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e" in namespace "projected-1954" to be "Succeeded or Failed"
Jan 19 23:07:53.032: INFO: Pod "downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.372236ms
Jan 19 23:07:55.035: INFO: Pod "downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013200145s
Jan 19 23:07:57.038: INFO: Pod "downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016301877s
STEP: Saw pod success
Jan 19 23:07:57.038: INFO: Pod "downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e" satisfied condition "Succeeded or Failed"
Jan 19 23:07:57.040: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e container client-container: <nil>
STEP: delete the pod
Jan 19 23:07:57.058: INFO: Waiting for pod downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e to disappear
Jan 19 23:07:57.066: INFO: Pod downwardapi-volume-3db9e894-a0d0-4a92-ac1d-bf840c04959e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:07:57.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1954" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":141,"skipped":2693,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:07:57.073: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5f26c50f-75f4-4209-bca4-e2277a07f4bd
STEP: Creating a pod to test consume secrets
Jan 19 23:07:57.146: INFO: Waiting up to 5m0s for pod "pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090" in namespace "secrets-1904" to be "Succeeded or Failed"
Jan 19 23:07:57.148: INFO: Pod "pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837496ms
Jan 19 23:07:59.151: INFO: Pod "pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004868746s
Jan 19 23:08:01.156: INFO: Pod "pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010289972s
STEP: Saw pod success
Jan 19 23:08:01.156: INFO: Pod "pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090" satisfied condition "Succeeded or Failed"
Jan 19 23:08:01.158: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:08:01.178: INFO: Waiting for pod pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090 to disappear
Jan 19 23:08:01.180: INFO: Pod pod-secrets-6c11565e-7b73-400c-9410-1ecc3e16f090 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:08:01.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1904" for this suite.
STEP: Destroying namespace "secret-namespace-1837" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2699,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:08:01.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:08:05.233: INFO: Deleting pod "var-expansion-e1ceec80-f7f1-4cf1-928c-5b189d6c3daf" in namespace "var-expansion-568"
Jan 19 23:08:05.237: INFO: Wait up to 5m0s for pod "var-expansion-e1ceec80-f7f1-4cf1-928c-5b189d6c3daf" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:08:09.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-568" for this suite.

• [SLOW TEST:8.061 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":143,"skipped":2712,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:08:09.250: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 19 23:08:09.290: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8715  c4f9d40f-56f7-4133-b272-2a6436ca00ca 73703 0 2022-01-19 23:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-19 23:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:08:09.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8715  c4f9d40f-56f7-4133-b272-2a6436ca00ca 73704 0 2022-01-19 23:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-19 23:08:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 19 23:08:09.301: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8715  c4f9d40f-56f7-4133-b272-2a6436ca00ca 73705 0 2022-01-19 23:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-19 23:08:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:08:09.301: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8715  c4f9d40f-56f7-4133-b272-2a6436ca00ca 73706 0 2022-01-19 23:08:09 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-01-19 23:08:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:08:09.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8715" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":144,"skipped":2716,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:08:09.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 19 23:08:09.351: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:08:09.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2930" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":145,"skipped":2721,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:08:09.368: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-c3dcb911-6522-4c91-b967-a211f4c634f5
STEP: Creating a pod to test consume secrets
Jan 19 23:08:09.412: INFO: Waiting up to 5m0s for pod "pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa" in namespace "secrets-1339" to be "Succeeded or Failed"
Jan 19 23:08:09.414: INFO: Pod "pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037199ms
Jan 19 23:08:11.417: INFO: Pod "pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004977132s
Jan 19 23:08:13.421: INFO: Pod "pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008914246s
STEP: Saw pod success
Jan 19 23:08:13.421: INFO: Pod "pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa" satisfied condition "Succeeded or Failed"
Jan 19 23:08:13.423: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:08:13.443: INFO: Waiting for pod pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa to disappear
Jan 19 23:08:13.445: INFO: Pod pod-secrets-63ad08c6-9cb8-4364-a883-300cd9fc2eaa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:08:13.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1339" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2736,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:08:13.451: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1233
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 23:08:13.480: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 23:08:13.554: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:08:15.557: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:08:17.560: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:19.558: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:21.558: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:23.559: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:25.561: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:27.560: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:29.562: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:31.557: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:08:33.561: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 19 23:08:33.564: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 19 23:08:33.567: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 19 23:08:33.571: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jan 19 23:08:37.599: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan 19 23:08:37.599: INFO: Going to poll 172.16.214.216 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:08:37.601: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.214.216 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1233 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:08:37.601: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:08:37.602: INFO: ExecWithOptions: Clientset creation
Jan 19 23:08:37.602: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1233/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.214.216+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:08:38.717: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 19 23:08:38.717: INFO: Going to poll 172.16.146.55 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:08:38.721: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.146.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1233 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:08:38.721: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:08:38.722: INFO: ExecWithOptions: Clientset creation
Jan 19 23:08:38.722: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1233/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.146.55+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:08:39.816: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 19 23:08:39.816: INFO: Going to poll 172.16.138.24 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:08:39.819: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.138.24 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1233 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:08:39.819: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:08:39.820: INFO: ExecWithOptions: Clientset creation
Jan 19 23:08:39.820: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1233/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.138.24+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:08:40.926: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 19 23:08:40.926: INFO: Going to poll 172.16.32.138 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:08:40.929: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.32.138 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1233 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:08:40.929: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:08:40.930: INFO: ExecWithOptions: Clientset creation
Jan 19 23:08:40.930: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1233/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.32.138+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:08:42.028: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:08:42.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1233" for this suite.

• [SLOW TEST:28.586 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":147,"skipped":2744,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:08:42.037: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:08:42.807: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 19 23:08:42.808: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 19 23:08:42.808: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 19 23:08:42.808: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 19 23:08:42.808: INFO: Checking APIGroup: apps
Jan 19 23:08:42.809: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 19 23:08:42.809: INFO: Versions found [{apps/v1 v1}]
Jan 19 23:08:42.809: INFO: apps/v1 matches apps/v1
Jan 19 23:08:42.809: INFO: Checking APIGroup: events.k8s.io
Jan 19 23:08:42.809: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 19 23:08:42.809: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 19 23:08:42.809: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 19 23:08:42.809: INFO: Checking APIGroup: authentication.k8s.io
Jan 19 23:08:42.810: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 19 23:08:42.810: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 19 23:08:42.810: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 19 23:08:42.810: INFO: Checking APIGroup: authorization.k8s.io
Jan 19 23:08:42.811: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 19 23:08:42.811: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 19 23:08:42.811: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 19 23:08:42.811: INFO: Checking APIGroup: autoscaling
Jan 19 23:08:42.812: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 19 23:08:42.812: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 19 23:08:42.812: INFO: autoscaling/v2 matches autoscaling/v2
Jan 19 23:08:42.812: INFO: Checking APIGroup: batch
Jan 19 23:08:42.812: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 19 23:08:42.812: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 19 23:08:42.812: INFO: batch/v1 matches batch/v1
Jan 19 23:08:42.812: INFO: Checking APIGroup: certificates.k8s.io
Jan 19 23:08:42.813: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 19 23:08:42.813: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 19 23:08:42.813: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 19 23:08:42.813: INFO: Checking APIGroup: networking.k8s.io
Jan 19 23:08:42.814: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 19 23:08:42.814: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 19 23:08:42.814: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 19 23:08:42.814: INFO: Checking APIGroup: policy
Jan 19 23:08:42.815: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 19 23:08:42.815: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 19 23:08:42.815: INFO: policy/v1 matches policy/v1
Jan 19 23:08:42.815: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 19 23:08:42.815: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 19 23:08:42.815: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 19 23:08:42.815: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 19 23:08:42.815: INFO: Checking APIGroup: storage.k8s.io
Jan 19 23:08:42.816: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 19 23:08:42.816: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 19 23:08:42.816: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 19 23:08:42.816: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 19 23:08:42.817: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 19 23:08:42.817: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 19 23:08:42.817: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 19 23:08:42.817: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 19 23:08:42.818: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 19 23:08:42.818: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 19 23:08:42.818: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 19 23:08:42.818: INFO: Checking APIGroup: scheduling.k8s.io
Jan 19 23:08:42.818: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 19 23:08:42.818: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 19 23:08:42.818: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 19 23:08:42.818: INFO: Checking APIGroup: coordination.k8s.io
Jan 19 23:08:42.819: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 19 23:08:42.819: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 19 23:08:42.819: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 19 23:08:42.819: INFO: Checking APIGroup: node.k8s.io
Jan 19 23:08:42.820: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 19 23:08:42.820: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 19 23:08:42.820: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 19 23:08:42.820: INFO: Checking APIGroup: discovery.k8s.io
Jan 19 23:08:42.820: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 19 23:08:42.821: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 19 23:08:42.821: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 19 23:08:42.821: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 19 23:08:42.822: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 19 23:08:42.822: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 19 23:08:42.822: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 19 23:08:42.822: INFO: Checking APIGroup: acme.cert-manager.io
Jan 19 23:08:42.822: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Jan 19 23:08:42.822: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Jan 19 23:08:42.822: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Jan 19 23:08:42.822: INFO: Checking APIGroup: cert-manager.io
Jan 19 23:08:42.823: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Jan 19 23:08:42.823: INFO: Versions found [{cert-manager.io/v1 v1}]
Jan 19 23:08:42.823: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Jan 19 23:08:42.823: INFO: Checking APIGroup: crd.projectcalico.org
Jan 19 23:08:42.824: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 19 23:08:42.824: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 19 23:08:42.824: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 19 23:08:42.824: INFO: Checking APIGroup: monitoring.coreos.com
Jan 19 23:08:42.825: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 19 23:08:42.825: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 19 23:08:42.825: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 19 23:08:42.825: INFO: Checking APIGroup: templates.gatekeeper.sh
Jan 19 23:08:42.825: INFO: PreferredVersion.GroupVersion: templates.gatekeeper.sh/v1
Jan 19 23:08:42.825: INFO: Versions found [{templates.gatekeeper.sh/v1 v1} {templates.gatekeeper.sh/v1beta1 v1beta1} {templates.gatekeeper.sh/v1alpha1 v1alpha1}]
Jan 19 23:08:42.825: INFO: templates.gatekeeper.sh/v1 matches templates.gatekeeper.sh/v1
Jan 19 23:08:42.825: INFO: Checking APIGroup: velero.io
Jan 19 23:08:42.826: INFO: PreferredVersion.GroupVersion: velero.io/v1
Jan 19 23:08:42.826: INFO: Versions found [{velero.io/v1 v1}]
Jan 19 23:08:42.826: INFO: velero.io/v1 matches velero.io/v1
Jan 19 23:08:42.826: INFO: Checking APIGroup: config.gatekeeper.sh
Jan 19 23:08:42.827: INFO: PreferredVersion.GroupVersion: config.gatekeeper.sh/v1alpha1
Jan 19 23:08:42.827: INFO: Versions found [{config.gatekeeper.sh/v1alpha1 v1alpha1}]
Jan 19 23:08:42.827: INFO: config.gatekeeper.sh/v1alpha1 matches config.gatekeeper.sh/v1alpha1
Jan 19 23:08:42.827: INFO: Checking APIGroup: constraints.gatekeeper.sh
Jan 19 23:08:42.828: INFO: PreferredVersion.GroupVersion: constraints.gatekeeper.sh/v1beta1
Jan 19 23:08:42.828: INFO: Versions found [{constraints.gatekeeper.sh/v1beta1 v1beta1} {constraints.gatekeeper.sh/v1alpha1 v1alpha1}]
Jan 19 23:08:42.828: INFO: constraints.gatekeeper.sh/v1beta1 matches constraints.gatekeeper.sh/v1beta1
Jan 19 23:08:42.828: INFO: Checking APIGroup: status.gatekeeper.sh
Jan 19 23:08:42.828: INFO: PreferredVersion.GroupVersion: status.gatekeeper.sh/v1beta1
Jan 19 23:08:42.828: INFO: Versions found [{status.gatekeeper.sh/v1beta1 v1beta1}]
Jan 19 23:08:42.828: INFO: status.gatekeeper.sh/v1beta1 matches status.gatekeeper.sh/v1beta1
Jan 19 23:08:42.828: INFO: Checking APIGroup: metrics.k8s.io
Jan 19 23:08:42.829: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 19 23:08:42.829: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 19 23:08:42.829: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:08:42.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3235" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":148,"skipped":2748,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:08:42.836: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-1f9e8a35-d1fc-4c4b-9822-3648ad5c9d39 in namespace container-probe-2887
Jan 19 23:08:46.882: INFO: Started pod liveness-1f9e8a35-d1fc-4c4b-9822-3648ad5c9d39 in namespace container-probe-2887
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 23:08:46.884: INFO: Initial restart count of pod liveness-1f9e8a35-d1fc-4c4b-9822-3648ad5c9d39 is 0
Jan 19 23:09:04.919: INFO: Restart count of pod container-probe-2887/liveness-1f9e8a35-d1fc-4c4b-9822-3648ad5c9d39 is now 1 (18.035572918s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:09:04.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2887" for this suite.

• [SLOW TEST:22.103 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":149,"skipped":2754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:09:04.939: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 19 23:09:04.984: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 23:10:05.043: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jan 19 23:10:05.061: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 19 23:10:05.068: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 19 23:10:05.087: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 19 23:10:05.100: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 19 23:10:05.120: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 19 23:10:05.130: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 19 23:10:05.158: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 19 23:10:05.169: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:10:13.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1944" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.331 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":150,"skipped":2788,"failed":0}
S
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:10:13.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 19 23:10:13.321: INFO: starting watch
STEP: patching
STEP: updating
Jan 19 23:10:13.330: INFO: waiting for watch events with expected annotations
Jan 19 23:10:13.330: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:10:13.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-197" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":151,"skipped":2789,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:10:13.356: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 19 23:10:13.399: INFO: Waiting up to 5m0s for pod "pod-db373441-3048-4d30-8fbe-be2508d0cab7" in namespace "emptydir-867" to be "Succeeded or Failed"
Jan 19 23:10:13.401: INFO: Pod "pod-db373441-3048-4d30-8fbe-be2508d0cab7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.937903ms
Jan 19 23:10:15.405: INFO: Pod "pod-db373441-3048-4d30-8fbe-be2508d0cab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005414959s
Jan 19 23:10:17.407: INFO: Pod "pod-db373441-3048-4d30-8fbe-be2508d0cab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007820209s
STEP: Saw pod success
Jan 19 23:10:17.407: INFO: Pod "pod-db373441-3048-4d30-8fbe-be2508d0cab7" satisfied condition "Succeeded or Failed"
Jan 19 23:10:17.409: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-db373441-3048-4d30-8fbe-be2508d0cab7 container test-container: <nil>
STEP: delete the pod
Jan 19 23:10:17.438: INFO: Waiting for pod pod-db373441-3048-4d30-8fbe-be2508d0cab7 to disappear
Jan 19 23:10:17.440: INFO: Pod pod-db373441-3048-4d30-8fbe-be2508d0cab7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:10:17.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-867" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":152,"skipped":2789,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:10:17.446: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 19 23:10:17.483: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 23:11:17.541: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:11:17.543: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:11:17.583: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 19 23:11:17.585: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:11:17.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3480" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:11:17.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8900" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.203 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":153,"skipped":2798,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:11:17.649: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:11:28.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9310" for this suite.

• [SLOW TEST:11.075 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":154,"skipped":2804,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:11:28.725: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:11:28.780: INFO: created pod
Jan 19 23:11:28.780: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7920" to be "Succeeded or Failed"
Jan 19 23:11:28.782: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.872794ms
Jan 19 23:11:30.786: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006036546s
Jan 19 23:11:32.790: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009840801s
STEP: Saw pod success
Jan 19 23:11:32.790: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 19 23:12:02.791: INFO: polling logs
Jan 19 23:12:02.804: INFO: Pod logs: 
2022/01/19 23:11:30 OK: Got token
2022/01/19 23:11:30 validating with in-cluster discovery
2022/01/19 23:11:30 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/01/19 23:11:30 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7920:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1642634488, NotBefore:1642633888, IssuedAt:1642633888, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7920", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a08508cf-3ac7-46fe-9b9f-72bb30ab7cf9"}}}
2022/01/19 23:11:30 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/01/19 23:11:30 OK: Validated signature on JWT
2022/01/19 23:11:30 OK: Got valid claims from token!
2022/01/19 23:11:30 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7920:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1642634488, NotBefore:1642633888, IssuedAt:1642633888, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7920", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a08508cf-3ac7-46fe-9b9f-72bb30ab7cf9"}}}

Jan 19 23:12:02.804: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:12:02.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7920" for this suite.

• [SLOW TEST:34.089 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":155,"skipped":2826,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:12:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:12:09.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-173" for this suite.

• [SLOW TEST:7.045 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":156,"skipped":2830,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:12:09.859: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jan 19 23:12:09.899: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 23:12:14.903: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jan 19 23:12:14.905: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jan 19 23:12:14.910: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jan 19 23:12:14.911: INFO: Observed &ReplicaSet event: ADDED
Jan 19 23:12:14.912: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 23:12:14.912: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 23:12:14.912: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 23:12:14.912: INFO: Found replicaset test-rs in namespace replicaset-8834 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 23:12:14.912: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jan 19 23:12:14.912: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 23:12:14.915: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jan 19 23:12:14.917: INFO: Observed &ReplicaSet event: ADDED
Jan 19 23:12:14.917: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 23:12:14.917: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 23:12:14.917: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 23:12:14.917: INFO: Observed replicaset test-rs in namespace replicaset-8834 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 23:12:14.918: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 23:12:14.918: INFO: Found replicaset test-rs in namespace replicaset-8834 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 19 23:12:14.918: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:12:14.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8834" for this suite.

• [SLOW TEST:5.065 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":157,"skipped":2834,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:12:14.925: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-0beaf373-913c-45d7-9e86-b3e4f70cadcf in namespace container-probe-8275
Jan 19 23:12:18.967: INFO: Started pod busybox-0beaf373-913c-45d7-9e86-b3e4f70cadcf in namespace container-probe-8275
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 23:12:18.968: INFO: Initial restart count of pod busybox-0beaf373-913c-45d7-9e86-b3e4f70cadcf is 0
Jan 19 23:13:07.083: INFO: Restart count of pod container-probe-8275/busybox-0beaf373-913c-45d7-9e86-b3e4f70cadcf is now 1 (48.114440014s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:13:07.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8275" for this suite.

• [SLOW TEST:52.171 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":2837,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:13:07.096: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 19 23:13:07.135: INFO: The status of Pod labelsupdate9cc3fa87-7d8c-434d-8d2c-c9aa4fafd8c7 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:13:09.140: INFO: The status of Pod labelsupdate9cc3fa87-7d8c-434d-8d2c-c9aa4fafd8c7 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:13:11.141: INFO: The status of Pod labelsupdate9cc3fa87-7d8c-434d-8d2c-c9aa4fafd8c7 is Running (Ready = true)
Jan 19 23:13:11.675: INFO: Successfully updated pod "labelsupdate9cc3fa87-7d8c-434d-8d2c-c9aa4fafd8c7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:13:13.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6566" for this suite.

• [SLOW TEST:6.599 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":2841,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:13:13.695: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 19 23:13:13.791: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 19 23:13:13.794: INFO: starting watch
STEP: patching
STEP: updating
Jan 19 23:13:13.827: INFO: waiting for watch events with expected annotations
Jan 19 23:13:13.827: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:13:13.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8815" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":160,"skipped":2843,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:13:13.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 19 23:13:13.882: INFO: Waiting up to 5m0s for pod "pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52" in namespace "emptydir-8181" to be "Succeeded or Failed"
Jan 19 23:13:13.884: INFO: Pod "pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722824ms
Jan 19 23:13:15.890: INFO: Pod "pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007825726s
Jan 19 23:13:17.896: INFO: Pod "pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013894901s
STEP: Saw pod success
Jan 19 23:13:17.896: INFO: Pod "pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52" satisfied condition "Succeeded or Failed"
Jan 19 23:13:17.898: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52 container test-container: <nil>
STEP: delete the pod
Jan 19 23:13:17.911: INFO: Waiting for pod pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52 to disappear
Jan 19 23:13:17.918: INFO: Pod pod-74dbdf8a-2c07-47c0-9fc2-54a5adce6a52 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:13:17.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8181" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":2859,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:13:17.925: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-34cdfc8f-854a-4062-8ed7-9a6857b64a36
STEP: Creating the pod
Jan 19 23:13:17.968: INFO: The status of Pod pod-configmaps-4a41a8a7-e8f3-4bdf-b49c-bf3b0a968629 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:13:19.971: INFO: The status of Pod pod-configmaps-4a41a8a7-e8f3-4bdf-b49c-bf3b0a968629 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:13:21.971: INFO: The status of Pod pod-configmaps-4a41a8a7-e8f3-4bdf-b49c-bf3b0a968629 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-34cdfc8f-854a-4062-8ed7-9a6857b64a36
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:14:52.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6569" for this suite.

• [SLOW TEST:94.405 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":2875,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:14:52.330: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34
Jan 19 23:14:52.373: INFO: Pod name my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34: Found 0 pods out of 1
Jan 19 23:14:57.377: INFO: Pod name my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34: Found 1 pods out of 1
Jan 19 23:14:57.377: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34" are running
Jan 19 23:14:57.379: INFO: Pod "my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34-vw4rc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:14:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:14:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:14:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:14:52 +0000 UTC Reason: Message:}])
Jan 19 23:14:57.379: INFO: Trying to dial the pod
Jan 19 23:15:02.392: INFO: Controller my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34: Got expected result from replica 1 [my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34-vw4rc]: "my-hostname-basic-e0c6efd8-1296-4580-aeda-e98075f85e34-vw4rc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:15:02.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1613" for this suite.

• [SLOW TEST:10.069 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":163,"skipped":2887,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:15:02.399: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:15:02.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58" in namespace "projected-1661" to be "Succeeded or Failed"
Jan 19 23:15:02.442: INFO: Pod "downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.202517ms
Jan 19 23:15:04.446: INFO: Pod "downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00606216s
Jan 19 23:15:06.449: INFO: Pod "downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009808176s
STEP: Saw pod success
Jan 19 23:15:06.449: INFO: Pod "downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58" satisfied condition "Succeeded or Failed"
Jan 19 23:15:06.451: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58 container client-container: <nil>
STEP: delete the pod
Jan 19 23:15:06.471: INFO: Waiting for pod downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58 to disappear
Jan 19 23:15:06.473: INFO: Pod downwardapi-volume-8a547cad-e1de-4b25-ad5d-218796190b58 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:15:06.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1661" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":2894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:15:06.480: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:15:17.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1376" for this suite.

• [SLOW TEST:11.198 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":165,"skipped":2934,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:15:17.678: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 23:15:17.739: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:15:17.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:15:17.741: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:15:18.745: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:15:18.747: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:15:18.747: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:15:19.755: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:15:19.757: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:15:19.757: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:15:20.744: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:15:20.747: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 23:15:20.747: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status
Jan 19 23:15:20.751: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jan 19 23:15:20.756: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jan 19 23:15:20.758: INFO: Observed &DaemonSet event: ADDED
Jan 19 23:15:20.758: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.758: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.758: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.758: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.758: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.759: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.759: INFO: Found daemon set daemon-set in namespace daemonsets-5889 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 23:15:20.759: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jan 19 23:15:20.764: INFO: Observed &DaemonSet event: ADDED
Jan 19 23:15:20.764: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.764: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.765: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.765: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.765: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.765: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.765: INFO: Observed daemon set daemon-set in namespace daemonsets-5889 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 23:15:20.766: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 23:15:20.766: INFO: Found daemon set daemon-set in namespace daemonsets-5889 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 19 23:15:20.766: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5889, will wait for the garbage collector to delete the pods
Jan 19 23:15:20.824: INFO: Deleting DaemonSet.extensions daemon-set took: 2.74963ms
Jan 19 23:15:20.925: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.471674ms
Jan 19 23:15:23.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:15:23.127: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 23:15:23.129: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75696"},"items":null}

Jan 19 23:15:23.131: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75696"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:15:23.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5889" for this suite.

• [SLOW TEST:5.469 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":166,"skipped":2938,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:15:23.147: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:15:23.668: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 23:15:25.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 15, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 15, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 15, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:15:28.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:15:28.690: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:15:31.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2682" for this suite.
STEP: Destroying namespace "webhook-2682-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.686 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":167,"skipped":2939,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:15:31.835: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-nn97
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 23:15:31.896: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nn97" in namespace "subpath-7068" to be "Succeeded or Failed"
Jan 19 23:15:31.898: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829774ms
Jan 19 23:15:33.901: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004381315s
Jan 19 23:15:35.904: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 4.007501716s
Jan 19 23:15:37.907: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 6.010300775s
Jan 19 23:15:39.909: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 8.012817606s
Jan 19 23:15:41.912: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 10.016075806s
Jan 19 23:15:43.916: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 12.019768961s
Jan 19 23:15:45.919: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 14.022377327s
Jan 19 23:15:47.926: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 16.029356535s
Jan 19 23:15:49.933: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 18.036228497s
Jan 19 23:15:51.936: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 20.039382783s
Jan 19 23:15:53.939: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Running", Reason="", readiness=true. Elapsed: 22.042776689s
Jan 19 23:15:55.945: INFO: Pod "pod-subpath-test-secret-nn97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.048498797s
STEP: Saw pod success
Jan 19 23:15:55.945: INFO: Pod "pod-subpath-test-secret-nn97" satisfied condition "Succeeded or Failed"
Jan 19 23:15:55.947: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-subpath-test-secret-nn97 container test-container-subpath-secret-nn97: <nil>
STEP: delete the pod
Jan 19 23:15:55.967: INFO: Waiting for pod pod-subpath-test-secret-nn97 to disappear
Jan 19 23:15:55.969: INFO: Pod pod-subpath-test-secret-nn97 no longer exists
STEP: Deleting pod pod-subpath-test-secret-nn97
Jan 19 23:15:55.969: INFO: Deleting pod "pod-subpath-test-secret-nn97" in namespace "subpath-7068"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:15:55.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7068" for this suite.

• [SLOW TEST:24.142 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":168,"skipped":2953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:15:55.978: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-c8226b21-2e7e-4529-959a-77faa738e1a1 in namespace container-probe-6420
Jan 19 23:16:00.020: INFO: Started pod liveness-c8226b21-2e7e-4529-959a-77faa738e1a1 in namespace container-probe-6420
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 23:16:00.022: INFO: Initial restart count of pod liveness-c8226b21-2e7e-4529-959a-77faa738e1a1 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:00.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6420" for this suite.

• [SLOW TEST:244.656 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":169,"skipped":3006,"failed":0}
SS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:00.634: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 19 23:20:00.695: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 19 23:20:00.698: INFO: starting watch
STEP: patching
STEP: updating
Jan 19 23:20:00.730: INFO: waiting for watch events with expected annotations
Jan 19 23:20:00.730: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:00.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3536" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":170,"skipped":3008,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:00.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 19 23:20:00.885: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-479  60a9bc1e-88ec-48da-9b2a-a935b1d6f84c 76487 0 2022-01-19 23:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-01-19 23:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:20:00.885: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-479  60a9bc1e-88ec-48da-9b2a-a935b1d6f84c 76488 0 2022-01-19 23:20:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-01-19 23:20:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:00.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-479" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":171,"skipped":3022,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:00.891: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-61049720-fd9f-4e54-a798-4e092c7d9c59
STEP: Creating a pod to test consume secrets
Jan 19 23:20:00.933: INFO: Waiting up to 5m0s for pod "pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c" in namespace "secrets-393" to be "Succeeded or Failed"
Jan 19 23:20:00.938: INFO: Pod "pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.800694ms
Jan 19 23:20:02.942: INFO: Pod "pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008853876s
Jan 19 23:20:04.946: INFO: Pod "pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012764656s
STEP: Saw pod success
Jan 19 23:20:04.946: INFO: Pod "pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c" satisfied condition "Succeeded or Failed"
Jan 19 23:20:04.948: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:20:04.982: INFO: Waiting for pod pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c to disappear
Jan 19 23:20:04.984: INFO: Pod pod-secrets-1cb5e10b-7219-42e7-9bcf-1e99e025657c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:04.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-393" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":3040,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:04.991: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-5e0ad150-d173-44da-aae8-1815d8aa0dae
STEP: Creating a pod to test consume configMaps
Jan 19 23:20:05.033: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875" in namespace "configmap-4767" to be "Succeeded or Failed"
Jan 19 23:20:05.035: INFO: Pod "pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016814ms
Jan 19 23:20:07.038: INFO: Pod "pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005017823s
Jan 19 23:20:09.043: INFO: Pod "pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010214634s
STEP: Saw pod success
Jan 19 23:20:09.043: INFO: Pod "pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875" satisfied condition "Succeeded or Failed"
Jan 19 23:20:09.045: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:20:09.062: INFO: Waiting for pod pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875 to disappear
Jan 19 23:20:09.069: INFO: Pod pod-configmaps-5e3bd2f4-174a-44f3-8c1a-7ff94d7b5875 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:09.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4767" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3048,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:09.075: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 19 23:20:09.119: INFO: Waiting up to 5m0s for pod "pod-634ca895-96b2-43b3-922d-5a5c29e419ec" in namespace "emptydir-4572" to be "Succeeded or Failed"
Jan 19 23:20:09.121: INFO: Pod "pod-634ca895-96b2-43b3-922d-5a5c29e419ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.95267ms
Jan 19 23:20:11.124: INFO: Pod "pod-634ca895-96b2-43b3-922d-5a5c29e419ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00484459s
Jan 19 23:20:13.127: INFO: Pod "pod-634ca895-96b2-43b3-922d-5a5c29e419ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00801527s
STEP: Saw pod success
Jan 19 23:20:13.127: INFO: Pod "pod-634ca895-96b2-43b3-922d-5a5c29e419ec" satisfied condition "Succeeded or Failed"
Jan 19 23:20:13.129: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-634ca895-96b2-43b3-922d-5a5c29e419ec container test-container: <nil>
STEP: delete the pod
Jan 19 23:20:13.139: INFO: Waiting for pod pod-634ca895-96b2-43b3-922d-5a5c29e419ec to disappear
Jan 19 23:20:13.148: INFO: Pod pod-634ca895-96b2-43b3-922d-5a5c29e419ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:13.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4572" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":3049,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:13.154: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 19 23:20:13.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5011 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 19 23:20:14.202: INFO: stderr: ""
Jan 19 23:20:14.202: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 19 23:20:14.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5011 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 19 23:20:15.643: INFO: stderr: ""
Jan 19 23:20:15.643: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 19 23:20:15.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5011 delete pods e2e-test-httpd-pod'
Jan 19 23:20:17.465: INFO: stderr: ""
Jan 19 23:20:17.465: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:17.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5011" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":175,"skipped":3064,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:17.476: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 19 23:20:17.516: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:20:19.519: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:20:21.519: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:20:22.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8585" for this suite.

• [SLOW TEST:5.066 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":176,"skipped":3083,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:20:22.542: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:20:22.594: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 23:20:22.603: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:22.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:20:22.605: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:20:23.609: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:23.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:20:23.612: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:20:24.610: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:24.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:20:24.612: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:20:25.610: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:25.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 23:20:25.612: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 19 23:20:25.638: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:25.638: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:25.638: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:25.638: INFO: Wrong image for pod: daemon-set-gwg44. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:25.643: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:26.646: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:26.646: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:26.646: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:26.649: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:27.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:27.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:27.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:27.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:28.646: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:28.646: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:28.646: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:28.646: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:28.649: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:29.647: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:29.647: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:29.647: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:29.647: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:29.650: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:30.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:30.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:30.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:30.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:30.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:31.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:31.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:31.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:31.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:31.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:32.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:32.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:32.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:32.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:32.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:33.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:33.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:33.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:33.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:33.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:34.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:34.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:34.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:34.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:34.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:35.647: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:35.647: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:35.647: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:35.647: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:35.650: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:36.647: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:36.647: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:36.647: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:36.647: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:36.650: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:37.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:37.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:37.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:37.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:37.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:38.647: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:38.647: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:38.647: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:38.647: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:38.650: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:39.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:39.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:39.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:39.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:39.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:40.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:40.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:40.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:40.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:40.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:41.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:41.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:41.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:41.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:41.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:42.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:42.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:42.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:42.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:42.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:43.650: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:43.650: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:43.650: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:43.650: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:43.653: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:44.646: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:44.646: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:44.646: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:44.646: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:44.649: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:45.646: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:45.646: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:45.646: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:45.646: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:45.649: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:46.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:46.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:46.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:46.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:46.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:47.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:47.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:47.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:47.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:47.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:48.647: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:48.647: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:48.647: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:48.647: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:48.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:49.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:49.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:49.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:49.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:49.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:50.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:50.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:50.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:50.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:50.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:51.646: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:51.646: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:51.646: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:51.646: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:51.649: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:52.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:52.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:52.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:52.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:52.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:53.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:53.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:53.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:53.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:53.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:54.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:54.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:54.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:54.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:54.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:55.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:55.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:55.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:55.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:55.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:56.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:56.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:56.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:56.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:56.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:57.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:57.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:57.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:57.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:57.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:58.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:58.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:58.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:58.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:58.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:20:59.648: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:59.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:59.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:20:59.648: INFO: Pod daemon-set-np69m is not available
Jan 19 23:20:59.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:00.649: INFO: Wrong image for pod: daemon-set-654ln. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:00.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:00.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:00.649: INFO: Pod daemon-set-np69m is not available
Jan 19 23:21:00.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:01.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:01.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:01.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:02.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:02.649: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:02.653: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:03.649: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:03.650: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:03.650: INFO: Pod daemon-set-pbh7v is not available
Jan 19 23:21:03.652: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:04.648: INFO: Wrong image for pod: daemon-set-c2nwh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:04.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:04.648: INFO: Pod daemon-set-pbh7v is not available
Jan 19 23:21:04.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:05.647: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:05.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:06.653: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:06.673: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:07.648: INFO: Pod daemon-set-7frhj is not available
Jan 19 23:21:07.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:07.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:08.648: INFO: Pod daemon-set-7frhj is not available
Jan 19 23:21:08.648: INFO: Wrong image for pod: daemon-set-ffgl8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 23:21:08.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:09.653: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:10.648: INFO: Pod daemon-set-5slc7 is not available
Jan 19 23:21:10.651: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 19 23:21:10.654: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:10.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:21:10.656: INFO: Node ip-10-0-1-62.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:21:11.660: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:11.662: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:21:11.662: INFO: Node ip-10-0-1-62.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:21:12.664: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:21:12.666: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 23:21:12.666: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-559, will wait for the garbage collector to delete the pods
Jan 19 23:21:12.732: INFO: Deleting DaemonSet.extensions daemon-set took: 3.749452ms
Jan 19 23:21:12.832: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.285241ms
Jan 19 23:21:15.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:21:15.136: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 23:21:15.138: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77080"},"items":null}

Jan 19 23:21:15.139: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77080"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:15.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-559" for this suite.

• [SLOW TEST:52.614 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":177,"skipped":3084,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:15.156: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-dbf285f6-b7c4-4a85-9be7-b7a4cef3423f
STEP: Creating configMap with name cm-test-opt-upd-743acebd-0707-4423-a2f7-c128149b1b0d
STEP: Creating the pod
Jan 19 23:21:15.208: INFO: The status of Pod pod-configmaps-f4b1b4d0-945d-4fb3-bf2d-99dd0325f4e5 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:21:17.212: INFO: The status of Pod pod-configmaps-f4b1b4d0-945d-4fb3-bf2d-99dd0325f4e5 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:21:19.212: INFO: The status of Pod pod-configmaps-f4b1b4d0-945d-4fb3-bf2d-99dd0325f4e5 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-dbf285f6-b7c4-4a85-9be7-b7a4cef3423f
STEP: Updating configmap cm-test-opt-upd-743acebd-0707-4423-a2f7-c128149b1b0d
STEP: Creating configMap with name cm-test-opt-create-021698f2-4e53-45d5-a9e9-33905aa0c8e0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:23.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9658" for this suite.

• [SLOW TEST:8.122 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":3087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:23.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 19 23:21:23.312: INFO: Waiting up to 5m0s for pod "pod-19d36222-3c84-4ddf-a670-bc9d238d7e98" in namespace "emptydir-9325" to be "Succeeded or Failed"
Jan 19 23:21:23.314: INFO: Pod "pod-19d36222-3c84-4ddf-a670-bc9d238d7e98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.933766ms
Jan 19 23:21:25.318: INFO: Pod "pod-19d36222-3c84-4ddf-a670-bc9d238d7e98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00635458s
Jan 19 23:21:27.322: INFO: Pod "pod-19d36222-3c84-4ddf-a670-bc9d238d7e98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009947274s
STEP: Saw pod success
Jan 19 23:21:27.322: INFO: Pod "pod-19d36222-3c84-4ddf-a670-bc9d238d7e98" satisfied condition "Succeeded or Failed"
Jan 19 23:21:27.323: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-19d36222-3c84-4ddf-a670-bc9d238d7e98 container test-container: <nil>
STEP: delete the pod
Jan 19 23:21:27.335: INFO: Waiting for pod pod-19d36222-3c84-4ddf-a670-bc9d238d7e98 to disappear
Jan 19 23:21:27.343: INFO: Pod pod-19d36222-3c84-4ddf-a670-bc9d238d7e98 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:27.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9325" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3123,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:27.349: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 19 23:21:27.383: INFO: Waiting up to 5m0s for pod "pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29" in namespace "emptydir-9822" to be "Succeeded or Failed"
Jan 19 23:21:27.385: INFO: Pod "pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839186ms
Jan 19 23:21:29.389: INFO: Pod "pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005756304s
Jan 19 23:21:31.393: INFO: Pod "pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010053322s
STEP: Saw pod success
Jan 19 23:21:31.393: INFO: Pod "pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29" satisfied condition "Succeeded or Failed"
Jan 19 23:21:31.395: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29 container test-container: <nil>
STEP: delete the pod
Jan 19 23:21:31.416: INFO: Waiting for pod pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29 to disappear
Jan 19 23:21:31.418: INFO: Pod pod-d88ae1fa-c778-49f2-b5bf-1acf52250e29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9822" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3135,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:31.424: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:21:31.462: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-625fc04c-50a7-4fb8-83f1-514c4ffd9d2b" in namespace "security-context-test-3872" to be "Succeeded or Failed"
Jan 19 23:21:31.464: INFO: Pod "busybox-readonly-false-625fc04c-50a7-4fb8-83f1-514c4ffd9d2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051851ms
Jan 19 23:21:33.468: INFO: Pod "busybox-readonly-false-625fc04c-50a7-4fb8-83f1-514c4ffd9d2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006166103s
Jan 19 23:21:35.473: INFO: Pod "busybox-readonly-false-625fc04c-50a7-4fb8-83f1-514c4ffd9d2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011331369s
Jan 19 23:21:35.473: INFO: Pod "busybox-readonly-false-625fc04c-50a7-4fb8-83f1-514c4ffd9d2b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:35.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3872" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":181,"skipped":3148,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:35.482: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 19 23:21:35.520: INFO: Waiting up to 5m0s for pod "pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d" in namespace "emptydir-4807" to be "Succeeded or Failed"
Jan 19 23:21:35.522: INFO: Pod "pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.896871ms
Jan 19 23:21:37.525: INFO: Pod "pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005031148s
Jan 19 23:21:39.533: INFO: Pod "pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012728866s
STEP: Saw pod success
Jan 19 23:21:39.533: INFO: Pod "pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d" satisfied condition "Succeeded or Failed"
Jan 19 23:21:39.534: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d container test-container: <nil>
STEP: delete the pod
Jan 19 23:21:39.556: INFO: Waiting for pod pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d to disappear
Jan 19 23:21:39.557: INFO: Pod pod-5ce589ca-4016-4c8b-b028-9f1d863cd05d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4807" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3168,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:39.564: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jan 19 23:21:39.624: INFO: observed Pod pod-test in namespace pods-7025 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 19 23:21:39.627: INFO: observed Pod pod-test in namespace pods-7025 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC  }]
Jan 19 23:21:39.636: INFO: observed Pod pod-test in namespace pods-7025 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC  }]
Jan 19 23:21:40.802: INFO: observed Pod pod-test in namespace pods-7025 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC  }]
Jan 19 23:21:41.787: INFO: Found Pod pod-test in namespace pods-7025 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-01-19 23:21:39 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jan 19 23:21:41.797: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 19 23:21:41.811: INFO: observed event type ADDED
Jan 19 23:21:41.811: INFO: observed event type MODIFIED
Jan 19 23:21:41.811: INFO: observed event type MODIFIED
Jan 19 23:21:41.811: INFO: observed event type MODIFIED
Jan 19 23:21:41.812: INFO: observed event type MODIFIED
Jan 19 23:21:41.812: INFO: observed event type MODIFIED
Jan 19 23:21:41.812: INFO: observed event type MODIFIED
Jan 19 23:21:41.812: INFO: observed event type MODIFIED
Jan 19 23:21:43.833: INFO: observed event type MODIFIED
Jan 19 23:21:44.069: INFO: observed event type MODIFIED
Jan 19 23:21:44.851: INFO: observed event type MODIFIED
Jan 19 23:21:44.857: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:44.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7025" for this suite.

• [SLOW TEST:5.311 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":183,"skipped":3183,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:44.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:55.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8156" for this suite.

• [SLOW TEST:11.066 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":184,"skipped":3190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:55.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jan 19 23:21:55.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-2995 api-versions'
Jan 19 23:21:56.053: INFO: stderr: ""
Jan 19 23:21:56.053: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert-manager.io/v1\ncertificates.k8s.io/v1\nconfig.gatekeeper.sh/v1alpha1\nconstraints.gatekeeper.sh/v1alpha1\nconstraints.gatekeeper.sh/v1beta1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstatus.gatekeeper.sh/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplates.gatekeeper.sh/v1\ntemplates.gatekeeper.sh/v1alpha1\ntemplates.gatekeeper.sh/v1beta1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:21:56.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2995" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":185,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:21:56.068: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 19 23:21:56.116: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:21:58.123: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:22:00.119: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.1.103 on the node which pod1 resides and expect scheduled
Jan 19 23:22:00.128: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:22:02.131: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:22:04.135: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.1.103 but use UDP protocol on the node which pod2 resides
Jan 19 23:22:04.147: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:22:06.151: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:22:08.150: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 19 23:22:08.157: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:22:10.160: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 19 23:22:10.162: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.1.103 http://127.0.0.1:54323/hostname] Namespace:hostport-4788 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:22:10.162: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:22:10.162: INFO: ExecWithOptions: Clientset creation
Jan 19 23:22:10.162: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4788/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.1.103+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.103, port: 54323
Jan 19 23:22:10.270: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.1.103:54323/hostname] Namespace:hostport-4788 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:22:10.270: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:22:10.271: INFO: ExecWithOptions: Clientset creation
Jan 19 23:22:10.271: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4788/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.1.103%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.1.103, port: 54323 UDP
Jan 19 23:22:10.372: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.1.103 54323] Namespace:hostport-4788 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:22:10.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:22:10.373: INFO: ExecWithOptions: Clientset creation
Jan 19 23:22:10.373: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4788/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.0.1.103+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:22:15.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4788" for this suite.

• [SLOW TEST:19.424 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":186,"skipped":3289,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:22:15.492: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jan 19 23:22:21.552: INFO: 80 pods remaining
Jan 19 23:22:21.552: INFO: 80 pods has nil DeletionTimestamp
Jan 19 23:22:21.552: INFO: 
Jan 19 23:22:22.553: INFO: 71 pods remaining
Jan 19 23:22:22.553: INFO: 71 pods has nil DeletionTimestamp
Jan 19 23:22:22.553: INFO: 
Jan 19 23:22:23.550: INFO: 60 pods remaining
Jan 19 23:22:23.550: INFO: 60 pods has nil DeletionTimestamp
Jan 19 23:22:23.550: INFO: 
Jan 19 23:22:24.551: INFO: 40 pods remaining
Jan 19 23:22:24.551: INFO: 40 pods has nil DeletionTimestamp
Jan 19 23:22:24.551: INFO: 
Jan 19 23:22:25.557: INFO: 31 pods remaining
Jan 19 23:22:25.557: INFO: 31 pods has nil DeletionTimestamp
Jan 19 23:22:25.557: INFO: 
Jan 19 23:22:26.549: INFO: 20 pods remaining
Jan 19 23:22:26.549: INFO: 20 pods has nil DeletionTimestamp
Jan 19 23:22:26.549: INFO: 
STEP: Gathering metrics
Jan 19 23:22:27.575: INFO: The status of Pod kube-controller-manager-ip-10-0-0-80.eu-central-1.compute.internal is Running (Ready = true)
Jan 19 23:22:27.873: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:22:27.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1600" for this suite.

• [SLOW TEST:12.388 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":187,"skipped":3306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:22:27.881: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:28:01.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5812" for this suite.

• [SLOW TEST:334.116 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":188,"skipped":3330,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:28:01.997: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 19 23:28:02.058: INFO: Waiting up to 5m0s for pod "pod-334cff23-4577-4a90-a94b-ce6cb872cf3c" in namespace "emptydir-2801" to be "Succeeded or Failed"
Jan 19 23:28:02.060: INFO: Pod "pod-334cff23-4577-4a90-a94b-ce6cb872cf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.962092ms
Jan 19 23:28:04.064: INFO: Pod "pod-334cff23-4577-4a90-a94b-ce6cb872cf3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006356846s
Jan 19 23:28:06.068: INFO: Pod "pod-334cff23-4577-4a90-a94b-ce6cb872cf3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010002296s
STEP: Saw pod success
Jan 19 23:28:06.068: INFO: Pod "pod-334cff23-4577-4a90-a94b-ce6cb872cf3c" satisfied condition "Succeeded or Failed"
Jan 19 23:28:06.070: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-334cff23-4577-4a90-a94b-ce6cb872cf3c container test-container: <nil>
STEP: delete the pod
Jan 19 23:28:06.092: INFO: Waiting for pod pod-334cff23-4577-4a90-a94b-ce6cb872cf3c to disappear
Jan 19 23:28:06.100: INFO: Pod pod-334cff23-4577-4a90-a94b-ce6cb872cf3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:28:06.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2801" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:28:06.108: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-0f02de14-7908-44ab-b6cb-f1c831c502d2
STEP: Creating a pod to test consume configMaps
Jan 19 23:28:06.152: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a" in namespace "projected-4576" to be "Succeeded or Failed"
Jan 19 23:28:06.154: INFO: Pod "pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.991832ms
Jan 19 23:28:08.156: INFO: Pod "pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004102324s
Jan 19 23:28:10.159: INFO: Pod "pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007480555s
STEP: Saw pod success
Jan 19 23:28:10.159: INFO: Pod "pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a" satisfied condition "Succeeded or Failed"
Jan 19 23:28:10.161: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:28:10.181: INFO: Waiting for pod pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a to disappear
Jan 19 23:28:10.183: INFO: Pod pod-projected-configmaps-efa6f8e9-a94b-4b5b-8215-14cd1d3f7e3a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:28:10.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4576" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3417,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:28:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-e9692017-8c35-4fb9-b5c7-b70899353c35
STEP: Creating a pod to test consume configMaps
Jan 19 23:28:10.233: INFO: Waiting up to 5m0s for pod "pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0" in namespace "configmap-9719" to be "Succeeded or Failed"
Jan 19 23:28:10.235: INFO: Pod "pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.940749ms
Jan 19 23:28:12.237: INFO: Pod "pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004643361s
Jan 19 23:28:14.242: INFO: Pod "pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009243138s
STEP: Saw pod success
Jan 19 23:28:14.242: INFO: Pod "pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0" satisfied condition "Succeeded or Failed"
Jan 19 23:28:14.244: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:28:14.264: INFO: Waiting for pod pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0 to disappear
Jan 19 23:28:14.266: INFO: Pod pod-configmaps-80d4d384-ae68-4838-be94-85b52f9cdad0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:28:14.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9719" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:28:14.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-pjhx
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 23:28:14.317: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pjhx" in namespace "subpath-4753" to be "Succeeded or Failed"
Jan 19 23:28:14.319: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.931108ms
Jan 19 23:28:16.323: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005280264s
Jan 19 23:28:18.326: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 4.008268518s
Jan 19 23:28:20.328: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 6.010826632s
Jan 19 23:28:22.333: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 8.01547238s
Jan 19 23:28:24.336: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 10.019040113s
Jan 19 23:28:26.340: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 12.022930186s
Jan 19 23:28:28.344: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 14.027117388s
Jan 19 23:28:30.350: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 16.033045789s
Jan 19 23:28:32.356: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 18.038434022s
Jan 19 23:28:34.363: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 20.045608714s
Jan 19 23:28:36.366: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Running", Reason="", readiness=true. Elapsed: 22.0483532s
Jan 19 23:28:38.369: INFO: Pod "pod-subpath-test-configmap-pjhx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.051574275s
STEP: Saw pod success
Jan 19 23:28:38.369: INFO: Pod "pod-subpath-test-configmap-pjhx" satisfied condition "Succeeded or Failed"
Jan 19 23:28:38.371: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-subpath-test-configmap-pjhx container test-container-subpath-configmap-pjhx: <nil>
STEP: delete the pod
Jan 19 23:28:38.384: INFO: Waiting for pod pod-subpath-test-configmap-pjhx to disappear
Jan 19 23:28:38.390: INFO: Pod pod-subpath-test-configmap-pjhx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pjhx
Jan 19 23:28:38.390: INFO: Deleting pod "pod-subpath-test-configmap-pjhx" in namespace "subpath-4753"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:28:38.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4753" for this suite.

• [SLOW TEST:24.125 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":192,"skipped":3460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:28:38.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jan 19 23:28:49.111: INFO: 68 pods remaining
Jan 19 23:28:49.111: INFO: 68 pods has nil DeletionTimestamp
Jan 19 23:28:49.111: INFO: 
STEP: Gathering metrics
Jan 19 23:28:54.145: INFO: The status of Pod kube-controller-manager-ip-10-0-0-80.eu-central-1.compute.internal is Running (Ready = true)
Jan 19 23:28:54.431: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 19 23:28:54.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-26n82" in namespace "gc-5334"
Jan 19 23:28:54.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-27w4b" in namespace "gc-5334"
Jan 19 23:28:54.452: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d976" in namespace "gc-5334"
Jan 19 23:28:54.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g47s" in namespace "gc-5334"
Jan 19 23:28:54.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j5l5" in namespace "gc-5334"
Jan 19 23:28:54.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jw96" in namespace "gc-5334"
Jan 19 23:28:54.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bfxv" in namespace "gc-5334"
Jan 19 23:28:54.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lzbd" in namespace "gc-5334"
Jan 19 23:28:54.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tlxm" in namespace "gc-5334"
Jan 19 23:28:54.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-526j7" in namespace "gc-5334"
Jan 19 23:28:54.563: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hgrn" in namespace "gc-5334"
Jan 19 23:28:54.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hwnf" in namespace "gc-5334"
Jan 19 23:28:54.587: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v2lq" in namespace "gc-5334"
Jan 19 23:28:54.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w9dr" in namespace "gc-5334"
Jan 19 23:28:54.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xzqb" in namespace "gc-5334"
Jan 19 23:28:54.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-66xfj" in namespace "gc-5334"
Jan 19 23:28:54.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fgl5" in namespace "gc-5334"
Jan 19 23:28:54.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nf7k" in namespace "gc-5334"
Jan 19 23:28:54.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-74wz4" in namespace "gc-5334"
Jan 19 23:28:54.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-7cfqk" in namespace "gc-5334"
Jan 19 23:28:54.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-7t6zk" in namespace "gc-5334"
Jan 19 23:28:54.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wv54" in namespace "gc-5334"
Jan 19 23:28:54.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-84mls" in namespace "gc-5334"
Jan 19 23:28:54.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-85zfw" in namespace "gc-5334"
Jan 19 23:28:54.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b4zm" in namespace "gc-5334"
Jan 19 23:28:54.828: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f2mp" in namespace "gc-5334"
Jan 19 23:28:54.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jqm4" in namespace "gc-5334"
Jan 19 23:28:54.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tvb9" in namespace "gc-5334"
Jan 19 23:28:54.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h9rj" in namespace "gc-5334"
Jan 19 23:28:54.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qgx8" in namespace "gc-5334"
Jan 19 23:28:54.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s7ng" in namespace "gc-5334"
Jan 19 23:28:54.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkbp5" in namespace "gc-5334"
Jan 19 23:28:54.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-bthhn" in namespace "gc-5334"
Jan 19 23:28:54.934: INFO: Deleting pod "simpletest-rc-to-be-deleted-bznbq" in namespace "gc-5334"
Jan 19 23:28:54.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-c457k" in namespace "gc-5334"
Jan 19 23:28:54.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmg67" in namespace "gc-5334"
Jan 19 23:28:54.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctl5h" in namespace "gc-5334"
Jan 19 23:28:54.983: INFO: Deleting pod "simpletest-rc-to-be-deleted-dblkk" in namespace "gc-5334"
Jan 19 23:28:54.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-df2mw" in namespace "gc-5334"
Jan 19 23:28:55.004: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfl6v" in namespace "gc-5334"
Jan 19 23:28:55.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-dk2k7" in namespace "gc-5334"
Jan 19 23:28:55.028: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkcv5" in namespace "gc-5334"
Jan 19 23:28:55.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzk6k" in namespace "gc-5334"
Jan 19 23:28:55.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5528" in namespace "gc-5334"
Jan 19 23:28:55.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-fn5v8" in namespace "gc-5334"
Jan 19 23:28:55.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4zng" in namespace "gc-5334"
Jan 19 23:28:55.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnc7b" in namespace "gc-5334"
Jan 19 23:28:55.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvbhm" in namespace "gc-5334"
Jan 19 23:28:55.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqr4v" in namespace "gc-5334"
Jan 19 23:28:55.135: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqtnc" in namespace "gc-5334"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:28:55.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5334" for this suite.

• [SLOW TEST:16.760 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":193,"skipped":3487,"failed":0}
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:28:55.159: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:28:55.186: INFO: Creating deployment "webserver-deployment"
Jan 19 23:28:55.194: INFO: Waiting for observed generation 1
Jan 19 23:28:57.199: INFO: Waiting for all required pods to come up
Jan 19 23:28:57.202: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 19 23:29:05.209: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 19 23:29:05.213: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 19 23:29:05.222: INFO: Updating deployment webserver-deployment
Jan 19 23:29:05.222: INFO: Waiting for observed generation 2
Jan 19 23:29:07.228: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 19 23:29:07.229: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 19 23:29:07.231: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 23:29:07.236: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 19 23:29:07.236: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 19 23:29:07.238: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 23:29:07.241: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 19 23:29:07.241: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 19 23:29:07.249: INFO: Updating deployment webserver-deployment
Jan 19 23:29:07.249: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 19 23:29:07.252: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 19 23:29:09.262: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 19 23:29:09.280: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3760  11e12114-e4ac-4785-bfdd-7318139921b3 82831 3 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e9e898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-01-19 23:29:07 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-01-19 23:29:07 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 19 23:29:09.283: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-3760  0f69cf32-67ba-4531-85ef-c93f3a90b59d 82828 3 2022-01-19 23:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 11e12114-e4ac-4785-bfdd-7318139921b3 0xc005a73be7 0xc005a73be8}] []  [{kube-controller-manager Update apps/v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11e12114-e4ac-4785-bfdd-7318139921b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a73c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:29:09.283: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 19 23:29:09.283: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-3760  302d3a7e-34e6-412e-91e1-1f056458fe9d 82818 3 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 11e12114-e4ac-4785-bfdd-7318139921b3 0xc005a73cf7 0xc005a73cf8}] []  [{kube-controller-manager Update apps/v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11e12114-e4ac-4785-bfdd-7318139921b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:28:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a73d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:29:09.288: INFO: Pod "webserver-deployment-566f96c878-2bxfq" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-2bxfq webserver-deployment-566f96c878- deployment-3760  db2aea85-33a4-42cc-847d-3c1e90166a6a 82931 0 2022-01-19 23:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:cda1be4242e612e585730fa1c7848572650fa6ea15f6a075179812064a2b0ef9 cni.projectcalico.org/podIP:172.16.214.217/32 cni.projectcalico.org/podIPs:172.16.214.217/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc003c5a1f7 0xc003c5a1f8}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.214.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrggf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrggf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-103.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.103,PodIP:172.16.214.217,StartTime:2022-01-19 23:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.214.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.288: INFO: Pod "webserver-deployment-566f96c878-8fpjq" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-8fpjq webserver-deployment-566f96c878- deployment-3760  971f3a26-c983-4755-b07e-8fe599b27d64 82908 0 2022-01-19 23:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:34d7ee4ffb56c50785a4f5f89c5b2b15220a5b25050704468121a3239b9a33de cni.projectcalico.org/podIP:172.16.138.35/32 cni.projectcalico.org/podIPs:172.16.138.35/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c00b0 0xc0064c00b1}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:29:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.138.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdbpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdbpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:172.16.138.35,StartTime:2022-01-19 23:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.138.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.288: INFO: Pod "webserver-deployment-566f96c878-9xdt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9xdt8 webserver-deployment-566f96c878- deployment-3760  402a91e6-9309-40b9-b98b-93cbaf5f82d7 82900 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:d44e96b45f9852dc18958508801b8bf4941407acd9a41e1e0abf431fcbbea5f1 cni.projectcalico.org/podIP:172.16.138.40/32 cni.projectcalico.org/podIPs:172.16.138.40/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c0300 0xc0064c0301}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gltp7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gltp7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.289: INFO: Pod "webserver-deployment-566f96c878-b88bj" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-b88bj webserver-deployment-566f96c878- deployment-3760  5cf48066-4e05-4cd4-8206-c9616c89341a 82865 0 2022-01-19 23:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:6f4a16148d3fbfd37664d5d97fa8ea0eede6f59f0623a988c6e30c27e30f2579 cni.projectcalico.org/podIP:172.16.146.1/32 cni.projectcalico.org/podIPs:172.16.146.1/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c0510 0xc0064c0511}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-01-19 23:29:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zj9pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zj9pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:,StartTime:2022-01-19 23:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.289: INFO: Pod "webserver-deployment-566f96c878-c8b8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-c8b8p webserver-deployment-566f96c878- deployment-3760  5359ab95-f1a7-406d-8fa2-b221764d7ffb 82928 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:30e00acf3c68f26de40225c0924935f269b1f0d99dbe63b477c3297d7500c667 cni.projectcalico.org/podIP:172.16.138.63/32 cni.projectcalico.org/podIPs:172.16.138.63/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c0730 0xc0064c0731}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8cg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8cg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.289: INFO: Pod "webserver-deployment-566f96c878-j9vc6" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-j9vc6 webserver-deployment-566f96c878- deployment-3760  f06c4fab-4a0f-4ecc-8156-c7fc9214bb2c 82932 0 2022-01-19 23:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:887c563c612fd1b7605f8fa8bec3ae74ec0cf5ee7e566176d80fd762297d3118 cni.projectcalico.org/podIP:172.16.32.134/32 cni.projectcalico.org/podIPs:172.16.32.134/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c0960 0xc0064c0961}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.32.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q55hg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q55hg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:172.16.32.134,StartTime:2022-01-19 23:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.32.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.289: INFO: Pod "webserver-deployment-566f96c878-jmrbl" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-jmrbl webserver-deployment-566f96c878- deployment-3760  4e846dd8-d584-4528-bbc9-e432cfe1016d 82846 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c0bb0 0xc0064c0bb1}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbhtx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbhtx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-103.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.103,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.289: INFO: Pod "webserver-deployment-566f96c878-llxdw" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-llxdw webserver-deployment-566f96c878- deployment-3760  ff7f8520-0b81-4d92-a66c-28a0ec3ac29b 82824 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c0d80 0xc0064c0d81}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5fmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5fmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.290: INFO: Pod "webserver-deployment-566f96c878-p6jd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-p6jd2 webserver-deployment-566f96c878- deployment-3760  2372ba13-eb51-4391-ac55-6a27e9b2aefb 82889 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:584bb901fd6349d0703afdbcaa38d8fd4b2b31a7189781eba71ed42bfe6dcf4c cni.projectcalico.org/podIP:172.16.214.215/32 cni.projectcalico.org/podIPs:172.16.214.215/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c0ee0 0xc0064c0ee1}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzvqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzvqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-103.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.103,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.290: INFO: Pod "webserver-deployment-566f96c878-pc9rr" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-pc9rr webserver-deployment-566f96c878- deployment-3760  c0f93b78-6103-4f41-aa1f-9b51cec833e2 82807 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c10d0 0xc0064c10d1}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tk9dp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tk9dp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.290: INFO: Pod "webserver-deployment-566f96c878-t2md2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-t2md2 webserver-deployment-566f96c878- deployment-3760  ac669f8e-6197-4fd7-b2b5-930166d62561 82901 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:7059153f8e6df142361d4926020783935404d2fcef81ebe02ebafcf5ce7f079c cni.projectcalico.org/podIP:172.16.32.140/32 cni.projectcalico.org/podIPs:172.16.32.140/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c1250 0xc0064c1251}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-svbrc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-svbrc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.290: INFO: Pod "webserver-deployment-566f96c878-txbvf" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-txbvf webserver-deployment-566f96c878- deployment-3760  6ef8b797-1678-4a4d-999b-b1a59325e7ee 82827 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c1440 0xc0064c1441}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5sc2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5sc2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.290: INFO: Pod "webserver-deployment-566f96c878-wrbng" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-wrbng webserver-deployment-566f96c878- deployment-3760  ae373757-4cef-4606-87e4-0163d208b6a3 82760 0 2022-01-19 23:29:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:27b3e14bc227247499e72e197136d1103ad50e46918fbc6149b115c873cd320f cni.projectcalico.org/podIP:172.16.146.30/32 cni.projectcalico.org/podIPs:172.16.146.30/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 0f69cf32-67ba-4531-85ef-c93f3a90b59d 0xc0064c1630 0xc0064c1631}] []  [{Go-http-client Update v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f69cf32-67ba-4531-85ef-c93f3a90b59d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b69x6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b69x6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:,StartTime:2022-01-19 23:29:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.291: INFO: Pod "webserver-deployment-5d9fdcc779-4g9sf" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4g9sf webserver-deployment-5d9fdcc779- deployment-3760  d5690421-5c1e-4426-b265-fb7acb7eb861 82384 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:63d40a7e65fdb7b36b86f560ab0cf80ea7858e6f96a579bdf020b67de9b86ada cni.projectcalico.org/podIP:172.16.214.195/32 cni.projectcalico.org/podIPs:172.16.214.195/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc0064c1820 0xc0064c1821}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:29:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.214.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67gtf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67gtf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-103.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.103,PodIP:172.16.214.195,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://37a019589c6de1f93af1d7346f2813c4ed6ad64d35b13ed71854014c9d83f90a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.214.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.291: INFO: Pod "webserver-deployment-5d9fdcc779-4hh8l" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4hh8l webserver-deployment-5d9fdcc779- deployment-3760  b8d10685-688d-496c-ac76-7091c9d5d054 82669 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:5b0ba43a5f024732e6b5e9f8ca70fc1424b0de36252c7c94ef43e4193fe56e39 cni.projectcalico.org/podIP:172.16.146.39/32 cni.projectcalico.org/podIPs:172.16.146.39/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc0064c1a50 0xc0064c1a51}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:29:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvrmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvrmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:172.16.146.39,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://04fcad122c272dbf5ddeb36b1e96a906e132139431d91fbc38e6612065e808b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.146.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.291: INFO: Pod "webserver-deployment-5d9fdcc779-5n8kb" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5n8kb webserver-deployment-5d9fdcc779- deployment-3760  b3cf6b20-b44c-45df-aa33-a4d97b67cdac 82222 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:872d4d3d8dd4177dce98506d82519b91d12954643180cfc9d277dff9034b8d9f cni.projectcalico.org/podIP:172.16.138.24/32 cni.projectcalico.org/podIPs:172.16.138.24/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc0064c1c80 0xc0064c1c81}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.138.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nntpj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nntpj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:172.16.138.24,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://93a0546b297673ce94107cd7b10092b2f71fd4a46d7773ca651010a506f926ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.138.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.291: INFO: Pod "webserver-deployment-5d9fdcc779-7l2vk" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7l2vk webserver-deployment-5d9fdcc779- deployment-3760  3e8841c4-fd5e-44e0-91ae-92ec95594771 82247 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:ab427d7df6608f90bafc20f9db42dc74f02bcd4f2055bdcf51bb71827cc025b2 cni.projectcalico.org/podIP:172.16.138.34/32 cni.projectcalico.org/podIPs:172.16.138.34/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc0064c1eb0 0xc0064c1eb1}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:29:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.138.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cggmd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cggmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:172.16.138.34,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://b4bb780cf310338a50afa8929d831eabc76fecc5861af16fc110315c8c44874e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.138.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.292: INFO: Pod "webserver-deployment-5d9fdcc779-8hbks" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-8hbks webserver-deployment-5d9fdcc779- deployment-3760  5c0164f6-09c0-400b-8e0a-18aa43a23cd4 82850 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0ca0b0 0xc00a0ca0b1}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5cj5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5cj5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.292: INFO: Pod "webserver-deployment-5d9fdcc779-8m6vs" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-8m6vs webserver-deployment-5d9fdcc779- deployment-3760  45a250eb-5a45-4441-b0c4-c2ce47d75e23 82935 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0ca260 0xc00a0ca261}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgtm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgtm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.292: INFO: Pod "webserver-deployment-5d9fdcc779-bj7kf" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-bj7kf webserver-deployment-5d9fdcc779- deployment-3760  0174538d-0a1c-4013-b42d-cc057f18ba3e 82904 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:4711096226fe07d798ec4c73c088663d57c77160a0bbfd895446e6296c04c50e cni.projectcalico.org/podIP:172.16.138.7/32 cni.projectcalico.org/podIPs:172.16.138.7/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0ca450 0xc00a0ca451}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fsgf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fsgf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.292: INFO: Pod "webserver-deployment-5d9fdcc779-gbvj9" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gbvj9 webserver-deployment-5d9fdcc779- deployment-3760  5bd0b526-fa81-43ea-bfb0-cb2207d65e8d 82207 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:fe3963f01883c879bb85420a113e3f2094b556d0e561dd737b6449d00e0ff6d1 cni.projectcalico.org/podIP:172.16.214.252/32 cni.projectcalico.org/podIPs:172.16.214.252/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0ca620 0xc00a0ca621}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.214.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trwbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trwbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-103.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.103,PodIP:172.16.214.252,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://74019ac11a4302f77fdd5451e9adfc8c7c3281732e27583dc47c7b5cc4309ae4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.214.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.293: INFO: Pod "webserver-deployment-5d9fdcc779-ggjmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ggjmh webserver-deployment-5d9fdcc779- deployment-3760  a239dae7-ffad-443e-a6d2-3fa4e72e436d 82814 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0ca820 0xc00a0ca821}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wcsdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wcsdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.293: INFO: Pod "webserver-deployment-5d9fdcc779-hlxj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-hlxj5 webserver-deployment-5d9fdcc779- deployment-3760  497c9373-2487-4b3b-9aef-785583d6ebd8 82927 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:d0a9f02f14c9e44f9356900330ba3a8ded3ec7fedfa76358432a06f59edbd6ab cni.projectcalico.org/podIP:172.16.32.141/32 cni.projectcalico.org/podIPs:172.16.32.141/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0ca990 0xc00a0ca991}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2trp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2trp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.293: INFO: Pod "webserver-deployment-5d9fdcc779-hpsbc" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-hpsbc webserver-deployment-5d9fdcc779- deployment-3760  fb7f4ac1-5fed-475a-8951-d7e01f319a0e 82377 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:37ab6263113e815e0468ad8fe9f798bde08d358ae0eed6279440f26bdf1a808e cni.projectcalico.org/podIP:172.16.138.29/32 cni.projectcalico.org/podIPs:172.16.138.29/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cab80 0xc00a0cab81}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:29:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.138.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77kd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77kd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:172.16.138.29,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://cc1b81d8e260105a7119a9c2d02bfc61256e0d76754ece29d8df5b4e7fd660c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.138.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.293: INFO: Pod "webserver-deployment-5d9fdcc779-j8j64" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-j8j64 webserver-deployment-5d9fdcc779- deployment-3760  6f01af54-096c-45ba-9228-d40b2777b106 82837 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cad80 0xc00a0cad81}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mx248,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mx248,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.293: INFO: Pod "webserver-deployment-5d9fdcc779-jqfq4" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-jqfq4 webserver-deployment-5d9fdcc779- deployment-3760  8f7c8664-2455-4565-846f-4ef3e28c8882 82920 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:6d57b9dfce45ff54fad31e100a0514c3c2d981d6985cf137105b18dca130e186 cni.projectcalico.org/podIP:172.16.138.52/32 cni.projectcalico.org/podIPs:172.16.138.52/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0caf50 0xc00a0caf51}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhqtq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhqtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.48,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.294: INFO: Pod "webserver-deployment-5d9fdcc779-kqstb" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-kqstb webserver-deployment-5d9fdcc779- deployment-3760  c00ef9e9-6996-4b01-8982-24dcc0d5824c 82787 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cb120 0xc00a0cb121}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qspwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qspwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.294: INFO: Pod "webserver-deployment-5d9fdcc779-pjgr2" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-pjgr2 webserver-deployment-5d9fdcc779- deployment-3760  9552fca8-4617-431b-84c0-c4ea9089c3ac 82218 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:c5a2c4a20a8a35a162f322be007433a8dddab907990854aadcc8ea9b28b49ade cni.projectcalico.org/podIP:172.16.32.138/32 cni.projectcalico.org/podIPs:172.16.32.138/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cb2f0 0xc00a0cb2f1}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.32.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6568,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6568,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:172.16.32.138,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://d082451d2fc2566b48f167c72c57992369dea3830e7ac746aee68848638c7d79,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.32.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.294: INFO: Pod "webserver-deployment-5d9fdcc779-pp4sf" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-pp4sf webserver-deployment-5d9fdcc779- deployment-3760  3146b525-f4af-4a64-aa35-1c81104d0d7e 82840 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cb4f0 0xc00a0cb4f1}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26gcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26gcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-103.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.103,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.294: INFO: Pod "webserver-deployment-5d9fdcc779-qks9x" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-qks9x webserver-deployment-5d9fdcc779- deployment-3760  712db6f5-011b-4340-8cf0-e899d19f584f 82808 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cb6a0 0xc00a0cb6a1}] []  [{kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2z4vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z4vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.295: INFO: Pod "webserver-deployment-5d9fdcc779-xjlns" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-xjlns webserver-deployment-5d9fdcc779- deployment-3760  3415c4ce-e267-4083-b0b1-343ea98ce64c 82212 0 2022-01-19 23:28:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:0b5a3a52365d7a5457ff294e6ea269b3ad0d7a5dddb01813f695331382a3606a cni.projectcalico.org/podIP:172.16.32.139/32 cni.projectcalico.org/podIPs:172.16.32.139/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cb810 0xc00a0cb811}] []  [{kube-controller-manager Update v1 2022-01-19 23:28:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:28:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.32.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sp5hb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sp5hb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-62.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:28:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.62,PodIP:172.16.32.139,StartTime:2022-01-19 23:28:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:28:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://4c45dd46efcf2de29eb6b795b050b30397ceb7665d37d6251637e356ebcab289,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.32.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.295: INFO: Pod "webserver-deployment-5d9fdcc779-zd7vg" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-zd7vg webserver-deployment-5d9fdcc779- deployment-3760  aecf3327-76b4-4491-973c-69580f8f41e0 82916 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:6f8e4d83cbf6a7ed756b3b40589f238f31496e8b5734ead0cfd9e68dbeb98bc2 cni.projectcalico.org/podIP:172.16.214.218/32 cni.projectcalico.org/podIPs:172.16.214.218/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cba10 0xc00a0cba11}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2fkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2fkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-103.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.103,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:29:09.295: INFO: Pod "webserver-deployment-5d9fdcc779-zrkcv" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-zrkcv webserver-deployment-5d9fdcc779- deployment-3760  22593c70-103f-4f29-86d1-39a07a97b392 82915 0 2022-01-19 23:29:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:2b3280c877b3cabd74a96b92f1215d6bb12911958279bd21e1838af9ced51bdc cni.projectcalico.org/podIP:172.16.146.16/32 cni.projectcalico.org/podIPs:172.16.146.16/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 302d3a7e-34e6-412e-91e1-1f056458fe9d 0xc00a0cbc00 0xc00a0cbc01}] []  [{Go-http-client Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-01-19 23:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"302d3a7e-34e6-412e-91e1-1f056458fe9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gnlzr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gnlzr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:29:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:,StartTime:2022-01-19 23:29:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:29:09.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3760" for this suite.

• [SLOW TEST:14.146 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":194,"skipped":3487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:29:09.305: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:29:09.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6" in namespace "downward-api-8479" to be "Succeeded or Failed"
Jan 19 23:29:09.343: INFO: Pod "downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060868ms
Jan 19 23:29:11.347: INFO: Pod "downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006493278s
Jan 19 23:29:13.351: INFO: Pod "downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009844766s
Jan 19 23:29:15.359: INFO: Pod "downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017989474s
STEP: Saw pod success
Jan 19 23:29:15.360: INFO: Pod "downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6" satisfied condition "Succeeded or Failed"
Jan 19 23:29:15.369: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6 container client-container: <nil>
STEP: delete the pod
Jan 19 23:29:15.450: INFO: Waiting for pod downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6 to disappear
Jan 19 23:29:15.469: INFO: Pod downwardapi-volume-202d5674-6bc1-4383-8673-82b2f9a85fc6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:29:15.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8479" for this suite.

• [SLOW TEST:6.201 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3520,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:29:15.508: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-r49g
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 23:29:15.607: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-r49g" in namespace "subpath-255" to be "Succeeded or Failed"
Jan 19 23:29:15.617: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Pending", Reason="", readiness=false. Elapsed: 10.321145ms
Jan 19 23:29:17.621: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013812288s
Jan 19 23:29:19.626: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 4.018794829s
Jan 19 23:29:21.628: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 6.021496322s
Jan 19 23:29:23.632: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 8.024880816s
Jan 19 23:29:25.635: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 10.028368892s
Jan 19 23:29:27.640: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 12.032999703s
Jan 19 23:29:29.643: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 14.036516857s
Jan 19 23:29:31.651: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 16.043629979s
Jan 19 23:29:33.655: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 18.04825833s
Jan 19 23:29:35.663: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 20.056365768s
Jan 19 23:29:37.669: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Running", Reason="", readiness=true. Elapsed: 22.061613338s
Jan 19 23:29:39.675: INFO: Pod "pod-subpath-test-downwardapi-r49g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.067565143s
STEP: Saw pod success
Jan 19 23:29:39.675: INFO: Pod "pod-subpath-test-downwardapi-r49g" satisfied condition "Succeeded or Failed"
Jan 19 23:29:39.676: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-r49g container test-container-subpath-downwardapi-r49g: <nil>
STEP: delete the pod
Jan 19 23:29:39.697: INFO: Waiting for pod pod-subpath-test-downwardapi-r49g to disappear
Jan 19 23:29:39.699: INFO: Pod pod-subpath-test-downwardapi-r49g no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-r49g
Jan 19 23:29:39.699: INFO: Deleting pod "pod-subpath-test-downwardapi-r49g" in namespace "subpath-255"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:29:39.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-255" for this suite.

• [SLOW TEST:24.199 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":196,"skipped":3536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:29:39.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:29:40.233: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 23:29:42.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 29, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 29, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 29, 40, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 29, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:29:45.256: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:29:45.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9215" for this suite.
STEP: Destroying namespace "webhook-9215-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.760 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":197,"skipped":3566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:29:45.467: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 19 23:29:45.509: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 23:29:45.515: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 23:29:45.517: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-103.eu-central-1.compute.internal before test
Jan 19 23:29:45.529: INFO: cert-manager-774679ff99-fg5j9 from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container cert-manager ready: true, restart count 0
Jan 19 23:29:45.529: INFO: gatekeeper-audit-59fcc9f644-7fmkr from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:29:45.529: INFO: gatekeeper-controller-manager-86677cdd6d-59r5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:29:45.529: INFO: calico-node-rpdnw from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:29:45.529: INFO: kube-proxy-l9kpk from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:29:45.529: INFO: minio-0 from kube-system started at 2022-01-19 22:19:21 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container minio ready: true, restart count 0
Jan 19 23:29:45.529: INFO: velero-restic-dr6p6 from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:29:45.529: INFO: elasticsearch-0 from logging started at 2022-01-19 22:19:23 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container elasticsearch ready: true, restart count 0
Jan 19 23:29:45.529: INFO: 	Container exporter ready: true, restart count 0
Jan 19 23:29:45.529: INFO: fluentbit-zwj8m from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:29:45.529: INFO: alertmanager-main-2 from monitoring started at 2022-01-19 22:40:34 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 23:29:45.529: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:29:45.529: INFO: goldpinger-96ctd from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:29:45.529: INFO: kube-proxy-metrics-wbqwc from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:29:45.529: INFO: node-exporter-tqbmk from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:29:45.529: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:29:45.529: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csw5s from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:29:45.529: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 23:29:45.529: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-153.eu-central-1.compute.internal before test
Jan 19 23:29:45.542: INFO: forbid-27377248-s97sc from cronjob-5812 started at 2022-01-19 23:28:01 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container c ready: false, restart count 0
Jan 19 23:29:45.542: INFO: calico-node-c7xgd from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:29:45.542: INFO: kube-proxy-9p6s2 from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:29:45.542: INFO: velero-restic-xtp74 from kube-system started at 2022-01-19 22:58:22 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:29:45.542: INFO: fluentbit-nz7dc from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:29:45.542: INFO: fluentd-1 from logging started at 2022-01-19 22:58:24 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container fluentd ready: true, restart count 0
Jan 19 23:29:45.542: INFO: goldpinger-9n9xc from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:29:45.542: INFO: kube-proxy-metrics-nx74g from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:29:45.542: INFO: node-exporter-ptzzd from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:29:45.542: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:29:45.542: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-nbt7p from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.542: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:29:45.542: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 23:29:45.542: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-48.eu-central-1.compute.internal before test
Jan 19 23:29:45.555: INFO: cert-manager-cainjector-66fcc559bf-wgm9t from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container cainjector ready: true, restart count 0
Jan 19 23:29:45.555: INFO: gatekeeper-controller-manager-86677cdd6d-fps5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:29:45.555: INFO: gatekeeper-policy-manager-7644bb5475-cwmpf from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container gatekeeper-policy-manager ready: true, restart count 0
Jan 19 23:29:45.555: INFO: nginx-ingress-controller-6bcf978d9-w2wlr from ingress-nginx started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 23:29:45.555: INFO: calico-node-c8np5 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:29:45.555: INFO: kube-proxy-prc9k from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:29:45.555: INFO: velero-restic-nkgcd from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:29:45.555: INFO: cerebro-6f88ff7888-plncf from logging started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container cerebro ready: true, restart count 0
Jan 19 23:29:45.555: INFO: fluentbit-wt6cx from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:29:45.555: INFO: fluentd-0 from logging started at 2022-01-19 22:19:52 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container fluentd ready: true, restart count 1
Jan 19 23:29:45.555: INFO: alertmanager-main-0 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 23:29:45.555: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:29:45.555: INFO: goldpinger-g9xm2 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:29:45.555: INFO: kube-proxy-metrics-z42ff from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:29:45.555: INFO: node-exporter-w54w5 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:29:45.555: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:29:45.555: INFO: prometheus-k8s-0 from monitoring started at 2022-01-19 22:21:16 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:29:45.555: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 23:29:45.555: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-qdwj2 from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.555: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:29:45.555: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 23:29:45.555: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-62.eu-central-1.compute.internal before test
Jan 19 23:29:45.568: INFO: cert-manager-webhook-77ffc44fc6-8fxlk from cert-manager started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container webhook ready: true, restart count 0
Jan 19 23:29:45.568: INFO: gatekeeper-controller-manager-86677cdd6d-mnx56 from gatekeeper-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:29:45.568: INFO: calico-kube-controllers-59c967558b-nwcpg from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 23:29:45.568: INFO: calico-node-jbfb7 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:29:45.568: INFO: coredns-64897985d-cftqm from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container coredns ready: true, restart count 0
Jan 19 23:29:45.568: INFO: coredns-64897985d-l9c4t from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container coredns ready: true, restart count 0
Jan 19 23:29:45.568: INFO: kube-proxy-7shjp from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:29:45.568: INFO: metrics-server-6f5f78d676-mnlwp from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 23:29:45.568: INFO: velero-6f84f7465-tgzks from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container velero ready: true, restart count 0
Jan 19 23:29:45.568: INFO: velero-restic-6xdjj from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:29:45.568: INFO: local-path-provisioner-5b94755fb4-8zcbz from local-path-storage started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.568: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jan 19 23:29:45.568: INFO: fluentbit-sj8hh from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:29:45.569: INFO: fluentd-2 from logging started at 2022-01-19 22:20:56 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container fluentd ready: true, restart count 0
Jan 19 23:29:45.569: INFO: kibana-8565fbd49b-pm9nr from logging started at 2022-01-19 22:40:29 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container kibana ready: true, restart count 0
Jan 19 23:29:45.569: INFO: 	Container kibana-index-patterns ready: true, restart count 0
Jan 19 23:29:45.569: INFO: alertmanager-main-1 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 23:29:45.569: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:29:45.569: INFO: goldpinger-ml8vn from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:29:45.569: INFO: grafana-7c96db944d-cq4n6 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container dashboard-sidecar ready: true, restart count 0
Jan 19 23:29:45.569: INFO: 	Container grafana ready: true, restart count 0
Jan 19 23:29:45.569: INFO: kube-proxy-metrics-lvbg9 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:29:45.569: INFO: kube-state-metrics-7456544d4b-wjgk8 from monitoring started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 23:29:45.569: INFO: node-exporter-x46g7 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:29:45.569: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:29:45.569: INFO: prometheus-operator-7db7f75fdb-624bl from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 23:29:45.569: INFO: sonobuoy from sonobuoy started at 2022-01-19 22:27:04 +0000 UTC (1 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 23:29:45.569: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csbhg from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:29:45.569: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:29:45.569: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16cbcfd0fc410e19], Reason = [FailedScheduling], Message = [0/5 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:29:46.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7223" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":198,"skipped":3597,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:29:46.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2328
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 23:29:46.637: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 23:29:46.693: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:29:48.696: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:29:50.698: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:29:52.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:29:54.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:29:56.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:29:58.695: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:30:00.696: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:30:02.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:30:04.697: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 19 23:30:06.706: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 19 23:30:06.710: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 19 23:30:06.724: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jan 19 23:30:06.727: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jan 19 23:30:10.780: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jan 19 23:30:10.780: INFO: Going to poll 172.16.214.219 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:30:10.781: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.214.219:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:30:10.781: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:30:10.782: INFO: ExecWithOptions: Clientset creation
Jan 19 23:30:10.782: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.214.219%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:30:10.892: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 19 23:30:10.892: INFO: Going to poll 172.16.146.45 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:30:10.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.146.45:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:30:10.894: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:30:10.895: INFO: ExecWithOptions: Clientset creation
Jan 19 23:30:10.895: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.146.45%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:30:11.002: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 19 23:30:11.002: INFO: Going to poll 172.16.138.56 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:30:11.004: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.138.56:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:30:11.004: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:30:11.005: INFO: ExecWithOptions: Clientset creation
Jan 19 23:30:11.005: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.138.56%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:30:11.111: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 19 23:30:11.111: INFO: Going to poll 172.16.32.142 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jan 19 23:30:11.113: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.32.142:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:30:11.113: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:30:11.113: INFO: ExecWithOptions: Clientset creation
Jan 19 23:30:11.113: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.32.142%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:30:11.218: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:30:11.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2328" for this suite.

• [SLOW TEST:24.616 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:30:11.225: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:30:11.271: INFO: The status of Pod busybox-host-aliasesdb799fc8-1bab-4444-8bc4-95721319b3b5 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:30:13.277: INFO: The status of Pod busybox-host-aliasesdb799fc8-1bab-4444-8bc4-95721319b3b5 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:30:15.275: INFO: The status of Pod busybox-host-aliasesdb799fc8-1bab-4444-8bc4-95721319b3b5 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:30:15.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9416" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3647,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:30:15.289: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-f85f9089-8205-42ed-914d-cfe40c1ed128
STEP: Creating a pod to test consume configMaps
Jan 19 23:30:15.335: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0" in namespace "projected-3594" to be "Succeeded or Failed"
Jan 19 23:30:15.337: INFO: Pod "pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.841896ms
Jan 19 23:30:17.340: INFO: Pod "pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004784993s
Jan 19 23:30:19.344: INFO: Pod "pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008874898s
STEP: Saw pod success
Jan 19 23:30:19.344: INFO: Pod "pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0" satisfied condition "Succeeded or Failed"
Jan 19 23:30:19.346: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:30:19.366: INFO: Waiting for pod pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0 to disappear
Jan 19 23:30:19.368: INFO: Pod pod-projected-configmaps-3fb71db2-be3b-4bac-80d4-7413f63773d0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:30:19.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3594" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3665,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:30:19.374: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jan 19 23:30:19.407: INFO: Waiting up to 5m0s for pod "client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33" in namespace "containers-7242" to be "Succeeded or Failed"
Jan 19 23:30:19.409: INFO: Pod "client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33": Phase="Pending", Reason="", readiness=false. Elapsed: 1.966878ms
Jan 19 23:30:21.412: INFO: Pod "client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005326415s
Jan 19 23:30:23.415: INFO: Pod "client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008402852s
STEP: Saw pod success
Jan 19 23:30:23.415: INFO: Pod "client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33" satisfied condition "Succeeded or Failed"
Jan 19 23:30:23.419: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:30:23.430: INFO: Waiting for pod client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33 to disappear
Jan 19 23:30:23.438: INFO: Pod client-containers-6a7b2e18-ae3e-4a08-be35-38c4cbc21e33 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:30:23.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7242" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":202,"skipped":3674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:30:23.444: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jan 19 23:30:23.478: INFO: The status of Pod pod-hostip-994934d9-03c1-40be-b8d4-fb3cba3c093d is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:30:25.481: INFO: The status of Pod pod-hostip-994934d9-03c1-40be-b8d4-fb3cba3c093d is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:30:27.483: INFO: The status of Pod pod-hostip-994934d9-03c1-40be-b8d4-fb3cba3c093d is Running (Ready = true)
Jan 19 23:30:27.487: INFO: Pod pod-hostip-994934d9-03c1-40be-b8d4-fb3cba3c093d has hostIP: 10.0.1.153
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:30:27.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7026" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":203,"skipped":3701,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:30:27.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:30:27.523: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Creating first CR 
Jan 19 23:30:30.074: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-19T23:30:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-19T23:30:30Z]] name:name1 resourceVersion:84018 uid:c379d76d-9b10-4d71-aee3-a3fe09a3b57e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 19 23:30:40.080: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-19T23:30:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-19T23:30:40Z]] name:name2 resourceVersion:84070 uid:d4def485-6679-4877-8298-f2c76fd8afee] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 19 23:30:50.089: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-19T23:30:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-19T23:30:50Z]] name:name1 resourceVersion:84097 uid:c379d76d-9b10-4d71-aee3-a3fe09a3b57e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 19 23:31:00.095: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-19T23:30:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-19T23:31:00Z]] name:name2 resourceVersion:84129 uid:d4def485-6679-4877-8298-f2c76fd8afee] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 19 23:31:10.103: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-19T23:30:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-19T23:30:50Z]] name:name1 resourceVersion:84152 uid:c379d76d-9b10-4d71-aee3-a3fe09a3b57e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 19 23:31:20.111: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-01-19T23:30:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-01-19T23:31:00Z]] name:name2 resourceVersion:84170 uid:d4def485-6679-4877-8298-f2c76fd8afee] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:31:30.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7303" for this suite.

• [SLOW TEST:63.131 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":204,"skipped":3705,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:31:30.625: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:31:30.664: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861" in namespace "downward-api-6537" to be "Succeeded or Failed"
Jan 19 23:31:30.666: INFO: Pod "downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119423ms
Jan 19 23:31:32.671: INFO: Pod "downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007505475s
Jan 19 23:31:34.674: INFO: Pod "downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010034302s
STEP: Saw pod success
Jan 19 23:31:34.674: INFO: Pod "downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861" satisfied condition "Succeeded or Failed"
Jan 19 23:31:34.676: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861 container client-container: <nil>
STEP: delete the pod
Jan 19 23:31:34.697: INFO: Waiting for pod downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861 to disappear
Jan 19 23:31:34.699: INFO: Pod downwardapi-volume-0e2e4cb2-728b-4a95-b69a-94e65329f861 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:31:34.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6537" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":205,"skipped":3708,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:31:34.705: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 19 23:31:34.766: INFO: Waiting up to 5m0s for pod "pod-c4538449-61ff-4e38-844e-275139db6594" in namespace "emptydir-4476" to be "Succeeded or Failed"
Jan 19 23:31:34.774: INFO: Pod "pod-c4538449-61ff-4e38-844e-275139db6594": Phase="Pending", Reason="", readiness=false. Elapsed: 7.914644ms
Jan 19 23:31:36.779: INFO: Pod "pod-c4538449-61ff-4e38-844e-275139db6594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012123862s
Jan 19 23:31:38.782: INFO: Pod "pod-c4538449-61ff-4e38-844e-275139db6594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015235627s
STEP: Saw pod success
Jan 19 23:31:38.782: INFO: Pod "pod-c4538449-61ff-4e38-844e-275139db6594" satisfied condition "Succeeded or Failed"
Jan 19 23:31:38.783: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-c4538449-61ff-4e38-844e-275139db6594 container test-container: <nil>
STEP: delete the pod
Jan 19 23:31:38.794: INFO: Waiting for pod pod-c4538449-61ff-4e38-844e-275139db6594 to disappear
Jan 19 23:31:38.803: INFO: Pod pod-c4538449-61ff-4e38-844e-275139db6594 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:31:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4476" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":206,"skipped":3708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:31:38.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:31:39.302: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 23:31:41.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 31, 39, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 31, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 31, 39, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 31, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:31:44.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:31:44.348: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3145-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:31:47.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4654" for this suite.
STEP: Destroying namespace "webhook-4654-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":207,"skipped":3744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:31:47.482: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-845
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:31:47.540: INFO: Found 0 stateful pods, waiting for 1
Jan 19 23:31:57.545: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jan 19 23:31:57.560: INFO: Found 1 stateful pods, waiting for 2
Jan 19 23:32:07.565: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 23:32:07.565: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 19 23:32:07.577: INFO: Deleting all statefulset in ns statefulset-845
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:07.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-845" for this suite.

• [SLOW TEST:20.111 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":208,"skipped":3773,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:07.594: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:32:07.650: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062" in namespace "downward-api-2346" to be "Succeeded or Failed"
Jan 19 23:32:07.652: INFO: Pod "downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062": Phase="Pending", Reason="", readiness=false. Elapsed: 1.878808ms
Jan 19 23:32:09.655: INFO: Pod "downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005234862s
Jan 19 23:32:11.659: INFO: Pod "downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00874689s
STEP: Saw pod success
Jan 19 23:32:11.659: INFO: Pod "downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062" satisfied condition "Succeeded or Failed"
Jan 19 23:32:11.661: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062 container client-container: <nil>
STEP: delete the pod
Jan 19 23:32:11.681: INFO: Waiting for pod downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062 to disappear
Jan 19 23:32:11.683: INFO: Pod downwardapi-volume-6255b48c-e7c3-47d0-920a-5b415f81d062 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:11.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2346" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":3775,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:11.689: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 19 23:32:11.762: INFO: The status of Pod pod-update-ecc8481f-63c0-4c48-a5ea-311c835c9513 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:32:13.766: INFO: The status of Pod pod-update-ecc8481f-63c0-4c48-a5ea-311c835c9513 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:32:15.767: INFO: The status of Pod pod-update-ecc8481f-63c0-4c48-a5ea-311c835c9513 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 19 23:32:16.285: INFO: Successfully updated pod "pod-update-ecc8481f-63c0-4c48-a5ea-311c835c9513"
STEP: verifying the updated pod is in kubernetes
Jan 19 23:32:16.288: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:16.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9642" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":3791,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:16.295: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-2247
STEP: creating replication controller nodeport-test in namespace services-2247
I0119 23:32:16.386162      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2247, replica count: 2
Jan 19 23:32:19.438: INFO: Creating new exec pod
I0119 23:32:19.438429      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 23:32:24.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2247 exec execpodmcq2p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 23:32:25.508: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 23:32:25.508: INFO: stdout: "nodeport-test-fnr59"
Jan 19 23:32:25.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2247 exec execpodmcq2p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.163.49 80'
Jan 19 23:32:25.677: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.163.49 80\nConnection to 10.101.163.49 80 port [tcp/http] succeeded!\n"
Jan 19 23:32:25.677: INFO: stdout: ""
Jan 19 23:32:26.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2247 exec execpodmcq2p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.163.49 80'
Jan 19 23:32:26.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.163.49 80\nConnection to 10.101.163.49 80 port [tcp/http] succeeded!\n"
Jan 19 23:32:26.845: INFO: stdout: "nodeport-test-bq64p"
Jan 19 23:32:26.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2247 exec execpodmcq2p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.103 32738'
Jan 19 23:32:26.992: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.103 32738\nConnection to 10.0.1.103 32738 port [tcp/*] succeeded!\n"
Jan 19 23:32:26.992: INFO: stdout: "nodeport-test-bq64p"
Jan 19 23:32:26.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2247 exec execpodmcq2p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.48 32738'
Jan 19 23:32:27.148: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.48 32738\nConnection to 10.0.1.48 32738 port [tcp/*] succeeded!\n"
Jan 19 23:32:27.148: INFO: stdout: ""
Jan 19 23:32:28.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2247 exec execpodmcq2p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.48 32738'
Jan 19 23:32:28.306: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.48 32738\nConnection to 10.0.1.48 32738 port [tcp/*] succeeded!\n"
Jan 19 23:32:28.306: INFO: stdout: "nodeport-test-fnr59"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:28.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2247" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.019 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":211,"skipped":3808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:28.314: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:32:28.344: INFO: Got root ca configmap in namespace "svcaccounts-5948"
Jan 19 23:32:28.347: INFO: Deleted root ca configmap in namespace "svcaccounts-5948"
STEP: waiting for a new root ca configmap created
Jan 19 23:32:28.850: INFO: Recreated root ca configmap in namespace "svcaccounts-5948"
Jan 19 23:32:28.854: INFO: Updated root ca configmap in namespace "svcaccounts-5948"
STEP: waiting for the root ca configmap reconciled
Jan 19 23:32:29.357: INFO: Reconciled root ca configmap in namespace "svcaccounts-5948"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:29.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5948" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":212,"skipped":3866,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:29.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:32:29.389: INFO: Creating ReplicaSet my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29
Jan 19 23:32:29.396: INFO: Pod name my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29: Found 0 pods out of 1
Jan 19 23:32:34.399: INFO: Pod name my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29: Found 1 pods out of 1
Jan 19 23:32:34.399: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29" is running
Jan 19 23:32:34.401: INFO: Pod "my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29-jfmdv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:32:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:32:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:32:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-01-19 23:32:29 +0000 UTC Reason: Message:}])
Jan 19 23:32:34.402: INFO: Trying to dial the pod
Jan 19 23:32:39.410: INFO: Controller my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29: Got expected result from replica 1 [my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29-jfmdv]: "my-hostname-basic-c85f1dc6-a381-42c3-80b2-f5357bef5c29-jfmdv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:39.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6295" for this suite.

• [SLOW TEST:10.053 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":213,"skipped":3876,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:39.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:32:39.452: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0" in namespace "projected-828" to be "Succeeded or Failed"
Jan 19 23:32:39.454: INFO: Pod "downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879262ms
Jan 19 23:32:41.459: INFO: Pod "downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006613036s
Jan 19 23:32:43.463: INFO: Pod "downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010758936s
STEP: Saw pod success
Jan 19 23:32:43.463: INFO: Pod "downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0" satisfied condition "Succeeded or Failed"
Jan 19 23:32:43.465: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0 container client-container: <nil>
STEP: delete the pod
Jan 19 23:32:43.476: INFO: Waiting for pod downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0 to disappear
Jan 19 23:32:43.485: INFO: Pod downwardapi-volume-b6f5a4df-8ba4-4b9c-a6a4-ae267465cdc0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:43.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-828" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":3917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:43.492: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jan 19 23:32:43.524: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jan 19 23:32:43.976: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 19 23:32:46.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:32:48.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:32:50.022: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:32:52.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:32:54.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:32:56.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 32, 44, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:32:58.146: INFO: Waited 120.504497ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 19 23:32:58.485: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:32:59.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-21" for this suite.

• [SLOW TEST:15.891 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":215,"skipped":3947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:32:59.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:05.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9705" for this suite.

• [SLOW TEST:6.079 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":216,"skipped":3973,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 19 23:33:05.499: INFO: Waiting up to 5m0s for pod "pod-4fc24d80-a116-4e21-af26-22cb248e9735" in namespace "emptydir-7335" to be "Succeeded or Failed"
Jan 19 23:33:05.502: INFO: Pod "pod-4fc24d80-a116-4e21-af26-22cb248e9735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.113887ms
Jan 19 23:33:07.506: INFO: Pod "pod-4fc24d80-a116-4e21-af26-22cb248e9735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006391562s
Jan 19 23:33:09.511: INFO: Pod "pod-4fc24d80-a116-4e21-af26-22cb248e9735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011596518s
STEP: Saw pod success
Jan 19 23:33:09.511: INFO: Pod "pod-4fc24d80-a116-4e21-af26-22cb248e9735" satisfied condition "Succeeded or Failed"
Jan 19 23:33:09.513: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-4fc24d80-a116-4e21-af26-22cb248e9735 container test-container: <nil>
STEP: delete the pod
Jan 19 23:33:09.529: INFO: Waiting for pod pod-4fc24d80-a116-4e21-af26-22cb248e9735 to disappear
Jan 19 23:33:09.538: INFO: Pod pod-4fc24d80-a116-4e21-af26-22cb248e9735 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:09.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7335" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":3975,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:09.544: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:33:09.572: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 19 23:33:09.579: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 23:33:14.589: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 19 23:33:14.589: INFO: Creating deployment "test-rolling-update-deployment"
Jan 19 23:33:14.594: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 19 23:33:14.598: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 19 23:33:16.604: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 19 23:33:16.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 33, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 33, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 33, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 33, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-796dbc4547\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:33:18.613: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 19 23:33:18.618: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7827  a9dcd411-1c2c-46c1-b6c1-27ff208e18e0 85196 1 2022-01-19 23:33:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-01-19 23:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:33:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008e43408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-19 23:33:14 +0000 UTC,LastTransitionTime:2022-01-19 23:33:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-01-19 23:33:16 +0000 UTC,LastTransitionTime:2022-01-19 23:33:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 23:33:18.620: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-7827  f48d6bbe-140d-4838-b4e1-6dea02546c8c 85186 1 2022-01-19 23:33:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a9dcd411-1c2c-46c1-b6c1-27ff208e18e0 0xc008e43907 0xc008e43908}] []  [{kube-controller-manager Update apps/v1 2022-01-19 23:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9dcd411-1c2c-46c1-b6c1-27ff208e18e0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:33:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008e439b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:33:18.620: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 19 23:33:18.620: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7827  16d74442-e473-4339-9a2f-3e269fe3f91c 85195 2 2022-01-19 23:33:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a9dcd411-1c2c-46c1-b6c1-27ff208e18e0 0xc008e437d7 0xc008e437d8}] []  [{e2e.test Update apps/v1 2022-01-19 23:33:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:33:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9dcd411-1c2c-46c1-b6c1-27ff208e18e0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:33:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008e43898 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:33:18.623: INFO: Pod "test-rolling-update-deployment-796dbc4547-bfc9p" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-bfc9p test-rolling-update-deployment-796dbc4547- deployment-7827  438e5139-730c-4af3-8ad1-9b7fa93a276d 85185 0 2022-01-19 23:33:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[cni.projectcalico.org/containerID:9550e25f5b4e435de9463fe8c4f39f2dacb58c10b7224095a480a0e909cfe44c cni.projectcalico.org/podIP:172.16.146.20/32 cni.projectcalico.org/podIPs:172.16.146.20/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 f48d6bbe-140d-4838-b4e1-6dea02546c8c 0xc006337537 0xc006337538}] []  [{kube-controller-manager Update v1 2022-01-19 23:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f48d6bbe-140d-4838-b4e1-6dea02546c8c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:33:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:33:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qvjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qvjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:33:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:33:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:33:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:33:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:172.16.146.20,StartTime:2022-01-19 23:33:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:33:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:docker://e6be8f0c7441d9e75988eca9794a05d8b9959e2511212014840b6654c1c39626,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.146.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:18.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7827" for this suite.

• [SLOW TEST:9.086 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":218,"skipped":3994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:18.630: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
STEP: creating an pod
Jan 19 23:33:18.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 19 23:33:18.720: INFO: stderr: ""
Jan 19 23:33:18.720: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jan 19 23:33:18.720: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 19 23:33:18.720: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1399" to be "running and ready, or succeeded"
Jan 19 23:33:18.744: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 24.205008ms
Jan 19 23:33:20.746: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026595781s
Jan 19 23:33:22.753: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.032966214s
Jan 19 23:33:22.753: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 19 23:33:22.753: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 19 23:33:22.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 logs logs-generator logs-generator'
Jan 19 23:33:22.812: INFO: stderr: ""
Jan 19 23:33:22.812: INFO: stdout: "I0119 23:33:20.310591       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7j4 303\nI0119 23:33:20.510732       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/9bf6 571\nI0119 23:33:20.710960       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/dms 431\nI0119 23:33:20.911368       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/lcl 481\nI0119 23:33:21.111633       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lbwq 454\nI0119 23:33:21.311063       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/dz7 409\nI0119 23:33:21.511483       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/srtt 543\nI0119 23:33:21.710784       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/62vs 548\nI0119 23:33:21.911220       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/cb89 426\nI0119 23:33:22.111649       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/5vq 486\nI0119 23:33:22.311082       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/6t8 279\nI0119 23:33:22.511547       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/75j6 241\nI0119 23:33:22.710697       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/psw 327\n"
STEP: limiting log lines
Jan 19 23:33:22.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 logs logs-generator logs-generator --tail=1'
Jan 19 23:33:22.867: INFO: stderr: ""
Jan 19 23:33:22.867: INFO: stdout: "I0119 23:33:22.710697       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/psw 327\n"
Jan 19 23:33:22.867: INFO: got output "I0119 23:33:22.710697       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/psw 327\n"
STEP: limiting log bytes
Jan 19 23:33:22.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 logs logs-generator logs-generator --limit-bytes=1'
Jan 19 23:33:22.923: INFO: stderr: ""
Jan 19 23:33:22.923: INFO: stdout: "I"
Jan 19 23:33:22.923: INFO: got output "I"
STEP: exposing timestamps
Jan 19 23:33:22.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 19 23:33:22.979: INFO: stderr: ""
Jan 19 23:33:22.979: INFO: stdout: "2022-01-19T23:33:22.911207384Z I0119 23:33:22.911094       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/bpcs 578\n"
Jan 19 23:33:22.979: INFO: got output "2022-01-19T23:33:22.911207384Z I0119 23:33:22.911094       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/bpcs 578\n"
STEP: restricting to a time range
Jan 19 23:33:25.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 logs logs-generator logs-generator --since=1s'
Jan 19 23:33:25.551: INFO: stderr: ""
Jan 19 23:33:25.551: INFO: stdout: "I0119 23:33:24.710662       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/6n92 473\nI0119 23:33:24.911081       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/dvzd 344\nI0119 23:33:25.111490       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/hb4 290\nI0119 23:33:25.311149       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/mtk 484\nI0119 23:33:25.511631       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/mff8 456\n"
Jan 19 23:33:25.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 logs logs-generator logs-generator --since=24h'
Jan 19 23:33:25.609: INFO: stderr: ""
Jan 19 23:33:25.609: INFO: stdout: "I0119 23:33:20.310591       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7j4 303\nI0119 23:33:20.510732       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/9bf6 571\nI0119 23:33:20.710960       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/dms 431\nI0119 23:33:20.911368       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/lcl 481\nI0119 23:33:21.111633       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lbwq 454\nI0119 23:33:21.311063       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/dz7 409\nI0119 23:33:21.511483       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/srtt 543\nI0119 23:33:21.710784       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/62vs 548\nI0119 23:33:21.911220       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/cb89 426\nI0119 23:33:22.111649       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/5vq 486\nI0119 23:33:22.311082       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/6t8 279\nI0119 23:33:22.511547       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/75j6 241\nI0119 23:33:22.710697       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/psw 327\nI0119 23:33:22.911094       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/bpcs 578\nI0119 23:33:23.111582       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/tk9l 414\nI0119 23:33:23.311025       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/5kd 532\nI0119 23:33:23.511497       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/b4md 554\nI0119 23:33:23.710804       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/dgt 514\nI0119 23:33:23.911230       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/cz57 419\nI0119 23:33:24.111447       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/s7mm 258\nI0119 23:33:24.310845       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/dhk 252\nI0119 23:33:24.511360       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/zvl 319\nI0119 23:33:24.710662       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/6n92 473\nI0119 23:33:24.911081       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/dvzd 344\nI0119 23:33:25.111490       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/hb4 290\nI0119 23:33:25.311149       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/mtk 484\nI0119 23:33:25.511631       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/mff8 456\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
Jan 19 23:33:25.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1399 delete pod logs-generator'
Jan 19 23:33:26.658: INFO: stderr: ""
Jan 19 23:33:26.658: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:26.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1399" for this suite.

• [SLOW TEST:8.037 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1406
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":219,"skipped":4074,"failed":0}
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:26.667: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:26.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9518" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":220,"skipped":4074,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:26.765: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jan 19 23:33:26.837: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1866 proxy --unix-socket=/tmp/kubectl-proxy-unix3199796519/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:26.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1866" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":221,"skipped":4081,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:26.884: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-350
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-350
STEP: creating replication controller externalsvc in namespace services-350
I0119 23:33:26.957606      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-350, replica count: 2
I0119 23:33:30.009146      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 19 23:33:30.024: INFO: Creating new exec pod
Jan 19 23:33:34.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-350 exec execpod4z8g8 -- /bin/sh -x -c nslookup clusterip-service.services-350.svc.cluster.local'
Jan 19 23:33:34.252: INFO: stderr: "+ nslookup clusterip-service.services-350.svc.cluster.local\n"
Jan 19 23:33:34.252: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-350.svc.cluster.local\tcanonical name = externalsvc.services-350.svc.cluster.local.\nName:\texternalsvc.services-350.svc.cluster.local\nAddress: 10.104.48.132\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-350, will wait for the garbage collector to delete the pods
Jan 19 23:33:34.309: INFO: Deleting ReplicationController externalsvc took: 3.043008ms
Jan 19 23:33:34.409: INFO: Terminating ReplicationController externalsvc pods took: 100.783594ms
Jan 19 23:33:36.320: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:36.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-350" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.457 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":222,"skipped":4086,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:36.342: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-96e9dfae-9a06-40e3-9e5a-2e97faa72af8
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:36.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4801" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":223,"skipped":4096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:36.387: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jan 19 23:33:36.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-6434 create -f -'
Jan 19 23:33:37.753: INFO: stderr: ""
Jan 19 23:33:37.753: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 19 23:33:37.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-6434 diff -f -'
Jan 19 23:33:37.968: INFO: rc: 1
Jan 19 23:33:37.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-6434 delete -f -'
Jan 19 23:33:38.021: INFO: stderr: ""
Jan 19 23:33:38.021: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:33:38.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6434" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":224,"skipped":4120,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:33:38.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-5236c2ca-7578-48a2-9e17-9df718256c6a in namespace container-probe-7695
Jan 19 23:33:42.068: INFO: Started pod liveness-5236c2ca-7578-48a2-9e17-9df718256c6a in namespace container-probe-7695
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 23:33:42.070: INFO: Initial restart count of pod liveness-5236c2ca-7578-48a2-9e17-9df718256c6a is 0
Jan 19 23:34:00.116: INFO: Restart count of pod container-probe-7695/liveness-5236c2ca-7578-48a2-9e17-9df718256c6a is now 1 (18.045812803s elapsed)
Jan 19 23:34:20.163: INFO: Restart count of pod container-probe-7695/liveness-5236c2ca-7578-48a2-9e17-9df718256c6a is now 2 (38.093164552s elapsed)
Jan 19 23:34:40.212: INFO: Restart count of pod container-probe-7695/liveness-5236c2ca-7578-48a2-9e17-9df718256c6a is now 3 (58.142248209s elapsed)
Jan 19 23:35:00.254: INFO: Restart count of pod container-probe-7695/liveness-5236c2ca-7578-48a2-9e17-9df718256c6a is now 4 (1m18.183589354s elapsed)
Jan 19 23:36:00.388: INFO: Restart count of pod container-probe-7695/liveness-5236c2ca-7578-48a2-9e17-9df718256c6a is now 5 (2m18.317353446s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:36:00.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7695" for this suite.

• [SLOW TEST:142.379 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4122,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:36:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:36:00.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9335" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":226,"skipped":4134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:36:00.460: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-d5c3dc7e-baff-4a39-af68-ca71cfba5085 in namespace container-probe-3762
Jan 19 23:36:04.507: INFO: Started pod test-webserver-d5c3dc7e-baff-4a39-af68-ca71cfba5085 in namespace container-probe-3762
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 23:36:04.509: INFO: Initial restart count of pod test-webserver-d5c3dc7e-baff-4a39-af68-ca71cfba5085 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:05.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3762" for this suite.

• [SLOW TEST:244.666 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4192,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:05.126: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:05.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9254" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":228,"skipped":4196,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:05.203: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:40:05.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4" in namespace "projected-9585" to be "Succeeded or Failed"
Jan 19 23:40:05.246: INFO: Pod "downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.559239ms
Jan 19 23:40:07.250: INFO: Pod "downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015464565s
Jan 19 23:40:09.254: INFO: Pod "downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019954278s
STEP: Saw pod success
Jan 19 23:40:09.254: INFO: Pod "downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4" satisfied condition "Succeeded or Failed"
Jan 19 23:40:09.256: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4 container client-container: <nil>
STEP: delete the pod
Jan 19 23:40:09.280: INFO: Waiting for pod downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4 to disappear
Jan 19 23:40:09.289: INFO: Pod downwardapi-volume-57e2b56a-4329-468d-a75a-568ee19ee2a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:09.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9585" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:09.296: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:40:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4172
I0119 23:40:09.327052      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4172, replica count: 1
I0119 23:40:10.378336      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 23:40:11.378625      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 23:40:12.379578      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 23:40:12.490: INFO: Created: latency-svc-p78bv
Jan 19 23:40:12.509: INFO: Got endpoints: latency-svc-p78bv [29.218654ms]
Jan 19 23:40:12.525: INFO: Created: latency-svc-xsns4
Jan 19 23:40:12.533: INFO: Got endpoints: latency-svc-xsns4 [23.1954ms]
Jan 19 23:40:12.536: INFO: Created: latency-svc-79mvq
Jan 19 23:40:12.544: INFO: Got endpoints: latency-svc-79mvq [33.778982ms]
Jan 19 23:40:12.547: INFO: Created: latency-svc-5gjhr
Jan 19 23:40:12.556: INFO: Got endpoints: latency-svc-5gjhr [45.819568ms]
Jan 19 23:40:12.560: INFO: Created: latency-svc-6b6hv
Jan 19 23:40:12.568: INFO: Got endpoints: latency-svc-6b6hv [57.556075ms]
Jan 19 23:40:12.578: INFO: Created: latency-svc-c5brh
Jan 19 23:40:12.587: INFO: Got endpoints: latency-svc-c5brh [76.683195ms]
Jan 19 23:40:12.590: INFO: Created: latency-svc-9lrqt
Jan 19 23:40:12.597: INFO: Got endpoints: latency-svc-9lrqt [86.864782ms]
Jan 19 23:40:12.602: INFO: Created: latency-svc-hd4gp
Jan 19 23:40:12.622: INFO: Got endpoints: latency-svc-hd4gp [111.394921ms]
Jan 19 23:40:12.626: INFO: Created: latency-svc-jjh7v
Jan 19 23:40:12.638: INFO: Created: latency-svc-zp7n7
Jan 19 23:40:12.638: INFO: Got endpoints: latency-svc-jjh7v [127.388442ms]
Jan 19 23:40:12.646: INFO: Got endpoints: latency-svc-zp7n7 [135.511013ms]
Jan 19 23:40:12.649: INFO: Created: latency-svc-86l42
Jan 19 23:40:12.657: INFO: Got endpoints: latency-svc-86l42 [146.292104ms]
Jan 19 23:40:12.660: INFO: Created: latency-svc-5p8xg
Jan 19 23:40:12.670: INFO: Got endpoints: latency-svc-5p8xg [158.723928ms]
Jan 19 23:40:12.674: INFO: Created: latency-svc-65fw7
Jan 19 23:40:12.683: INFO: Got endpoints: latency-svc-65fw7 [171.53076ms]
Jan 19 23:40:12.692: INFO: Created: latency-svc-7qxr5
Jan 19 23:40:12.706: INFO: Got endpoints: latency-svc-7qxr5 [194.377426ms]
Jan 19 23:40:12.710: INFO: Created: latency-svc-5xp77
Jan 19 23:40:12.722: INFO: Got endpoints: latency-svc-5xp77 [211.00011ms]
Jan 19 23:40:12.745: INFO: Created: latency-svc-gtw6x
Jan 19 23:40:12.756: INFO: Got endpoints: latency-svc-gtw6x [245.071204ms]
Jan 19 23:40:12.757: INFO: Created: latency-svc-vpbgl
Jan 19 23:40:12.776: INFO: Got endpoints: latency-svc-vpbgl [242.648258ms]
Jan 19 23:40:12.794: INFO: Created: latency-svc-szblc
Jan 19 23:40:12.803: INFO: Got endpoints: latency-svc-szblc [259.164358ms]
Jan 19 23:40:12.806: INFO: Created: latency-svc-h2xzd
Jan 19 23:40:12.816: INFO: Got endpoints: latency-svc-h2xzd [259.668263ms]
Jan 19 23:40:12.826: INFO: Created: latency-svc-b65lv
Jan 19 23:40:12.835: INFO: Got endpoints: latency-svc-b65lv [266.97052ms]
Jan 19 23:40:12.839: INFO: Created: latency-svc-5cmmz
Jan 19 23:40:12.859: INFO: Got endpoints: latency-svc-5cmmz [271.656702ms]
Jan 19 23:40:12.862: INFO: Created: latency-svc-qjjl8
Jan 19 23:40:12.866: INFO: Got endpoints: latency-svc-qjjl8 [268.466451ms]
Jan 19 23:40:12.875: INFO: Created: latency-svc-95llb
Jan 19 23:40:12.878: INFO: Created: latency-svc-f8q5n
Jan 19 23:40:12.888: INFO: Got endpoints: latency-svc-f8q5n [249.544713ms]
Jan 19 23:40:12.888: INFO: Got endpoints: latency-svc-95llb [265.706369ms]
Jan 19 23:40:12.890: INFO: Created: latency-svc-vshn8
Jan 19 23:40:12.898: INFO: Got endpoints: latency-svc-vshn8 [251.610348ms]
Jan 19 23:40:12.901: INFO: Created: latency-svc-thrjn
Jan 19 23:40:12.909: INFO: Got endpoints: latency-svc-thrjn [252.057719ms]
Jan 19 23:40:12.914: INFO: Created: latency-svc-7bct5
Jan 19 23:40:12.922: INFO: Got endpoints: latency-svc-7bct5 [252.411968ms]
Jan 19 23:40:12.933: INFO: Created: latency-svc-vtl8r
Jan 19 23:40:12.942: INFO: Got endpoints: latency-svc-vtl8r [259.47489ms]
Jan 19 23:40:12.947: INFO: Created: latency-svc-v245k
Jan 19 23:40:12.954: INFO: Got endpoints: latency-svc-v245k [248.048089ms]
Jan 19 23:40:12.958: INFO: Created: latency-svc-ngkx5
Jan 19 23:40:12.981: INFO: Got endpoints: latency-svc-ngkx5 [258.700573ms]
Jan 19 23:40:12.985: INFO: Created: latency-svc-cm7zf
Jan 19 23:40:12.997: INFO: Created: latency-svc-v6kts
Jan 19 23:40:12.997: INFO: Got endpoints: latency-svc-cm7zf [240.976317ms]
Jan 19 23:40:13.006: INFO: Got endpoints: latency-svc-v6kts [230.647157ms]
Jan 19 23:40:13.009: INFO: Created: latency-svc-j6zr6
Jan 19 23:40:13.017: INFO: Got endpoints: latency-svc-j6zr6 [214.404109ms]
Jan 19 23:40:13.020: INFO: Created: latency-svc-tnn2l
Jan 19 23:40:13.028: INFO: Got endpoints: latency-svc-tnn2l [212.434908ms]
Jan 19 23:40:13.031: INFO: Created: latency-svc-z5zdp
Jan 19 23:40:13.039: INFO: Got endpoints: latency-svc-z5zdp [204.237901ms]
Jan 19 23:40:13.043: INFO: Created: latency-svc-j5hdn
Jan 19 23:40:13.052: INFO: Got endpoints: latency-svc-j5hdn [192.805888ms]
Jan 19 23:40:13.054: INFO: Created: latency-svc-zhch5
Jan 19 23:40:13.063: INFO: Got endpoints: latency-svc-zhch5 [197.198381ms]
Jan 19 23:40:13.066: INFO: Created: latency-svc-dbb62
Jan 19 23:40:13.075: INFO: Got endpoints: latency-svc-dbb62 [187.434532ms]
Jan 19 23:40:13.096: INFO: Created: latency-svc-qjkfk
Jan 19 23:40:13.102: INFO: Got endpoints: latency-svc-qjkfk [213.938994ms]
Jan 19 23:40:13.111: INFO: Created: latency-svc-9265h
Jan 19 23:40:13.113: INFO: Created: latency-svc-l5ctr
Jan 19 23:40:13.122: INFO: Got endpoints: latency-svc-9265h [224.227454ms]
Jan 19 23:40:13.122: INFO: Got endpoints: latency-svc-l5ctr [212.836064ms]
Jan 19 23:40:13.127: INFO: Created: latency-svc-gcx5k
Jan 19 23:40:13.133: INFO: Got endpoints: latency-svc-gcx5k [210.690151ms]
Jan 19 23:40:13.136: INFO: Created: latency-svc-45l9g
Jan 19 23:40:13.147: INFO: Got endpoints: latency-svc-45l9g [204.303648ms]
Jan 19 23:40:13.148: INFO: Created: latency-svc-swn77
Jan 19 23:40:13.167: INFO: Created: latency-svc-x78cr
Jan 19 23:40:13.179: INFO: Created: latency-svc-6gflh
Jan 19 23:40:13.190: INFO: Created: latency-svc-b2sz4
Jan 19 23:40:13.202: INFO: Got endpoints: latency-svc-swn77 [247.916763ms]
Jan 19 23:40:13.205: INFO: Created: latency-svc-k5c4m
Jan 19 23:40:13.214: INFO: Created: latency-svc-wxh5q
Jan 19 23:40:13.226: INFO: Created: latency-svc-sj4zl
Jan 19 23:40:13.238: INFO: Created: latency-svc-8xqj2
Jan 19 23:40:13.249: INFO: Got endpoints: latency-svc-x78cr [267.884799ms]
Jan 19 23:40:13.249: INFO: Created: latency-svc-hx8nc
Jan 19 23:40:13.268: INFO: Created: latency-svc-ls759
Jan 19 23:40:13.280: INFO: Created: latency-svc-prtcs
Jan 19 23:40:13.290: INFO: Created: latency-svc-n4lfv
Jan 19 23:40:13.299: INFO: Got endpoints: latency-svc-6gflh [301.14253ms]
Jan 19 23:40:13.322: INFO: Created: latency-svc-qr7ln
Jan 19 23:40:13.325: INFO: Created: latency-svc-xrc8w
Jan 19 23:40:13.334: INFO: Created: latency-svc-rfr9r
Jan 19 23:40:13.346: INFO: Got endpoints: latency-svc-b2sz4 [339.44771ms]
Jan 19 23:40:13.355: INFO: Created: latency-svc-lm62t
Jan 19 23:40:13.357: INFO: Created: latency-svc-swhgt
Jan 19 23:40:13.377: INFO: Created: latency-svc-xltwd
Jan 19 23:40:13.399: INFO: Got endpoints: latency-svc-k5c4m [381.653218ms]
Jan 19 23:40:13.400: INFO: Created: latency-svc-47rgp
Jan 19 23:40:13.419: INFO: Created: latency-svc-c5dwf
Jan 19 23:40:13.445: INFO: Got endpoints: latency-svc-wxh5q [416.647701ms]
Jan 19 23:40:13.454: INFO: Created: latency-svc-rfjqn
Jan 19 23:40:13.496: INFO: Got endpoints: latency-svc-sj4zl [456.952657ms]
Jan 19 23:40:13.514: INFO: Created: latency-svc-5mc24
Jan 19 23:40:13.545: INFO: Got endpoints: latency-svc-8xqj2 [493.755373ms]
Jan 19 23:40:13.555: INFO: Created: latency-svc-745q8
Jan 19 23:40:13.595: INFO: Got endpoints: latency-svc-hx8nc [532.035344ms]
Jan 19 23:40:13.608: INFO: Created: latency-svc-brbdz
Jan 19 23:40:13.646: INFO: Got endpoints: latency-svc-ls759 [570.584508ms]
Jan 19 23:40:13.656: INFO: Created: latency-svc-blcrk
Jan 19 23:40:13.695: INFO: Got endpoints: latency-svc-prtcs [593.212712ms]
Jan 19 23:40:13.717: INFO: Created: latency-svc-cc89h
Jan 19 23:40:13.757: INFO: Got endpoints: latency-svc-n4lfv [634.62722ms]
Jan 19 23:40:13.794: INFO: Created: latency-svc-vpzfj
Jan 19 23:40:13.803: INFO: Got endpoints: latency-svc-qr7ln [680.110607ms]
Jan 19 23:40:13.811: INFO: Created: latency-svc-6tdmp
Jan 19 23:40:13.846: INFO: Got endpoints: latency-svc-xrc8w [713.227646ms]
Jan 19 23:40:13.865: INFO: Created: latency-svc-shptj
Jan 19 23:40:13.896: INFO: Got endpoints: latency-svc-rfr9r [749.108888ms]
Jan 19 23:40:13.905: INFO: Created: latency-svc-j7fws
Jan 19 23:40:13.945: INFO: Got endpoints: latency-svc-lm62t [743.571595ms]
Jan 19 23:40:13.978: INFO: Created: latency-svc-rkn9d
Jan 19 23:40:13.998: INFO: Got endpoints: latency-svc-swhgt [748.737895ms]
Jan 19 23:40:14.008: INFO: Created: latency-svc-ngrtm
Jan 19 23:40:14.050: INFO: Got endpoints: latency-svc-xltwd [751.099476ms]
Jan 19 23:40:14.064: INFO: Created: latency-svc-szw2d
Jan 19 23:40:14.097: INFO: Got endpoints: latency-svc-47rgp [751.451469ms]
Jan 19 23:40:14.108: INFO: Created: latency-svc-qw9rw
Jan 19 23:40:14.147: INFO: Got endpoints: latency-svc-c5dwf [747.216372ms]
Jan 19 23:40:14.158: INFO: Created: latency-svc-j2nb4
Jan 19 23:40:14.197: INFO: Got endpoints: latency-svc-rfjqn [752.0096ms]
Jan 19 23:40:14.210: INFO: Created: latency-svc-rfnxz
Jan 19 23:40:14.247: INFO: Got endpoints: latency-svc-5mc24 [750.951135ms]
Jan 19 23:40:14.257: INFO: Created: latency-svc-knc5h
Jan 19 23:40:14.298: INFO: Got endpoints: latency-svc-745q8 [752.262081ms]
Jan 19 23:40:14.314: INFO: Created: latency-svc-448lc
Jan 19 23:40:14.348: INFO: Got endpoints: latency-svc-brbdz [752.244189ms]
Jan 19 23:40:14.363: INFO: Created: latency-svc-jz8w5
Jan 19 23:40:14.406: INFO: Got endpoints: latency-svc-blcrk [758.977304ms]
Jan 19 23:40:14.448: INFO: Got endpoints: latency-svc-cc89h [752.927792ms]
Jan 19 23:40:14.462: INFO: Created: latency-svc-zfxv4
Jan 19 23:40:14.482: INFO: Created: latency-svc-rdq54
Jan 19 23:40:14.496: INFO: Got endpoints: latency-svc-vpzfj [738.974549ms]
Jan 19 23:40:14.510: INFO: Created: latency-svc-cc7vf
Jan 19 23:40:14.546: INFO: Got endpoints: latency-svc-6tdmp [743.174653ms]
Jan 19 23:40:14.562: INFO: Created: latency-svc-s89jb
Jan 19 23:40:14.595: INFO: Got endpoints: latency-svc-shptj [748.973938ms]
Jan 19 23:40:14.611: INFO: Created: latency-svc-9rmft
Jan 19 23:40:14.647: INFO: Got endpoints: latency-svc-j7fws [750.45327ms]
Jan 19 23:40:14.661: INFO: Created: latency-svc-ss9f8
Jan 19 23:40:14.701: INFO: Got endpoints: latency-svc-rkn9d [755.37294ms]
Jan 19 23:40:14.715: INFO: Created: latency-svc-8pjlb
Jan 19 23:40:14.750: INFO: Got endpoints: latency-svc-ngrtm [752.21605ms]
Jan 19 23:40:14.772: INFO: Created: latency-svc-f9wfg
Jan 19 23:40:14.795: INFO: Got endpoints: latency-svc-szw2d [744.764318ms]
Jan 19 23:40:14.834: INFO: Created: latency-svc-k957r
Jan 19 23:40:14.845: INFO: Got endpoints: latency-svc-qw9rw [747.952388ms]
Jan 19 23:40:14.855: INFO: Created: latency-svc-k878f
Jan 19 23:40:14.896: INFO: Got endpoints: latency-svc-j2nb4 [749.412382ms]
Jan 19 23:40:14.909: INFO: Created: latency-svc-c6tc2
Jan 19 23:40:14.947: INFO: Got endpoints: latency-svc-rfnxz [749.621843ms]
Jan 19 23:40:14.954: INFO: Created: latency-svc-knt4k
Jan 19 23:40:14.995: INFO: Got endpoints: latency-svc-knc5h [747.750877ms]
Jan 19 23:40:15.008: INFO: Created: latency-svc-27st5
Jan 19 23:40:15.045: INFO: Got endpoints: latency-svc-448lc [747.395664ms]
Jan 19 23:40:15.062: INFO: Created: latency-svc-qndtl
Jan 19 23:40:15.096: INFO: Got endpoints: latency-svc-jz8w5 [748.186918ms]
Jan 19 23:40:15.107: INFO: Created: latency-svc-qgkls
Jan 19 23:40:15.147: INFO: Got endpoints: latency-svc-zfxv4 [699.169732ms]
Jan 19 23:40:15.161: INFO: Created: latency-svc-sj2z2
Jan 19 23:40:15.196: INFO: Got endpoints: latency-svc-rdq54 [747.896922ms]
Jan 19 23:40:15.204: INFO: Created: latency-svc-vjjwl
Jan 19 23:40:15.245: INFO: Got endpoints: latency-svc-cc7vf [749.297345ms]
Jan 19 23:40:15.258: INFO: Created: latency-svc-brtdz
Jan 19 23:40:15.295: INFO: Got endpoints: latency-svc-s89jb [749.662407ms]
Jan 19 23:40:15.303: INFO: Created: latency-svc-sp2wx
Jan 19 23:40:15.349: INFO: Got endpoints: latency-svc-9rmft [751.36794ms]
Jan 19 23:40:15.360: INFO: Created: latency-svc-vqk9z
Jan 19 23:40:15.396: INFO: Got endpoints: latency-svc-ss9f8 [749.114737ms]
Jan 19 23:40:15.404: INFO: Created: latency-svc-wsm8d
Jan 19 23:40:15.444: INFO: Got endpoints: latency-svc-8pjlb [742.718496ms]
Jan 19 23:40:15.463: INFO: Created: latency-svc-bvzb2
Jan 19 23:40:15.496: INFO: Got endpoints: latency-svc-f9wfg [745.262662ms]
Jan 19 23:40:15.508: INFO: Created: latency-svc-bglhg
Jan 19 23:40:15.546: INFO: Got endpoints: latency-svc-k957r [751.615692ms]
Jan 19 23:40:15.563: INFO: Created: latency-svc-22djk
Jan 19 23:40:15.596: INFO: Got endpoints: latency-svc-k878f [750.500651ms]
Jan 19 23:40:15.609: INFO: Created: latency-svc-wcwn6
Jan 19 23:40:15.646: INFO: Got endpoints: latency-svc-c6tc2 [749.546426ms]
Jan 19 23:40:15.654: INFO: Created: latency-svc-klngw
Jan 19 23:40:15.695: INFO: Got endpoints: latency-svc-knt4k [748.336708ms]
Jan 19 23:40:15.708: INFO: Created: latency-svc-5b5nt
Jan 19 23:40:15.753: INFO: Got endpoints: latency-svc-27st5 [757.708676ms]
Jan 19 23:40:15.797: INFO: Got endpoints: latency-svc-qndtl [751.781785ms]
Jan 19 23:40:15.797: INFO: Created: latency-svc-xsv95
Jan 19 23:40:15.805: INFO: Created: latency-svc-cm28l
Jan 19 23:40:15.845: INFO: Got endpoints: latency-svc-qgkls [749.202732ms]
Jan 19 23:40:15.857: INFO: Created: latency-svc-nd6l9
Jan 19 23:40:15.894: INFO: Got endpoints: latency-svc-sj2z2 [746.968449ms]
Jan 19 23:40:15.904: INFO: Created: latency-svc-jd2mh
Jan 19 23:40:15.945: INFO: Got endpoints: latency-svc-vjjwl [749.486119ms]
Jan 19 23:40:15.959: INFO: Created: latency-svc-vzlxd
Jan 19 23:40:15.995: INFO: Got endpoints: latency-svc-brtdz [749.488554ms]
Jan 19 23:40:16.006: INFO: Created: latency-svc-28xsf
Jan 19 23:40:16.048: INFO: Got endpoints: latency-svc-sp2wx [752.300791ms]
Jan 19 23:40:16.063: INFO: Created: latency-svc-mj48z
Jan 19 23:40:16.095: INFO: Got endpoints: latency-svc-vqk9z [746.399329ms]
Jan 19 23:40:16.117: INFO: Created: latency-svc-7gkfh
Jan 19 23:40:16.147: INFO: Got endpoints: latency-svc-wsm8d [750.852351ms]
Jan 19 23:40:16.155: INFO: Created: latency-svc-z4srz
Jan 19 23:40:16.195: INFO: Got endpoints: latency-svc-bvzb2 [750.339352ms]
Jan 19 23:40:16.209: INFO: Created: latency-svc-tv4bc
Jan 19 23:40:16.245: INFO: Got endpoints: latency-svc-bglhg [749.846081ms]
Jan 19 23:40:16.254: INFO: Created: latency-svc-s8c69
Jan 19 23:40:16.297: INFO: Got endpoints: latency-svc-22djk [750.793642ms]
Jan 19 23:40:16.313: INFO: Created: latency-svc-wfw2s
Jan 19 23:40:16.347: INFO: Got endpoints: latency-svc-wcwn6 [751.288694ms]
Jan 19 23:40:16.358: INFO: Created: latency-svc-fshbt
Jan 19 23:40:16.395: INFO: Got endpoints: latency-svc-klngw [749.344621ms]
Jan 19 23:40:16.403: INFO: Created: latency-svc-5fwrw
Jan 19 23:40:16.451: INFO: Got endpoints: latency-svc-5b5nt [755.931559ms]
Jan 19 23:40:16.463: INFO: Created: latency-svc-w4x2q
Jan 19 23:40:16.495: INFO: Got endpoints: latency-svc-xsv95 [741.63252ms]
Jan 19 23:40:16.504: INFO: Created: latency-svc-rlmlz
Jan 19 23:40:16.548: INFO: Got endpoints: latency-svc-cm28l [750.664449ms]
Jan 19 23:40:16.563: INFO: Created: latency-svc-xkwhg
Jan 19 23:40:16.596: INFO: Got endpoints: latency-svc-nd6l9 [750.798885ms]
Jan 19 23:40:16.606: INFO: Created: latency-svc-q7b49
Jan 19 23:40:16.646: INFO: Got endpoints: latency-svc-jd2mh [751.095929ms]
Jan 19 23:40:16.669: INFO: Created: latency-svc-fp772
Jan 19 23:40:16.695: INFO: Got endpoints: latency-svc-vzlxd [749.344278ms]
Jan 19 23:40:16.710: INFO: Created: latency-svc-8c765
Jan 19 23:40:16.747: INFO: Got endpoints: latency-svc-28xsf [752.232117ms]
Jan 19 23:40:16.779: INFO: Created: latency-svc-8zn9k
Jan 19 23:40:16.796: INFO: Got endpoints: latency-svc-mj48z [748.208669ms]
Jan 19 23:40:16.813: INFO: Created: latency-svc-8kxfp
Jan 19 23:40:16.845: INFO: Got endpoints: latency-svc-7gkfh [750.231128ms]
Jan 19 23:40:16.865: INFO: Created: latency-svc-ll2bz
Jan 19 23:40:16.896: INFO: Got endpoints: latency-svc-z4srz [749.374389ms]
Jan 19 23:40:16.904: INFO: Created: latency-svc-klb5b
Jan 19 23:40:16.945: INFO: Got endpoints: latency-svc-tv4bc [750.587243ms]
Jan 19 23:40:16.958: INFO: Created: latency-svc-qxlkm
Jan 19 23:40:16.996: INFO: Got endpoints: latency-svc-s8c69 [750.313523ms]
Jan 19 23:40:17.004: INFO: Created: latency-svc-ndprs
Jan 19 23:40:17.046: INFO: Got endpoints: latency-svc-wfw2s [748.760716ms]
Jan 19 23:40:17.062: INFO: Created: latency-svc-ng8gk
Jan 19 23:40:17.095: INFO: Got endpoints: latency-svc-fshbt [748.183561ms]
Jan 19 23:40:17.106: INFO: Created: latency-svc-nfm6g
Jan 19 23:40:17.146: INFO: Got endpoints: latency-svc-5fwrw [750.988783ms]
Jan 19 23:40:17.154: INFO: Created: latency-svc-gbp7p
Jan 19 23:40:17.196: INFO: Got endpoints: latency-svc-w4x2q [745.040027ms]
Jan 19 23:40:17.208: INFO: Created: latency-svc-88xl8
Jan 19 23:40:17.245: INFO: Got endpoints: latency-svc-rlmlz [749.96598ms]
Jan 19 23:40:17.253: INFO: Created: latency-svc-dhrl8
Jan 19 23:40:17.295: INFO: Got endpoints: latency-svc-xkwhg [746.74572ms]
Jan 19 23:40:17.308: INFO: Created: latency-svc-j4dh9
Jan 19 23:40:17.345: INFO: Got endpoints: latency-svc-q7b49 [749.321686ms]
Jan 19 23:40:17.353: INFO: Created: latency-svc-zv9ld
Jan 19 23:40:17.395: INFO: Got endpoints: latency-svc-fp772 [749.787386ms]
Jan 19 23:40:17.412: INFO: Created: latency-svc-74bj6
Jan 19 23:40:17.446: INFO: Got endpoints: latency-svc-8c765 [751.111505ms]
Jan 19 23:40:17.459: INFO: Created: latency-svc-vrnrb
Jan 19 23:40:17.499: INFO: Got endpoints: latency-svc-8zn9k [751.3918ms]
Jan 19 23:40:17.508: INFO: Created: latency-svc-46ssp
Jan 19 23:40:17.547: INFO: Got endpoints: latency-svc-8kxfp [750.744623ms]
Jan 19 23:40:17.559: INFO: Created: latency-svc-8s6fz
Jan 19 23:40:17.595: INFO: Got endpoints: latency-svc-ll2bz [749.935612ms]
Jan 19 23:40:17.604: INFO: Created: latency-svc-zj5hf
Jan 19 23:40:17.647: INFO: Got endpoints: latency-svc-klb5b [750.576174ms]
Jan 19 23:40:17.661: INFO: Created: latency-svc-52rzn
Jan 19 23:40:17.695: INFO: Got endpoints: latency-svc-qxlkm [749.277752ms]
Jan 19 23:40:17.703: INFO: Created: latency-svc-z5w4p
Jan 19 23:40:17.749: INFO: Got endpoints: latency-svc-ndprs [753.180203ms]
Jan 19 23:40:17.787: INFO: Created: latency-svc-z2f89
Jan 19 23:40:17.797: INFO: Got endpoints: latency-svc-ng8gk [750.648511ms]
Jan 19 23:40:17.810: INFO: Created: latency-svc-p47wf
Jan 19 23:40:17.845: INFO: Got endpoints: latency-svc-nfm6g [750.020839ms]
Jan 19 23:40:17.859: INFO: Created: latency-svc-27fdv
Jan 19 23:40:17.895: INFO: Got endpoints: latency-svc-gbp7p [749.12813ms]
Jan 19 23:40:17.913: INFO: Created: latency-svc-mrrpn
Jan 19 23:40:17.946: INFO: Got endpoints: latency-svc-88xl8 [749.514827ms]
Jan 19 23:40:17.959: INFO: Created: latency-svc-4chm2
Jan 19 23:40:17.996: INFO: Got endpoints: latency-svc-dhrl8 [750.920636ms]
Jan 19 23:40:18.005: INFO: Created: latency-svc-c2hp6
Jan 19 23:40:18.045: INFO: Got endpoints: latency-svc-j4dh9 [750.303577ms]
Jan 19 23:40:18.058: INFO: Created: latency-svc-gh7l8
Jan 19 23:40:18.095: INFO: Got endpoints: latency-svc-zv9ld [749.428433ms]
Jan 19 23:40:18.105: INFO: Created: latency-svc-8crcs
Jan 19 23:40:18.147: INFO: Got endpoints: latency-svc-74bj6 [751.743691ms]
Jan 19 23:40:18.162: INFO: Created: latency-svc-wsgff
Jan 19 23:40:18.195: INFO: Got endpoints: latency-svc-vrnrb [749.122042ms]
Jan 19 23:40:18.205: INFO: Created: latency-svc-24p4k
Jan 19 23:40:18.245: INFO: Got endpoints: latency-svc-46ssp [746.76201ms]
Jan 19 23:40:18.262: INFO: Created: latency-svc-72gkb
Jan 19 23:40:18.295: INFO: Got endpoints: latency-svc-8s6fz [748.077038ms]
Jan 19 23:40:18.308: INFO: Created: latency-svc-c9cwr
Jan 19 23:40:18.346: INFO: Got endpoints: latency-svc-zj5hf [750.520439ms]
Jan 19 23:40:18.355: INFO: Created: latency-svc-ch5ln
Jan 19 23:40:18.397: INFO: Got endpoints: latency-svc-52rzn [749.929532ms]
Jan 19 23:40:18.409: INFO: Created: latency-svc-j2lbv
Jan 19 23:40:18.445: INFO: Got endpoints: latency-svc-z5w4p [750.305867ms]
Jan 19 23:40:18.454: INFO: Created: latency-svc-nt4xh
Jan 19 23:40:18.497: INFO: Got endpoints: latency-svc-z2f89 [747.494811ms]
Jan 19 23:40:18.509: INFO: Created: latency-svc-2brkr
Jan 19 23:40:18.546: INFO: Got endpoints: latency-svc-p47wf [748.35848ms]
Jan 19 23:40:18.556: INFO: Created: latency-svc-dpf7r
Jan 19 23:40:18.597: INFO: Got endpoints: latency-svc-27fdv [751.58505ms]
Jan 19 23:40:18.612: INFO: Created: latency-svc-mm2k2
Jan 19 23:40:18.646: INFO: Got endpoints: latency-svc-mrrpn [750.209533ms]
Jan 19 23:40:18.658: INFO: Created: latency-svc-5c4gw
Jan 19 23:40:18.696: INFO: Got endpoints: latency-svc-4chm2 [750.779899ms]
Jan 19 23:40:18.714: INFO: Created: latency-svc-rg7j6
Jan 19 23:40:18.753: INFO: Got endpoints: latency-svc-c2hp6 [756.723757ms]
Jan 19 23:40:18.774: INFO: Created: latency-svc-2pbn6
Jan 19 23:40:18.795: INFO: Got endpoints: latency-svc-gh7l8 [749.846008ms]
Jan 19 23:40:18.814: INFO: Created: latency-svc-q9mqv
Jan 19 23:40:18.846: INFO: Got endpoints: latency-svc-8crcs [750.945311ms]
Jan 19 23:40:18.863: INFO: Created: latency-svc-qcsnv
Jan 19 23:40:18.895: INFO: Got endpoints: latency-svc-wsgff [747.638273ms]
Jan 19 23:40:18.911: INFO: Created: latency-svc-7c5md
Jan 19 23:40:18.946: INFO: Got endpoints: latency-svc-24p4k [750.284225ms]
Jan 19 23:40:18.959: INFO: Created: latency-svc-k56zq
Jan 19 23:40:18.995: INFO: Got endpoints: latency-svc-72gkb [749.660874ms]
Jan 19 23:40:19.004: INFO: Created: latency-svc-brv9v
Jan 19 23:40:19.045: INFO: Got endpoints: latency-svc-c9cwr [750.492785ms]
Jan 19 23:40:19.059: INFO: Created: latency-svc-8rznt
Jan 19 23:40:19.096: INFO: Got endpoints: latency-svc-ch5ln [750.067188ms]
Jan 19 23:40:19.105: INFO: Created: latency-svc-xqmgc
Jan 19 23:40:19.145: INFO: Got endpoints: latency-svc-j2lbv [748.227556ms]
Jan 19 23:40:19.162: INFO: Created: latency-svc-4chdm
Jan 19 23:40:19.196: INFO: Got endpoints: latency-svc-nt4xh [750.579835ms]
Jan 19 23:40:19.208: INFO: Created: latency-svc-nvctb
Jan 19 23:40:19.246: INFO: Got endpoints: latency-svc-2brkr [749.064805ms]
Jan 19 23:40:19.255: INFO: Created: latency-svc-t24rq
Jan 19 23:40:19.295: INFO: Got endpoints: latency-svc-dpf7r [749.2948ms]
Jan 19 23:40:19.312: INFO: Created: latency-svc-qt2cs
Jan 19 23:40:19.345: INFO: Got endpoints: latency-svc-mm2k2 [747.771142ms]
Jan 19 23:40:19.354: INFO: Created: latency-svc-r964l
Jan 19 23:40:19.398: INFO: Got endpoints: latency-svc-5c4gw [752.646751ms]
Jan 19 23:40:19.413: INFO: Created: latency-svc-x5kg5
Jan 19 23:40:19.447: INFO: Got endpoints: latency-svc-rg7j6 [750.398448ms]
Jan 19 23:40:19.458: INFO: Created: latency-svc-cns4l
Jan 19 23:40:19.501: INFO: Got endpoints: latency-svc-2pbn6 [748.105316ms]
Jan 19 23:40:19.512: INFO: Created: latency-svc-n25z2
Jan 19 23:40:19.547: INFO: Got endpoints: latency-svc-q9mqv [752.451365ms]
Jan 19 23:40:19.562: INFO: Created: latency-svc-75j4t
Jan 19 23:40:19.595: INFO: Got endpoints: latency-svc-qcsnv [749.007763ms]
Jan 19 23:40:19.605: INFO: Created: latency-svc-5z8dk
Jan 19 23:40:19.646: INFO: Got endpoints: latency-svc-7c5md [750.649795ms]
Jan 19 23:40:19.662: INFO: Created: latency-svc-sldxs
Jan 19 23:40:19.695: INFO: Got endpoints: latency-svc-k56zq [749.378681ms]
Jan 19 23:40:19.722: INFO: Created: latency-svc-hbljs
Jan 19 23:40:19.751: INFO: Got endpoints: latency-svc-brv9v [755.358761ms]
Jan 19 23:40:19.784: INFO: Created: latency-svc-f8gbh
Jan 19 23:40:19.796: INFO: Got endpoints: latency-svc-8rznt [750.297746ms]
Jan 19 23:40:19.832: INFO: Created: latency-svc-z8t2w
Jan 19 23:40:19.846: INFO: Got endpoints: latency-svc-xqmgc [750.192336ms]
Jan 19 23:40:19.854: INFO: Created: latency-svc-4cd2j
Jan 19 23:40:19.897: INFO: Got endpoints: latency-svc-4chdm [752.421306ms]
Jan 19 23:40:19.912: INFO: Created: latency-svc-hl8ps
Jan 19 23:40:19.945: INFO: Got endpoints: latency-svc-nvctb [748.554353ms]
Jan 19 23:40:19.954: INFO: Created: latency-svc-zvjzj
Jan 19 23:40:19.997: INFO: Got endpoints: latency-svc-t24rq [751.105142ms]
Jan 19 23:40:20.011: INFO: Created: latency-svc-qbfmm
Jan 19 23:40:20.046: INFO: Got endpoints: latency-svc-qt2cs [750.690649ms]
Jan 19 23:40:20.059: INFO: Created: latency-svc-dz47b
Jan 19 23:40:20.095: INFO: Got endpoints: latency-svc-r964l [750.517623ms]
Jan 19 23:40:20.106: INFO: Created: latency-svc-4srgj
Jan 19 23:40:20.145: INFO: Got endpoints: latency-svc-x5kg5 [746.77256ms]
Jan 19 23:40:20.159: INFO: Created: latency-svc-lj7ff
Jan 19 23:40:20.194: INFO: Got endpoints: latency-svc-cns4l [747.584289ms]
Jan 19 23:40:20.204: INFO: Created: latency-svc-2vrq9
Jan 19 23:40:20.247: INFO: Got endpoints: latency-svc-n25z2 [746.640379ms]
Jan 19 23:40:20.259: INFO: Created: latency-svc-snzlr
Jan 19 23:40:20.295: INFO: Got endpoints: latency-svc-75j4t [747.995402ms]
Jan 19 23:40:20.304: INFO: Created: latency-svc-f7th2
Jan 19 23:40:20.349: INFO: Got endpoints: latency-svc-5z8dk [754.110136ms]
Jan 19 23:40:20.405: INFO: Got endpoints: latency-svc-sldxs [759.733396ms]
Jan 19 23:40:20.446: INFO: Got endpoints: latency-svc-hbljs [750.820767ms]
Jan 19 23:40:20.495: INFO: Got endpoints: latency-svc-f8gbh [744.423439ms]
Jan 19 23:40:20.547: INFO: Got endpoints: latency-svc-z8t2w [751.272893ms]
Jan 19 23:40:20.595: INFO: Got endpoints: latency-svc-4cd2j [748.777932ms]
Jan 19 23:40:20.645: INFO: Got endpoints: latency-svc-hl8ps [748.008586ms]
Jan 19 23:40:20.696: INFO: Got endpoints: latency-svc-zvjzj [751.325523ms]
Jan 19 23:40:20.745: INFO: Got endpoints: latency-svc-qbfmm [747.765094ms]
Jan 19 23:40:20.795: INFO: Got endpoints: latency-svc-dz47b [749.055115ms]
Jan 19 23:40:20.845: INFO: Got endpoints: latency-svc-4srgj [749.117099ms]
Jan 19 23:40:20.895: INFO: Got endpoints: latency-svc-lj7ff [749.840624ms]
Jan 19 23:40:20.946: INFO: Got endpoints: latency-svc-2vrq9 [751.522761ms]
Jan 19 23:40:20.997: INFO: Got endpoints: latency-svc-snzlr [749.121719ms]
Jan 19 23:40:21.045: INFO: Got endpoints: latency-svc-f7th2 [750.036998ms]
Jan 19 23:40:21.045: INFO: Latencies: [23.1954ms 33.778982ms 45.819568ms 57.556075ms 76.683195ms 86.864782ms 111.394921ms 127.388442ms 135.511013ms 146.292104ms 158.723928ms 171.53076ms 187.434532ms 192.805888ms 194.377426ms 197.198381ms 204.237901ms 204.303648ms 210.690151ms 211.00011ms 212.434908ms 212.836064ms 213.938994ms 214.404109ms 224.227454ms 230.647157ms 240.976317ms 242.648258ms 245.071204ms 247.916763ms 248.048089ms 249.544713ms 251.610348ms 252.057719ms 252.411968ms 258.700573ms 259.164358ms 259.47489ms 259.668263ms 265.706369ms 266.97052ms 267.884799ms 268.466451ms 271.656702ms 301.14253ms 339.44771ms 381.653218ms 416.647701ms 456.952657ms 493.755373ms 532.035344ms 570.584508ms 593.212712ms 634.62722ms 680.110607ms 699.169732ms 713.227646ms 738.974549ms 741.63252ms 742.718496ms 743.174653ms 743.571595ms 744.423439ms 744.764318ms 745.040027ms 745.262662ms 746.399329ms 746.640379ms 746.74572ms 746.76201ms 746.77256ms 746.968449ms 747.216372ms 747.395664ms 747.494811ms 747.584289ms 747.638273ms 747.750877ms 747.765094ms 747.771142ms 747.896922ms 747.952388ms 747.995402ms 748.008586ms 748.077038ms 748.105316ms 748.183561ms 748.186918ms 748.208669ms 748.227556ms 748.336708ms 748.35848ms 748.554353ms 748.737895ms 748.760716ms 748.777932ms 748.973938ms 749.007763ms 749.055115ms 749.064805ms 749.108888ms 749.114737ms 749.117099ms 749.121719ms 749.122042ms 749.12813ms 749.202732ms 749.277752ms 749.2948ms 749.297345ms 749.321686ms 749.344278ms 749.344621ms 749.374389ms 749.378681ms 749.412382ms 749.428433ms 749.486119ms 749.488554ms 749.514827ms 749.546426ms 749.621843ms 749.660874ms 749.662407ms 749.787386ms 749.840624ms 749.846008ms 749.846081ms 749.929532ms 749.935612ms 749.96598ms 750.020839ms 750.036998ms 750.067188ms 750.192336ms 750.209533ms 750.231128ms 750.284225ms 750.297746ms 750.303577ms 750.305867ms 750.313523ms 750.339352ms 750.398448ms 750.45327ms 750.492785ms 750.500651ms 750.517623ms 750.520439ms 750.576174ms 750.579835ms 750.587243ms 750.648511ms 750.649795ms 750.664449ms 750.690649ms 750.744623ms 750.779899ms 750.793642ms 750.798885ms 750.820767ms 750.852351ms 750.920636ms 750.945311ms 750.951135ms 750.988783ms 751.095929ms 751.099476ms 751.105142ms 751.111505ms 751.272893ms 751.288694ms 751.325523ms 751.36794ms 751.3918ms 751.451469ms 751.522761ms 751.58505ms 751.615692ms 751.743691ms 751.781785ms 752.0096ms 752.21605ms 752.232117ms 752.244189ms 752.262081ms 752.300791ms 752.421306ms 752.451365ms 752.646751ms 752.927792ms 753.180203ms 754.110136ms 755.358761ms 755.37294ms 755.931559ms 756.723757ms 757.708676ms 758.977304ms 759.733396ms]
Jan 19 23:40:21.046: INFO: 50 %ile: 749.108888ms
Jan 19 23:40:21.046: INFO: 90 %ile: 751.781785ms
Jan 19 23:40:21.046: INFO: 99 %ile: 758.977304ms
Jan 19 23:40:21.046: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:21.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4172" for this suite.

• [SLOW TEST:11.758 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":230,"skipped":4242,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:21.054: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:40:21.093: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c" in namespace "projected-3409" to be "Succeeded or Failed"
Jan 19 23:40:21.096: INFO: Pod "downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.885869ms
Jan 19 23:40:23.106: INFO: Pod "downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012540676s
Jan 19 23:40:25.110: INFO: Pod "downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016084283s
STEP: Saw pod success
Jan 19 23:40:25.110: INFO: Pod "downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c" satisfied condition "Succeeded or Failed"
Jan 19 23:40:25.111: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c container client-container: <nil>
STEP: delete the pod
Jan 19 23:40:25.123: INFO: Waiting for pod downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c to disappear
Jan 19 23:40:25.131: INFO: Pod downwardapi-volume-c53eda4e-0412-40fb-83a4-76ed6d933a3c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:25.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3409" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":231,"skipped":4255,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:25.137: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-535
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-535
Jan 19 23:40:25.185: INFO: Found 0 stateful pods, waiting for 1
Jan 19 23:40:35.188: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 19 23:40:35.208: INFO: Deleting all statefulset in ns statefulset-535
Jan 19 23:40:35.210: INFO: Scaling statefulset ss to 0
Jan 19 23:40:45.247: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 23:40:45.249: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:45.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-535" for this suite.

• [SLOW TEST:20.126 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":232,"skipped":4273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:45.264: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:45.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4341" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":233,"skipped":4308,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:45.340: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-51baf87a-fb51-4d4f-9514-ec56603942d7
STEP: Creating a pod to test consume secrets
Jan 19 23:40:45.377: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229" in namespace "projected-6383" to be "Succeeded or Failed"
Jan 19 23:40:45.380: INFO: Pod "pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023559ms
Jan 19 23:40:47.384: INFO: Pod "pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006552629s
Jan 19 23:40:49.388: INFO: Pod "pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010604749s
STEP: Saw pod success
Jan 19 23:40:49.388: INFO: Pod "pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229" satisfied condition "Succeeded or Failed"
Jan 19 23:40:49.390: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:40:49.410: INFO: Waiting for pod pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229 to disappear
Jan 19 23:40:49.412: INFO: Pod pod-projected-secrets-60da322c-af65-49ae-bc1b-ae17e5174229 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:40:49.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6383" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:40:49.418: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 19 23:40:49.446: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:40:54.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:41:14.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8584" for this suite.

• [SLOW TEST:25.076 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":235,"skipped":4352,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:41:14.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:41:14.522: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:41:15.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7614" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":236,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:41:15.068: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4814
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4814
I0119 23:41:15.135133      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4814, replica count: 2
Jan 19 23:41:18.187: INFO: Creating new exec pod
I0119 23:41:18.187013      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 23:41:23.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4814 exec execpodx7f77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 23:41:23.362: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 23:41:23.362: INFO: stdout: ""
Jan 19 23:41:24.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4814 exec execpodx7f77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 23:41:24.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 23:41:24.512: INFO: stdout: ""
Jan 19 23:41:25.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4814 exec execpodx7f77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 23:41:25.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 23:41:25.520: INFO: stdout: "externalname-service-nwzmq"
Jan 19 23:41:25.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4814 exec execpodx7f77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.27.76 80'
Jan 19 23:41:25.664: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.27.76 80\nConnection to 10.97.27.76 80 port [tcp/http] succeeded!\n"
Jan 19 23:41:25.664: INFO: stdout: ""
Jan 19 23:41:26.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4814 exec execpodx7f77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.27.76 80'
Jan 19 23:41:26.814: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.27.76 80\nConnection to 10.97.27.76 80 port [tcp/http] succeeded!\n"
Jan 19 23:41:26.814: INFO: stdout: ""
Jan 19 23:41:27.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-4814 exec execpodx7f77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.27.76 80'
Jan 19 23:41:27.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.27.76 80\nConnection to 10.97.27.76 80 port [tcp/http] succeeded!\n"
Jan 19 23:41:27.818: INFO: stdout: "externalname-service-lkgrk"
Jan 19 23:41:27.818: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:41:27.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4814" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.774 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":237,"skipped":4380,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:41:27.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:41:27.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4329 create -f -'
Jan 19 23:41:29.151: INFO: stderr: ""
Jan 19 23:41:29.151: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 19 23:41:29.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4329 create -f -'
Jan 19 23:41:29.355: INFO: stderr: ""
Jan 19 23:41:29.355: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 19 23:41:30.360: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:41:30.360: INFO: Found 0 / 1
Jan 19 23:41:31.359: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:41:31.359: INFO: Found 0 / 1
Jan 19 23:41:32.360: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:41:32.360: INFO: Found 1 / 1
Jan 19 23:41:32.360: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 23:41:32.362: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:41:32.362: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 23:41:32.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4329 describe pod agnhost-primary-qxw5w'
Jan 19 23:41:32.422: INFO: stderr: ""
Jan 19 23:41:32.422: INFO: stdout: "Name:         agnhost-primary-qxw5w\nNamespace:    kubectl-4329\nPriority:     0\nNode:         ip-10-0-1-153.eu-central-1.compute.internal/10.0.1.153\nStart Time:   Wed, 19 Jan 2022 23:41:29 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: eb9dfea456c406c3df651944dc99c28e4fa1d33545ab3d3931f07be859ad2404\n              cni.projectcalico.org/podIP: 172.16.146.28/32\n              cni.projectcalico.org/podIPs: 172.16.146.28/32\nStatus:       Running\nIP:           172.16.146.28\nIPs:\n  IP:           172.16.146.28\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://e384a47718f3b1bbf11fc928b928ec7573376a466e80a7eff63ccf5aa719c48f\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 19 Jan 2022 23:41:30 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d8fbr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-d8fbr:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-4329/agnhost-primary-qxw5w to ip-10-0-1-153.eu-central-1.compute.internal\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 19 23:41:32.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4329 describe rc agnhost-primary'
Jan 19 23:41:32.481: INFO: stderr: ""
Jan 19 23:41:32.481: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4329\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-qxw5w\n"
Jan 19 23:41:32.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4329 describe service agnhost-primary'
Jan 19 23:41:32.539: INFO: stderr: ""
Jan 19 23:41:32.539: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4329\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.101.27.178\nIPs:               10.101.27.178\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.146.28:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 19 23:41:32.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4329 describe node ip-10-0-0-80.eu-central-1.compute.internal'
Jan 19 23:41:32.619: INFO: stderr: ""
Jan 19 23:41:32.619: INFO: stdout: "Name:               ip-10-0-0-80.eu-central-1.compute.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-0-80.eu-central-1.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.80/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.149.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 19 Jan 2022 18:05:00 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-0-80.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 19 Jan 2022 23:41:29 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 19 Jan 2022 18:06:07 +0000   Wed, 19 Jan 2022 18:06:07 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 19 Jan 2022 23:39:11 +0000   Wed, 19 Jan 2022 18:05:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 19 Jan 2022 23:39:11 +0000   Wed, 19 Jan 2022 18:05:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 19 Jan 2022 23:39:11 +0000   Wed, 19 Jan 2022 18:05:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 19 Jan 2022 23:39:11 +0000   Wed, 19 Jan 2022 18:06:02 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.0.80\n  Hostname:    ip-10-0-0-80.eu-central-1.compute.internal\nCapacity:\n  cpu:                8\n  ephemeral-storage:  101583780Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32939324Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  93619611493\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32836924Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 89e002fc4fcd49f195bd414d16607d34\n  System UUID:                EC2C4BD0-30CA-66C5-45C2-3C83BA7C9543\n  Boot ID:                    e230d447-c78e-4154-835f-18a2f55ec210\n  Kernel Version:             4.15.0-1058-aws\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.9\n  Kubelet Version:            v1.23.0\n  Kube-Proxy Version:         v1.23.0\nPodCIDR:                      10.0.0.0/24\nPodCIDRs:                     10.0.0.0/24\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                  ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-2lsxv                                                     250m (3%)     0 (0%)      0 (0%)           0 (0%)         5h35m\n  kube-system                 etcd-ip-10-0-0-80.eu-central-1.compute.internal                       100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         5h36m\n  kube-system                 kube-apiserver-ip-10-0-0-80.eu-central-1.compute.internal             250m (3%)     0 (0%)      0 (0%)           0 (0%)         5h36m\n  kube-system                 kube-controller-manager-ip-10-0-0-80.eu-central-1.compute.internal    200m (2%)     0 (0%)      0 (0%)           0 (0%)         5h36m\n  kube-system                 kube-proxy-ktwds                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h36m\n  kube-system                 kube-scheduler-ip-10-0-0-80.eu-central-1.compute.internal             100m (1%)     0 (0%)      0 (0%)           0 (0%)         5h36m\n  logging                     fluentbit-jwpmp                                                       100m (1%)     200m (2%)   50M (0%)         100M (0%)      82m\n  monitoring                  goldpinger-5q648                                                      1m (0%)       0 (0%)      40Mi (0%)        80Mi (0%)      82m\n  monitoring                  kube-proxy-metrics-qlr9d                                              10m (0%)      0 (0%)      20Mi (0%)        40Mi (0%)      82m\n  monitoring                  node-exporter-x8rs9                                                   112m (1%)     270m (3%)   200Mi (0%)       220Mi (0%)     82m\n  sonobuoy                    sonobuoy-e2e-job-e35c13774c034cac                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-c77vp               0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1123m (14%)     470m (5%)\n  memory             427487360 (1%)  456515840 (1%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:\n  Type     Reason                                            Age    From        Message\n  ----     ------                                            ----   ----        -------\n  Warning  listen tcp4 :30658: bind: address already in use  50m    kube-proxy  can't open port \"nodePort for services-1258/affinity-nodeport-timeout\" (:30658/tcp4), skipping it\n  Warning  listen tcp4 :31935: bind: address already in use  47m    kube-proxy  can't open port \"nodePort for services-2868/nodeport-service\" (:31935/tcp4), skipping it\n  Warning  listen tcp4 :32621: bind: address already in use  45m    kube-proxy  can't open port \"nodePort for services-4582/externalname-service:http\" (:32621/tcp4), skipping it\n  Warning  listen tcp4 :31663: bind: address already in use  26m    kube-proxy  can't open port \"nodePort for resourcequota-1376/test-service-np\" (:31663/tcp4), skipping it\n  Warning  listen tcp4 :32738: bind: address already in use  9m16s  kube-proxy  can't open port \"nodePort for services-2247/nodeport-test:http\" (:32738/tcp4), skipping it\n"
Jan 19 23:41:32.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4329 describe namespace kubectl-4329'
Jan 19 23:41:32.677: INFO: stderr: ""
Jan 19 23:41:32.677: INFO: stdout: "Name:         kubectl-4329\nLabels:       e2e-framework=kubectl\n              e2e-run=51a0ad83-197b-44da-b76e-3062d64f5602\n              kubernetes.io/metadata.name=kubectl-4329\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:41:32.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4329" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":238,"skipped":4392,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:41:32.684: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 19 23:41:32.720: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:41:37.938: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:41:57.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8623" for this suite.

• [SLOW TEST:25.111 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":239,"skipped":4392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:41:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-6019
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6019 to expose endpoints map[]
Jan 19 23:41:57.847: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 19 23:41:58.853: INFO: successfully validated that service multi-endpoint-test in namespace services-6019 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6019
Jan 19 23:41:58.862: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:00.869: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:02.865: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6019 to expose endpoints map[pod1:[100]]
Jan 19 23:42:02.874: INFO: successfully validated that service multi-endpoint-test in namespace services-6019 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6019
Jan 19 23:42:02.882: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:04.885: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:06.889: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6019 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 19 23:42:06.899: INFO: successfully validated that service multi-endpoint-test in namespace services-6019 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jan 19 23:42:06.899: INFO: Creating new exec pod
Jan 19 23:42:11.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-6019 exec execpoddldjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 19 23:42:12.065: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 19 23:42:12.065: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:42:12.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-6019 exec execpoddldjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.149.210 80'
Jan 19 23:42:12.203: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.149.210 80\nConnection to 10.102.149.210 80 port [tcp/http] succeeded!\n"
Jan 19 23:42:12.203: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:42:12.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-6019 exec execpoddldjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 19 23:42:12.352: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 19 23:42:12.352: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:42:12.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-6019 exec execpoddldjw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.149.210 81'
Jan 19 23:42:12.519: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.149.210 81\nConnection to 10.102.149.210 81 port [tcp/*] succeeded!\n"
Jan 19 23:42:12.519: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6019
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6019 to expose endpoints map[pod2:[101]]
Jan 19 23:42:12.548: INFO: successfully validated that service multi-endpoint-test in namespace services-6019 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6019
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6019 to expose endpoints map[]
Jan 19 23:42:13.568: INFO: successfully validated that service multi-endpoint-test in namespace services-6019 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:42:13.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6019" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.796 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":240,"skipped":4433,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:42:13.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-320cff49-88b4-402a-9562-e69f34248e98
STEP: Creating a pod to test consume secrets
Jan 19 23:42:13.636: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81" in namespace "projected-1456" to be "Succeeded or Failed"
Jan 19 23:42:13.638: INFO: Pod "pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949847ms
Jan 19 23:42:15.641: INFO: Pod "pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004748283s
Jan 19 23:42:17.644: INFO: Pod "pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008073837s
STEP: Saw pod success
Jan 19 23:42:17.644: INFO: Pod "pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81" satisfied condition "Succeeded or Failed"
Jan 19 23:42:17.646: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:42:17.666: INFO: Waiting for pod pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81 to disappear
Jan 19 23:42:17.668: INFO: Pod pod-projected-secrets-ba007d77-8fb4-4151-a370-cf8fa8fb2d81 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:42:17.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1456" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:42:17.675: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 19 23:42:17.712: INFO: The status of Pod labelsupdate1118a5c1-f9f7-40e4-b925-b8899f379411 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:19.714: INFO: The status of Pod labelsupdate1118a5c1-f9f7-40e4-b925-b8899f379411 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:21.715: INFO: The status of Pod labelsupdate1118a5c1-f9f7-40e4-b925-b8899f379411 is Running (Ready = true)
Jan 19 23:42:22.236: INFO: Successfully updated pod "labelsupdate1118a5c1-f9f7-40e4-b925-b8899f379411"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:42:24.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9282" for this suite.

• [SLOW TEST:6.587 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4469,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:42:24.261: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-3b9bc506-099c-4da7-b1e8-8f5893c976f7
STEP: Creating configMap with name cm-test-opt-upd-e018f538-5192-4437-8360-2e9fbf5a41ff
STEP: Creating the pod
Jan 19 23:42:24.320: INFO: The status of Pod pod-projected-configmaps-9a03b25a-c903-4216-a65a-692a07ef6dbc is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:26.324: INFO: The status of Pod pod-projected-configmaps-9a03b25a-c903-4216-a65a-692a07ef6dbc is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:42:28.326: INFO: The status of Pod pod-projected-configmaps-9a03b25a-c903-4216-a65a-692a07ef6dbc is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-3b9bc506-099c-4da7-b1e8-8f5893c976f7
STEP: Updating configmap cm-test-opt-upd-e018f538-5192-4437-8360-2e9fbf5a41ff
STEP: Creating configMap with name cm-test-opt-create-55415696-3c23-4120-a729-48e151153bb5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:42:32.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4957" for this suite.

• [SLOW TEST:8.128 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4471,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:42:32.390: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 19 23:42:32.415: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 23:42:32.424: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 23:42:32.426: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-103.eu-central-1.compute.internal before test
Jan 19 23:42:32.436: INFO: cert-manager-774679ff99-fg5j9 from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container cert-manager ready: true, restart count 0
Jan 19 23:42:32.436: INFO: gatekeeper-audit-59fcc9f644-7fmkr from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:42:32.436: INFO: gatekeeper-controller-manager-86677cdd6d-59r5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:42:32.436: INFO: calico-node-rpdnw from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:42:32.436: INFO: kube-proxy-l9kpk from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:42:32.436: INFO: minio-0 from kube-system started at 2022-01-19 22:19:21 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container minio ready: true, restart count 0
Jan 19 23:42:32.436: INFO: velero-restic-dr6p6 from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:42:32.436: INFO: elasticsearch-0 from logging started at 2022-01-19 22:19:23 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container elasticsearch ready: true, restart count 0
Jan 19 23:42:32.436: INFO: 	Container exporter ready: true, restart count 0
Jan 19 23:42:32.436: INFO: fluentbit-zwj8m from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:42:32.436: INFO: alertmanager-main-2 from monitoring started at 2022-01-19 22:40:34 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 23:42:32.436: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:42:32.436: INFO: goldpinger-96ctd from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:42:32.436: INFO: kube-proxy-metrics-wbqwc from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:42:32.436: INFO: node-exporter-tqbmk from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:42:32.436: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:42:32.436: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csw5s from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.436: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:42:32.436: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 23:42:32.436: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-153.eu-central-1.compute.internal before test
Jan 19 23:42:32.455: INFO: calico-node-c7xgd from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:42:32.455: INFO: kube-proxy-9p6s2 from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:42:32.455: INFO: velero-restic-xtp74 from kube-system started at 2022-01-19 22:58:22 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:42:32.455: INFO: fluentbit-nz7dc from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:42:32.455: INFO: fluentd-1 from logging started at 2022-01-19 22:58:24 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container fluentd ready: true, restart count 0
Jan 19 23:42:32.455: INFO: goldpinger-9n9xc from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:42:32.455: INFO: kube-proxy-metrics-nx74g from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:42:32.455: INFO: node-exporter-ptzzd from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:42:32.455: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:42:32.455: INFO: pod-projected-configmaps-9a03b25a-c903-4216-a65a-692a07ef6dbc from projected-4957 started at 2022-01-19 23:42:24 +0000 UTC (3 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container createcm-volume-test ready: true, restart count 0
Jan 19 23:42:32.455: INFO: 	Container delcm-volume-test ready: true, restart count 0
Jan 19 23:42:32.455: INFO: 	Container updcm-volume-test ready: true, restart count 0
Jan 19 23:42:32.455: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-nbt7p from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:42:32.455: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 23:42:32.455: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-48.eu-central-1.compute.internal before test
Jan 19 23:42:32.503: INFO: cert-manager-cainjector-66fcc559bf-wgm9t from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container cainjector ready: true, restart count 0
Jan 19 23:42:32.503: INFO: gatekeeper-controller-manager-86677cdd6d-fps5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:42:32.503: INFO: gatekeeper-policy-manager-7644bb5475-cwmpf from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container gatekeeper-policy-manager ready: true, restart count 0
Jan 19 23:42:32.503: INFO: nginx-ingress-controller-6bcf978d9-w2wlr from ingress-nginx started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 23:42:32.503: INFO: calico-node-c8np5 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:42:32.503: INFO: kube-proxy-prc9k from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:42:32.503: INFO: velero-restic-nkgcd from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:42:32.503: INFO: cerebro-6f88ff7888-plncf from logging started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container cerebro ready: true, restart count 0
Jan 19 23:42:32.503: INFO: fluentbit-wt6cx from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:42:32.503: INFO: fluentd-0 from logging started at 2022-01-19 22:19:52 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container fluentd ready: true, restart count 1
Jan 19 23:42:32.503: INFO: alertmanager-main-0 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 23:42:32.503: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:42:32.503: INFO: goldpinger-g9xm2 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:42:32.503: INFO: kube-proxy-metrics-z42ff from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:42:32.503: INFO: node-exporter-w54w5 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:42:32.503: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:42:32.503: INFO: prometheus-k8s-0 from monitoring started at 2022-01-19 22:21:16 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:42:32.503: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 23:42:32.503: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-qdwj2 from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:42:32.503: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 23:42:32.503: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-62.eu-central-1.compute.internal before test
Jan 19 23:42:32.517: INFO: cert-manager-webhook-77ffc44fc6-8fxlk from cert-manager started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container webhook ready: true, restart count 0
Jan 19 23:42:32.517: INFO: gatekeeper-controller-manager-86677cdd6d-mnx56 from gatekeeper-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container manager ready: true, restart count 0
Jan 19 23:42:32.517: INFO: calico-kube-controllers-59c967558b-nwcpg from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 23:42:32.517: INFO: calico-node-jbfb7 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 23:42:32.517: INFO: coredns-64897985d-cftqm from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container coredns ready: true, restart count 0
Jan 19 23:42:32.517: INFO: coredns-64897985d-l9c4t from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container coredns ready: true, restart count 0
Jan 19 23:42:32.517: INFO: kube-proxy-7shjp from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 23:42:32.517: INFO: metrics-server-6f5f78d676-mnlwp from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 23:42:32.517: INFO: velero-6f84f7465-tgzks from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container velero ready: true, restart count 0
Jan 19 23:42:32.517: INFO: velero-restic-6xdjj from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container restic ready: true, restart count 0
Jan 19 23:42:32.517: INFO: local-path-provisioner-5b94755fb4-8zcbz from local-path-storage started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jan 19 23:42:32.517: INFO: fluentbit-sj8hh from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 19 23:42:32.517: INFO: fluentd-2 from logging started at 2022-01-19 22:20:56 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container fluentd ready: true, restart count 0
Jan 19 23:42:32.517: INFO: kibana-8565fbd49b-pm9nr from logging started at 2022-01-19 22:40:29 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container kibana ready: true, restart count 0
Jan 19 23:42:32.517: INFO: 	Container kibana-index-patterns ready: true, restart count 0
Jan 19 23:42:32.517: INFO: alertmanager-main-1 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container alertmanager ready: true, restart count 0
Jan 19 23:42:32.517: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 23:42:32.517: INFO: goldpinger-ml8vn from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container goldpinger ready: true, restart count 0
Jan 19 23:42:32.517: INFO: grafana-7c96db944d-cq4n6 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container dashboard-sidecar ready: true, restart count 0
Jan 19 23:42:32.517: INFO: 	Container grafana ready: true, restart count 0
Jan 19 23:42:32.517: INFO: kube-proxy-metrics-lvbg9 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 19 23:42:32.517: INFO: kube-state-metrics-7456544d4b-wjgk8 from monitoring started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 23:42:32.517: INFO: node-exporter-x46g7 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 23:42:32.517: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 23:42:32.517: INFO: prometheus-operator-7db7f75fdb-624bl from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 23:42:32.517: INFO: sonobuoy from sonobuoy started at 2022-01-19 22:27:04 +0000 UTC (1 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 23:42:32.517: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csbhg from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 19 23:42:32.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 23:42:32.517: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-13ca3ca8-32c2-4640-b84f-7dadea321546 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.1.153 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-13ca3ca8-32c2-4640-b84f-7dadea321546 off the node ip-10-0-1-153.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-13ca3ca8-32c2-4640-b84f-7dadea321546
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:47:40.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1998" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.216 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":244,"skipped":4483,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:47:40.607: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-2529
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2529 to expose endpoints map[]
Jan 19 23:47:40.655: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 19 23:47:41.661: INFO: successfully validated that service endpoint-test2 in namespace services-2529 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2529
Jan 19 23:47:41.670: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:47:43.675: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:47:45.675: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2529 to expose endpoints map[pod1:[80]]
Jan 19 23:47:45.684: INFO: successfully validated that service endpoint-test2 in namespace services-2529 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jan 19 23:47:45.684: INFO: Creating new exec pod
Jan 19 23:47:50.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2529 exec execpodkb5r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 23:47:51.744: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 23:47:51.744: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:47:51.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2529 exec execpodkb5r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.226.24 80'
Jan 19 23:47:51.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.226.24 80\nConnection to 10.108.226.24 80 port [tcp/http] succeeded!\n"
Jan 19 23:47:51.895: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2529
Jan 19 23:47:51.904: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:47:53.906: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:47:55.908: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2529 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 19 23:47:55.918: INFO: successfully validated that service endpoint-test2 in namespace services-2529 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jan 19 23:47:56.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2529 exec execpodkb5r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 23:47:57.076: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 23:47:57.076: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:47:57.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2529 exec execpodkb5r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.226.24 80'
Jan 19 23:47:57.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.226.24 80\nConnection to 10.108.226.24 80 port [tcp/http] succeeded!\n"
Jan 19 23:47:57.228: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2529
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2529 to expose endpoints map[pod2:[80]]
Jan 19 23:47:57.260: INFO: successfully validated that service endpoint-test2 in namespace services-2529 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jan 19 23:47:58.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2529 exec execpodkb5r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 23:47:58.420: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 23:47:58.420: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:47:58.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-2529 exec execpodkb5r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.226.24 80'
Jan 19 23:47:58.563: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.226.24 80\nConnection to 10.108.226.24 80 port [tcp/http] succeeded!\n"
Jan 19 23:47:58.563: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2529
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2529 to expose endpoints map[]
Jan 19 23:47:59.584: INFO: successfully validated that service endpoint-test2 in namespace services-2529 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:47:59.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2529" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:19.005 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":245,"skipped":4497,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:47:59.611: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jan 19 23:47:59.653: INFO: created test-event-1
Jan 19 23:47:59.657: INFO: created test-event-2
Jan 19 23:47:59.661: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 19 23:47:59.663: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 19 23:47:59.674: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:47:59.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6406" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":246,"skipped":4513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:47:59.687: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 23:47:59.794: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:47:59.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:47:59.802: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:00.807: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:00.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:48:00.809: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:01.806: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:01.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:48:01.809: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:02.808: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:02.811: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 23:48:02.811: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 19 23:48:02.823: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:02.825: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:48:02.825: INFO: Node ip-10-0-1-62.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:03.830: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:03.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:48:03.833: INFO: Node ip-10-0-1-62.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:04.830: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:04.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:48:04.833: INFO: Node ip-10-0-1-62.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:05.829: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:05.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:48:05.831: INFO: Node ip-10-0-1-62.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:06.830: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:06.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 19 23:48:06.832: INFO: Node ip-10-0-1-62.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:48:07.830: INFO: DaemonSet pods can't tolerate node ip-10-0-0-80.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 23:48:07.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jan 19 23:48:07.832: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-210, will wait for the garbage collector to delete the pods
Jan 19 23:48:07.889: INFO: Deleting DaemonSet.extensions daemon-set took: 3.110414ms
Jan 19 23:48:07.990: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.722135ms
Jan 19 23:48:10.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:48:10.393: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 23:48:10.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"90215"},"items":null}

Jan 19 23:48:10.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"90215"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:10.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-210" for this suite.

• [SLOW TEST:10.726 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":247,"skipped":4548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:48:10.445: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 19 23:48:15.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-7674 --namespace=crd-publish-openapi-7674 create -f -'
Jan 19 23:48:17.500: INFO: stderr: ""
Jan 19 23:48:17.500: INFO: stdout: "e2e-test-crd-publish-openapi-4050-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 23:48:17.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-7674 --namespace=crd-publish-openapi-7674 delete e2e-test-crd-publish-openapi-4050-crds test-cr'
Jan 19 23:48:17.558: INFO: stderr: ""
Jan 19 23:48:17.558: INFO: stdout: "e2e-test-crd-publish-openapi-4050-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 19 23:48:17.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-7674 --namespace=crd-publish-openapi-7674 apply -f -'
Jan 19 23:48:17.772: INFO: stderr: ""
Jan 19 23:48:17.772: INFO: stdout: "e2e-test-crd-publish-openapi-4050-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 23:48:17.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-7674 --namespace=crd-publish-openapi-7674 delete e2e-test-crd-publish-openapi-4050-crds test-cr'
Jan 19 23:48:17.827: INFO: stderr: ""
Jan 19 23:48:17.827: INFO: stdout: "e2e-test-crd-publish-openapi-4050-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 19 23:48:17.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=crd-publish-openapi-7674 explain e2e-test-crd-publish-openapi-4050-crds'
Jan 19 23:48:18.022: INFO: stderr: ""
Jan 19 23:48:18.022: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4050-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:23.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7674" for this suite.

• [SLOW TEST:12.775 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":248,"skipped":4611,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:23.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 19 23:48:23.219: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:27.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2556" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":249,"skipped":4623,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:27.792: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:32.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6214" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":250,"skipped":4635,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:32.222: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jan 19 23:48:32.248: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-4114 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:32.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4114" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":251,"skipped":4635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:32.300: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-2478/configmap-test-1d688fe4-39ec-4d20-8b33-35242ae696be
STEP: Creating a pod to test consume configMaps
Jan 19 23:48:32.340: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b" in namespace "configmap-2478" to be "Succeeded or Failed"
Jan 19 23:48:32.342: INFO: Pod "pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860805ms
Jan 19 23:48:34.344: INFO: Pod "pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004629882s
Jan 19 23:48:36.349: INFO: Pod "pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009322411s
STEP: Saw pod success
Jan 19 23:48:36.349: INFO: Pod "pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b" satisfied condition "Succeeded or Failed"
Jan 19 23:48:36.352: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b container env-test: <nil>
STEP: delete the pod
Jan 19 23:48:36.375: INFO: Waiting for pod pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b to disappear
Jan 19 23:48:36.383: INFO: Pod pod-configmaps-d7e18a15-9eb7-445a-92cd-ab79ecd3858b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:36.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2478" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:36.391: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 19 23:48:46.479: INFO: The status of Pod kube-controller-manager-ip-10-0-0-80.eu-central-1.compute.internal is Running (Ready = true)
Jan 19 23:48:46.748: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:46.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4097" for this suite.

• [SLOW TEST:10.366 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":253,"skipped":4746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:46.756: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8938
STEP: creating service affinity-nodeport in namespace services-8938
STEP: creating replication controller affinity-nodeport in namespace services-8938
I0119 23:48:46.824115      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8938, replica count: 3
I0119 23:48:49.875085      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 23:48:49.883: INFO: Creating new exec pod
Jan 19 23:48:54.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-8938 exec execpod-affinity5h6j7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 19 23:48:55.064: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 19 23:48:55.064: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:48:55.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-8938 exec execpod-affinity5h6j7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.49.212 80'
Jan 19 23:48:55.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.49.212 80\nConnection to 10.96.49.212 80 port [tcp/http] succeeded!\n"
Jan 19 23:48:55.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:48:55.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-8938 exec execpod-affinity5h6j7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.103 32429'
Jan 19 23:48:55.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.103 32429\nConnection to 10.0.1.103 32429 port [tcp/*] succeeded!\n"
Jan 19 23:48:55.363: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:48:55.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-8938 exec execpod-affinity5h6j7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.1.153 32429'
Jan 19 23:48:55.514: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.1.153 32429\nConnection to 10.0.1.153 32429 port [tcp/*] succeeded!\n"
Jan 19 23:48:55.515: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 23:48:55.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=services-8938 exec execpod-affinity5h6j7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.103:32429/ ; done'
Jan 19 23:48:55.741: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.103:32429/\n"
Jan 19 23:48:55.741: INFO: stdout: "\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v\naffinity-nodeport-mkn5v"
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Received response from host: affinity-nodeport-mkn5v
Jan 19 23:48:55.741: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8938, will wait for the garbage collector to delete the pods
Jan 19 23:48:55.809: INFO: Deleting ReplicationController affinity-nodeport took: 3.313873ms
Jan 19 23:48:55.910: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.004123ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:48:58.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8938" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.888 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":254,"skipped":4789,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:48:58.644: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 19 23:48:58.682: INFO: Waiting up to 5m0s for pod "downward-api-c8118ff9-2e79-407f-b670-8b47591f6371" in namespace "downward-api-284" to be "Succeeded or Failed"
Jan 19 23:48:58.683: INFO: Pod "downward-api-c8118ff9-2e79-407f-b670-8b47591f6371": Phase="Pending", Reason="", readiness=false. Elapsed: 1.801354ms
Jan 19 23:49:00.687: INFO: Pod "downward-api-c8118ff9-2e79-407f-b670-8b47591f6371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005688826s
Jan 19 23:49:02.693: INFO: Pod "downward-api-c8118ff9-2e79-407f-b670-8b47591f6371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011712862s
STEP: Saw pod success
Jan 19 23:49:02.693: INFO: Pod "downward-api-c8118ff9-2e79-407f-b670-8b47591f6371" satisfied condition "Succeeded or Failed"
Jan 19 23:49:02.695: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downward-api-c8118ff9-2e79-407f-b670-8b47591f6371 container dapi-container: <nil>
STEP: delete the pod
Jan 19 23:49:02.716: INFO: Waiting for pod downward-api-c8118ff9-2e79-407f-b670-8b47591f6371 to disappear
Jan 19 23:49:02.724: INFO: Pod downward-api-c8118ff9-2e79-407f-b670-8b47591f6371 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:49:02.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-284" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":4796,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:49:02.730: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:49:02.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676" in namespace "downward-api-7282" to be "Succeeded or Failed"
Jan 19 23:49:02.800: INFO: Pod "downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869293ms
Jan 19 23:49:04.803: INFO: Pod "downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005127713s
Jan 19 23:49:06.806: INFO: Pod "downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008417889s
STEP: Saw pod success
Jan 19 23:49:06.807: INFO: Pod "downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676" satisfied condition "Succeeded or Failed"
Jan 19 23:49:06.808: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676 container client-container: <nil>
STEP: delete the pod
Jan 19 23:49:06.834: INFO: Waiting for pod downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676 to disappear
Jan 19 23:49:06.836: INFO: Pod downwardapi-volume-85721638-cea6-4feb-8248-88328bf0a676 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:49:06.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7282" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:49:06.843: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jan 19 23:49:06.895: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 23:50:06.951: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:50:06.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 19 23:50:11.010: INFO: found a healthy node: ip-10-0-1-153.eu-central-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:50:27.070: INFO: pods created so far: [1 1 1]
Jan 19 23:50:27.070: INFO: length of pods created so far: 3
Jan 19 23:50:33.079: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:50:40.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5706" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:50:40.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7164" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.301 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":257,"skipped":4871,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:50:40.144: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 19 23:50:40.171: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:50:44.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1022" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":258,"skipped":4880,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:50:44.331: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 19 23:51:04.501: INFO: EndpointSlice for Service endpointslice-3227/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:14.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3227" for this suite.

• [SLOW TEST:30.186 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":259,"skipped":4881,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:14.517: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jan 19 23:51:15.077: INFO: created pod pod-service-account-defaultsa
Jan 19 23:51:15.077: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 19 23:51:15.088: INFO: created pod pod-service-account-mountsa
Jan 19 23:51:15.088: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 19 23:51:15.099: INFO: created pod pod-service-account-nomountsa
Jan 19 23:51:15.100: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 19 23:51:15.109: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 19 23:51:15.110: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 19 23:51:15.120: INFO: created pod pod-service-account-mountsa-mountspec
Jan 19 23:51:15.120: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 19 23:51:15.131: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 19 23:51:15.132: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 19 23:51:15.141: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 19 23:51:15.142: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 19 23:51:15.151: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 19 23:51:15.151: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 19 23:51:15.161: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 19 23:51:15.161: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:15.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4699" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":260,"skipped":4885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:15.176: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:51:15.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78948c58f6\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:51:17.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:51:19.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:51:21.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:51:24.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:24.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-788" for this suite.
STEP: Destroying namespace "webhook-788-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.429 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":261,"skipped":4938,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:24.606: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 19 23:51:24.690: INFO: running pods: 0 < 3
Jan 19 23:51:26.695: INFO: running pods: 0 < 3
Jan 19 23:51:28.700: INFO: running pods: 0 < 3
Jan 19 23:51:30.693: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:32.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7022" for this suite.

• [SLOW TEST:8.099 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":262,"skipped":4950,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:32.706: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 19 23:51:33.859: INFO: The status of Pod kube-controller-manager-ip-10-0-0-80.eu-central-1.compute.internal is Running (Ready = true)
Jan 19 23:51:34.192: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:34.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2393" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":263,"skipped":4953,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:34.200: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:51:34.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 23:51:36.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 51, 34, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 51, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:51:39.702: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:49.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2320" for this suite.
STEP: Destroying namespace "webhook-2320-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.635 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":264,"skipped":4966,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:49.835: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jan 19 23:51:49.886: INFO: Waiting up to 5m0s for pod "var-expansion-4e11ac2a-5632-4606-865b-90f20aa57583" in namespace "var-expansion-8281" to be "Succeeded or Failed"
Jan 19 23:51:49.888: INFO: Pod "var-expansion-4e11ac2a-5632-4606-865b-90f20aa57583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014153ms
Jan 19 23:51:51.892: INFO: Pod "var-expansion-4e11ac2a-5632-4606-865b-90f20aa57583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005600261s
STEP: Saw pod success
Jan 19 23:51:51.892: INFO: Pod "var-expansion-4e11ac2a-5632-4606-865b-90f20aa57583" satisfied condition "Succeeded or Failed"
Jan 19 23:51:51.894: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod var-expansion-4e11ac2a-5632-4606-865b-90f20aa57583 container dapi-container: <nil>
STEP: delete the pod
Jan 19 23:51:51.924: INFO: Waiting for pod var-expansion-4e11ac2a-5632-4606-865b-90f20aa57583 to disappear
Jan 19 23:51:51.932: INFO: Pod var-expansion-4e11ac2a-5632-4606-865b-90f20aa57583 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:51.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8281" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":265,"skipped":4978,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:51.938: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 23:51:55.988: INFO: DNS probes using dns-315/dns-test-f2327824-e7bf-43ed-804c-585316bda942 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:51:56.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-315" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":266,"skipped":4987,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:51:56.010: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 19 23:51:56.050: INFO: The status of Pod annotationupdated6053799-d26b-4db8-91ec-c115659c094c is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:51:58.053: INFO: The status of Pod annotationupdated6053799-d26b-4db8-91ec-c115659c094c is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:52:00.054: INFO: The status of Pod annotationupdated6053799-d26b-4db8-91ec-c115659c094c is Running (Ready = true)
Jan 19 23:52:00.575: INFO: Successfully updated pod "annotationupdated6053799-d26b-4db8-91ec-c115659c094c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:02.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9335" for this suite.

• [SLOW TEST:6.589 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":4997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:02.600: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:52:02.642: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 19 23:52:07.645: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 19 23:52:07.645: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 19 23:52:09.648: INFO: Creating deployment "test-rollover-deployment"
Jan 19 23:52:09.655: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 19 23:52:11.665: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 19 23:52:11.668: INFO: Ensure that both replica sets have 1 created replica
Jan 19 23:52:11.672: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 19 23:52:11.680: INFO: Updating deployment test-rollover-deployment
Jan 19 23:52:11.680: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 19 23:52:13.688: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 19 23:52:13.692: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 19 23:52:13.695: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 23:52:13.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:52:15.702: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 23:52:15.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:52:17.703: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 23:52:17.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:52:19.700: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 23:52:19.701: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:52:21.701: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 23:52:21.701: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:52:23.702: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 23:52:23.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 52, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 52, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 23:52:25.701: INFO: 
Jan 19 23:52:25.701: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 19 23:52:25.707: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9355  bbc9b03d-51f4-47f3-b20f-8a6ae0033b6f 92285 2 2022-01-19 23:52:09 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-01-19 23:52:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:52:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a697758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-19 23:52:09 +0000 UTC,LastTransitionTime:2022-01-19 23:52:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-01-19 23:52:24 +0000 UTC,LastTransitionTime:2022-01-19 23:52:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 23:52:25.709: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-9355  521b8c44-0dcc-430f-bd5b-6ffd47340578 92274 2 2022-01-19 23:52:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bbc9b03d-51f4-47f3-b20f-8a6ae0033b6f 0xc009456e57 0xc009456e58}] []  [{kube-controller-manager Update apps/v1 2022-01-19 23:52:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc9b03d-51f4-47f3-b20f-8a6ae0033b6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:52:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009456f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:52:25.709: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 19 23:52:25.709: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9355  fb77f79e-4965-4a87-a7c3-7f3b83926429 92284 2 2022-01-19 23:52:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bbc9b03d-51f4-47f3-b20f-8a6ae0033b6f 0xc009456d27 0xc009456d28}] []  [{e2e.test Update apps/v1 2022-01-19 23:52:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:52:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc9b03d-51f4-47f3-b20f-8a6ae0033b6f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:52:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc009456de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:52:25.709: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-9355  4fa93d95-1521-4c97-af33-6dc34920ac36 92230 2 2022-01-19 23:52:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bbc9b03d-51f4-47f3-b20f-8a6ae0033b6f 0xc009456f97 0xc009456f98}] []  [{kube-controller-manager Update apps/v1 2022-01-19 23:52:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc9b03d-51f4-47f3-b20f-8a6ae0033b6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-19 23:52:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009457048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 23:52:25.711: INFO: Pod "test-rollover-deployment-668b7f667d-77j8n" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-77j8n test-rollover-deployment-668b7f667d- deployment-9355  fbfbf628-470d-449b-8882-80c71eb099be 92252 0 2022-01-19 23:52:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[cni.projectcalico.org/containerID:1073609b683d74ed4fa418fb02bbf1aa99cd93ecceb8a2e9736bdb0a9a84efef cni.projectcalico.org/podIP:172.16.146.6/32 cni.projectcalico.org/podIPs:172.16.146.6/32] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d 521b8c44-0dcc-430f-bd5b-6ffd47340578 0xc0094575f7 0xc0094575f8}] []  [{kube-controller-manager Update v1 2022-01-19 23:52:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"521b8c44-0dcc-430f-bd5b-6ffd47340578\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-19 23:52:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-19 23:52:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-675b6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-675b6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:52:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:52:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:52:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-19 23:52:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:172.16.146.6,StartTime:2022-01-19 23:52:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-19 23:52:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:docker://8bb0316b82b6816b201adbf9141a71feaeb82072f495fcd489e6abeeebb8c21a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.146.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:25.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9355" for this suite.

• [SLOW TEST:23.117 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":268,"skipped":5035,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:25.718: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3111.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3111.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3111.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3111.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 23:52:29.787: INFO: DNS probes using dns-3111/dns-test-dc1c90a8-11c6-4270-9721-0f7dd175cac9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:29.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3111" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":269,"skipped":5035,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:29.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:52:31.867: INFO: Deleting pod "var-expansion-ff08227e-d768-45f3-bf47-dace05f1d0b6" in namespace "var-expansion-7733"
Jan 19 23:52:31.870: INFO: Wait up to 5m0s for pod "var-expansion-ff08227e-d768-45f3-bf47-dace05f1d0b6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:35.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7733" for this suite.

• [SLOW TEST:6.059 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":270,"skipped":5043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:35.881: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 19 23:52:35.918: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:52:37.923: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:52:39.922: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 19 23:52:39.932: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:52:41.936: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:52:43.939: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 19 23:52:43.941: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:43.941: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:43.941: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:43.941: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.043: INFO: Exec stderr: ""
Jan 19 23:52:44.043: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.044: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.044: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.180: INFO: Exec stderr: ""
Jan 19 23:52:44.180: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.180: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.181: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.181: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.285: INFO: Exec stderr: ""
Jan 19 23:52:44.285: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.285: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.285: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.285: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.383: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 19 23:52:44.383: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.384: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.384: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.480: INFO: Exec stderr: ""
Jan 19 23:52:44.480: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.480: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.480: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.480: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.569: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 19 23:52:44.569: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.569: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.569: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.569: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.660: INFO: Exec stderr: ""
Jan 19 23:52:44.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.660: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.661: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.661: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.752: INFO: Exec stderr: ""
Jan 19 23:52:44.752: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.752: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.752: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.752: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.844: INFO: Exec stderr: ""
Jan 19 23:52:44.844: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1023 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:52:44.844: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:52:44.844: INFO: ExecWithOptions: Clientset creation
Jan 19 23:52:44.844: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1023/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:52:44.936: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:44.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1023" for this suite.

• [SLOW TEST:9.062 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":5082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:44.945: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 19 23:52:44.983: INFO: Waiting up to 5m0s for pod "pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b" in namespace "emptydir-6264" to be "Succeeded or Failed"
Jan 19 23:52:44.985: INFO: Pod "pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693021ms
Jan 19 23:52:46.988: INFO: Pod "pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00490249s
Jan 19 23:52:48.991: INFO: Pod "pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007894984s
STEP: Saw pod success
Jan 19 23:52:48.991: INFO: Pod "pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b" satisfied condition "Succeeded or Failed"
Jan 19 23:52:48.993: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b container test-container: <nil>
STEP: delete the pod
Jan 19 23:52:49.005: INFO: Waiting for pod pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b to disappear
Jan 19 23:52:49.013: INFO: Pod pod-e7ff713e-210c-4163-aa54-a75e40a6ce7b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:49.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6264" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":5181,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:49.020: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-b631d26b-bb3e-4629-8ba8-1685a9852820
STEP: Creating a pod to test consume configMaps
Jan 19 23:52:49.068: INFO: Waiting up to 5m0s for pod "pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130" in namespace "configmap-5463" to be "Succeeded or Failed"
Jan 19 23:52:49.070: INFO: Pod "pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130": Phase="Pending", Reason="", readiness=false. Elapsed: 1.997909ms
Jan 19 23:52:51.073: INFO: Pod "pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004945896s
Jan 19 23:52:53.076: INFO: Pod "pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008401514s
STEP: Saw pod success
Jan 19 23:52:53.076: INFO: Pod "pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130" satisfied condition "Succeeded or Failed"
Jan 19 23:52:53.078: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:52:53.090: INFO: Waiting for pod pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130 to disappear
Jan 19 23:52:53.098: INFO: Pod pod-configmaps-4fc5b756-b31b-49aa-a26b-92ec70e44130 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:53.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5463" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":5199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:53.105: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jan 19 23:52:53.139: INFO: Waiting up to 5m0s for pod "var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48" in namespace "var-expansion-4845" to be "Succeeded or Failed"
Jan 19 23:52:53.143: INFO: Pod "var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705169ms
Jan 19 23:52:55.146: INFO: Pod "var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007146335s
Jan 19 23:52:57.150: INFO: Pod "var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010471134s
STEP: Saw pod success
Jan 19 23:52:57.150: INFO: Pod "var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48" satisfied condition "Succeeded or Failed"
Jan 19 23:52:57.151: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48 container dapi-container: <nil>
STEP: delete the pod
Jan 19 23:52:57.171: INFO: Waiting for pod var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48 to disappear
Jan 19 23:52:57.173: INFO: Pod var-expansion-4e4b47e4-ccba-4ba8-8ba8-238e390e0c48 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:57.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4845" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":274,"skipped":5241,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:57.179: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:52:57.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-2563 version'
Jan 19 23:52:57.266: INFO: stderr: ""
Jan 19 23:52:57.266: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:41:01Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.1\", GitCommit:\"86ec240af8cbd1b60bcc4c03c20da9b98005b92e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T11:34:54Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:57.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2563" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":275,"skipped":5245,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:57.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:52:57.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-737" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":276,"skipped":5250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:52:57.308: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jan 19 23:52:57.334: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:53:25.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9084" for this suite.

• [SLOW TEST:28.223 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":277,"skipped":5294,"failed":0}
SS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:53:25.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-3870
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3870
STEP: Deleting pre-stop pod
Jan 19 23:53:38.606: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:53:38.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3870" for this suite.

• [SLOW TEST:13.099 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":278,"skipped":5296,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:53:38.631: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 19 23:53:38.676: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2303  47bef54b-9bd8-4029-a011-e50e57f5b438 92853 0 2022-01-19 23:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-19 23:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:53:38.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2303  47bef54b-9bd8-4029-a011-e50e57f5b438 92854 0 2022-01-19 23:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-19 23:53:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:53:38.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2303  47bef54b-9bd8-4029-a011-e50e57f5b438 92855 0 2022-01-19 23:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-19 23:53:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 19 23:53:48.704: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2303  47bef54b-9bd8-4029-a011-e50e57f5b438 92895 0 2022-01-19 23:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-19 23:53:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:53:48.704: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2303  47bef54b-9bd8-4029-a011-e50e57f5b438 92896 0 2022-01-19 23:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-19 23:53:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:53:48.705: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2303  47bef54b-9bd8-4029-a011-e50e57f5b438 92897 0 2022-01-19 23:53:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-01-19 23:53:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:53:48.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2303" for this suite.

• [SLOW TEST:10.088 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":279,"skipped":5297,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:53:48.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 19 23:53:48.782: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-788  063988c6-a5fc-43b1-a6fd-11160610c1ce 92904 0 2022-01-19 23:53:48 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-01-19 23:53:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kpbfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kpbfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 23:53:48.784: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:53:50.789: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:53:52.790: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 19 23:53:52.790: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-788 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:53:52.790: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:53:52.791: INFO: ExecWithOptions: Clientset creation
Jan 19 23:53:52.791: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-788/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Jan 19 23:53:52.922: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-788 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:53:52.922: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:53:52.922: INFO: ExecWithOptions: Clientset creation
Jan 19 23:53:52.922: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-788/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jan 19 23:53:53.025: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:53:53.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-788" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":280,"skipped":5307,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:53:53.047: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:53:53.082: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c" in namespace "downward-api-8902" to be "Succeeded or Failed"
Jan 19 23:53:53.084: INFO: Pod "downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.9364ms
Jan 19 23:53:55.087: INFO: Pod "downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005231709s
Jan 19 23:53:57.093: INFO: Pod "downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011262718s
STEP: Saw pod success
Jan 19 23:53:57.093: INFO: Pod "downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c" satisfied condition "Succeeded or Failed"
Jan 19 23:53:57.095: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c container client-container: <nil>
STEP: delete the pod
Jan 19 23:53:57.120: INFO: Waiting for pod downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c to disappear
Jan 19 23:53:57.128: INFO: Pod downwardapi-volume-aeb65ef9-0565-4fa8-a49a-fe0c3e715a6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:53:57.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8902" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5314,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:53:57.135: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jan 19 23:53:57.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 create -f -'
Jan 19 23:53:58.116: INFO: stderr: ""
Jan 19 23:53:58.116: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 23:53:58.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 23:53:58.188: INFO: stderr: ""
Jan 19 23:53:58.188: INFO: stdout: "update-demo-nautilus-cpmlv update-demo-nautilus-mlwjc "
Jan 19 23:53:58.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:53:58.254: INFO: stderr: ""
Jan 19 23:53:58.254: INFO: stdout: ""
Jan 19 23:53:58.254: INFO: update-demo-nautilus-cpmlv is created but not running
Jan 19 23:54:03.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 23:54:03.313: INFO: stderr: ""
Jan 19 23:54:03.313: INFO: stdout: "update-demo-nautilus-cpmlv update-demo-nautilus-mlwjc "
Jan 19 23:54:03.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:54:03.365: INFO: stderr: ""
Jan 19 23:54:03.365: INFO: stdout: "true"
Jan 19 23:54:03.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 23:54:03.416: INFO: stderr: ""
Jan 19 23:54:03.416: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 19 23:54:03.416: INFO: validating pod update-demo-nautilus-cpmlv
Jan 19 23:54:03.419: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 23:54:03.419: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 23:54:03.419: INFO: update-demo-nautilus-cpmlv is verified up and running
Jan 19 23:54:03.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-mlwjc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:54:03.469: INFO: stderr: ""
Jan 19 23:54:03.469: INFO: stdout: "true"
Jan 19 23:54:03.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-mlwjc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 23:54:03.531: INFO: stderr: ""
Jan 19 23:54:03.531: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 19 23:54:03.531: INFO: validating pod update-demo-nautilus-mlwjc
Jan 19 23:54:03.534: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 23:54:03.534: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 23:54:03.534: INFO: update-demo-nautilus-mlwjc is verified up and running
STEP: scaling down the replication controller
Jan 19 23:54:03.536: INFO: scanned /root for discovery docs: <nil>
Jan 19 23:54:03.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 19 23:54:04.603: INFO: stderr: ""
Jan 19 23:54:04.603: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 23:54:04.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 23:54:04.659: INFO: stderr: ""
Jan 19 23:54:04.659: INFO: stdout: "update-demo-nautilus-cpmlv "
Jan 19 23:54:04.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:54:04.719: INFO: stderr: ""
Jan 19 23:54:04.719: INFO: stdout: "true"
Jan 19 23:54:04.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 23:54:04.772: INFO: stderr: ""
Jan 19 23:54:04.772: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 19 23:54:04.772: INFO: validating pod update-demo-nautilus-cpmlv
Jan 19 23:54:04.774: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 23:54:04.774: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 23:54:04.774: INFO: update-demo-nautilus-cpmlv is verified up and running
STEP: scaling up the replication controller
Jan 19 23:54:04.776: INFO: scanned /root for discovery docs: <nil>
Jan 19 23:54:04.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 19 23:54:05.841: INFO: stderr: ""
Jan 19 23:54:05.842: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 23:54:05.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 23:54:05.897: INFO: stderr: ""
Jan 19 23:54:05.897: INFO: stdout: "update-demo-nautilus-cpmlv update-demo-nautilus-xrkk7 "
Jan 19 23:54:05.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:54:05.949: INFO: stderr: ""
Jan 19 23:54:05.950: INFO: stdout: "true"
Jan 19 23:54:05.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 23:54:06.001: INFO: stderr: ""
Jan 19 23:54:06.001: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 19 23:54:06.001: INFO: validating pod update-demo-nautilus-cpmlv
Jan 19 23:54:06.004: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 23:54:06.004: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 23:54:06.004: INFO: update-demo-nautilus-cpmlv is verified up and running
Jan 19 23:54:06.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-xrkk7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:54:06.060: INFO: stderr: ""
Jan 19 23:54:06.060: INFO: stdout: ""
Jan 19 23:54:06.060: INFO: update-demo-nautilus-xrkk7 is created but not running
Jan 19 23:54:11.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 23:54:11.119: INFO: stderr: ""
Jan 19 23:54:11.119: INFO: stdout: "update-demo-nautilus-cpmlv update-demo-nautilus-xrkk7 "
Jan 19 23:54:11.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:54:11.171: INFO: stderr: ""
Jan 19 23:54:11.171: INFO: stdout: "true"
Jan 19 23:54:11.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-cpmlv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 23:54:11.222: INFO: stderr: ""
Jan 19 23:54:11.222: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 19 23:54:11.222: INFO: validating pod update-demo-nautilus-cpmlv
Jan 19 23:54:11.225: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 23:54:11.225: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 23:54:11.225: INFO: update-demo-nautilus-cpmlv is verified up and running
Jan 19 23:54:11.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-xrkk7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 23:54:11.275: INFO: stderr: ""
Jan 19 23:54:11.275: INFO: stdout: "true"
Jan 19 23:54:11.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods update-demo-nautilus-xrkk7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 23:54:11.325: INFO: stderr: ""
Jan 19 23:54:11.325: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 19 23:54:11.325: INFO: validating pod update-demo-nautilus-xrkk7
Jan 19 23:54:11.328: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 23:54:11.328: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 23:54:11.328: INFO: update-demo-nautilus-xrkk7 is verified up and running
STEP: using delete to clean up resources
Jan 19 23:54:11.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 delete --grace-period=0 --force -f -'
Jan 19 23:54:11.380: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 23:54:11.380: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 19 23:54:11.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get rc,svc -l name=update-demo --no-headers'
Jan 19 23:54:11.447: INFO: stderr: "No resources found in kubectl-7663 namespace.\n"
Jan 19 23:54:11.447: INFO: stdout: ""
Jan 19 23:54:11.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7663 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 23:54:11.503: INFO: stderr: ""
Jan 19 23:54:11.503: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:11.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7663" for this suite.

• [SLOW TEST:14.377 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":282,"skipped":5320,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:11.512: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-d4bca41d-0ddf-42a6-a1f5-549bd6b00dfb
STEP: Creating a pod to test consume secrets
Jan 19 23:54:11.555: INFO: Waiting up to 5m0s for pod "pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512" in namespace "secrets-7846" to be "Succeeded or Failed"
Jan 19 23:54:11.557: INFO: Pod "pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738488ms
Jan 19 23:54:13.560: INFO: Pod "pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00503068s
Jan 19 23:54:15.564: INFO: Pod "pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009421277s
STEP: Saw pod success
Jan 19 23:54:15.564: INFO: Pod "pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512" satisfied condition "Succeeded or Failed"
Jan 19 23:54:15.566: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:54:15.587: INFO: Waiting for pod pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512 to disappear
Jan 19 23:54:15.589: INFO: Pod pod-secrets-36935889-92d5-4dcd-8887-2a8473afd512 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:15.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7846" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5323,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:15.596: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-efd7a9ab-ad05-46be-b2ee-f2d7cf22c79f
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:15.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1386" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":284,"skipped":5327,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:15.636: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:54:16.013: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 23:54:18.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 54, 16, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 54, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 54, 16, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 54, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:54:21.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:54:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-950-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:24.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3264" for this suite.
STEP: Destroying namespace "webhook-3264-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":285,"skipped":5336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:24.185: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:54:24.243: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 19 23:54:24.250: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:24.250: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jan 19 23:54:24.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:24.273: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:54:25.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:25.277: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:54:26.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:26.276: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:54:27.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 23:54:27.276: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 19 23:54:27.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 23:54:27.293: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 19 23:54:28.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:28.298: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 19 23:54:28.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:28.307: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:54:29.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:29.312: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:54:30.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:30.310: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:54:31.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:31.310: INFO: Node ip-10-0-1-103.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Jan 19 23:54:32.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 23:54:32.310: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7935, will wait for the garbage collector to delete the pods
Jan 19 23:54:32.368: INFO: Deleting DaemonSet.extensions daemon-set took: 3.256587ms
Jan 19 23:54:32.469: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.98372ms
Jan 19 23:54:34.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 23:54:34.775: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 23:54:34.777: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"93393"},"items":null}

Jan 19 23:54:34.778: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"93393"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:34.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7935" for this suite.

• [SLOW TEST:10.622 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":286,"skipped":5370,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:34.807: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 19 23:54:34.842: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 19 23:54:39.845: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:40.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1207" for this suite.

• [SLOW TEST:6.066 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":287,"skipped":5390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:40.874: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:54:41.753: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 19 23:54:43.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 54, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 54, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 54, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 54, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:54:46.778: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:54:46.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:49.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9457" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.097 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":288,"skipped":5476,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:49.972: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 19 23:54:50.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9" in namespace "downward-api-8314" to be "Succeeded or Failed"
Jan 19 23:54:50.024: INFO: Pod "downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50756ms
Jan 19 23:54:52.027: INFO: Pod "downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005478877s
Jan 19 23:54:54.032: INFO: Pod "downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010278563s
STEP: Saw pod success
Jan 19 23:54:54.032: INFO: Pod "downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9" satisfied condition "Succeeded or Failed"
Jan 19 23:54:54.034: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9 container client-container: <nil>
STEP: delete the pod
Jan 19 23:54:54.048: INFO: Waiting for pod downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9 to disappear
Jan 19 23:54:54.055: INFO: Pod downwardapi-volume-c03b2552-b382-4b06-bbd2-8f3bcc4b67a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:54.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8314" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5479,"failed":0}
SSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:54.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:54:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5293" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":290,"skipped":5484,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:54:54.130: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 19 23:54:54.165: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 19 23:54:54.171: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 19 23:54:54.171: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 19 23:54:54.179: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 19 23:54:54.179: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 19 23:54:54.189: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 19 23:54:54.189: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 19 23:55:01.224: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:55:01.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-563" for this suite.

• [SLOW TEST:7.112 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":291,"skipped":5500,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:55:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 19 23:55:01.279: INFO: Waiting up to 5m0s for pod "downward-api-05369919-608b-4f60-a854-45adf6279287" in namespace "downward-api-9978" to be "Succeeded or Failed"
Jan 19 23:55:01.296: INFO: Pod "downward-api-05369919-608b-4f60-a854-45adf6279287": Phase="Pending", Reason="", readiness=false. Elapsed: 16.704095ms
Jan 19 23:55:03.302: INFO: Pod "downward-api-05369919-608b-4f60-a854-45adf6279287": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022535093s
Jan 19 23:55:05.306: INFO: Pod "downward-api-05369919-608b-4f60-a854-45adf6279287": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027163196s
STEP: Saw pod success
Jan 19 23:55:05.307: INFO: Pod "downward-api-05369919-608b-4f60-a854-45adf6279287" satisfied condition "Succeeded or Failed"
Jan 19 23:55:05.308: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downward-api-05369919-608b-4f60-a854-45adf6279287 container dapi-container: <nil>
STEP: delete the pod
Jan 19 23:55:05.329: INFO: Waiting for pod downward-api-05369919-608b-4f60-a854-45adf6279287 to disappear
Jan 19 23:55:05.335: INFO: Pod downward-api-05369919-608b-4f60-a854-45adf6279287 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:55:05.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9978" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5511,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:55:05.342: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 19 23:55:05.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93734 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:55:05.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93734 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 19 23:55:05.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93735 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:55:05.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93735 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 19 23:55:05.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93736 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:55:05.398: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93736 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 19 23:55:05.400: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93737 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:55:05.401: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  c0c23f83-0fb8-4127-9f31-7557ada48515 93737 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 19 23:55:05.404: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  902c7ed2-7b69-45a8-ae75-8160d84ad652 93738 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:55:05.404: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  902c7ed2-7b69-45a8-ae75-8160d84ad652 93738 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 19 23:55:15.414: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  902c7ed2-7b69-45a8-ae75-8160d84ad652 93799 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 23:55:15.414: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  902c7ed2-7b69-45a8-ae75-8160d84ad652 93799 0 2022-01-19 23:55:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-01-19 23:55:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:55:25.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9373" for this suite.

• [SLOW TEST:20.080 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":293,"skipped":5538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:55:25.423: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jan 19 23:55:25.454: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:55:52.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6486" for this suite.

• [SLOW TEST:26.674 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":294,"skipped":5565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:55:52.097: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jan 19 23:55:52.143: INFO: namespace kubectl-7298
Jan 19 23:55:52.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7298 create -f -'
Jan 19 23:55:53.034: INFO: stderr: ""
Jan 19 23:55:53.034: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 19 23:55:54.037: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:55:54.037: INFO: Found 0 / 1
Jan 19 23:55:55.038: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:55:55.038: INFO: Found 0 / 1
Jan 19 23:55:56.039: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:55:56.039: INFO: Found 1 / 1
Jan 19 23:55:56.039: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 23:55:56.041: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 23:55:56.041: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 23:55:56.041: INFO: wait on agnhost-primary startup in kubectl-7298 
Jan 19 23:55:56.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7298 logs agnhost-primary-59z5w agnhost-primary'
Jan 19 23:55:56.101: INFO: stderr: ""
Jan 19 23:55:56.101: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 19 23:55:56.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7298 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 19 23:55:56.166: INFO: stderr: ""
Jan 19 23:55:56.166: INFO: stdout: "service/rm2 exposed\n"
Jan 19 23:55:56.172: INFO: Service rm2 in namespace kubectl-7298 found.
STEP: exposing service
Jan 19 23:55:58.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-7298 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 19 23:55:58.249: INFO: stderr: ""
Jan 19 23:55:58.249: INFO: stdout: "service/rm3 exposed\n"
Jan 19 23:55:58.256: INFO: Service rm3 in namespace kubectl-7298 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:00.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7298" for this suite.

• [SLOW TEST:8.172 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":295,"skipped":5598,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:00.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 19 23:56:00.304: INFO: Waiting up to 5m0s for pod "pod-5355acbe-dae9-449e-9438-9926b71daebb" in namespace "emptydir-2297" to be "Succeeded or Failed"
Jan 19 23:56:00.306: INFO: Pod "pod-5355acbe-dae9-449e-9438-9926b71daebb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.91727ms
Jan 19 23:56:02.310: INFO: Pod "pod-5355acbe-dae9-449e-9438-9926b71daebb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005413141s
Jan 19 23:56:04.314: INFO: Pod "pod-5355acbe-dae9-449e-9438-9926b71daebb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009958287s
STEP: Saw pod success
Jan 19 23:56:04.314: INFO: Pod "pod-5355acbe-dae9-449e-9438-9926b71daebb" satisfied condition "Succeeded or Failed"
Jan 19 23:56:04.316: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-5355acbe-dae9-449e-9438-9926b71daebb container test-container: <nil>
STEP: delete the pod
Jan 19 23:56:04.329: INFO: Waiting for pod pod-5355acbe-dae9-449e-9438-9926b71daebb to disappear
Jan 19 23:56:04.337: INFO: Pod pod-5355acbe-dae9-449e-9438-9926b71daebb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:04.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2297" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5618,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:04.343: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 19 23:56:04.381: INFO: Waiting up to 5m0s for pod "downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f" in namespace "downward-api-1061" to be "Succeeded or Failed"
Jan 19 23:56:04.391: INFO: Pod "downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.202042ms
Jan 19 23:56:06.394: INFO: Pod "downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013361656s
Jan 19 23:56:08.398: INFO: Pod "downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01664799s
STEP: Saw pod success
Jan 19 23:56:08.398: INFO: Pod "downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f" satisfied condition "Succeeded or Failed"
Jan 19 23:56:08.400: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f container dapi-container: <nil>
STEP: delete the pod
Jan 19 23:56:08.415: INFO: Waiting for pod downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f to disappear
Jan 19 23:56:08.423: INFO: Pod downward-api-c2a0f437-ec16-4415-b974-b706570fcf0f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:08.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1061" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":297,"skipped":5618,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:08.429: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:12.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9587" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":298,"skipped":5622,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:12.477: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-7690ab6a-7f43-4a4c-9878-7c990494f922
STEP: Creating a pod to test consume secrets
Jan 19 23:56:12.514: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a" in namespace "projected-5499" to be "Succeeded or Failed"
Jan 19 23:56:12.516: INFO: Pod "pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.881415ms
Jan 19 23:56:14.518: INFO: Pod "pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004787666s
Jan 19 23:56:16.522: INFO: Pod "pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008368068s
STEP: Saw pod success
Jan 19 23:56:16.522: INFO: Pod "pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a" satisfied condition "Succeeded or Failed"
Jan 19 23:56:16.524: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 23:56:16.535: INFO: Waiting for pod pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a to disappear
Jan 19 23:56:16.543: INFO: Pod pod-projected-secrets-0e49df0c-d227-4085-b555-12b693178b9a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:16.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5499" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:16.550: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jan 19 23:56:16.586: INFO: Waiting up to 5m0s for pod "client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472" in namespace "containers-7140" to be "Succeeded or Failed"
Jan 19 23:56:16.588: INFO: Pod "client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472": Phase="Pending", Reason="", readiness=false. Elapsed: 1.862751ms
Jan 19 23:56:18.590: INFO: Pod "client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004515267s
Jan 19 23:56:20.594: INFO: Pod "client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0083138s
STEP: Saw pod success
Jan 19 23:56:20.594: INFO: Pod "client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472" satisfied condition "Succeeded or Failed"
Jan 19 23:56:20.596: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472 container agnhost-container: <nil>
STEP: delete the pod
Jan 19 23:56:20.608: INFO: Waiting for pod client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472 to disappear
Jan 19 23:56:20.610: INFO: Pod client-containers-bcabe17a-808f-45be-a6dd-bc224e7eb472 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:20.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7140" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:20.616: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:56:21.347: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 23:56:23.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 56, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 56, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 56, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 56, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:56:26.368: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:26.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8692" for this suite.
STEP: Destroying namespace "webhook-8692-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.801 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":301,"skipped":5748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:26.418: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:56:26.466: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 19 23:56:31.474: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 19 23:56:31.474: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 19 23:56:31.488: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6942  97f44811-81da-42d4-963b-54c3a5de60a0 94321 1 2022-01-19 23:56:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-01-19 23:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007be3c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 19 23:56:31.490: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:31.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6942" for this suite.

• [SLOW TEST:5.097 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":302,"skipped":5779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:31.516: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:31.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-824" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":303,"skipped":5808,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:31.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jan 19 23:56:31.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-394 cluster-info'
Jan 19 23:56:31.669: INFO: stderr: ""
Jan 19 23:56:31.669: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:31.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-394" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":304,"skipped":5824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:31.678: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jan 19 23:56:31.741: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jan 19 23:56:33.755: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jan 19 23:56:35.767: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:56:37.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-4679" for this suite.

• [SLOW TEST:6.098 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":305,"skipped":5852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:56:37.777: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 19 23:56:41.827: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3192 PodName:var-expansion-9c0e53a9-fec1-4817-b4bc-0d7fc5f1027f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:56:41.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:56:41.827: INFO: ExecWithOptions: Clientset creation
Jan 19 23:56:41.827: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3192/pods/var-expansion-9c0e53a9-fec1-4817-b4bc-0d7fc5f1027f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Jan 19 23:56:41.919: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3192 PodName:var-expansion-9c0e53a9-fec1-4817-b4bc-0d7fc5f1027f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 23:56:41.919: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
Jan 19 23:56:41.919: INFO: ExecWithOptions: Clientset creation
Jan 19 23:56:41.920: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3192/pods/var-expansion-9c0e53a9-fec1-4817-b4bc-0d7fc5f1027f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Jan 19 23:56:42.539: INFO: Successfully updated pod "var-expansion-9c0e53a9-fec1-4817-b4bc-0d7fc5f1027f"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 19 23:56:42.542: INFO: Deleting pod "var-expansion-9c0e53a9-fec1-4817-b4bc-0d7fc5f1027f" in namespace "var-expansion-3192"
Jan 19 23:56:42.547: INFO: Wait up to 5m0s for pod "var-expansion-9c0e53a9-fec1-4817-b4bc-0d7fc5f1027f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:57:14.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3192" for this suite.

• [SLOW TEST:36.788 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":306,"skipped":5876,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:57:14.566: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 23:57:15.364: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 23:57:17.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 19, 23, 57, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 57, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 19, 23, 57, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 19, 23, 57, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 23:57:20.387: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:57:20.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4975" for this suite.
STEP: Destroying namespace "webhook-4975-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.917 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":307,"skipped":5894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:57:20.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jan 19 23:57:20.533: INFO: The status of Pod annotationupdate7b866b8d-53dd-40b5-bfc5-9681fd2f3969 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:57:22.538: INFO: The status of Pod annotationupdate7b866b8d-53dd-40b5-bfc5-9681fd2f3969 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:57:24.538: INFO: The status of Pod annotationupdate7b866b8d-53dd-40b5-bfc5-9681fd2f3969 is Running (Ready = true)
Jan 19 23:57:25.057: INFO: Successfully updated pod "annotationupdate7b866b8d-53dd-40b5-bfc5-9681fd2f3969"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:57:27.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-185" for this suite.

• [SLOW TEST:6.594 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":5926,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:57:27.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:57:40.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2969" for this suite.

• [SLOW TEST:13.116 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":309,"skipped":5928,"failed":0}
S
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:57:40.194: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 19 23:57:40.220: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 19 23:57:40.229: INFO: The status of Pod pod-logs-websocket-b717e3f9-8a1c-4d37-a755-1e95322fcec6 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:57:42.233: INFO: The status of Pod pod-logs-websocket-b717e3f9-8a1c-4d37-a755-1e95322fcec6 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 23:57:44.234: INFO: The status of Pod pod-logs-websocket-b717e3f9-8a1c-4d37-a755-1e95322fcec6 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:57:44.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2643" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:57:44.253: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 23:57:47.301: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 19 23:57:47.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5675" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5953,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 19 23:57:47.323: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-6e2ed996-9fd1-4e0c-b9ff-2d5a138ca767 in namespace container-probe-1240
Jan 19 23:57:51.365: INFO: Started pod busybox-6e2ed996-9fd1-4e0c-b9ff-2d5a138ca767 in namespace container-probe-1240
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 23:57:51.367: INFO: Initial restart count of pod busybox-6e2ed996-9fd1-4e0c-b9ff-2d5a138ca767 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:01:51.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1240" for this suite.

• [SLOW TEST:244.627 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":5957,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:01:51.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:02:51.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5100" for this suite.

• [SLOW TEST:60.053 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":5973,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:02:52.003: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 20 00:02:52.036: INFO: Creating simple deployment test-new-deployment
Jan 20 00:02:52.044: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Jan 20 00:02:54.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.January, 20, 0, 2, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 20, 0, 2, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.January, 20, 0, 2, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.January, 20, 0, 2, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-5d9fdcc779\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jan 20 00:02:56.085: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3936  a59d9cfa-1431-4f0b-a216-7c6ac761345a 95557 3 2022-01-20 00:02:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-01-20 00:02:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-20 00:02:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b600e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-01-20 00:02:54 +0000 UTC,LastTransitionTime:2022-01-20 00:02:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-01-20 00:02:54 +0000 UTC,LastTransitionTime:2022-01-20 00:02:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 20 00:02:56.096: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-3936  04cd2796-d367-460f-befe-c6f9347a9013 95562 2 2022-01-20 00:02:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a59d9cfa-1431-4f0b-a216-7c6ac761345a 0xc002b60547 0xc002b60548}] []  [{kube-controller-manager Update apps/v1 2022-01-20 00:02:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a59d9cfa-1431-4f0b-a216-7c6ac761345a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-01-20 00:02:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b605d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 20 00:02:56.098: INFO: Pod "test-new-deployment-5d9fdcc779-dkk8k" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-dkk8k test-new-deployment-5d9fdcc779- deployment-3936  d2867c03-86f0-41c6-8517-51d4a4f19baa 95548 0 2022-01-20 00:02:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:cc220a7ff99d4aba30f56492316ae77fb5231f04efdc5d2f49a075910729403d cni.projectcalico.org/podIP:172.16.146.51/32 cni.projectcalico.org/podIPs:172.16.146.51/32] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 04cd2796-d367-460f-befe-c6f9347a9013 0xc002e9f457 0xc002e9f458}] []  [{kube-controller-manager Update v1 2022-01-20 00:02:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04cd2796-d367-460f-befe-c6f9347a9013\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-01-20 00:02:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-01-20 00:02:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.146.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fjxr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fjxr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-153.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-20 00:02:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-20 00:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-20 00:02:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-20 00:02:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.153,PodIP:172.16.146.51,StartTime:2022-01-20 00:02:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-01-20 00:02:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://ed948121b60f7c20d20de1230be16d9d7ed96a6e1c88e98cd431c96cc27e8df4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.146.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 20 00:02:56.099: INFO: Pod "test-new-deployment-5d9fdcc779-wk8pb" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-wk8pb test-new-deployment-5d9fdcc779- deployment-3936  15b5ed56-11a1-4644-8931-88d472d5de1f 95561 0 2022-01-20 00:02:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 04cd2796-d367-460f-befe-c6f9347a9013 0xc002e9f930 0xc002e9f931}] []  [{kube-controller-manager Update v1 2022-01-20 00:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04cd2796-d367-460f-befe-c6f9347a9013\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7rc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7rc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-48.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-01-20 00:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:02:56.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3936" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":314,"skipped":5979,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:02:56.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 20 00:02:56.152: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 20 00:02:56.162: INFO: Waiting for terminating namespaces to be deleted...
Jan 20 00:02:56.164: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-103.eu-central-1.compute.internal before test
Jan 20 00:02:56.174: INFO: cert-manager-774679ff99-fg5j9 from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container cert-manager ready: true, restart count 0
Jan 20 00:02:56.174: INFO: gatekeeper-audit-59fcc9f644-7fmkr from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container manager ready: true, restart count 0
Jan 20 00:02:56.174: INFO: gatekeeper-controller-manager-86677cdd6d-59r5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container manager ready: true, restart count 0
Jan 20 00:02:56.174: INFO: calico-node-rpdnw from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container calico-node ready: true, restart count 0
Jan 20 00:02:56.174: INFO: kube-proxy-l9kpk from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 20 00:02:56.174: INFO: minio-0 from kube-system started at 2022-01-19 22:19:21 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container minio ready: true, restart count 0
Jan 20 00:02:56.174: INFO: velero-restic-dr6p6 from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container restic ready: true, restart count 0
Jan 20 00:02:56.174: INFO: elasticsearch-0 from logging started at 2022-01-19 22:19:23 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container elasticsearch ready: true, restart count 0
Jan 20 00:02:56.174: INFO: 	Container exporter ready: true, restart count 0
Jan 20 00:02:56.174: INFO: fluentbit-zwj8m from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 20 00:02:56.174: INFO: alertmanager-main-2 from monitoring started at 2022-01-19 22:40:34 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container alertmanager ready: true, restart count 0
Jan 20 00:02:56.174: INFO: 	Container config-reloader ready: true, restart count 0
Jan 20 00:02:56.174: INFO: goldpinger-96ctd from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container goldpinger ready: true, restart count 0
Jan 20 00:02:56.174: INFO: kube-proxy-metrics-wbqwc from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 20 00:02:56.174: INFO: node-exporter-tqbmk from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 20 00:02:56.174: INFO: 	Container node-exporter ready: true, restart count 0
Jan 20 00:02:56.174: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csw5s from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.174: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 20 00:02:56.174: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 20 00:02:56.174: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-153.eu-central-1.compute.internal before test
Jan 20 00:02:56.185: INFO: test-webserver-5b0e067c-3742-4daf-98d8-5f10c5a2e7b3 from container-probe-5100 started at 2022-01-20 00:01:51 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container test-webserver ready: false, restart count 0
Jan 20 00:02:56.185: INFO: test-new-deployment-5d9fdcc779-dkk8k from deployment-3936 started at 2022-01-20 00:02:52 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container httpd ready: true, restart count 0
Jan 20 00:02:56.185: INFO: calico-node-c7xgd from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container calico-node ready: true, restart count 0
Jan 20 00:02:56.185: INFO: kube-proxy-9p6s2 from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 20 00:02:56.185: INFO: velero-restic-xtp74 from kube-system started at 2022-01-19 22:58:22 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container restic ready: true, restart count 0
Jan 20 00:02:56.185: INFO: fluentbit-nz7dc from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 20 00:02:56.185: INFO: fluentd-1 from logging started at 2022-01-19 22:58:24 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container fluentd ready: true, restart count 0
Jan 20 00:02:56.185: INFO: goldpinger-9n9xc from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container goldpinger ready: true, restart count 0
Jan 20 00:02:56.185: INFO: kube-proxy-metrics-nx74g from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 20 00:02:56.185: INFO: node-exporter-ptzzd from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 20 00:02:56.185: INFO: 	Container node-exporter ready: true, restart count 0
Jan 20 00:02:56.185: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-nbt7p from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.185: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 20 00:02:56.185: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 20 00:02:56.185: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-48.eu-central-1.compute.internal before test
Jan 20 00:02:56.195: INFO: cert-manager-cainjector-66fcc559bf-wgm9t from cert-manager started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container cainjector ready: true, restart count 0
Jan 20 00:02:56.195: INFO: test-new-deployment-5d9fdcc779-wk8pb from deployment-3936 started at 2022-01-20 00:02:56 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container httpd ready: false, restart count 0
Jan 20 00:02:56.195: INFO: gatekeeper-controller-manager-86677cdd6d-fps5h from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container manager ready: true, restart count 0
Jan 20 00:02:56.195: INFO: gatekeeper-policy-manager-7644bb5475-cwmpf from gatekeeper-system started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container gatekeeper-policy-manager ready: true, restart count 0
Jan 20 00:02:56.195: INFO: nginx-ingress-controller-6bcf978d9-w2wlr from ingress-nginx started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 20 00:02:56.195: INFO: calico-node-c8np5 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container calico-node ready: true, restart count 0
Jan 20 00:02:56.195: INFO: kube-proxy-prc9k from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 20 00:02:56.195: INFO: velero-restic-nkgcd from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container restic ready: true, restart count 0
Jan 20 00:02:56.195: INFO: cerebro-6f88ff7888-plncf from logging started at 2022-01-19 22:19:11 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container cerebro ready: true, restart count 0
Jan 20 00:02:56.195: INFO: fluentbit-wt6cx from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 20 00:02:56.195: INFO: fluentd-0 from logging started at 2022-01-19 22:19:52 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container fluentd ready: true, restart count 1
Jan 20 00:02:56.195: INFO: alertmanager-main-0 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container alertmanager ready: true, restart count 0
Jan 20 00:02:56.195: INFO: 	Container config-reloader ready: true, restart count 0
Jan 20 00:02:56.195: INFO: goldpinger-g9xm2 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container goldpinger ready: true, restart count 0
Jan 20 00:02:56.195: INFO: kube-proxy-metrics-z42ff from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 20 00:02:56.195: INFO: node-exporter-w54w5 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 20 00:02:56.195: INFO: 	Container node-exporter ready: true, restart count 0
Jan 20 00:02:56.195: INFO: prometheus-k8s-0 from monitoring started at 2022-01-19 22:21:16 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container config-reloader ready: true, restart count 0
Jan 20 00:02:56.195: INFO: 	Container prometheus ready: true, restart count 0
Jan 20 00:02:56.195: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-qdwj2 from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 20 00:02:56.195: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 20 00:02:56.195: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-62.eu-central-1.compute.internal before test
Jan 20 00:02:56.212: INFO: cert-manager-webhook-77ffc44fc6-8fxlk from cert-manager started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container webhook ready: true, restart count 0
Jan 20 00:02:56.213: INFO: gatekeeper-controller-manager-86677cdd6d-mnx56 from gatekeeper-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container manager ready: true, restart count 0
Jan 20 00:02:56.213: INFO: calico-kube-controllers-59c967558b-nwcpg from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 20 00:02:56.213: INFO: calico-node-jbfb7 from kube-system started at 2022-01-19 18:05:43 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container calico-node ready: true, restart count 0
Jan 20 00:02:56.213: INFO: coredns-64897985d-cftqm from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container coredns ready: true, restart count 0
Jan 20 00:02:56.213: INFO: coredns-64897985d-l9c4t from kube-system started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container coredns ready: true, restart count 0
Jan 20 00:02:56.213: INFO: kube-proxy-7shjp from kube-system started at 2022-01-19 18:05:18 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 20 00:02:56.213: INFO: metrics-server-6f5f78d676-mnlwp from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container metrics-server ready: true, restart count 0
Jan 20 00:02:56.213: INFO: velero-6f84f7465-tgzks from kube-system started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container velero ready: true, restart count 0
Jan 20 00:02:56.213: INFO: velero-restic-6xdjj from kube-system started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container restic ready: true, restart count 0
Jan 20 00:02:56.213: INFO: local-path-provisioner-5b94755fb4-8zcbz from local-path-storage started at 2022-01-19 18:05:57 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container local-path-provisioner ready: true, restart count 0
Jan 20 00:02:56.213: INFO: fluentbit-sj8hh from logging started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 20 00:02:56.213: INFO: fluentd-2 from logging started at 2022-01-19 22:20:56 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container fluentd ready: true, restart count 0
Jan 20 00:02:56.213: INFO: kibana-8565fbd49b-pm9nr from logging started at 2022-01-19 22:40:29 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container kibana ready: true, restart count 0
Jan 20 00:02:56.213: INFO: 	Container kibana-index-patterns ready: true, restart count 0
Jan 20 00:02:56.213: INFO: alertmanager-main-1 from monitoring started at 2022-01-19 22:21:10 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container alertmanager ready: true, restart count 0
Jan 20 00:02:56.213: INFO: 	Container config-reloader ready: true, restart count 0
Jan 20 00:02:56.213: INFO: goldpinger-ml8vn from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container goldpinger ready: true, restart count 0
Jan 20 00:02:56.213: INFO: grafana-7c96db944d-cq4n6 from monitoring started at 2022-01-19 22:19:12 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container dashboard-sidecar ready: true, restart count 0
Jan 20 00:02:56.213: INFO: 	Container grafana ready: true, restart count 0
Jan 20 00:02:56.213: INFO: kube-proxy-metrics-lvbg9 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container kube-proxy-metrics ready: true, restart count 0
Jan 20 00:02:56.213: INFO: kube-state-metrics-7456544d4b-wjgk8 from monitoring started at 2022-01-19 22:40:29 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 20 00:02:56.213: INFO: node-exporter-x46g7 from monitoring started at 2022-01-19 22:19:13 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 20 00:02:56.213: INFO: 	Container node-exporter ready: true, restart count 0
Jan 20 00:02:56.213: INFO: prometheus-operator-7db7f75fdb-624bl from monitoring started at 2022-01-19 22:19:12 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 20 00:02:56.213: INFO: sonobuoy from sonobuoy started at 2022-01-19 22:27:04 +0000 UTC (1 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 20 00:02:56.213: INFO: sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csbhg from sonobuoy started at 2022-01-19 22:27:06 +0000 UTC (2 container statuses recorded)
Jan 20 00:02:56.213: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 20 00:02:56.213: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node ip-10-0-1-103.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-10-0-1-153.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-10-0-1-48.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod cert-manager-774679ff99-fg5j9 requesting resource cpu=50m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod cert-manager-cainjector-66fcc559bf-wgm9t requesting resource cpu=50m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod cert-manager-webhook-77ffc44fc6-8fxlk requesting resource cpu=50m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod test-webserver-5b0e067c-3742-4daf-98d8-5f10c5a2e7b3 requesting resource cpu=0m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod test-new-deployment-5d9fdcc779-dkk8k requesting resource cpu=0m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod test-new-deployment-5d9fdcc779-wk8pb requesting resource cpu=0m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod gatekeeper-audit-59fcc9f644-7fmkr requesting resource cpu=100m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod gatekeeper-controller-manager-86677cdd6d-59r5h requesting resource cpu=100m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod gatekeeper-controller-manager-86677cdd6d-fps5h requesting resource cpu=100m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod gatekeeper-controller-manager-86677cdd6d-mnx56 requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod gatekeeper-policy-manager-7644bb5475-cwmpf requesting resource cpu=100m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod nginx-ingress-controller-6bcf978d9-w2wlr requesting resource cpu=100m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod calico-kube-controllers-59c967558b-nwcpg requesting resource cpu=0m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod calico-node-c7xgd requesting resource cpu=250m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod calico-node-c8np5 requesting resource cpu=250m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod calico-node-jbfb7 requesting resource cpu=250m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod calico-node-rpdnw requesting resource cpu=250m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod coredns-64897985d-cftqm requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod coredns-64897985d-l9c4t requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-7shjp requesting resource cpu=0m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-9p6s2 requesting resource cpu=0m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-l9kpk requesting resource cpu=0m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-prc9k requesting resource cpu=0m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod metrics-server-6f5f78d676-mnlwp requesting resource cpu=50m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod minio-0 requesting resource cpu=0m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod velero-6f84f7465-tgzks requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod velero-restic-6xdjj requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod velero-restic-dr6p6 requesting resource cpu=100m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod velero-restic-nkgcd requesting resource cpu=100m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod velero-restic-xtp74 requesting resource cpu=100m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod local-path-provisioner-5b94755fb4-8zcbz requesting resource cpu=0m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod cerebro-6f88ff7888-plncf requesting resource cpu=100m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod elasticsearch-0 requesting resource cpu=1600m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod fluentbit-nz7dc requesting resource cpu=100m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod fluentbit-sj8hh requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod fluentbit-wt6cx requesting resource cpu=100m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod fluentbit-zwj8m requesting resource cpu=100m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod fluentd-0 requesting resource cpu=300m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod fluentd-1 requesting resource cpu=300m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod fluentd-2 requesting resource cpu=300m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kibana-8565fbd49b-pm9nr requesting resource cpu=110m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod alertmanager-main-0 requesting resource cpu=250m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod alertmanager-main-1 requesting resource cpu=250m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod alertmanager-main-2 requesting resource cpu=250m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod goldpinger-96ctd requesting resource cpu=1m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod goldpinger-9n9xc requesting resource cpu=1m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod goldpinger-g9xm2 requesting resource cpu=1m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod goldpinger-ml8vn requesting resource cpu=1m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod grafana-7c96db944d-cq4n6 requesting resource cpu=150m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-metrics-lvbg9 requesting resource cpu=10m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-metrics-nx74g requesting resource cpu=10m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-metrics-wbqwc requesting resource cpu=10m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-proxy-metrics-z42ff requesting resource cpu=10m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod kube-state-metrics-7456544d4b-wjgk8 requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod node-exporter-ptzzd requesting resource cpu=112m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod node-exporter-tqbmk requesting resource cpu=112m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod node-exporter-w54w5 requesting resource cpu=112m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod node-exporter-x46g7 requesting resource cpu=112m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod prometheus-k8s-0 requesting resource cpu=600m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod prometheus-operator-7db7f75fdb-624bl requesting resource cpu=100m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csbhg requesting resource cpu=0m on Node ip-10-0-1-62.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-csw5s requesting resource cpu=0m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-nbt7p requesting resource cpu=0m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.363: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b6223d5918a4cde-qdwj2 requesting resource cpu=0m on Node ip-10-0-1-48.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Jan 20 00:02:56.363: INFO: Creating a pod which consumes cpu=928m on Node ip-10-0-1-103.eu-central-1.compute.internal
Jan 20 00:02:56.371: INFO: Creating a pod which consumes cpu=2188m on Node ip-10-0-1-153.eu-central-1.compute.internal
Jan 20 00:02:56.383: INFO: Creating a pod which consumes cpu=1278m on Node ip-10-0-1-48.eu-central-1.compute.internal
Jan 20 00:02:56.395: INFO: Creating a pod which consumes cpu=1341m on Node ip-10-0-1-62.eu-central-1.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03afe422-2076-4624-b030-774b4af5a276.16cbd1a081d5f0d4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1225/filler-pod-03afe422-2076-4624-b030-774b4af5a276 to ip-10-0-1-62.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03afe422-2076-4624-b030-774b4af5a276.16cbd1a0d3d74dd5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03afe422-2076-4624-b030-774b4af5a276.16cbd1a0db851dc8], Reason = [Created], Message = [Created container filler-pod-03afe422-2076-4624-b030-774b4af5a276]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03afe422-2076-4624-b030-774b4af5a276.16cbd1a0e72ceecc], Reason = [Started], Message = [Started container filler-pod-03afe422-2076-4624-b030-774b4af5a276]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2992969a-89ab-484b-afb1-6a7070658616.16cbd1a081286d9a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1225/filler-pod-2992969a-89ab-484b-afb1-6a7070658616 to ip-10-0-1-48.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2992969a-89ab-484b-afb1-6a7070658616.16cbd1a0cf3ec8d7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2992969a-89ab-484b-afb1-6a7070658616.16cbd1a0d634b7b2], Reason = [Created], Message = [Created container filler-pod-2992969a-89ab-484b-afb1-6a7070658616]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2992969a-89ab-484b-afb1-6a7070658616.16cbd1a0e17f1888], Reason = [Started], Message = [Started container filler-pod-2992969a-89ab-484b-afb1-6a7070658616]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcbd6964-65b0-4cc5-ba15-333175dfae34.16cbd1a07fcdb6b3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1225/filler-pod-bcbd6964-65b0-4cc5-ba15-333175dfae34 to ip-10-0-1-103.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcbd6964-65b0-4cc5-ba15-333175dfae34.16cbd1a0d06a66bb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcbd6964-65b0-4cc5-ba15-333175dfae34.16cbd1a0d7be86ce], Reason = [Created], Message = [Created container filler-pod-bcbd6964-65b0-4cc5-ba15-333175dfae34]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bcbd6964-65b0-4cc5-ba15-333175dfae34.16cbd1a0e4499c74], Reason = [Started], Message = [Started container filler-pod-bcbd6964-65b0-4cc5-ba15-333175dfae34]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cde4b331-31f5-4fee-a31f-e0fae035afa0.16cbd1a08079ae5b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1225/filler-pod-cde4b331-31f5-4fee-a31f-e0fae035afa0 to ip-10-0-1-153.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cde4b331-31f5-4fee-a31f-e0fae035afa0.16cbd1a0d239936b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cde4b331-31f5-4fee-a31f-e0fae035afa0.16cbd1a0d8b21bf6], Reason = [Created], Message = [Created container filler-pod-cde4b331-31f5-4fee-a31f-e0fae035afa0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cde4b331-31f5-4fee-a31f-e0fae035afa0.16cbd1a0e3211a58], Reason = [Started], Message = [Started container filler-pod-cde4b331-31f5-4fee-a31f-e0fae035afa0]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16cbd1a1715428c6], Reason = [FailedScheduling], Message = [0/5 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-1-153.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-1-48.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-1-62.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-1-103.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:03:01.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1225" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.399 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":315,"skipped":5991,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:03:01.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 20 00:03:01.548: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cf3778f8-fd59-475b-8238-0c32a7aecf78" in namespace "security-context-test-2848" to be "Succeeded or Failed"
Jan 20 00:03:01.550: INFO: Pod "alpine-nnp-false-cf3778f8-fd59-475b-8238-0c32a7aecf78": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890707ms
Jan 20 00:03:03.555: INFO: Pod "alpine-nnp-false-cf3778f8-fd59-475b-8238-0c32a7aecf78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006270995s
Jan 20 00:03:05.560: INFO: Pod "alpine-nnp-false-cf3778f8-fd59-475b-8238-0c32a7aecf78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011207963s
Jan 20 00:03:05.560: INFO: Pod "alpine-nnp-false-cf3778f8-fd59-475b-8238-0c32a7aecf78" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:03:05.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2848" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":6008,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:03:05.582: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 20 00:03:05.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64" in namespace "downward-api-519" to be "Succeeded or Failed"
Jan 20 00:03:05.620: INFO: Pod "downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309502ms
Jan 20 00:03:07.623: INFO: Pod "downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005173744s
Jan 20 00:03:09.629: INFO: Pod "downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010558792s
STEP: Saw pod success
Jan 20 00:03:09.629: INFO: Pod "downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64" satisfied condition "Succeeded or Failed"
Jan 20 00:03:09.631: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64 container client-container: <nil>
STEP: delete the pod
Jan 20 00:03:09.654: INFO: Waiting for pod downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64 to disappear
Jan 20 00:03:09.656: INFO: Pod downwardapi-volume-670a5e5a-7a03-482e-8f4a-f0f8ab6d7d64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:03:09.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-519" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":6008,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:03:09.663: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 20 00:03:09.705: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:03:11.709: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:03:13.709: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 20 00:03:13.725: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:03:15.728: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:03:17.728: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 20 00:03:17.752: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 20 00:03:17.754: INFO: Pod pod-with-poststart-http-hook still exists
Jan 20 00:03:19.754: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 20 00:03:19.760: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:03:19.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3916" for this suite.

• [SLOW TEST:10.103 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":6037,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:03:19.766: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 20 00:03:22.828: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:03:22.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4320" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":6038,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:03:22.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-07ac9e2b-baba-48bc-87ba-e6872ee96e05
STEP: Creating a pod to test consume configMaps
Jan 20 00:03:22.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47" in namespace "projected-133" to be "Succeeded or Failed"
Jan 20 00:03:22.896: INFO: Pod "pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.894167ms
Jan 20 00:03:24.900: INFO: Pod "pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007732281s
Jan 20 00:03:26.908: INFO: Pod "pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015657244s
STEP: Saw pod success
Jan 20 00:03:26.908: INFO: Pod "pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47" satisfied condition "Succeeded or Failed"
Jan 20 00:03:26.909: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 20 00:03:26.930: INFO: Waiting for pod pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47 to disappear
Jan 20 00:03:26.932: INFO: Pod pod-projected-configmaps-2d1091e9-3d98-4e45-8ef6-d1ae05f20b47 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:03:26.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-133" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":6058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:03:26.939: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1571
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 20 00:03:26.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1851 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 20 00:03:27.937: INFO: stderr: ""
Jan 20 00:03:27.937: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 20 00:03:32.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1851 get pod e2e-test-httpd-pod -o json'
Jan 20 00:03:33.045: INFO: stderr: ""
Jan 20 00:03:33.045: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"447a6635acddc84ab93aee807f03eec6b1074de851f9aef50cebf078aa9b5289\",\n            \"cni.projectcalico.org/podIP\": \"172.16.146.16/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.146.16/32\"\n        },\n        \"creationTimestamp\": \"2022-01-20T00:03:27Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1851\",\n        \"resourceVersion\": \"96024\",\n        \"uid\": \"af023d44-371f-4a31-a253-1f5f1d3306d6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lhwz6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-1-153.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lhwz6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-20T00:03:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-20T00:03:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-20T00:03:30Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-01-20T00:03:27Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://30cfc4df043c5e1f12773dc24286f7c6f9fbdfda520f26da6efa141e1cb5ed7f\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-01-20T00:03:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.1.153\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.146.16\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.146.16\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-01-20T00:03:27Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 20 00:03:33.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1851 replace -f -'
Jan 20 00:03:33.241: INFO: stderr: ""
Jan 20 00:03:33.241: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1575
Jan 20 00:03:33.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-1851 delete pods e2e-test-httpd-pod'
Jan 20 00:03:35.550: INFO: stderr: ""
Jan 20 00:03:35.550: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:03:35.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1851" for this suite.

• [SLOW TEST:8.622 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":321,"skipped":6096,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:03:35.561: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8081.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8081.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 20 00:03:39.625: INFO: DNS probes using dns-test-23b393c2-7162-40cc-83d9-2a00f27e39fd succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8081.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8081.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 20 00:03:43.669: INFO: File wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:43.671: INFO: File jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:43.671: INFO: Lookups using dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 failed for: [wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local]

Jan 20 00:03:48.676: INFO: File wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:48.679: INFO: File jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:48.679: INFO: Lookups using dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 failed for: [wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local]

Jan 20 00:03:53.677: INFO: File wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:53.679: INFO: File jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:53.679: INFO: Lookups using dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 failed for: [wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local]

Jan 20 00:03:58.675: INFO: File wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:58.678: INFO: File jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:03:58.678: INFO: Lookups using dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 failed for: [wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local]

Jan 20 00:04:03.674: INFO: File wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:04:03.676: INFO: File jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:04:03.676: INFO: Lookups using dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 failed for: [wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local]

Jan 20 00:04:08.677: INFO: File wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local from pod  dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 20 00:04:08.679: INFO: Lookups using dns-8081/dns-test-82759928-608e-4f22-b132-f97dc0985c88 failed for: [wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local]

Jan 20 00:04:13.678: INFO: DNS probes using dns-test-82759928-608e-4f22-b132-f97dc0985c88 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8081.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8081.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8081.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8081.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 20 00:04:17.769: INFO: DNS probes using dns-test-c49680eb-99e0-4c44-b6dc-31bc86d07ac3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:04:17.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8081" for this suite.

• [SLOW TEST:42.236 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":322,"skipped":6117,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:04:17.798: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 20 00:04:17.837: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 20 00:04:19.863: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:04:20.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4571" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":323,"skipped":6118,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:04:20.875: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 20 00:04:23.930: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:04:23.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4424" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":324,"skipped":6127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:04:23.950: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 20 00:04:24.258: INFO: Pod name wrapped-volume-race-56de5ecb-5217-49ad-9663-12a451d7cdea: Found 1 pods out of 5
Jan 20 00:04:29.265: INFO: Pod name wrapped-volume-race-56de5ecb-5217-49ad-9663-12a451d7cdea: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-56de5ecb-5217-49ad-9663-12a451d7cdea in namespace emptydir-wrapper-7976, will wait for the garbage collector to delete the pods
Jan 20 00:04:41.337: INFO: Deleting ReplicationController wrapped-volume-race-56de5ecb-5217-49ad-9663-12a451d7cdea took: 3.580263ms
Jan 20 00:04:41.438: INFO: Terminating ReplicationController wrapped-volume-race-56de5ecb-5217-49ad-9663-12a451d7cdea pods took: 101.145644ms
STEP: Creating RC which spawns configmap-volume pods
Jan 20 00:04:45.162: INFO: Pod name wrapped-volume-race-c3e032ad-a22b-4a0b-9733-4b0c8b5b242f: Found 0 pods out of 5
Jan 20 00:04:50.167: INFO: Pod name wrapped-volume-race-c3e032ad-a22b-4a0b-9733-4b0c8b5b242f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c3e032ad-a22b-4a0b-9733-4b0c8b5b242f in namespace emptydir-wrapper-7976, will wait for the garbage collector to delete the pods
Jan 20 00:05:02.239: INFO: Deleting ReplicationController wrapped-volume-race-c3e032ad-a22b-4a0b-9733-4b0c8b5b242f took: 4.271728ms
Jan 20 00:05:02.340: INFO: Terminating ReplicationController wrapped-volume-race-c3e032ad-a22b-4a0b-9733-4b0c8b5b242f pods took: 101.097754ms
STEP: Creating RC which spawns configmap-volume pods
Jan 20 00:05:05.767: INFO: Pod name wrapped-volume-race-41349260-722c-47a1-9f45-ab1f89da9fbc: Found 0 pods out of 5
Jan 20 00:05:10.772: INFO: Pod name wrapped-volume-race-41349260-722c-47a1-9f45-ab1f89da9fbc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-41349260-722c-47a1-9f45-ab1f89da9fbc in namespace emptydir-wrapper-7976, will wait for the garbage collector to delete the pods
Jan 20 00:05:22.847: INFO: Deleting ReplicationController wrapped-volume-race-41349260-722c-47a1-9f45-ab1f89da9fbc took: 3.444776ms
Jan 20 00:05:22.948: INFO: Terminating ReplicationController wrapped-volume-race-41349260-722c-47a1-9f45-ab1f89da9fbc pods took: 100.663757ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:05:26.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7976" for this suite.

• [SLOW TEST:62.477 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":325,"skipped":6158,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:05:26.427: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-cb32fcaa-22ea-458a-99f1-e2f0befbc999
STEP: Creating secret with name s-test-opt-upd-59d60cea-fe67-4543-8b8b-d9b19a6b4313
STEP: Creating the pod
Jan 20 00:05:26.477: INFO: The status of Pod pod-projected-secrets-1ab12e4a-e22e-4c7c-bd4b-221c7a58c6ef is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:05:28.481: INFO: The status of Pod pod-projected-secrets-1ab12e4a-e22e-4c7c-bd4b-221c7a58c6ef is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:05:30.482: INFO: The status of Pod pod-projected-secrets-1ab12e4a-e22e-4c7c-bd4b-221c7a58c6ef is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-cb32fcaa-22ea-458a-99f1-e2f0befbc999
STEP: Updating secret s-test-opt-upd-59d60cea-fe67-4543-8b8b-d9b19a6b4313
STEP: Creating secret with name s-test-opt-create-35d55695-719a-4bf2-b4ea-37970f611447
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:05:34.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-84" for this suite.

• [SLOW TEST:8.133 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6164,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:05:34.560: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jan 20 00:05:34.596: INFO: Waiting up to 5m0s for pod "downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1" in namespace "downward-api-3102" to be "Succeeded or Failed"
Jan 20 00:05:34.599: INFO: Pod "downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.287491ms
Jan 20 00:05:36.605: INFO: Pod "downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008096134s
Jan 20 00:05:38.610: INFO: Pod "downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0137346s
STEP: Saw pod success
Jan 20 00:05:38.610: INFO: Pod "downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1" satisfied condition "Succeeded or Failed"
Jan 20 00:05:38.612: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1 container dapi-container: <nil>
STEP: delete the pod
Jan 20 00:05:38.624: INFO: Waiting for pod downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1 to disappear
Jan 20 00:05:38.633: INFO: Pod downward-api-2be2b3eb-10bc-4f52-a2c3-545ff730c6c1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:05:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3102" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":6170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:05:38.640: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9510
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jan 20 00:05:38.690: INFO: Found 0 stateful pods, waiting for 3
Jan 20 00:05:48.695: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 20 00:05:48.695: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 20 00:05:48.695: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 20 00:05:48.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-9510 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 20 00:05:48.879: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 20 00:05:48.879: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 20 00:05:48.879: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 20 00:05:58.912: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 20 00:06:08.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-9510 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 20 00:06:09.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 20 00:06:09.098: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 20 00:06:09.098: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 20 00:06:19.116: INFO: Waiting for StatefulSet statefulset-9510/ss2 to complete update
STEP: Rolling back to a previous revision
Jan 20 00:06:29.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-9510 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 20 00:06:29.291: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 20 00:06:29.292: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 20 00:06:29.292: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 20 00:06:39.322: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 20 00:06:49.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=statefulset-9510 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 20 00:06:49.513: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 20 00:06:49.513: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 20 00:06:49.513: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 20 00:06:59.528: INFO: Waiting for StatefulSet statefulset-9510/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jan 20 00:07:09.534: INFO: Deleting all statefulset in ns statefulset-9510
Jan 20 00:07:09.535: INFO: Scaling statefulset ss2 to 0
Jan 20 00:07:19.553: INFO: Waiting for statefulset status.replicas updated to 0
Jan 20 00:07:19.555: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:19.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9510" for this suite.

• [SLOW TEST:100.933 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":328,"skipped":6200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:19.574: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a collection of services
Jan 20 00:07:19.608: INFO: Creating e2e-svc-a-d9269
Jan 20 00:07:19.619: INFO: Creating e2e-svc-b-lqp8d
Jan 20 00:07:19.633: INFO: Creating e2e-svc-c-dxww4
STEP: deleting service collection
Jan 20 00:07:19.690: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:19.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2692" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":329,"skipped":6223,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:19.697: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:23.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8221" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":330,"skipped":6234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:23.807: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jan 20 00:07:23.848: INFO: Found Service test-service-kt55j in namespace services-9216 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 20 00:07:23.848: INFO: Service test-service-kt55j created
STEP: Getting /status
Jan 20 00:07:23.855: INFO: Service test-service-kt55j has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 20 00:07:23.860: INFO: observed Service test-service-kt55j in namespace services-9216 with annotations: map[] & LoadBalancer: {[]}
Jan 20 00:07:23.860: INFO: Found Service test-service-kt55j in namespace services-9216 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 20 00:07:23.860: INFO: Service test-service-kt55j has service status patched
STEP: updating the ServiceStatus
Jan 20 00:07:23.865: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 20 00:07:23.867: INFO: Observed Service test-service-kt55j in namespace services-9216 with annotations: map[] & Conditions: {[]}
Jan 20 00:07:23.867: INFO: Observed event: &Service{ObjectMeta:{test-service-kt55j  services-9216  8778d72e-f081-4c76-9b55-24f4646b6552 98324 0 2022-01-20 00:07:23 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-01-20 00:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-01-20 00:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.107.71.164,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.107.71.164],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 20 00:07:23.867: INFO: Found Service test-service-kt55j in namespace services-9216 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 20 00:07:23.867: INFO: Service test-service-kt55j has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 20 00:07:23.873: INFO: observed Service test-service-kt55j in namespace services-9216 with labels: map[test-service-static:true]
Jan 20 00:07:23.873: INFO: observed Service test-service-kt55j in namespace services-9216 with labels: map[test-service-static:true]
Jan 20 00:07:23.873: INFO: observed Service test-service-kt55j in namespace services-9216 with labels: map[test-service-static:true]
Jan 20 00:07:23.874: INFO: Found Service test-service-kt55j in namespace services-9216 with labels: map[test-service:patched test-service-static:true]
Jan 20 00:07:23.874: INFO: Service test-service-kt55j patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 20 00:07:23.884: INFO: Observed event: ADDED
Jan 20 00:07:23.884: INFO: Observed event: MODIFIED
Jan 20 00:07:23.884: INFO: Observed event: MODIFIED
Jan 20 00:07:23.884: INFO: Observed event: MODIFIED
Jan 20 00:07:23.884: INFO: Found Service test-service-kt55j in namespace services-9216 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 20 00:07:23.884: INFO: Service test-service-kt55j deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:23.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9216" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":331,"skipped":6272,"failed":0}

------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:23.912: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jan 20 00:07:23.959: INFO: created test-pod-1
Jan 20 00:07:27.964: INFO: running and ready test-pod-1
Jan 20 00:07:27.969: INFO: created test-pod-2
Jan 20 00:07:31.977: INFO: running and ready test-pod-2
Jan 20 00:07:31.984: INFO: created test-pod-3
Jan 20 00:07:35.992: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Jan 20 00:07:36.024: INFO: Pod quantity 3 is different from expected quantity 0
Jan 20 00:07:37.028: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:38.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7260" for this suite.

• [SLOW TEST:14.123 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":332,"skipped":6272,"failed":0}
S
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:38.035: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jan 20 00:07:38.067: INFO: Major version: 1
STEP: Confirm minor version
Jan 20 00:07:38.067: INFO: cleanMinorVersion: 23
Jan 20 00:07:38.067: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:38.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7542" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":333,"skipped":6273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:38.075: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 20 00:07:38.115: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:07:40.119: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:07:42.119: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 20 00:07:42.129: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:07:44.133: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:07:46.135: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 20 00:07:46.141: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 20 00:07:46.143: INFO: Pod pod-with-prestop-http-hook still exists
Jan 20 00:07:48.144: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 20 00:07:48.146: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8996" for this suite.

• [SLOW TEST:10.093 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":6333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:48.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jan 20 00:07:48.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54" in namespace "projected-5744" to be "Succeeded or Failed"
Jan 20 00:07:48.206: INFO: Pod "downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54": Phase="Pending", Reason="", readiness=false. Elapsed: 1.989711ms
Jan 20 00:07:50.209: INFO: Pod "downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004555119s
Jan 20 00:07:52.214: INFO: Pod "downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009225654s
STEP: Saw pod success
Jan 20 00:07:52.214: INFO: Pod "downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54" satisfied condition "Succeeded or Failed"
Jan 20 00:07:52.216: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54 container client-container: <nil>
STEP: delete the pod
Jan 20 00:07:52.236: INFO: Waiting for pod downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54 to disappear
Jan 20 00:07:52.238: INFO: Pod downwardapi-volume-427bc7c5-401a-47fd-89eb-767e1705eb54 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:52.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5744" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":6362,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:52.244: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-bd7eed3f-776a-4c66-a466-5ab992119914
STEP: Creating a pod to test consume secrets
Jan 20 00:07:52.283: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680" in namespace "projected-3078" to be "Succeeded or Failed"
Jan 20 00:07:52.285: INFO: Pod "pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833394ms
Jan 20 00:07:54.287: INFO: Pod "pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004594976s
Jan 20 00:07:56.291: INFO: Pod "pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00870869s
STEP: Saw pod success
Jan 20 00:07:56.292: INFO: Pod "pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680" satisfied condition "Succeeded or Failed"
Jan 20 00:07:56.293: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 20 00:07:56.313: INFO: Waiting for pod pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680 to disappear
Jan 20 00:07:56.315: INFO: Pod pod-projected-secrets-4d99617b-ba44-42ff-9778-c5fd3d94f680 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:07:56.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3078" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:07:56.323: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jan 20 00:07:56.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 create -f -'
Jan 20 00:07:56.553: INFO: stderr: ""
Jan 20 00:07:56.553: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 20 00:07:56.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 20 00:07:56.622: INFO: stderr: ""
Jan 20 00:07:56.622: INFO: stdout: "update-demo-nautilus-jkrpj update-demo-nautilus-rln54 "
Jan 20 00:07:56.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods update-demo-nautilus-jkrpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 20 00:07:56.674: INFO: stderr: ""
Jan 20 00:07:56.674: INFO: stdout: ""
Jan 20 00:07:56.674: INFO: update-demo-nautilus-jkrpj is created but not running
Jan 20 00:08:01.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 20 00:08:01.736: INFO: stderr: ""
Jan 20 00:08:01.736: INFO: stdout: "update-demo-nautilus-jkrpj update-demo-nautilus-rln54 "
Jan 20 00:08:01.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods update-demo-nautilus-jkrpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 20 00:08:01.791: INFO: stderr: ""
Jan 20 00:08:01.792: INFO: stdout: "true"
Jan 20 00:08:01.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods update-demo-nautilus-jkrpj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 20 00:08:01.847: INFO: stderr: ""
Jan 20 00:08:01.847: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 20 00:08:01.847: INFO: validating pod update-demo-nautilus-jkrpj
Jan 20 00:08:01.850: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 20 00:08:01.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 20 00:08:01.851: INFO: update-demo-nautilus-jkrpj is verified up and running
Jan 20 00:08:01.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods update-demo-nautilus-rln54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 20 00:08:01.906: INFO: stderr: ""
Jan 20 00:08:01.906: INFO: stdout: "true"
Jan 20 00:08:01.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods update-demo-nautilus-rln54 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 20 00:08:01.963: INFO: stderr: ""
Jan 20 00:08:01.963: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 20 00:08:01.963: INFO: validating pod update-demo-nautilus-rln54
Jan 20 00:08:01.967: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 20 00:08:01.967: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 20 00:08:01.967: INFO: update-demo-nautilus-rln54 is verified up and running
STEP: using delete to clean up resources
Jan 20 00:08:01.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 delete --grace-period=0 --force -f -'
Jan 20 00:08:02.027: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 20 00:08:02.027: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 20 00:08:02.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get rc,svc -l name=update-demo --no-headers'
Jan 20 00:08:02.098: INFO: stderr: "No resources found in kubectl-5932 namespace.\n"
Jan 20 00:08:02.098: INFO: stdout: ""
Jan 20 00:08:02.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4283518553 --namespace=kubectl-5932 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 20 00:08:02.165: INFO: stderr: ""
Jan 20 00:08:02.165: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:02.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5932" for this suite.

• [SLOW TEST:5.849 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":337,"skipped":6386,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:02.172: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jan 20 00:08:02.199: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:05.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9932" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":338,"skipped":6394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:05.340: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jan 20 00:08:05.376: INFO: The status of Pod server-envvars-08e1ab75-0a4a-421a-95dc-09add30f0421 is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:08:07.382: INFO: The status of Pod server-envvars-08e1ab75-0a4a-421a-95dc-09add30f0421 is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:08:09.380: INFO: The status of Pod server-envvars-08e1ab75-0a4a-421a-95dc-09add30f0421 is Running (Ready = true)
Jan 20 00:08:09.404: INFO: Waiting up to 5m0s for pod "client-envvars-fed30e89-500f-4613-a392-8fd5d161155e" in namespace "pods-3562" to be "Succeeded or Failed"
Jan 20 00:08:09.410: INFO: Pod "client-envvars-fed30e89-500f-4613-a392-8fd5d161155e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.423052ms
Jan 20 00:08:11.413: INFO: Pod "client-envvars-fed30e89-500f-4613-a392-8fd5d161155e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009106683s
Jan 20 00:08:13.420: INFO: Pod "client-envvars-fed30e89-500f-4613-a392-8fd5d161155e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01603755s
STEP: Saw pod success
Jan 20 00:08:13.420: INFO: Pod "client-envvars-fed30e89-500f-4613-a392-8fd5d161155e" satisfied condition "Succeeded or Failed"
Jan 20 00:08:13.422: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod client-envvars-fed30e89-500f-4613-a392-8fd5d161155e container env3cont: <nil>
STEP: delete the pod
Jan 20 00:08:13.447: INFO: Waiting for pod client-envvars-fed30e89-500f-4613-a392-8fd5d161155e to disappear
Jan 20 00:08:13.449: INFO: Pod client-envvars-fed30e89-500f-4613-a392-8fd5d161155e no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:13.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3562" for this suite.

• [SLOW TEST:8.114 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":339,"skipped":6498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jan 20 00:08:13.505: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:08:15.510: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:08:17.510: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jan 20 00:08:17.520: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:08:19.524: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 20 00:08:21.523: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 20 00:08:21.551: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 20 00:08:21.554: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 20 00:08:23.554: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 20 00:08:23.561: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:23.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3803" for this suite.

• [SLOW TEST:10.113 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":6560,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:23.569: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:23.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3834" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":341,"skipped":6566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:23.629: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jan 20 00:08:23.662: INFO: Waiting up to 5m0s for pod "var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106" in namespace "var-expansion-4952" to be "Succeeded or Failed"
Jan 20 00:08:23.664: INFO: Pod "var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913955ms
Jan 20 00:08:25.667: INFO: Pod "var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004236238s
Jan 20 00:08:27.672: INFO: Pod "var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00959179s
STEP: Saw pod success
Jan 20 00:08:27.672: INFO: Pod "var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106" satisfied condition "Succeeded or Failed"
Jan 20 00:08:27.674: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106 container dapi-container: <nil>
STEP: delete the pod
Jan 20 00:08:27.685: INFO: Waiting for pod var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106 to disappear
Jan 20 00:08:27.686: INFO: Pod var-expansion-2a2cb208-fca4-4d3a-9518-e6f12d1d1106 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:27.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4952" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":6628,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:27.692: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jan 20 00:08:27.758: INFO: Waiting up to 5m0s for pod "var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190" in namespace "var-expansion-3412" to be "Succeeded or Failed"
Jan 20 00:08:27.769: INFO: Pod "var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190": Phase="Pending", Reason="", readiness=false. Elapsed: 10.678322ms
Jan 20 00:08:29.771: INFO: Pod "var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012822072s
Jan 20 00:08:31.777: INFO: Pod "var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018622013s
STEP: Saw pod success
Jan 20 00:08:31.777: INFO: Pod "var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190" satisfied condition "Succeeded or Failed"
Jan 20 00:08:31.779: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190 container dapi-container: <nil>
STEP: delete the pod
Jan 20 00:08:31.810: INFO: Waiting for pod var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190 to disappear
Jan 20 00:08:31.812: INFO: Pod var-expansion-39820151-4fcb-438d-9ee3-717e8b5c5190 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:31.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3412" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6628,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:31.818: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:08:31.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-260" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":344,"skipped":6641,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:08:31.882: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 20 00:09:12.025: INFO: The status of Pod kube-controller-manager-ip-10-0-0-80.eu-central-1.compute.internal is Running (Ready = true)
Jan 20 00:09:12.355: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 20 00:09:12.355: INFO: Deleting pod "simpletest.rc-2mq4j" in namespace "gc-9607"
Jan 20 00:09:12.371: INFO: Deleting pod "simpletest.rc-2svck" in namespace "gc-9607"
Jan 20 00:09:12.385: INFO: Deleting pod "simpletest.rc-2wn9n" in namespace "gc-9607"
Jan 20 00:09:12.401: INFO: Deleting pod "simpletest.rc-488p6" in namespace "gc-9607"
Jan 20 00:09:12.406: INFO: Deleting pod "simpletest.rc-4rfnq" in namespace "gc-9607"
Jan 20 00:09:12.418: INFO: Deleting pod "simpletest.rc-564sm" in namespace "gc-9607"
Jan 20 00:09:12.429: INFO: Deleting pod "simpletest.rc-56grc" in namespace "gc-9607"
Jan 20 00:09:12.448: INFO: Deleting pod "simpletest.rc-58m6d" in namespace "gc-9607"
Jan 20 00:09:12.462: INFO: Deleting pod "simpletest.rc-64rk8" in namespace "gc-9607"
Jan 20 00:09:12.467: INFO: Deleting pod "simpletest.rc-65jkx" in namespace "gc-9607"
Jan 20 00:09:12.479: INFO: Deleting pod "simpletest.rc-6796p" in namespace "gc-9607"
Jan 20 00:09:12.491: INFO: Deleting pod "simpletest.rc-6czzm" in namespace "gc-9607"
Jan 20 00:09:12.513: INFO: Deleting pod "simpletest.rc-6hmbz" in namespace "gc-9607"
Jan 20 00:09:12.517: INFO: Deleting pod "simpletest.rc-6kpwh" in namespace "gc-9607"
Jan 20 00:09:12.538: INFO: Deleting pod "simpletest.rc-6sf2b" in namespace "gc-9607"
Jan 20 00:09:12.544: INFO: Deleting pod "simpletest.rc-79rfz" in namespace "gc-9607"
Jan 20 00:09:12.555: INFO: Deleting pod "simpletest.rc-7kb9h" in namespace "gc-9607"
Jan 20 00:09:12.575: INFO: Deleting pod "simpletest.rc-82r7k" in namespace "gc-9607"
Jan 20 00:09:12.581: INFO: Deleting pod "simpletest.rc-84lmx" in namespace "gc-9607"
Jan 20 00:09:12.592: INFO: Deleting pod "simpletest.rc-8576s" in namespace "gc-9607"
Jan 20 00:09:12.615: INFO: Deleting pod "simpletest.rc-8bffc" in namespace "gc-9607"
Jan 20 00:09:12.629: INFO: Deleting pod "simpletest.rc-989dz" in namespace "gc-9607"
Jan 20 00:09:12.642: INFO: Deleting pod "simpletest.rc-9z57k" in namespace "gc-9607"
Jan 20 00:09:12.656: INFO: Deleting pod "simpletest.rc-bmb5t" in namespace "gc-9607"
Jan 20 00:09:12.667: INFO: Deleting pod "simpletest.rc-bzp8t" in namespace "gc-9607"
Jan 20 00:09:12.690: INFO: Deleting pod "simpletest.rc-c7cc2" in namespace "gc-9607"
Jan 20 00:09:12.696: INFO: Deleting pod "simpletest.rc-cq679" in namespace "gc-9607"
Jan 20 00:09:12.739: INFO: Deleting pod "simpletest.rc-crgww" in namespace "gc-9607"
Jan 20 00:09:12.774: INFO: Deleting pod "simpletest.rc-cw4hz" in namespace "gc-9607"
Jan 20 00:09:12.812: INFO: Deleting pod "simpletest.rc-d4bqg" in namespace "gc-9607"
Jan 20 00:09:12.817: INFO: Deleting pod "simpletest.rc-d4q64" in namespace "gc-9607"
Jan 20 00:09:12.829: INFO: Deleting pod "simpletest.rc-d5d2h" in namespace "gc-9607"
Jan 20 00:09:12.850: INFO: Deleting pod "simpletest.rc-dbp8t" in namespace "gc-9607"
Jan 20 00:09:12.864: INFO: Deleting pod "simpletest.rc-ds7lp" in namespace "gc-9607"
Jan 20 00:09:12.879: INFO: Deleting pod "simpletest.rc-dtmrw" in namespace "gc-9607"
Jan 20 00:09:12.884: INFO: Deleting pod "simpletest.rc-dw27h" in namespace "gc-9607"
Jan 20 00:09:12.897: INFO: Deleting pod "simpletest.rc-dwx45" in namespace "gc-9607"
Jan 20 00:09:12.908: INFO: Deleting pod "simpletest.rc-dzhzk" in namespace "gc-9607"
Jan 20 00:09:12.928: INFO: Deleting pod "simpletest.rc-f55gg" in namespace "gc-9607"
Jan 20 00:09:12.944: INFO: Deleting pod "simpletest.rc-fprrw" in namespace "gc-9607"
Jan 20 00:09:12.949: INFO: Deleting pod "simpletest.rc-gfk6x" in namespace "gc-9607"
Jan 20 00:09:12.970: INFO: Deleting pod "simpletest.rc-gpp56" in namespace "gc-9607"
Jan 20 00:09:12.981: INFO: Deleting pod "simpletest.rc-h64x4" in namespace "gc-9607"
Jan 20 00:09:12.997: INFO: Deleting pod "simpletest.rc-hcgt2" in namespace "gc-9607"
Jan 20 00:09:13.008: INFO: Deleting pod "simpletest.rc-hdjt7" in namespace "gc-9607"
Jan 20 00:09:13.015: INFO: Deleting pod "simpletest.rc-hfzjf" in namespace "gc-9607"
Jan 20 00:09:13.026: INFO: Deleting pod "simpletest.rc-hn452" in namespace "gc-9607"
Jan 20 00:09:13.047: INFO: Deleting pod "simpletest.rc-hq76j" in namespace "gc-9607"
Jan 20 00:09:13.052: INFO: Deleting pod "simpletest.rc-jrp5d" in namespace "gc-9607"
Jan 20 00:09:13.071: INFO: Deleting pod "simpletest.rc-jv9cl" in namespace "gc-9607"
Jan 20 00:09:13.085: INFO: Deleting pod "simpletest.rc-jzrcb" in namespace "gc-9607"
Jan 20 00:09:13.097: INFO: Deleting pod "simpletest.rc-k6fl8" in namespace "gc-9607"
Jan 20 00:09:13.103: INFO: Deleting pod "simpletest.rc-kbqlh" in namespace "gc-9607"
Jan 20 00:09:13.115: INFO: Deleting pod "simpletest.rc-kmggl" in namespace "gc-9607"
Jan 20 00:09:13.126: INFO: Deleting pod "simpletest.rc-kz9gf" in namespace "gc-9607"
Jan 20 00:09:13.150: INFO: Deleting pod "simpletest.rc-lj89v" in namespace "gc-9607"
Jan 20 00:09:13.165: INFO: Deleting pod "simpletest.rc-lsz54" in namespace "gc-9607"
Jan 20 00:09:13.180: INFO: Deleting pod "simpletest.rc-m6jw7" in namespace "gc-9607"
Jan 20 00:09:13.194: INFO: Deleting pod "simpletest.rc-mbd2l" in namespace "gc-9607"
Jan 20 00:09:13.208: INFO: Deleting pod "simpletest.rc-mjq92" in namespace "gc-9607"
Jan 20 00:09:13.214: INFO: Deleting pod "simpletest.rc-mq8zf" in namespace "gc-9607"
Jan 20 00:09:13.234: INFO: Deleting pod "simpletest.rc-mrrv6" in namespace "gc-9607"
Jan 20 00:09:13.240: INFO: Deleting pod "simpletest.rc-mvm94" in namespace "gc-9607"
Jan 20 00:09:13.254: INFO: Deleting pod "simpletest.rc-nj95k" in namespace "gc-9607"
Jan 20 00:09:13.274: INFO: Deleting pod "simpletest.rc-nkmcl" in namespace "gc-9607"
Jan 20 00:09:13.289: INFO: Deleting pod "simpletest.rc-ntmrt" in namespace "gc-9607"
Jan 20 00:09:13.304: INFO: Deleting pod "simpletest.rc-p7bgt" in namespace "gc-9607"
Jan 20 00:09:13.318: INFO: Deleting pod "simpletest.rc-pnbgq" in namespace "gc-9607"
Jan 20 00:09:13.332: INFO: Deleting pod "simpletest.rc-pwx4q" in namespace "gc-9607"
Jan 20 00:09:13.338: INFO: Deleting pod "simpletest.rc-q9lvx" in namespace "gc-9607"
Jan 20 00:09:13.361: INFO: Deleting pod "simpletest.rc-r5lxs" in namespace "gc-9607"
Jan 20 00:09:13.410: INFO: Deleting pod "simpletest.rc-r6gz7" in namespace "gc-9607"
Jan 20 00:09:13.471: INFO: Deleting pod "simpletest.rc-rfx92" in namespace "gc-9607"
Jan 20 00:09:13.512: INFO: Deleting pod "simpletest.rc-rllkl" in namespace "gc-9607"
Jan 20 00:09:13.570: INFO: Deleting pod "simpletest.rc-rp6v7" in namespace "gc-9607"
Jan 20 00:09:13.611: INFO: Deleting pod "simpletest.rc-rwzkd" in namespace "gc-9607"
Jan 20 00:09:13.664: INFO: Deleting pod "simpletest.rc-rzl58" in namespace "gc-9607"
Jan 20 00:09:13.715: INFO: Deleting pod "simpletest.rc-sdgxv" in namespace "gc-9607"
Jan 20 00:09:13.771: INFO: Deleting pod "simpletest.rc-sfx7k" in namespace "gc-9607"
Jan 20 00:09:13.820: INFO: Deleting pod "simpletest.rc-tcxpt" in namespace "gc-9607"
Jan 20 00:09:13.864: INFO: Deleting pod "simpletest.rc-tg6rb" in namespace "gc-9607"
Jan 20 00:09:13.912: INFO: Deleting pod "simpletest.rc-thfd7" in namespace "gc-9607"
Jan 20 00:09:13.964: INFO: Deleting pod "simpletest.rc-tl222" in namespace "gc-9607"
Jan 20 00:09:14.013: INFO: Deleting pod "simpletest.rc-tlndl" in namespace "gc-9607"
Jan 20 00:09:14.074: INFO: Deleting pod "simpletest.rc-tlqv7" in namespace "gc-9607"
Jan 20 00:09:14.112: INFO: Deleting pod "simpletest.rc-tlz4w" in namespace "gc-9607"
Jan 20 00:09:14.162: INFO: Deleting pod "simpletest.rc-vgr69" in namespace "gc-9607"
Jan 20 00:09:14.221: INFO: Deleting pod "simpletest.rc-w6mfb" in namespace "gc-9607"
Jan 20 00:09:14.262: INFO: Deleting pod "simpletest.rc-wtbsx" in namespace "gc-9607"
Jan 20 00:09:14.321: INFO: Deleting pod "simpletest.rc-xdfbq" in namespace "gc-9607"
Jan 20 00:09:14.361: INFO: Deleting pod "simpletest.rc-xp7c8" in namespace "gc-9607"
Jan 20 00:09:14.420: INFO: Deleting pod "simpletest.rc-z2p69" in namespace "gc-9607"
Jan 20 00:09:14.469: INFO: Deleting pod "simpletest.rc-z7lf4" in namespace "gc-9607"
Jan 20 00:09:14.512: INFO: Deleting pod "simpletest.rc-zd2qh" in namespace "gc-9607"
Jan 20 00:09:14.563: INFO: Deleting pod "simpletest.rc-zffsn" in namespace "gc-9607"
Jan 20 00:09:14.612: INFO: Deleting pod "simpletest.rc-zprk8" in namespace "gc-9607"
Jan 20 00:09:14.673: INFO: Deleting pod "simpletest.rc-zt65z" in namespace "gc-9607"
Jan 20 00:09:14.734: INFO: Deleting pod "simpletest.rc-zt9px" in namespace "gc-9607"
Jan 20 00:09:14.818: INFO: Deleting pod "simpletest.rc-zxw7n" in namespace "gc-9607"
Jan 20 00:09:14.830: INFO: Deleting pod "simpletest.rc-zzql7" in namespace "gc-9607"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:09:14.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9607" for this suite.

• [SLOW TEST:43.077 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":345,"skipped":6642,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jan 20 00:09:14.959: INFO: >>> kubeConfig: /tmp/kubeconfig-4283518553
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-a2778e56-5eef-4be9-9210-bdb805db5993
STEP: Creating a pod to test consume configMaps
Jan 20 00:09:15.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0" in namespace "projected-6573" to be "Succeeded or Failed"
Jan 20 00:09:15.004: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080592ms
Jan 20 00:09:17.009: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006490763s
Jan 20 00:09:19.014: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011760002s
Jan 20 00:09:21.017: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014642257s
Jan 20 00:09:23.022: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019997248s
Jan 20 00:09:25.025: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022711481s
Jan 20 00:09:27.029: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.026338155s
STEP: Saw pod success
Jan 20 00:09:27.029: INFO: Pod "pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0" satisfied condition "Succeeded or Failed"
Jan 20 00:09:27.030: INFO: Trying to get logs from node ip-10-0-1-153.eu-central-1.compute.internal pod pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0 container agnhost-container: <nil>
STEP: delete the pod
Jan 20 00:09:27.050: INFO: Waiting for pod pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0 to disappear
Jan 20 00:09:27.052: INFO: Pod pod-projected-configmaps-49b29eef-51a1-4067-b995-cf0bce094bd0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jan 20 00:09:27.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6573" for this suite.

• [SLOW TEST:12.100 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJan 20 00:09:27.059: INFO: Running AfterSuite actions on all nodes
Jan 20 00:09:27.059: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Jan 20 00:09:27.059: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jan 20 00:09:27.059: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Jan 20 00:09:27.059: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 20 00:09:27.059: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 20 00:09:27.059: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 20 00:09:27.059: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jan 20 00:09:27.059: INFO: Running AfterSuite actions on node 1
Jan 20 00:09:27.059: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6696,"failed":0}

Ran 346 of 7042 Specs in 6129.866 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6696 Skipped
PASS

Ginkgo ran 1 suite in 1h42m12.235246562s
Test Suite Passed
