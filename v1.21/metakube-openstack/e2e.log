I1013 10:42:56.007879      20 e2e.go:129] Starting e2e run "82f6921c-2ad0-4123-a70f-7a2a5853eec8" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1634121774 - Will randomize all specs
Will run 339 of 5771 specs

Oct 13 10:42:56.021: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 10:42:56.023: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Oct 13 10:42:56.077: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct 13 10:42:56.173: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct 13 10:42:56.173: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Oct 13 10:42:56.173: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Oct 13 10:42:56.204: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Oct 13 10:42:56.204: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Oct 13 10:42:56.204: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Oct 13 10:42:56.204: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Oct 13 10:42:56.204: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Oct 13 10:42:56.204: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'syseleven-node-problem-detector' (0 seconds elapsed)
Oct 13 10:42:56.204: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
Oct 13 10:42:56.204: INFO: e2e test version: v1.21.3
Oct 13 10:42:56.210: INFO: kube-apiserver version: v1.21.3
Oct 13 10:42:56.210: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 10:42:56.223: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:42:56.223: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
W1013 10:42:56.320413      20 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Oct 13 10:42:56.321: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Oct 13 10:42:56.345: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 10:42:56.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333" in namespace "projected-6177" to be "Succeeded or Failed"
Oct 13 10:42:56.394: INFO: Pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333": Phase="Pending", Reason="", readiness=false. Elapsed: 14.15765ms
Oct 13 10:42:58.409: INFO: Pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029775794s
Oct 13 10:43:00.422: INFO: Pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042680999s
Oct 13 10:43:02.440: INFO: Pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059988966s
Oct 13 10:43:04.463: INFO: Pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082899828s
Oct 13 10:43:06.477: INFO: Pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.097350889s
STEP: Saw pod success
Oct 13 10:43:06.477: INFO: Pod "downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333" satisfied condition "Succeeded or Failed"
Oct 13 10:43:06.489: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333 container client-container: <nil>
STEP: delete the pod
Oct 13 10:43:06.557: INFO: Waiting for pod downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333 to disappear
Oct 13 10:43:06.566: INFO: Pod downwardapi-volume-abe4b5f0-6cbd-4e6d-967d-dcd0ea2ed333 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:43:06.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6177" for this suite.

• [SLOW TEST:10.376 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":1,"skipped":25,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:43:06.600: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Oct 13 10:43:16.938: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Oct 13 10:43:16.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xgrd" in namespace "gc-610"
W1013 10:43:16.938635      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1013 10:43:16.938683      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1013 10:43:16.938691      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 13 10:43:16.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-98gkg" in namespace "gc-610"
Oct 13 10:43:17.003: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcbwn" in namespace "gc-610"
Oct 13 10:43:17.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-m5fst" in namespace "gc-610"
Oct 13 10:43:17.055: INFO: Deleting pod "simpletest-rc-to-be-deleted-mpwlb" in namespace "gc-610"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:43:17.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-610" for this suite.

• [SLOW TEST:10.537 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":2,"skipped":28,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:43:17.138: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 13 10:43:17.228: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:43:25.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5570" for this suite.

• [SLOW TEST:8.718 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":3,"skipped":36,"failed":0}
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:43:25.857: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:43:25.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1649" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":4,"skipped":37,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:43:26.007: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 10:43:26.078: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 13 10:43:29.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-3868 --namespace=crd-publish-openapi-3868 create -f -'
Oct 13 10:43:31.359: INFO: stderr: ""
Oct 13 10:43:31.359: INFO: stdout: "e2e-test-crd-publish-openapi-9586-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 13 10:43:31.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-3868 --namespace=crd-publish-openapi-3868 delete e2e-test-crd-publish-openapi-9586-crds test-cr'
Oct 13 10:43:31.479: INFO: stderr: ""
Oct 13 10:43:31.479: INFO: stdout: "e2e-test-crd-publish-openapi-9586-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Oct 13 10:43:31.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-3868 --namespace=crd-publish-openapi-3868 apply -f -'
Oct 13 10:43:31.825: INFO: stderr: ""
Oct 13 10:43:31.825: INFO: stdout: "e2e-test-crd-publish-openapi-9586-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Oct 13 10:43:31.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-3868 --namespace=crd-publish-openapi-3868 delete e2e-test-crd-publish-openapi-9586-crds test-cr'
Oct 13 10:43:31.943: INFO: stderr: ""
Oct 13 10:43:31.944: INFO: stdout: "e2e-test-crd-publish-openapi-9586-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 13 10:43:31.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-3868 explain e2e-test-crd-publish-openapi-9586-crds'
Oct 13 10:43:32.317: INFO: stderr: ""
Oct 13 10:43:32.317: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9586-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:43:35.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3868" for this suite.

• [SLOW TEST:9.360 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":5,"skipped":54,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:43:35.367: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 10:43:35.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468" in namespace "projected-7110" to be "Succeeded or Failed"
Oct 13 10:43:35.499: INFO: Pod "downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468": Phase="Pending", Reason="", readiness=false. Elapsed: 11.386351ms
Oct 13 10:43:37.518: INFO: Pod "downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030583912s
Oct 13 10:43:39.535: INFO: Pod "downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047236324s
STEP: Saw pod success
Oct 13 10:43:39.535: INFO: Pod "downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468" satisfied condition "Succeeded or Failed"
Oct 13 10:43:39.545: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468 container client-container: <nil>
STEP: delete the pod
Oct 13 10:43:39.645: INFO: Waiting for pod downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468 to disappear
Oct 13 10:43:39.654: INFO: Pod downwardapi-volume-0e68a00b-b8b3-46b1-88a7-102ef8f0b468 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:43:39.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7110" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":6,"skipped":74,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:43:39.684: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Oct 13 10:43:45.819: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2286 PodName:pod-sharedvolume-8de93c0f-5716-48b9-8799-0de4b85afa79 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 10:43:45.820: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 10:43:46.164: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:43:46.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2286" for this suite.

• [SLOW TEST:6.514 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":7,"skipped":110,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:43:46.200: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-b9e6d674-b803-4efa-939d-4af9a62616b7 in namespace container-probe-5335
Oct 13 10:43:50.325: INFO: Started pod liveness-b9e6d674-b803-4efa-939d-4af9a62616b7 in namespace container-probe-5335
STEP: checking the pod's current state and verifying that restartCount is present
Oct 13 10:43:50.334: INFO: Initial restart count of pod liveness-b9e6d674-b803-4efa-939d-4af9a62616b7 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:47:50.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5335" for this suite.

• [SLOW TEST:244.485 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":8,"skipped":116,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:47:50.688: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-9d2f246a-9209-4131-9195-05bf89fc226d
STEP: Creating a pod to test consume secrets
Oct 13 10:47:50.904: INFO: Waiting up to 5m0s for pod "pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887" in namespace "secrets-9949" to be "Succeeded or Failed"
Oct 13 10:47:50.915: INFO: Pod "pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887": Phase="Pending", Reason="", readiness=false. Elapsed: 10.605764ms
Oct 13 10:47:52.937: INFO: Pod "pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031926026s
Oct 13 10:47:54.959: INFO: Pod "pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053980015s
STEP: Saw pod success
Oct 13 10:47:54.959: INFO: Pod "pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887" satisfied condition "Succeeded or Failed"
Oct 13 10:47:54.970: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887 container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 10:47:55.069: INFO: Waiting for pod pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887 to disappear
Oct 13 10:47:55.080: INFO: Pod pod-secrets-ce9d371b-a3fe-4717-9fc9-89b1318a8887 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:47:55.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9949" for this suite.
STEP: Destroying namespace "secret-namespace-4093" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":9,"skipped":123,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:47:55.133: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 10:47:55.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a" in namespace "projected-2576" to be "Succeeded or Failed"
Oct 13 10:47:55.249: INFO: Pod "downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.403061ms
Oct 13 10:47:57.263: INFO: Pod "downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024307643s
Oct 13 10:47:59.283: INFO: Pod "downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044581715s
STEP: Saw pod success
Oct 13 10:47:59.284: INFO: Pod "downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a" satisfied condition "Succeeded or Failed"
Oct 13 10:47:59.294: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a container client-container: <nil>
STEP: delete the pod
Oct 13 10:47:59.394: INFO: Waiting for pod downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a to disappear
Oct 13 10:47:59.404: INFO: Pod downwardapi-volume-e7df2adc-478d-4dc1-83b9-29ef1a95ce3a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:47:59.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2576" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":10,"skipped":135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:47:59.439: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 10:47:59.554: INFO: The status of Pod busybox-host-aliases48b95d0a-e0d9-44e9-af81-58ea9e32a14e is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:48:01.574: INFO: The status of Pod busybox-host-aliases48b95d0a-e0d9-44e9-af81-58ea9e32a14e is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:48:03.574: INFO: The status of Pod busybox-host-aliases48b95d0a-e0d9-44e9-af81-58ea9e32a14e is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:48:03.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9313" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":11,"skipped":172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:48:03.653: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 10:48:04.077: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Oct 13 10:48:06.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718884, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718884, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718884, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718884, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 10:48:09.164: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
Oct 13 10:48:09.221: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:48:09.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7493" for this suite.
STEP: Destroying namespace "webhook-7493-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.117 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":12,"skipped":201,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:48:09.771: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 10:48:10.179: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 10:48:12.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718890, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718890, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718890, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718890, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 10:48:15.263: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:48:15.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9661" for this suite.
STEP: Destroying namespace "webhook-9661-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.938 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":13,"skipped":220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:48:15.710: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 13 10:48:15.799: INFO: PodSpec: initContainers in spec.initContainers
Oct 13 10:49:12.230: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d7a79835-34bc-428d-bb36-3c2ed888b3ad", GenerateName:"", Namespace:"init-container-1239", SelfLink:"", UID:"d626e833-c2a6-4b3e-a9e8-79ca6739af82", ResourceVersion:"371338", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63769718895, loc:(*time.Location)(0x9ddf5a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"799438422"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.25.0.31/32", "cni.projectcalico.org/podIPs":"172.25.0.31/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002e86768), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002e86780)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002e86798), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002e867b0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002e867c8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002e867e0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zfnq6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00312ac40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfnq6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfnq6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfnq6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002ead9b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003c57d50), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002eada30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002eada50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002eada58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002eada5c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003329420), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718895, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718895, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718895, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769718895, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.1.246", PodIP:"172.25.0.31", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.0.31"}}, StartTime:(*v1.Time)(0xc002e86810), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003c57e30)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003c57ea0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"docker://929fbdb31f07a85e05a21a138719d0bb7512184a074e84dd168d27af8c608404", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00312acc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00312aca0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002eadadf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:49:12.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1239" for this suite.

• [SLOW TEST:56.560 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":14,"skipped":254,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:49:12.274: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 13 10:49:15.469: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:49:15.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5798" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":15,"skipped":261,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:49:15.546: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 10:49:15.640: INFO: Creating pod...
Oct 13 10:49:15.675: INFO: Pod Quantity: 1 Status: Pending
Oct 13 10:49:16.690: INFO: Pod Quantity: 1 Status: Pending
Oct 13 10:49:17.687: INFO: Pod Quantity: 1 Status: Pending
Oct 13 10:49:18.692: INFO: Pod Status: Running
Oct 13 10:49:18.692: INFO: Creating service...
Oct 13 10:49:18.720: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/pods/agnhost/proxy/some/path/with/DELETE
Oct 13 10:49:18.743: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Oct 13 10:49:18.743: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/pods/agnhost/proxy/some/path/with/GET
Oct 13 10:49:18.756: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Oct 13 10:49:18.756: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/pods/agnhost/proxy/some/path/with/HEAD
Oct 13 10:49:18.853: INFO: http.Client request:HEAD | StatusCode:200
Oct 13 10:49:18.853: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/pods/agnhost/proxy/some/path/with/OPTIONS
Oct 13 10:49:18.869: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Oct 13 10:49:18.869: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/pods/agnhost/proxy/some/path/with/PATCH
Oct 13 10:49:18.884: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Oct 13 10:49:18.884: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/pods/agnhost/proxy/some/path/with/POST
Oct 13 10:49:18.902: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Oct 13 10:49:18.902: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/pods/agnhost/proxy/some/path/with/PUT
Oct 13 10:49:18.920: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Oct 13 10:49:18.920: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/services/test-service/proxy/some/path/with/DELETE
Oct 13 10:49:18.943: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Oct 13 10:49:18.943: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/services/test-service/proxy/some/path/with/GET
Oct 13 10:49:18.964: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Oct 13 10:49:18.964: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/services/test-service/proxy/some/path/with/HEAD
Oct 13 10:49:18.988: INFO: http.Client request:HEAD | StatusCode:200
Oct 13 10:49:18.988: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/services/test-service/proxy/some/path/with/OPTIONS
Oct 13 10:49:19.012: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Oct 13 10:49:19.012: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/services/test-service/proxy/some/path/with/PATCH
Oct 13 10:49:19.033: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Oct 13 10:49:19.033: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/services/test-service/proxy/some/path/with/POST
Oct 13 10:49:19.063: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Oct 13 10:49:19.063: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-2537/services/test-service/proxy/some/path/with/PUT
Oct 13 10:49:19.083: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:49:19.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2537" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":16,"skipped":261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:49:19.122: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6634
STEP: creating service affinity-nodeport in namespace services-6634
STEP: creating replication controller affinity-nodeport in namespace services-6634
I1013 10:49:19.243376      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-6634, replica count: 3
I1013 10:49:22.294611      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 10:49:25.295342      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 10:49:28.295942      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 10:49:28.332: INFO: Creating new exec pod
Oct 13 10:49:33.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-6634 exec execpod-affinityn5h4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Oct 13 10:49:33.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Oct 13 10:49:33.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:49:33.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-6634 exec execpod-affinityn5h4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.24.95 80'
Oct 13 10:49:34.491: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.24.95 80\nConnection to 10.240.24.95 80 port [tcp/http] succeeded!\n"
Oct 13 10:49:34.491: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:49:34.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-6634 exec execpod-affinityn5h4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.102 30730'
Oct 13 10:49:35.006: INFO: stderr: "+ nc -v -t -w 2 192.168.1.102 30730\n+ echo hostName\nConnection to 192.168.1.102 30730 port [tcp/*] succeeded!\n"
Oct 13 10:49:35.006: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:49:35.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-6634 exec execpod-affinityn5h4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.246 30730'
Oct 13 10:49:35.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.246 30730\nConnection to 192.168.1.246 30730 port [tcp/*] succeeded!\n"
Oct 13 10:49:35.438: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:49:35.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-6634 exec execpod-affinityn5h4g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.102:30730/ ; done'
Oct 13 10:49:36.099: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30730/\n"
Oct 13 10:49:36.099: INFO: stdout: "\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm\naffinity-nodeport-l6nsm"
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Received response from host: affinity-nodeport-l6nsm
Oct 13 10:49:36.099: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6634, will wait for the garbage collector to delete the pods
Oct 13 10:49:36.218: INFO: Deleting ReplicationController affinity-nodeport took: 17.907342ms
Oct 13 10:49:36.320: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.546239ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:49:41.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6634" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:22.580 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":17,"skipped":296,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:49:41.709: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6681
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 13 10:49:41.810: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 13 10:49:41.886: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:49:43.909: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:49:45.909: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:49:47.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:49:49.905: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:49:51.909: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:49:53.909: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:49:55.904: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:49:57.897: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:49:59.907: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:50:01.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 10:50:03.905: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 13 10:50:03.923: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 13 10:50:08.005: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 13 10:50:08.005: INFO: Breadth first check of 172.25.1.14 on host 192.168.1.102...
Oct 13 10:50:08.017: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.38:9080/dial?request=hostname&protocol=udp&host=172.25.1.14&port=8081&tries=1'] Namespace:pod-network-test-6681 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 10:50:08.017: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 10:50:08.522: INFO: Waiting for responses: map[]
Oct 13 10:50:08.523: INFO: reached 172.25.1.14 after 0/1 tries
Oct 13 10:50:08.523: INFO: Breadth first check of 172.25.0.37 on host 192.168.1.246...
Oct 13 10:50:08.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.38:9080/dial?request=hostname&protocol=udp&host=172.25.0.37&port=8081&tries=1'] Namespace:pod-network-test-6681 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 10:50:08.539: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 10:50:08.952: INFO: Waiting for responses: map[]
Oct 13 10:50:08.952: INFO: reached 172.25.0.37 after 0/1 tries
Oct 13 10:50:08.952: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:08.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6681" for this suite.

• [SLOW TEST:27.287 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":18,"skipped":301,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:09.005: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 13 10:50:09.170: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 13 10:50:09.187: INFO: starting watch
STEP: patching
STEP: updating
Oct 13 10:50:09.225: INFO: waiting for watch events with expected annotations
Oct 13 10:50:09.225: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:09.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7646" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":19,"skipped":314,"failed":0}
S
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Oct 13 10:50:09.451: INFO: created test-pod-1
Oct 13 10:50:09.465: INFO: created test-pod-2
Oct 13 10:50:09.475: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:09.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8055" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":20,"skipped":315,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:09.593: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 13 10:50:09.678: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:17.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3749" for this suite.

• [SLOW TEST:8.280 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":21,"skipped":332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:17.874: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6429/configmap-test-3e8586d7-8596-4691-93bd-29b1fbe950d9
STEP: Creating a pod to test consume configMaps
Oct 13 10:50:17.992: INFO: Waiting up to 5m0s for pod "pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a" in namespace "configmap-6429" to be "Succeeded or Failed"
Oct 13 10:50:18.003: INFO: Pod "pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.34719ms
Oct 13 10:50:20.018: INFO: Pod "pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025023385s
Oct 13 10:50:22.032: INFO: Pod "pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039293883s
Oct 13 10:50:24.050: INFO: Pod "pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057717628s
STEP: Saw pod success
Oct 13 10:50:24.051: INFO: Pod "pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a" satisfied condition "Succeeded or Failed"
Oct 13 10:50:24.062: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a container env-test: <nil>
STEP: delete the pod
Oct 13 10:50:24.157: INFO: Waiting for pod pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a to disappear
Oct 13 10:50:24.167: INFO: Pod pod-configmaps-29ec5bb5-bb6d-4a21-90e8-e127a0b4ff4a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:24.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6429" for this suite.

• [SLOW TEST:6.320 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":22,"skipped":363,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:24.194: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Oct 13 10:50:24.289: INFO: Pod name pod-release: Found 0 pods out of 1
Oct 13 10:50:29.302: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:30.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7766" for this suite.

• [SLOW TEST:6.197 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":23,"skipped":369,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:30.391: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Oct 13 10:50:30.527: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:30.527: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:30.539: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:30.539: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:30.561: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:30.561: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:30.607: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:30.607: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Oct 13 10:50:34.023: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Oct 13 10:50:34.023: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Oct 13 10:50:38.623: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Oct 13 10:50:38.655: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Oct 13 10:50:38.661: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.661: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.661: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.661: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 0
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.662: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.670: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.670: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.701: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.701: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.736: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.736: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:38.746: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:38.746: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:41.756: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:41.756: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:41.801: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
STEP: listing Deployments
Oct 13 10:50:41.821: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Oct 13 10:50:41.842: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Oct 13 10:50:41.855: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:41.865: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:41.877: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:41.893: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:41.902: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:44.362: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:44.861: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:44.919: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:44.927: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Oct 13 10:50:53.602: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Oct 13 10:50:53.690: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:53.690: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:53.690: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:53.690: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:53.690: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 1
Oct 13 10:50:53.691: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:53.691: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 3
Oct 13 10:50:53.691: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:53.691: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 2
Oct 13 10:50:53.691: INFO: observed Deployment test-deployment in namespace deployment-6367 with ReadyReplicas 3
STEP: deleting the Deployment
Oct 13 10:50:53.715: INFO: observed event type MODIFIED
Oct 13 10:50:53.715: INFO: observed event type MODIFIED
Oct 13 10:50:53.715: INFO: observed event type MODIFIED
Oct 13 10:50:53.716: INFO: observed event type MODIFIED
Oct 13 10:50:53.716: INFO: observed event type MODIFIED
Oct 13 10:50:53.716: INFO: observed event type MODIFIED
Oct 13 10:50:53.716: INFO: observed event type MODIFIED
Oct 13 10:50:53.717: INFO: observed event type MODIFIED
Oct 13 10:50:53.717: INFO: observed event type MODIFIED
Oct 13 10:50:53.717: INFO: observed event type MODIFIED
Oct 13 10:50:53.717: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Oct 13 10:50:53.727: INFO: Log out all the ReplicaSets if there is no deployment created
Oct 13 10:50:53.736: INFO: ReplicaSet "test-deployment-748588b7cd":
&ReplicaSet{ObjectMeta:{test-deployment-748588b7cd  deployment-6367  ff07e9ab-4594-4b2e-bc89-15347fde22b3 372400 4 2021-10-13 10:50:38 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e9872a7f-925a-4039-a66e-0270f1aea1d8 0xc00322e397 0xc00322e398}] []  [{kube-controller-manager Update apps/v1 2021-10-13 10:50:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9872a7f-925a-4039-a66e-0270f1aea1d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 748588b7cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.4.1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00322e400 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Oct 13 10:50:53.753: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-6367  909f4301-7c0f-4340-83d9-3281623fae02 372272 3 2021-10-13 10:50:30 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e9872a7f-925a-4039-a66e-0270f1aea1d8 0xc00322e467 0xc00322e468}] []  [{kube-controller-manager Update apps/v1 2021-10-13 10:50:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9872a7f-925a-4039-a66e-0270f1aea1d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00322e4d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Oct 13 10:50:53.765: INFO: ReplicaSet "test-deployment-85d87c6f4b":
&ReplicaSet{ObjectMeta:{test-deployment-85d87c6f4b  deployment-6367  2c1fea78-b383-47de-8cd4-234230dd9a20 372391 2 2021-10-13 10:50:41 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e9872a7f-925a-4039-a66e-0270f1aea1d8 0xc00322e537 0xc00322e538}] []  [{kube-controller-manager Update apps/v1 2021-10-13 10:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9872a7f-925a-4039-a66e-0270f1aea1d8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 85d87c6f4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00322e5a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Oct 13 10:50:53.788: INFO: pod: "test-deployment-85d87c6f4b-7mcnf":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-7mcnf test-deployment-85d87c6f4b- deployment-6367  da200912-8e61-43fa-a4ab-64752c8d9cef 372407 0 2021-10-13 10:50:44 +0000 UTC 2021-10-13 10:50:53 +0000 UTC 0xc00322ebc8 map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:172.25.1.17/32 cni.projectcalico.org/podIPs:172.25.1.17/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 2c1fea78-b383-47de-8cd4-234230dd9a20 0xc00322ec17 0xc00322ec18}] []  [{kube-controller-manager Update v1 2021-10-13 10:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c1fea78-b383-47de-8cd4-234230dd9a20\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 10:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 10:50:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lqjdp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lqjdp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:172.25.1.17,StartTime:2021-10-13 10:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 10:50:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://295dfc7f98158c28533cf99af70a1759a83a8e2d7b1eb882c5965500bf3db8fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Oct 13 10:50:53.788: INFO: pod: "test-deployment-85d87c6f4b-8s2x8":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-8s2x8 test-deployment-85d87c6f4b- deployment-6367  2054b77f-eb22-4149-8ce9-e50b6b7aaac3 372408 0 2021-10-13 10:50:41 +0000 UTC 2021-10-13 10:50:53 +0000 UTC 0xc00322edf0 map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[cni.projectcalico.org/podIP:172.25.0.45/32 cni.projectcalico.org/podIPs:172.25.0.45/32] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 2c1fea78-b383-47de-8cd4-234230dd9a20 0xc00322ee47 0xc00322ee48}] []  [{kube-controller-manager Update v1 2021-10-13 10:50:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c1fea78-b383-47de-8cd4-234230dd9a20\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 10:50:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 10:50:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7frq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7frq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 10:50:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.45,StartTime:2021-10-13 10:50:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 10:50:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://9602b3ffb308451405cc8b4c48ccd1a8939553002a28b553a81c57ea57a57c25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:53.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6367" for this suite.

• [SLOW TEST:23.427 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":24,"skipped":375,"failed":0}
SSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:53.818: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 13 10:50:54.000: INFO: starting watch
STEP: patching
STEP: updating
Oct 13 10:50:54.032: INFO: waiting for watch events with expected annotations
Oct 13 10:50:54.032: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:50:54.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1325" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":25,"skipped":378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:50:54.156: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-e64ad9b2-2c1b-4498-8158-e3b388492c4b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:51:00.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6527" for this suite.

• [SLOW TEST:6.298 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":26,"skipped":401,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:51:00.454: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3144
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3144
I1013 10:51:00.627163      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3144, replica count: 2
I1013 10:51:03.678705      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 10:51:06.679116      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 10:51:06.679: INFO: Creating new exec pod
Oct 13 10:51:11.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Oct 13 10:51:12.315: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 13 10:51:12.315: INFO: stdout: ""
Oct 13 10:51:13.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Oct 13 10:51:13.822: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 13 10:51:13.822: INFO: stdout: "externalname-service-pvfmz"
Oct 13 10:51:13.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.117 80'
Oct 13 10:51:14.300: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.18.117 80\nConnection to 10.240.18.117 80 port [tcp/http] succeeded!\n"
Oct 13 10:51:14.300: INFO: stdout: ""
Oct 13 10:51:15.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.117 80'
Oct 13 10:51:15.826: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.18.117 80\nConnection to 10.240.18.117 80 port [tcp/http] succeeded!\n"
Oct 13 10:51:15.826: INFO: stdout: ""
Oct 13 10:51:16.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.117 80'
Oct 13 10:51:16.766: INFO: stderr: "+ nc -v -t -w 2 10.240.18.117 80\nConnection to 10.240.18.117 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Oct 13 10:51:16.766: INFO: stdout: "externalname-service-v46h5"
Oct 13 10:51:16.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.102 32188'
Oct 13 10:51:17.279: INFO: stderr: "+ nc -v -t -w 2 192.168.1.102 32188\n+ echo hostName\nConnection to 192.168.1.102 32188 port [tcp/*] succeeded!\n"
Oct 13 10:51:17.279: INFO: stdout: "externalname-service-v46h5"
Oct 13 10:51:17.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.246 32188'
Oct 13 10:51:17.744: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.246 32188\nConnection to 192.168.1.246 32188 port [tcp/*] succeeded!\n"
Oct 13 10:51:17.744: INFO: stdout: ""
Oct 13 10:51:18.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3144 exec execpod9kxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.246 32188'
Oct 13 10:51:19.240: INFO: stderr: "+ nc -v -t -w 2 192.168.1.246 32188\n+ echo hostName\nConnection to 192.168.1.246 32188 port [tcp/*] succeeded!\n"
Oct 13 10:51:19.240: INFO: stdout: "externalname-service-pvfmz"
Oct 13 10:51:19.240: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:51:19.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3144" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:18.877 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":27,"skipped":403,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:51:19.331: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-199
Oct 13 10:51:19.453: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:51:21.471: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:51:23.476: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Oct 13 10:51:23.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 13 10:51:23.891: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Oct 13 10:51:23.891: INFO: stdout: "ipvs"
Oct 13 10:51:23.891: INFO: proxyMode: ipvs
Oct 13 10:51:23.921: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 13 10:51:23.934: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-199
STEP: creating replication controller affinity-nodeport-timeout in namespace services-199
I1013 10:51:23.987557      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-199, replica count: 3
I1013 10:51:27.038216      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 10:51:30.040819      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 10:51:30.085: INFO: Creating new exec pod
Oct 13 10:51:35.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec execpod-affinityvk4d2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Oct 13 10:51:35.732: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-timeout 80\n+ echo hostName\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Oct 13 10:51:35.732: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:51:35.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec execpod-affinityvk4d2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.21.219 80'
Oct 13 10:51:36.186: INFO: stderr: "+ nc -v -t -w 2 10.240.21.219 80\n+ echo hostName\nConnection to 10.240.21.219 80 port [tcp/http] succeeded!\n"
Oct 13 10:51:36.186: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:51:36.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec execpod-affinityvk4d2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.102 30904'
Oct 13 10:51:36.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.102 30904\nConnection to 192.168.1.102 30904 port [tcp/*] succeeded!\n"
Oct 13 10:51:36.707: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:51:36.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec execpod-affinityvk4d2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.246 30904'
Oct 13 10:51:37.204: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.246 30904\nConnection to 192.168.1.246 30904 port [tcp/*] succeeded!\n"
Oct 13 10:51:37.204: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 10:51:37.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec execpod-affinityvk4d2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.102:30904/ ; done'
Oct 13 10:51:37.743: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n"
Oct 13 10:51:37.743: INFO: stdout: "\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b\naffinity-nodeport-timeout-pg56b"
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Received response from host: affinity-nodeport-timeout-pg56b
Oct 13 10:51:37.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec execpod-affinityvk4d2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.102:30904/'
Oct 13 10:51:38.200: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n"
Oct 13 10:51:38.200: INFO: stdout: "affinity-nodeport-timeout-pg56b"
Oct 13 10:53:48.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-199 exec execpod-affinityvk4d2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.1.102:30904/'
Oct 13 10:53:50.309: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.1.102:30904/\n"
Oct 13 10:53:50.309: INFO: stdout: "affinity-nodeport-timeout-r44ws"
Oct 13 10:53:50.309: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-199, will wait for the garbage collector to delete the pods
Oct 13 10:53:50.436: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 17.829028ms
Oct 13 10:53:50.539: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 102.287052ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:54:07.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-199" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:168.000 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":28,"skipped":403,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:54:07.334: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:54:07.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2383" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":29,"skipped":422,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:54:07.584: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 10:54:07.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6" in namespace "downward-api-8941" to be "Succeeded or Failed"
Oct 13 10:54:07.700: INFO: Pod "downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.950227ms
Oct 13 10:54:09.721: INFO: Pod "downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033973397s
Oct 13 10:54:11.739: INFO: Pod "downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051641446s
Oct 13 10:54:13.758: INFO: Pod "downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070209595s
STEP: Saw pod success
Oct 13 10:54:13.758: INFO: Pod "downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6" satisfied condition "Succeeded or Failed"
Oct 13 10:54:13.770: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6 container client-container: <nil>
STEP: delete the pod
Oct 13 10:54:13.865: INFO: Waiting for pod downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6 to disappear
Oct 13 10:54:13.876: INFO: Pod downwardapi-volume-99cf2526-323d-4265-b10b-7f06866b2ff6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:54:13.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8941" for this suite.

• [SLOW TEST:6.332 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":30,"skipped":435,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:54:13.925: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Oct 13 10:54:15.219: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:54:15.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1013 10:54:15.219612      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1013 10:54:15.219642      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1013 10:54:15.219646      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4270" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":31,"skipped":448,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:54:15.254: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:55:15.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3856" for this suite.

• [SLOW TEST:60.159 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":32,"skipped":461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:55:15.425: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:55:15.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4136" for this suite.
STEP: Destroying namespace "nspatchtest-46241914-c82f-44c1-9ec4-1644d1da7840-479" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":33,"skipped":487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:55:15.682: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W1013 10:55:15.780209      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:57:01.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5245" for this suite.

• [SLOW TEST:106.201 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":34,"skipped":523,"failed":0}
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:57:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 10:57:01.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e" in namespace "downward-api-3711" to be "Succeeded or Failed"
Oct 13 10:57:02.005: INFO: Pod "downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.244805ms
Oct 13 10:57:04.022: INFO: Pod "downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030271938s
Oct 13 10:57:06.039: INFO: Pod "downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046971586s
Oct 13 10:57:08.053: INFO: Pod "downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061425839s
STEP: Saw pod success
Oct 13 10:57:08.054: INFO: Pod "downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e" satisfied condition "Succeeded or Failed"
Oct 13 10:57:08.066: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e container client-container: <nil>
STEP: delete the pod
Oct 13 10:57:08.161: INFO: Waiting for pod downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e to disappear
Oct 13 10:57:08.173: INFO: Pod downwardapi-volume-dd5d186c-5651-4373-90b5-c0c713c3f91e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:57:08.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3711" for this suite.

• [SLOW TEST:6.326 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":35,"skipped":523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:57:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Oct 13 10:57:14.901: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8326 pod-service-account-de8b5b6e-b632-4d84-8891-441ca39b0d4b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Oct 13 10:57:15.437: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8326 pod-service-account-de8b5b6e-b632-4d84-8891-441ca39b0d4b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Oct 13 10:57:15.922: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8326 pod-service-account-de8b5b6e-b632-4d84-8891-441ca39b0d4b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:57:16.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8326" for this suite.

• [SLOW TEST:8.243 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":36,"skipped":562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:57:16.461: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:57:23.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1362" for this suite.

• [SLOW TEST:7.186 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":37,"skipped":584,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:57:23.653: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Oct 13 10:57:24.369: INFO: Pod name wrapped-volume-race-1a9cc56a-0fe1-4aa3-bed6-eb943722265c: Found 0 pods out of 5
Oct 13 10:57:29.406: INFO: Pod name wrapped-volume-race-1a9cc56a-0fe1-4aa3-bed6-eb943722265c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1a9cc56a-0fe1-4aa3-bed6-eb943722265c in namespace emptydir-wrapper-7368, will wait for the garbage collector to delete the pods
Oct 13 10:57:45.630: INFO: Deleting ReplicationController wrapped-volume-race-1a9cc56a-0fe1-4aa3-bed6-eb943722265c took: 17.096556ms
Oct 13 10:57:45.831: INFO: Terminating ReplicationController wrapped-volume-race-1a9cc56a-0fe1-4aa3-bed6-eb943722265c pods took: 200.616659ms
STEP: Creating RC which spawns configmap-volume pods
Oct 13 10:57:59.183: INFO: Pod name wrapped-volume-race-2299c55d-0a6b-4b09-b2a8-79cce8c599f8: Found 0 pods out of 5
Oct 13 10:58:04.220: INFO: Pod name wrapped-volume-race-2299c55d-0a6b-4b09-b2a8-79cce8c599f8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2299c55d-0a6b-4b09-b2a8-79cce8c599f8 in namespace emptydir-wrapper-7368, will wait for the garbage collector to delete the pods
Oct 13 10:58:20.401: INFO: Deleting ReplicationController wrapped-volume-race-2299c55d-0a6b-4b09-b2a8-79cce8c599f8 took: 20.147351ms
Oct 13 10:58:20.501: INFO: Terminating ReplicationController wrapped-volume-race-2299c55d-0a6b-4b09-b2a8-79cce8c599f8 pods took: 100.68083ms
STEP: Creating RC which spawns configmap-volume pods
Oct 13 10:58:37.392: INFO: Pod name wrapped-volume-race-8caefe4a-9a24-4585-a1ed-f160f940370c: Found 0 pods out of 5
Oct 13 10:58:42.421: INFO: Pod name wrapped-volume-race-8caefe4a-9a24-4585-a1ed-f160f940370c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8caefe4a-9a24-4585-a1ed-f160f940370c in namespace emptydir-wrapper-7368, will wait for the garbage collector to delete the pods
Oct 13 10:58:58.600: INFO: Deleting ReplicationController wrapped-volume-race-8caefe4a-9a24-4585-a1ed-f160f940370c took: 21.978309ms
Oct 13 10:58:58.701: INFO: Terminating ReplicationController wrapped-volume-race-8caefe4a-9a24-4585-a1ed-f160f940370c pods took: 101.407357ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:59:18.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7368" for this suite.

• [SLOW TEST:114.475 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":38,"skipped":599,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:59:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-16695be3-319f-4976-9cfc-1980f478deed
STEP: Creating a pod to test consume configMaps
Oct 13 10:59:18.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2" in namespace "configmap-4001" to be "Succeeded or Failed"
Oct 13 10:59:18.265: INFO: Pod "pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57664ms
Oct 13 10:59:20.283: INFO: Pod "pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027349967s
Oct 13 10:59:22.300: INFO: Pod "pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044610499s
Oct 13 10:59:24.313: INFO: Pod "pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057565479s
STEP: Saw pod success
Oct 13 10:59:24.313: INFO: Pod "pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2" satisfied condition "Succeeded or Failed"
Oct 13 10:59:24.323: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 10:59:24.404: INFO: Waiting for pod pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2 to disappear
Oct 13 10:59:24.420: INFO: Pod pod-configmaps-cd3001c0-06a8-4133-ac1c-35251deb22b2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:59:24.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4001" for this suite.

• [SLOW TEST:6.316 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":39,"skipped":618,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:59:24.451: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Oct 13 10:59:24.555: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Oct 13 10:59:24.568: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 13 10:59:24.568: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Oct 13 10:59:24.587: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Oct 13 10:59:24.587: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Oct 13 10:59:24.604: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Oct 13 10:59:24.604: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Oct 13 10:59:31.708: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:59:31.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2150" for this suite.

• [SLOW TEST:7.322 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":40,"skipped":630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:59:31.783: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-613
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-613 to expose endpoints map[]
Oct 13 10:59:31.922: INFO: successfully validated that service multi-endpoint-test in namespace services-613 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-613
Oct 13 10:59:31.957: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:59:33.972: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:59:35.979: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-613 to expose endpoints map[pod1:[100]]
Oct 13 10:59:36.027: INFO: successfully validated that service multi-endpoint-test in namespace services-613 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-613
Oct 13 10:59:36.053: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:59:38.072: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 10:59:40.070: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-613 to expose endpoints map[pod1:[100] pod2:[101]]
Oct 13 10:59:40.125: INFO: successfully validated that service multi-endpoint-test in namespace services-613 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-613
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-613 to expose endpoints map[pod2:[101]]
Oct 13 10:59:40.191: INFO: successfully validated that service multi-endpoint-test in namespace services-613 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-613
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-613 to expose endpoints map[]
Oct 13 10:59:40.245: INFO: successfully validated that service multi-endpoint-test in namespace services-613 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 10:59:40.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-613" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:8.527 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":41,"skipped":659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 10:59:40.311: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-wtsn
STEP: Creating a pod to test atomic-volume-subpath
Oct 13 10:59:40.438: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wtsn" in namespace "subpath-8793" to be "Succeeded or Failed"
Oct 13 10:59:40.448: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073237ms
Oct 13 10:59:42.465: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026488538s
Oct 13 10:59:44.491: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052320342s
Oct 13 10:59:46.510: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 6.071727388s
Oct 13 10:59:48.539: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 8.10072361s
Oct 13 10:59:50.557: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 10.119072319s
Oct 13 10:59:52.575: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 12.13631544s
Oct 13 10:59:54.596: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 14.157650002s
Oct 13 10:59:56.615: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 16.176753883s
Oct 13 10:59:58.632: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 18.193759263s
Oct 13 11:00:00.650: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 20.211171657s
Oct 13 11:00:02.669: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 22.230444652s
Oct 13 11:00:04.684: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Running", Reason="", readiness=true. Elapsed: 24.245347271s
Oct 13 11:00:06.697: INFO: Pod "pod-subpath-test-secret-wtsn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.258175484s
STEP: Saw pod success
Oct 13 11:00:06.697: INFO: Pod "pod-subpath-test-secret-wtsn" satisfied condition "Succeeded or Failed"
Oct 13 11:00:06.708: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-subpath-test-secret-wtsn container test-container-subpath-secret-wtsn: <nil>
STEP: delete the pod
Oct 13 11:00:06.770: INFO: Waiting for pod pod-subpath-test-secret-wtsn to disappear
Oct 13 11:00:06.780: INFO: Pod pod-subpath-test-secret-wtsn no longer exists
STEP: Deleting pod pod-subpath-test-secret-wtsn
Oct 13 11:00:06.780: INFO: Deleting pod "pod-subpath-test-secret-wtsn" in namespace "subpath-8793"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:00:06.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8793" for this suite.

• [SLOW TEST:26.511 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":42,"skipped":698,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:00:06.822: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-4a5f40b8-1765-4e2b-9ae6-1cb03d74885f
STEP: Creating a pod to test consume secrets
Oct 13 11:00:06.938: INFO: Waiting up to 5m0s for pod "pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923" in namespace "secrets-2863" to be "Succeeded or Failed"
Oct 13 11:00:06.950: INFO: Pod "pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923": Phase="Pending", Reason="", readiness=false. Elapsed: 11.219058ms
Oct 13 11:00:08.972: INFO: Pod "pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033829623s
Oct 13 11:00:10.994: INFO: Pod "pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055347573s
STEP: Saw pod success
Oct 13 11:00:10.994: INFO: Pod "pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923" satisfied condition "Succeeded or Failed"
Oct 13 11:00:11.005: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923 container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:00:11.111: INFO: Waiting for pod pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923 to disappear
Oct 13 11:00:11.120: INFO: Pod pod-secrets-e4111b02-1a72-48f7-b1d6-de1fe00bd923 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:00:11.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2863" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":43,"skipped":698,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:00:11.150: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 13 11:00:11.267: INFO: The status of Pod annotationupdateda82fa0e-1e03-4f58-9910-f5050c90ae2d is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:00:13.287: INFO: The status of Pod annotationupdateda82fa0e-1e03-4f58-9910-f5050c90ae2d is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:00:15.288: INFO: The status of Pod annotationupdateda82fa0e-1e03-4f58-9910-f5050c90ae2d is Running (Ready = true)
Oct 13 11:00:15.905: INFO: Successfully updated pod "annotationupdateda82fa0e-1e03-4f58-9910-f5050c90ae2d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:00:20.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4856" for this suite.

• [SLOW TEST:8.904 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":44,"skipped":702,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:00:20.055: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 13 11:00:20.187: INFO: The status of Pod pod-update-b53327a0-549e-44b5-975c-82f6fec7b4a8 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:00:22.203: INFO: The status of Pod pod-update-b53327a0-549e-44b5-975c-82f6fec7b4a8 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:00:24.208: INFO: The status of Pod pod-update-b53327a0-549e-44b5-975c-82f6fec7b4a8 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 13 11:00:24.771: INFO: Successfully updated pod "pod-update-b53327a0-549e-44b5-975c-82f6fec7b4a8"
STEP: verifying the updated pod is in kubernetes
Oct 13 11:00:24.796: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:00:24.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-401" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":45,"skipped":708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:00:24.835: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3874, will wait for the garbage collector to delete the pods
Oct 13 11:00:31.050: INFO: Deleting Job.batch foo took: 16.605047ms
Oct 13 11:00:31.150: INFO: Terminating Job.batch foo pods took: 100.81415ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:01:17.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3874" for this suite.

• [SLOW TEST:52.369 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":46,"skipped":734,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:01:17.215: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:01:17.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc" in namespace "downward-api-8635" to be "Succeeded or Failed"
Oct 13 11:01:17.327: INFO: Pod "downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.20166ms
Oct 13 11:01:19.346: INFO: Pod "downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026627827s
Oct 13 11:01:21.369: INFO: Pod "downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049429144s
STEP: Saw pod success
Oct 13 11:01:21.369: INFO: Pod "downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc" satisfied condition "Succeeded or Failed"
Oct 13 11:01:21.378: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc container client-container: <nil>
STEP: delete the pod
Oct 13 11:01:21.447: INFO: Waiting for pod downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc to disappear
Oct 13 11:01:21.457: INFO: Pod downwardapi-volume-f9cde9b5-41a8-4978-a5b7-83c262502afc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:01:21.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8635" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":47,"skipped":751,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:01:21.496: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:01:21.614: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a" in namespace "security-context-test-9653" to be "Succeeded or Failed"
Oct 13 11:01:21.626: INFO: Pod "busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.809304ms
Oct 13 11:01:23.645: INFO: Pod "busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030747785s
Oct 13 11:01:25.663: INFO: Pod "busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048671799s
Oct 13 11:01:25.663: INFO: Pod "busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a" satisfied condition "Succeeded or Failed"
Oct 13 11:01:25.882: INFO: Got logs for pod "busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:01:25.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9653" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":48,"skipped":753,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:01:25.918: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 13 11:01:26.013: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 13 11:01:26.041: INFO: Waiting for terminating namespaces to be deleted...
Oct 13 11:01:26.052: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf before test
Oct 13 11:01:26.093: INFO: canal-b5jhz from kube-system started at 2021-10-12 14:23:37 +0000 UTC (2 container statuses recorded)
Oct 13 11:01:26.093: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 11:01:26.094: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 11:01:26.094: INFO: csi-cinder-nodeplugin-2p26p from kube-system started at 2021-10-12 14:23:37 +0000 UTC (3 container statuses recorded)
Oct 13 11:01:26.094: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:01:26.094: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:01:26.094: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 11:01:26.094: INFO: kube-proxy-7p6hr from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 11:01:26.095: INFO: node-exporter-qjdpc from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 11:01:26.095: INFO: node-local-dns-tsflg from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 11:01:26.095: INFO: syseleven-node-problem-detector-rqc8m from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 11:01:26.095: INFO: user-ssh-keys-agent-mnrhx from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 11:01:26.095: INFO: sonobuoy from sonobuoy started at 2021-10-13 10:42:23 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 13 11:01:26.095: INFO: sonobuoy-e2e-job-c24ca988246e4377 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container e2e ready: true, restart count 0
Oct 13 11:01:26.095: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:01:26.095: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-qvznk from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:01:26.095: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 13 11:01:26.095: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-57pxk from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.095: INFO: 	Container controller ready: true, restart count 0
Oct 13 11:01:26.095: INFO: syseleven-ingress-nginx-ingress-defaultbackend-7b7489f769-xc4lv from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.096: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Oct 13 11:01:26.096: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr before test
Oct 13 11:01:26.127: INFO: canal-zmkkq from kube-system started at 2021-10-12 14:17:59 +0000 UTC (2 container statuses recorded)
Oct 13 11:01:26.127: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 11:01:26.127: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 11:01:26.127: INFO: cluster-autoscaler-74b76d7f55-xqfpn from kube-system started at 2021-10-12 14:19:42 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.127: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Oct 13 11:01:26.127: INFO: coredns-7df5db5d6-qm65b from kube-system started at 2021-10-12 14:18:40 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.128: INFO: 	Container coredns ready: true, restart count 0
Oct 13 11:01:26.128: INFO: coredns-7df5db5d6-rztzx from kube-system started at 2021-10-12 14:18:40 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.128: INFO: 	Container coredns ready: true, restart count 0
Oct 13 11:01:26.128: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-10-12 14:18:40 +0000 UTC (6 container statuses recorded)
Oct 13 11:01:26.128: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:01:26.128: INFO: 	Container csi-attacher ready: true, restart count 0
Oct 13 11:01:26.128: INFO: 	Container csi-provisioner ready: true, restart count 0
Oct 13 11:01:26.128: INFO: 	Container csi-resizer ready: true, restart count 0
Oct 13 11:01:26.128: INFO: 	Container csi-snapshotter ready: true, restart count 0
Oct 13 11:01:26.129: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:01:26.129: INFO: csi-cinder-nodeplugin-2wxbc from kube-system started at 2021-10-12 14:18:00 +0000 UTC (3 container statuses recorded)
Oct 13 11:01:26.129: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:01:26.129: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:01:26.129: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 11:01:26.129: INFO: dns-autoscaler-6cc9cd7f9f-p7cpb from kube-system started at 2021-10-12 14:19:42 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.129: INFO: 	Container autoscaler ready: true, restart count 0
Oct 13 11:01:26.129: INFO: kube-proxy-ft8xt from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.129: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 11:01:26.129: INFO: node-exporter-jxsls from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.129: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 11:01:26.130: INFO: node-local-dns-ct8kv from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.130: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 11:01:26.130: INFO: openvpn-client-58d7dddf79-7nhns from kube-system started at 2021-10-12 14:18:40 +0000 UTC (2 container statuses recorded)
Oct 13 11:01:26.130: INFO: 	Container dnat-controller ready: true, restart count 0
Oct 13 11:01:26.130: INFO: 	Container openvpn-client ready: true, restart count 0
Oct 13 11:01:26.130: INFO: syseleven-node-problem-detector-2dvkn from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.130: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 11:01:26.130: INFO: user-ssh-keys-agent-2kfhw from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.130: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 11:01:26.130: INFO: busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a from security-context-test-9653 started at 2021-10-13 11:01:21 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.130: INFO: 	Container busybox-privileged-false-56e1834c-57e2-46d8-89b3-51fd3132be0a ready: false, restart count 0
Oct 13 11:01:26.131: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-fpst9 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:01:26.131: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:01:26.131: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 13 11:01:26.131: INFO: helm-operator-6b9895c4c5-nbgvj from syseleven-helm-operator started at 2021-10-12 14:21:27 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.131: INFO: 	Container helm-operator ready: true, restart count 0
Oct 13 11:01:26.131: INFO: syseleven-helm-exporter-66c559868f-v79j4 from syseleven-helm-operator started at 2021-10-12 14:21:43 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.131: INFO: 	Container helm-exporter ready: true, restart count 0
Oct 13 11:01:26.131: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-2d4dv from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:01:26.131: INFO: 	Container controller ready: true, restart count 0
Oct 13 11:01:26.131: INFO: kubernetes-dashboard-6b6956d96c-sl4th from syseleven-kubernetes-dashboard started at 2021-10-12 14:21:43 +0000 UTC (2 container statuses recorded)
Oct 13 11:01:26.132: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 13 11:01:26.132: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6aab7a27-eda4-44d2-a66a-778fa43ba6bf 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.1.246 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6aab7a27-eda4-44d2-a66a-778fa43ba6bf off the node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6aab7a27-eda4-44d2-a66a-778fa43ba6bf
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:06:34.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2369" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.565 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":49,"skipped":757,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:06:34.484: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-a115c989-fcf5-4221-84d7-3561a21eabc1
STEP: Creating secret with name s-test-opt-upd-9aeb2b6a-b1b4-4d21-b9ff-cef8afb89d7a
STEP: Creating the pod
Oct 13 11:06:34.650: INFO: The status of Pod pod-secrets-714976aa-035e-43c5-b25f-c8b216178b38 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:06:36.670: INFO: The status of Pod pod-secrets-714976aa-035e-43c5-b25f-c8b216178b38 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:06:38.666: INFO: The status of Pod pod-secrets-714976aa-035e-43c5-b25f-c8b216178b38 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:06:40.662: INFO: The status of Pod pod-secrets-714976aa-035e-43c5-b25f-c8b216178b38 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-a115c989-fcf5-4221-84d7-3561a21eabc1
STEP: Updating secret s-test-opt-upd-9aeb2b6a-b1b4-4d21-b9ff-cef8afb89d7a
STEP: Creating secret with name s-test-opt-create-2772db75-dbf2-420b-a06d-2e3daa2ffbcf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:08:00.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9167" for this suite.

• [SLOW TEST:86.236 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":50,"skipped":770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:08:00.728: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Oct 13 11:08:06.966: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:08:06.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1013 11:08:06.966666      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1013 11:08:06.966695      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1013 11:08:06.966698      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5708" for this suite.

• [SLOW TEST:6.273 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":51,"skipped":796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:08:07.024: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:08:07.106: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 13 11:08:07.139: INFO: The status of Pod pod-logs-websocket-7f596435-1c34-4f79-b77b-f3644b89342c is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:08:09.161: INFO: The status of Pod pod-logs-websocket-7f596435-1c34-4f79-b77b-f3644b89342c is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:08:11.152: INFO: The status of Pod pod-logs-websocket-7f596435-1c34-4f79-b77b-f3644b89342c is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:08:13.149: INFO: The status of Pod pod-logs-websocket-7f596435-1c34-4f79-b77b-f3644b89342c is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:08:15.163: INFO: The status of Pod pod-logs-websocket-7f596435-1c34-4f79-b77b-f3644b89342c is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:08:15.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3831" for this suite.

• [SLOW TEST:8.287 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":52,"skipped":831,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:08:15.314: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:08:15.877: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 11:08:17.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769720095, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769720095, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769720095, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769720095, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:08:20.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:08:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7354" for this suite.
STEP: Destroying namespace "webhook-7354-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.824 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":53,"skipped":849,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:08:21.139: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Oct 13 11:08:21.272: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6  1845bc18-2c96-464b-915e-32baece104ae 379045 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:08:21.272: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6  1845bc18-2c96-464b-915e-32baece104ae 379046 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Oct 13 11:08:21.316: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6  1845bc18-2c96-464b-915e-32baece104ae 379047 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:08:21.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6  1845bc18-2c96-464b-915e-32baece104ae 379048 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:08:21.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":54,"skipped":857,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:08:21.352: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Oct 13 11:08:21.464: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379058 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:08:21.464: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379058 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Oct 13 11:08:31.498: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379135 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:08:31.499: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379135 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Oct 13 11:08:41.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379177 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:08:41.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379177 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Oct 13 11:08:51.553: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379218 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:08:51.553: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2210  393c38fd-e3de-4929-b661-03f5ca99dec9 379218 0 2021-10-13 11:08:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-10-13 11:08:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Oct 13 11:09:01.582: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2210  ff391903-a98d-4885-b039-f1af9c2fd3fe 379264 0 2021-10-13 11:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-13 11:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:09:01.582: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2210  ff391903-a98d-4885-b039-f1af9c2fd3fe 379264 0 2021-10-13 11:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-13 11:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Oct 13 11:09:11.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2210  ff391903-a98d-4885-b039-f1af9c2fd3fe 379303 0 2021-10-13 11:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-13 11:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:09:11.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2210  ff391903-a98d-4885-b039-f1af9c2fd3fe 379303 0 2021-10-13 11:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-10-13 11:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:09:21.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2210" for this suite.

• [SLOW TEST:60.291 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":55,"skipped":861,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:09:21.649: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-42f5bf78-c9f8-42d7-87cb-21adb5d24f0a
STEP: Creating a pod to test consume configMaps
Oct 13 11:09:21.752: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b" in namespace "projected-727" to be "Succeeded or Failed"
Oct 13 11:09:21.762: INFO: Pod "pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.376637ms
Oct 13 11:09:23.778: INFO: Pod "pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025914348s
Oct 13 11:09:25.796: INFO: Pod "pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044591361s
STEP: Saw pod success
Oct 13 11:09:25.796: INFO: Pod "pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b" satisfied condition "Succeeded or Failed"
Oct 13 11:09:25.810: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b container agnhost-container: <nil>
STEP: delete the pod
Oct 13 11:09:25.906: INFO: Waiting for pod pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b to disappear
Oct 13 11:09:25.917: INFO: Pod pod-projected-configmaps-d747ecac-9d6b-4e20-864e-213ecbb0053b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:09:25.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-727" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":56,"skipped":873,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:09:25.956: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-1fbb8ece-e38c-4c45-b784-7a16d40a108c in namespace container-probe-2064
Oct 13 11:09:30.097: INFO: Started pod busybox-1fbb8ece-e38c-4c45-b784-7a16d40a108c in namespace container-probe-2064
STEP: checking the pod's current state and verifying that restartCount is present
Oct 13 11:09:30.109: INFO: Initial restart count of pod busybox-1fbb8ece-e38c-4c45-b784-7a16d40a108c is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:13:30.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2064" for this suite.

• [SLOW TEST:244.475 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":57,"skipped":876,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:13:30.431: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:13:30.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f" in namespace "projected-8883" to be "Succeeded or Failed"
Oct 13 11:13:30.615: INFO: Pod "downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.210748ms
Oct 13 11:13:32.629: INFO: Pod "downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025851775s
Oct 13 11:13:34.645: INFO: Pod "downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041562866s
STEP: Saw pod success
Oct 13 11:13:34.645: INFO: Pod "downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f" satisfied condition "Succeeded or Failed"
Oct 13 11:13:34.657: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f container client-container: <nil>
STEP: delete the pod
Oct 13 11:13:34.753: INFO: Waiting for pod downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f to disappear
Oct 13 11:13:34.762: INFO: Pod downwardapi-volume-2b6b773c-c79e-46f3-acd6-02efb03f640f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:13:34.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8883" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":58,"skipped":887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:13:34.803: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 13 11:13:34.916: INFO: Waiting up to 5m0s for pod "pod-07d70029-ba43-4d53-a28f-47950088e7cd" in namespace "emptydir-3066" to be "Succeeded or Failed"
Oct 13 11:13:34.930: INFO: Pod "pod-07d70029-ba43-4d53-a28f-47950088e7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.278464ms
Oct 13 11:13:36.950: INFO: Pod "pod-07d70029-ba43-4d53-a28f-47950088e7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033050483s
Oct 13 11:13:38.969: INFO: Pod "pod-07d70029-ba43-4d53-a28f-47950088e7cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052174471s
STEP: Saw pod success
Oct 13 11:13:38.969: INFO: Pod "pod-07d70029-ba43-4d53-a28f-47950088e7cd" satisfied condition "Succeeded or Failed"
Oct 13 11:13:38.978: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-07d70029-ba43-4d53-a28f-47950088e7cd container test-container: <nil>
STEP: delete the pod
Oct 13 11:13:39.174: INFO: Waiting for pod pod-07d70029-ba43-4d53-a28f-47950088e7cd to disappear
Oct 13 11:13:39.187: INFO: Pod pod-07d70029-ba43-4d53-a28f-47950088e7cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:13:39.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3066" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":59,"skipped":909,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:13:39.221: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-59f533a2-6bf8-471b-9d2a-da1f2c75a844
STEP: Creating the pod
Oct 13 11:13:39.381: INFO: The status of Pod pod-configmaps-0d69b0f0-df13-4ada-9445-f2a829d72333 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:13:41.403: INFO: The status of Pod pod-configmaps-0d69b0f0-df13-4ada-9445-f2a829d72333 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:13:43.399: INFO: The status of Pod pod-configmaps-0d69b0f0-df13-4ada-9445-f2a829d72333 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-59f533a2-6bf8-471b-9d2a-da1f2c75a844
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:13:47.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3438" for this suite.

• [SLOW TEST:8.415 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":60,"skipped":924,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:13:47.646: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 13 11:13:47.866: INFO: Number of nodes with available pods: 0
Oct 13 11:13:47.866: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:13:48.897: INFO: Number of nodes with available pods: 0
Oct 13 11:13:48.897: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:13:49.898: INFO: Number of nodes with available pods: 0
Oct 13 11:13:49.898: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:13:50.900: INFO: Number of nodes with available pods: 0
Oct 13 11:13:50.900: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:13:51.901: INFO: Number of nodes with available pods: 2
Oct 13 11:13:51.901: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Oct 13 11:13:52.010: INFO: Number of nodes with available pods: 1
Oct 13 11:13:52.010: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:13:53.036: INFO: Number of nodes with available pods: 1
Oct 13 11:13:53.036: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:13:54.038: INFO: Number of nodes with available pods: 1
Oct 13 11:13:54.039: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:13:55.036: INFO: Number of nodes with available pods: 1
Oct 13 11:13:55.036: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:13:56.049: INFO: Number of nodes with available pods: 1
Oct 13 11:13:56.049: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:13:57.044: INFO: Number of nodes with available pods: 1
Oct 13 11:13:57.044: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:13:58.043: INFO: Number of nodes with available pods: 1
Oct 13 11:13:58.043: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:13:59.036: INFO: Number of nodes with available pods: 1
Oct 13 11:13:59.037: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:00.047: INFO: Number of nodes with available pods: 1
Oct 13 11:14:00.047: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:01.045: INFO: Number of nodes with available pods: 1
Oct 13 11:14:01.045: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:02.059: INFO: Number of nodes with available pods: 1
Oct 13 11:14:02.059: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:03.035: INFO: Number of nodes with available pods: 1
Oct 13 11:14:03.035: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:04.038: INFO: Number of nodes with available pods: 1
Oct 13 11:14:04.038: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:05.040: INFO: Number of nodes with available pods: 1
Oct 13 11:14:05.040: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:06.045: INFO: Number of nodes with available pods: 1
Oct 13 11:14:06.045: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:07.038: INFO: Number of nodes with available pods: 1
Oct 13 11:14:07.038: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:08.049: INFO: Number of nodes with available pods: 1
Oct 13 11:14:08.049: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:09.044: INFO: Number of nodes with available pods: 1
Oct 13 11:14:09.044: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:10.043: INFO: Number of nodes with available pods: 1
Oct 13 11:14:10.044: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 11:14:11.034: INFO: Number of nodes with available pods: 2
Oct 13 11:14:11.034: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5950, will wait for the garbage collector to delete the pods
Oct 13 11:14:11.126: INFO: Deleting DaemonSet.extensions daemon-set took: 16.678877ms
Oct 13 11:14:11.226: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.434453ms
Oct 13 11:14:18.953: INFO: Number of nodes with available pods: 0
Oct 13 11:14:18.953: INFO: Number of running nodes: 0, number of available pods: 0
Oct 13 11:14:18.968: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"380789"},"items":null}

Oct 13 11:14:18.980: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"380789"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:14:19.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5950" for this suite.

• [SLOW TEST:31.411 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":61,"skipped":943,"failed":0}
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:14:19.057: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-465e4365-338d-4278-a77b-5831277c1b9a
STEP: Creating secret with name secret-projected-all-test-volume-4b30539b-3823-44ef-a114-070ab858c1cc
STEP: Creating a pod to test Check all projections for projected volume plugin
Oct 13 11:14:19.172: INFO: Waiting up to 5m0s for pod "projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e" in namespace "projected-3224" to be "Succeeded or Failed"
Oct 13 11:14:19.186: INFO: Pod "projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.323385ms
Oct 13 11:14:21.203: INFO: Pod "projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029391977s
Oct 13 11:14:23.222: INFO: Pod "projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048799124s
STEP: Saw pod success
Oct 13 11:14:23.222: INFO: Pod "projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e" satisfied condition "Succeeded or Failed"
Oct 13 11:14:23.232: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e container projected-all-volume-test: <nil>
STEP: delete the pod
Oct 13 11:14:23.327: INFO: Waiting for pod projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e to disappear
Oct 13 11:14:23.337: INFO: Pod projected-volume-9feae73f-e018-4af1-9e20-0f6040a4ca9e no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:14:23.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3224" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":62,"skipped":947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:14:23.377: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-3934
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3934
STEP: Creating statefulset with conflicting port in namespace statefulset-3934
STEP: Waiting until pod test-pod will start running in namespace statefulset-3934
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3934
Oct 13 11:14:29.547: INFO: Observed stateful pod in namespace: statefulset-3934, name: ss-0, uid: 49698903-a2b4-43c9-89eb-e1e93de6f808, status phase: Pending. Waiting for statefulset controller to delete.
Oct 13 11:14:29.830: INFO: Observed stateful pod in namespace: statefulset-3934, name: ss-0, uid: 49698903-a2b4-43c9-89eb-e1e93de6f808, status phase: Failed. Waiting for statefulset controller to delete.
Oct 13 11:14:29.854: INFO: Observed stateful pod in namespace: statefulset-3934, name: ss-0, uid: 49698903-a2b4-43c9-89eb-e1e93de6f808, status phase: Failed. Waiting for statefulset controller to delete.
Oct 13 11:14:29.859: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3934
STEP: Removing pod with conflicting port in namespace statefulset-3934
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3934 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Oct 13 11:14:35.950: INFO: Deleting all statefulset in ns statefulset-3934
Oct 13 11:14:35.961: INFO: Scaling statefulset ss to 0
Oct 13 11:14:46.026: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 11:14:46.039: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:14:46.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3934" for this suite.

• [SLOW TEST:22.741 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":63,"skipped":974,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:14:46.121: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:14:46.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5897" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":64,"skipped":983,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:14:46.439: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-28f8e24b-0864-4304-8f9b-d082a9370f56
STEP: Creating configMap with name cm-test-opt-upd-e048b0fb-020c-47fb-bd36-efd3c003c1b9
STEP: Creating the pod
Oct 13 11:14:46.597: INFO: The status of Pod pod-configmaps-b6d1b647-77f2-47c5-a735-770c43055480 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:14:48.613: INFO: The status of Pod pod-configmaps-b6d1b647-77f2-47c5-a735-770c43055480 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:14:50.618: INFO: The status of Pod pod-configmaps-b6d1b647-77f2-47c5-a735-770c43055480 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:14:52.611: INFO: The status of Pod pod-configmaps-b6d1b647-77f2-47c5-a735-770c43055480 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-28f8e24b-0864-4304-8f9b-d082a9370f56
STEP: Updating configmap cm-test-opt-upd-e048b0fb-020c-47fb-bd36-efd3c003c1b9
STEP: Creating configMap with name cm-test-opt-create-11b9a4c5-9fb9-4582-9d94-10176df741ec
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:16:08.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4385" for this suite.

• [SLOW TEST:82.177 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":65,"skipped":996,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:16:08.625: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-7412b04d-2660-4176-9f6f-b3f6512fe7b6
STEP: Creating a pod to test consume configMaps
Oct 13 11:16:08.766: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe" in namespace "configmap-4048" to be "Succeeded or Failed"
Oct 13 11:16:08.778: INFO: Pod "pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.956916ms
Oct 13 11:16:10.802: INFO: Pod "pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036142216s
Oct 13 11:16:12.825: INFO: Pod "pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058548316s
Oct 13 11:16:14.843: INFO: Pod "pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076567228s
STEP: Saw pod success
Oct 13 11:16:14.843: INFO: Pod "pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe" satisfied condition "Succeeded or Failed"
Oct 13 11:16:14.853: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe container configmap-volume-test: <nil>
STEP: delete the pod
Oct 13 11:16:14.953: INFO: Waiting for pod pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe to disappear
Oct 13 11:16:14.964: INFO: Pod pod-configmaps-5a4360f9-1f35-4729-8dfa-a4ea37e2cdfe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:16:14.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4048" for this suite.

• [SLOW TEST:6.383 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":66,"skipped":1009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:16:15.011: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-t7ssz in namespace proxy-6488
I1013 11:16:15.139964      20 runners.go:190] Created replication controller with name: proxy-service-t7ssz, namespace: proxy-6488, replica count: 1
I1013 11:16:16.190504      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:16:17.192639      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:16:18.193857      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:16:19.194705      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:16:20.195203      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:16:21.195575      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1013 11:16:22.196119      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1013 11:16:23.196553      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1013 11:16:24.197022      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1013 11:16:25.197521      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1013 11:16:26.198500      20 runners.go:190] proxy-service-t7ssz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 11:16:26.216: INFO: setup took 11.118780195s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Oct 13 11:16:26.238: INFO: (0) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.607282ms)
Oct 13 11:16:26.244: INFO: (0) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 26.779224ms)
Oct 13 11:16:26.244: INFO: (0) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 27.332367ms)
Oct 13 11:16:26.254: INFO: (0) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 37.235566ms)
Oct 13 11:16:26.255: INFO: (0) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 38.269873ms)
Oct 13 11:16:26.255: INFO: (0) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 38.284948ms)
Oct 13 11:16:26.255: INFO: (0) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 38.350626ms)
Oct 13 11:16:26.256: INFO: (0) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 38.633899ms)
Oct 13 11:16:26.256: INFO: (0) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 39.300541ms)
Oct 13 11:16:26.256: INFO: (0) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 39.722222ms)
Oct 13 11:16:26.256: INFO: (0) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 39.670452ms)
Oct 13 11:16:26.257: INFO: (0) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 40.512635ms)
Oct 13 11:16:26.257: INFO: (0) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 40.928915ms)
Oct 13 11:16:26.263: INFO: (0) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 45.570224ms)
Oct 13 11:16:26.263: INFO: (0) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 46.009876ms)
Oct 13 11:16:26.263: INFO: (0) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 46.537676ms)
Oct 13 11:16:26.285: INFO: (1) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 20.649997ms)
Oct 13 11:16:26.285: INFO: (1) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 20.897366ms)
Oct 13 11:16:26.286: INFO: (1) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 21.681247ms)
Oct 13 11:16:26.286: INFO: (1) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.493193ms)
Oct 13 11:16:26.286: INFO: (1) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 20.84417ms)
Oct 13 11:16:26.286: INFO: (1) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 21.627973ms)
Oct 13 11:16:26.289: INFO: (1) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 23.948704ms)
Oct 13 11:16:26.289: INFO: (1) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 24.970594ms)
Oct 13 11:16:26.292: INFO: (1) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 28.604143ms)
Oct 13 11:16:26.292: INFO: (1) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 27.591095ms)
Oct 13 11:16:26.293: INFO: (1) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 27.812244ms)
Oct 13 11:16:26.294: INFO: (1) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 29.766834ms)
Oct 13 11:16:26.297: INFO: (1) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 32.729674ms)
Oct 13 11:16:26.301: INFO: (1) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 37.063803ms)
Oct 13 11:16:26.302: INFO: (1) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 36.858522ms)
Oct 13 11:16:26.305: INFO: (1) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 39.868936ms)
Oct 13 11:16:26.327: INFO: (2) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.659395ms)
Oct 13 11:16:26.327: INFO: (2) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 22.031581ms)
Oct 13 11:16:26.327: INFO: (2) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 21.898593ms)
Oct 13 11:16:26.328: INFO: (2) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 22.617698ms)
Oct 13 11:16:26.328: INFO: (2) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 22.173007ms)
Oct 13 11:16:26.328: INFO: (2) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 22.762064ms)
Oct 13 11:16:26.328: INFO: (2) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 22.748817ms)
Oct 13 11:16:26.331: INFO: (2) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 25.813335ms)
Oct 13 11:16:26.332: INFO: (2) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 26.599635ms)
Oct 13 11:16:26.332: INFO: (2) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 26.648117ms)
Oct 13 11:16:26.332: INFO: (2) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 27.013743ms)
Oct 13 11:16:26.334: INFO: (2) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 27.971024ms)
Oct 13 11:16:26.339: INFO: (2) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 33.262713ms)
Oct 13 11:16:26.339: INFO: (2) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 34.162177ms)
Oct 13 11:16:26.340: INFO: (2) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 34.999729ms)
Oct 13 11:16:26.341: INFO: (2) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 35.469529ms)
Oct 13 11:16:26.363: INFO: (3) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.973149ms)
Oct 13 11:16:26.364: INFO: (3) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 21.309642ms)
Oct 13 11:16:26.363: INFO: (3) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 22.289692ms)
Oct 13 11:16:26.364: INFO: (3) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 21.414275ms)
Oct 13 11:16:26.365: INFO: (3) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.926672ms)
Oct 13 11:16:26.365: INFO: (3) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 22.157133ms)
Oct 13 11:16:26.365: INFO: (3) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 22.838948ms)
Oct 13 11:16:26.366: INFO: (3) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 23.035191ms)
Oct 13 11:16:26.366: INFO: (3) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 24.321213ms)
Oct 13 11:16:26.366: INFO: (3) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 23.776713ms)
Oct 13 11:16:26.369: INFO: (3) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 27.289162ms)
Oct 13 11:16:26.373: INFO: (3) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 30.590534ms)
Oct 13 11:16:26.373: INFO: (3) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 31.676748ms)
Oct 13 11:16:26.373: INFO: (3) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 30.74856ms)
Oct 13 11:16:26.376: INFO: (3) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 33.221542ms)
Oct 13 11:16:26.376: INFO: (3) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 33.949359ms)
Oct 13 11:16:26.437: INFO: (4) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 60.292588ms)
Oct 13 11:16:26.437: INFO: (4) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 60.523345ms)
Oct 13 11:16:26.437: INFO: (4) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 61.110284ms)
Oct 13 11:16:26.440: INFO: (4) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 63.540046ms)
Oct 13 11:16:26.440: INFO: (4) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 63.519369ms)
Oct 13 11:16:26.440: INFO: (4) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 63.429799ms)
Oct 13 11:16:26.441: INFO: (4) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 64.546323ms)
Oct 13 11:16:26.442: INFO: (4) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 65.054446ms)
Oct 13 11:16:26.442: INFO: (4) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 64.619932ms)
Oct 13 11:16:26.442: INFO: (4) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 65.520956ms)
Oct 13 11:16:26.445: INFO: (4) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 68.693416ms)
Oct 13 11:16:26.445: INFO: (4) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 68.651078ms)
Oct 13 11:16:26.446: INFO: (4) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 69.611937ms)
Oct 13 11:16:26.446: INFO: (4) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 69.499149ms)
Oct 13 11:16:26.448: INFO: (4) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 71.435165ms)
Oct 13 11:16:26.448: INFO: (4) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 71.429775ms)
Oct 13 11:16:26.467: INFO: (5) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 18.749227ms)
Oct 13 11:16:26.467: INFO: (5) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 18.7564ms)
Oct 13 11:16:26.470: INFO: (5) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.028063ms)
Oct 13 11:16:26.470: INFO: (5) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 21.401678ms)
Oct 13 11:16:26.470: INFO: (5) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 21.752782ms)
Oct 13 11:16:26.474: INFO: (5) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 25.455514ms)
Oct 13 11:16:26.474: INFO: (5) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 25.341302ms)
Oct 13 11:16:26.474: INFO: (5) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 25.481449ms)
Oct 13 11:16:26.475: INFO: (5) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 25.433773ms)
Oct 13 11:16:26.475: INFO: (5) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 25.327636ms)
Oct 13 11:16:26.478: INFO: (5) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 28.652709ms)
Oct 13 11:16:26.478: INFO: (5) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 28.711684ms)
Oct 13 11:16:26.482: INFO: (5) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 33.776447ms)
Oct 13 11:16:26.487: INFO: (5) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 38.516081ms)
Oct 13 11:16:26.488: INFO: (5) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 38.401351ms)
Oct 13 11:16:26.488: INFO: (5) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 38.978982ms)
Oct 13 11:16:26.512: INFO: (6) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 24.384436ms)
Oct 13 11:16:26.512: INFO: (6) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 24.3694ms)
Oct 13 11:16:26.512: INFO: (6) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 23.760464ms)
Oct 13 11:16:26.512: INFO: (6) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 24.312759ms)
Oct 13 11:16:26.517: INFO: (6) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 29.361295ms)
Oct 13 11:16:26.517: INFO: (6) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 29.001724ms)
Oct 13 11:16:26.518: INFO: (6) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 29.438046ms)
Oct 13 11:16:26.518: INFO: (6) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 29.275397ms)
Oct 13 11:16:26.518: INFO: (6) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 29.251495ms)
Oct 13 11:16:26.518: INFO: (6) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 29.420049ms)
Oct 13 11:16:26.521: INFO: (6) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 32.758255ms)
Oct 13 11:16:26.522: INFO: (6) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 33.553818ms)
Oct 13 11:16:26.522: INFO: (6) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 33.453669ms)
Oct 13 11:16:26.522: INFO: (6) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 33.500021ms)
Oct 13 11:16:26.522: INFO: (6) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 34.19004ms)
Oct 13 11:16:26.528: INFO: (6) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 39.924387ms)
Oct 13 11:16:26.550: INFO: (7) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 22.139327ms)
Oct 13 11:16:26.551: INFO: (7) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 22.594384ms)
Oct 13 11:16:26.551: INFO: (7) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.938672ms)
Oct 13 11:16:26.551: INFO: (7) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 22.159269ms)
Oct 13 11:16:26.551: INFO: (7) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 22.839714ms)
Oct 13 11:16:26.552: INFO: (7) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 22.897079ms)
Oct 13 11:16:26.553: INFO: (7) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 24.714893ms)
Oct 13 11:16:26.557: INFO: (7) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 28.736402ms)
Oct 13 11:16:26.557: INFO: (7) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 28.527126ms)
Oct 13 11:16:26.557: INFO: (7) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 28.663439ms)
Oct 13 11:16:26.558: INFO: (7) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 28.613703ms)
Oct 13 11:16:26.558: INFO: (7) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 28.836045ms)
Oct 13 11:16:26.558: INFO: (7) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 28.945022ms)
Oct 13 11:16:26.601: INFO: (7) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 72.710229ms)
Oct 13 11:16:26.601: INFO: (7) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 72.822336ms)
Oct 13 11:16:26.608: INFO: (7) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 79.259841ms)
Oct 13 11:16:26.627: INFO: (8) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 17.993397ms)
Oct 13 11:16:26.628: INFO: (8) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 18.175003ms)
Oct 13 11:16:26.628: INFO: (8) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 19.530352ms)
Oct 13 11:16:26.629: INFO: (8) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 19.43538ms)
Oct 13 11:16:26.629: INFO: (8) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 20.570752ms)
Oct 13 11:16:26.629: INFO: (8) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 19.856867ms)
Oct 13 11:16:26.632: INFO: (8) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 22.901877ms)
Oct 13 11:16:26.632: INFO: (8) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 23.354193ms)
Oct 13 11:16:26.632: INFO: (8) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 22.678379ms)
Oct 13 11:16:26.632: INFO: (8) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 23.088109ms)
Oct 13 11:16:26.632: INFO: (8) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 22.847707ms)
Oct 13 11:16:26.632: INFO: (8) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 22.807854ms)
Oct 13 11:16:26.633: INFO: (8) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 23.635284ms)
Oct 13 11:16:26.633: INFO: (8) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 25.080755ms)
Oct 13 11:16:26.638: INFO: (8) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 28.199667ms)
Oct 13 11:16:26.641: INFO: (8) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 31.378974ms)
Oct 13 11:16:26.664: INFO: (9) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 22.39605ms)
Oct 13 11:16:26.664: INFO: (9) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 21.94605ms)
Oct 13 11:16:26.664: INFO: (9) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 21.829831ms)
Oct 13 11:16:26.664: INFO: (9) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 21.567426ms)
Oct 13 11:16:26.664: INFO: (9) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 22.171212ms)
Oct 13 11:16:26.664: INFO: (9) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 21.688761ms)
Oct 13 11:16:26.668: INFO: (9) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 26.383306ms)
Oct 13 11:16:26.668: INFO: (9) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 26.791227ms)
Oct 13 11:16:26.669: INFO: (9) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 27.250164ms)
Oct 13 11:16:26.669: INFO: (9) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 26.462212ms)
Oct 13 11:16:26.669: INFO: (9) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 27.355195ms)
Oct 13 11:16:26.669: INFO: (9) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 27.302983ms)
Oct 13 11:16:26.674: INFO: (9) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 31.399595ms)
Oct 13 11:16:26.678: INFO: (9) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 35.586897ms)
Oct 13 11:16:26.679: INFO: (9) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 37.017759ms)
Oct 13 11:16:26.679: INFO: (9) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 37.17763ms)
Oct 13 11:16:26.704: INFO: (10) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 24.927198ms)
Oct 13 11:16:26.705: INFO: (10) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 25.07818ms)
Oct 13 11:16:26.705: INFO: (10) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 24.681317ms)
Oct 13 11:16:26.705: INFO: (10) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 25.241399ms)
Oct 13 11:16:26.705: INFO: (10) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 25.954964ms)
Oct 13 11:16:26.706: INFO: (10) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 25.823314ms)
Oct 13 11:16:26.706: INFO: (10) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 26.169882ms)
Oct 13 11:16:26.741: INFO: (10) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 61.586127ms)
Oct 13 11:16:26.741: INFO: (10) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 61.349428ms)
Oct 13 11:16:26.742: INFO: (10) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 61.841129ms)
Oct 13 11:16:26.742: INFO: (10) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 61.85378ms)
Oct 13 11:16:26.742: INFO: (10) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 62.28982ms)
Oct 13 11:16:26.746: INFO: (10) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 66.645897ms)
Oct 13 11:16:26.746: INFO: (10) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 66.413875ms)
Oct 13 11:16:26.747: INFO: (10) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 67.559045ms)
Oct 13 11:16:26.801: INFO: (10) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 121.019992ms)
Oct 13 11:16:26.826: INFO: (11) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 23.908092ms)
Oct 13 11:16:26.826: INFO: (11) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 23.800182ms)
Oct 13 11:16:26.826: INFO: (11) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 23.840254ms)
Oct 13 11:16:26.826: INFO: (11) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 23.475055ms)
Oct 13 11:16:26.826: INFO: (11) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 25.31773ms)
Oct 13 11:16:26.826: INFO: (11) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 23.527959ms)
Oct 13 11:16:26.827: INFO: (11) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 24.08532ms)
Oct 13 11:16:26.827: INFO: (11) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 23.858824ms)
Oct 13 11:16:26.827: INFO: (11) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 25.605007ms)
Oct 13 11:16:26.827: INFO: (11) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 25.344713ms)
Oct 13 11:16:26.827: INFO: (11) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 25.118026ms)
Oct 13 11:16:26.832: INFO: (11) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 30.245734ms)
Oct 13 11:16:26.833: INFO: (11) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 31.191931ms)
Oct 13 11:16:26.833: INFO: (11) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 29.83249ms)
Oct 13 11:16:26.835: INFO: (11) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 32.576761ms)
Oct 13 11:16:26.835: INFO: (11) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 32.392055ms)
Oct 13 11:16:26.856: INFO: (12) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.237729ms)
Oct 13 11:16:26.860: INFO: (12) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 24.7296ms)
Oct 13 11:16:26.861: INFO: (12) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 25.649301ms)
Oct 13 11:16:26.861: INFO: (12) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 25.685348ms)
Oct 13 11:16:26.861: INFO: (12) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 25.808902ms)
Oct 13 11:16:26.864: INFO: (12) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 29.100767ms)
Oct 13 11:16:26.864: INFO: (12) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 28.755711ms)
Oct 13 11:16:26.864: INFO: (12) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 28.599825ms)
Oct 13 11:16:26.864: INFO: (12) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 28.636028ms)
Oct 13 11:16:26.870: INFO: (12) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 34.399938ms)
Oct 13 11:16:26.870: INFO: (12) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 34.586999ms)
Oct 13 11:16:26.870: INFO: (12) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 35.03787ms)
Oct 13 11:16:26.870: INFO: (12) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 35.319306ms)
Oct 13 11:16:26.875: INFO: (12) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 39.004545ms)
Oct 13 11:16:26.876: INFO: (12) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 40.860478ms)
Oct 13 11:16:26.880: INFO: (12) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 44.347835ms)
Oct 13 11:16:26.902: INFO: (13) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 21.776153ms)
Oct 13 11:16:26.902: INFO: (13) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 22.08408ms)
Oct 13 11:16:26.902: INFO: (13) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 21.978764ms)
Oct 13 11:16:26.902: INFO: (13) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 22.171097ms)
Oct 13 11:16:26.902: INFO: (13) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 22.118726ms)
Oct 13 11:16:26.904: INFO: (13) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 23.913928ms)
Oct 13 11:16:26.904: INFO: (13) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 23.92555ms)
Oct 13 11:16:26.904: INFO: (13) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 24.051254ms)
Oct 13 11:16:26.904: INFO: (13) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 23.990073ms)
Oct 13 11:16:26.904: INFO: (13) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 24.288632ms)
Oct 13 11:16:26.908: INFO: (13) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 27.748422ms)
Oct 13 11:16:26.910: INFO: (13) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 29.609255ms)
Oct 13 11:16:26.910: INFO: (13) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 29.877485ms)
Oct 13 11:16:26.910: INFO: (13) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 29.957196ms)
Oct 13 11:16:26.910: INFO: (13) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 30.107814ms)
Oct 13 11:16:26.910: INFO: (13) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 30.401725ms)
Oct 13 11:16:26.932: INFO: (14) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 21.195083ms)
Oct 13 11:16:26.932: INFO: (14) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 21.239636ms)
Oct 13 11:16:26.932: INFO: (14) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 21.245004ms)
Oct 13 11:16:26.932: INFO: (14) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 21.686829ms)
Oct 13 11:16:26.932: INFO: (14) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 21.541131ms)
Oct 13 11:16:26.934: INFO: (14) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 23.109531ms)
Oct 13 11:16:26.934: INFO: (14) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 23.455334ms)
Oct 13 11:16:26.935: INFO: (14) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 23.88209ms)
Oct 13 11:16:26.935: INFO: (14) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 23.736182ms)
Oct 13 11:16:26.935: INFO: (14) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 24.001181ms)
Oct 13 11:16:26.937: INFO: (14) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 25.632077ms)
Oct 13 11:16:26.940: INFO: (14) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 29.678067ms)
Oct 13 11:16:26.944: INFO: (14) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 32.673671ms)
Oct 13 11:16:26.950: INFO: (14) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 39.432789ms)
Oct 13 11:16:26.950: INFO: (14) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 39.418844ms)
Oct 13 11:16:26.950: INFO: (14) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 39.581021ms)
Oct 13 11:16:26.972: INFO: (15) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 21.984341ms)
Oct 13 11:16:26.974: INFO: (15) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 23.143994ms)
Oct 13 11:16:26.974: INFO: (15) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 23.081157ms)
Oct 13 11:16:26.974: INFO: (15) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 23.134802ms)
Oct 13 11:16:26.974: INFO: (15) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 23.442421ms)
Oct 13 11:16:26.974: INFO: (15) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 23.553008ms)
Oct 13 11:16:26.975: INFO: (15) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 24.512092ms)
Oct 13 11:16:26.976: INFO: (15) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 25.256495ms)
Oct 13 11:16:26.976: INFO: (15) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 25.086927ms)
Oct 13 11:16:26.976: INFO: (15) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 25.534744ms)
Oct 13 11:16:27.017: INFO: (15) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 65.64178ms)
Oct 13 11:16:27.017: INFO: (15) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 66.270193ms)
Oct 13 11:16:27.017: INFO: (15) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 66.239122ms)
Oct 13 11:16:27.018: INFO: (15) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 66.891986ms)
Oct 13 11:16:27.020: INFO: (15) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 69.690746ms)
Oct 13 11:16:27.113: INFO: (15) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 162.540186ms)
Oct 13 11:16:27.133: INFO: (16) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 19.939659ms)
Oct 13 11:16:27.133: INFO: (16) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 19.473078ms)
Oct 13 11:16:27.134: INFO: (16) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 19.185035ms)
Oct 13 11:16:27.136: INFO: (16) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 20.964443ms)
Oct 13 11:16:27.140: INFO: (16) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 24.358023ms)
Oct 13 11:16:27.141: INFO: (16) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 25.317963ms)
Oct 13 11:16:27.142: INFO: (16) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 26.762216ms)
Oct 13 11:16:27.142: INFO: (16) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 26.391137ms)
Oct 13 11:16:27.143: INFO: (16) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 26.610299ms)
Oct 13 11:16:27.144: INFO: (16) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 28.103618ms)
Oct 13 11:16:27.143: INFO: (16) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 26.729092ms)
Oct 13 11:16:27.144: INFO: (16) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 27.774584ms)
Oct 13 11:16:27.145: INFO: (16) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 30.339276ms)
Oct 13 11:16:27.148: INFO: (16) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 31.89433ms)
Oct 13 11:16:27.148: INFO: (16) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 32.107549ms)
Oct 13 11:16:27.148: INFO: (16) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 32.92221ms)
Oct 13 11:16:27.209: INFO: (17) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 60.504684ms)
Oct 13 11:16:27.209: INFO: (17) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 60.79787ms)
Oct 13 11:16:27.213: INFO: (17) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 64.101765ms)
Oct 13 11:16:27.213: INFO: (17) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 64.972297ms)
Oct 13 11:16:27.213: INFO: (17) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 64.277912ms)
Oct 13 11:16:27.214: INFO: (17) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 64.511809ms)
Oct 13 11:16:27.213: INFO: (17) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 64.642483ms)
Oct 13 11:16:27.214: INFO: (17) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 64.884325ms)
Oct 13 11:16:27.214: INFO: (17) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 65.270421ms)
Oct 13 11:16:27.215: INFO: (17) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 65.677543ms)
Oct 13 11:16:27.215: INFO: (17) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 66.082237ms)
Oct 13 11:16:27.215: INFO: (17) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 66.099214ms)
Oct 13 11:16:27.216: INFO: (17) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 67.787325ms)
Oct 13 11:16:27.219: INFO: (17) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 69.976208ms)
Oct 13 11:16:27.226: INFO: (17) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 77.402015ms)
Oct 13 11:16:27.226: INFO: (17) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 77.73369ms)
Oct 13 11:16:27.245: INFO: (18) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 18.024962ms)
Oct 13 11:16:27.245: INFO: (18) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 17.723244ms)
Oct 13 11:16:27.245: INFO: (18) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 17.408023ms)
Oct 13 11:16:27.246: INFO: (18) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 18.301266ms)
Oct 13 11:16:27.247: INFO: (18) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 19.698671ms)
Oct 13 11:16:27.247: INFO: (18) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 19.916462ms)
Oct 13 11:16:27.247: INFO: (18) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 19.788624ms)
Oct 13 11:16:27.247: INFO: (18) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 20.495037ms)
Oct 13 11:16:27.247: INFO: (18) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 19.655394ms)
Oct 13 11:16:27.248: INFO: (18) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 20.216984ms)
Oct 13 11:16:27.251: INFO: (18) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 24.541206ms)
Oct 13 11:16:27.252: INFO: (18) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 23.770841ms)
Oct 13 11:16:27.255: INFO: (18) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 27.5587ms)
Oct 13 11:16:27.255: INFO: (18) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 27.313981ms)
Oct 13 11:16:27.255: INFO: (18) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 27.520666ms)
Oct 13 11:16:27.255: INFO: (18) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 27.627465ms)
Oct 13 11:16:27.277: INFO: (19) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 21.828724ms)
Oct 13 11:16:27.277: INFO: (19) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 21.953984ms)
Oct 13 11:16:27.277: INFO: (19) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">test<... (200; 21.880147ms)
Oct 13 11:16:27.277: INFO: (19) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz:162/proxy/: bar (200; 22.006706ms)
Oct 13 11:16:27.278: INFO: (19) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:462/proxy/: tls qux (200; 22.068336ms)
Oct 13 11:16:27.278: INFO: (19) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:1080/proxy/rewriteme">... (200; 22.407217ms)
Oct 13 11:16:27.278: INFO: (19) /api/v1/namespaces/proxy-6488/pods/http:proxy-service-t7ssz-ppdsz:160/proxy/: foo (200; 22.566389ms)
Oct 13 11:16:27.279: INFO: (19) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:460/proxy/: tls baz (200; 23.319427ms)
Oct 13 11:16:27.279: INFO: (19) /api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/https:proxy-service-t7ssz-ppdsz:443/proxy/tlsrewritem... (200; 23.339518ms)
Oct 13 11:16:27.284: INFO: (19) /api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/: <a href="/api/v1/namespaces/proxy-6488/pods/proxy-service-t7ssz-ppdsz/proxy/rewriteme">test</a> (200; 28.468379ms)
Oct 13 11:16:27.284: INFO: (19) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname2/proxy/: bar (200; 28.817467ms)
Oct 13 11:16:27.284: INFO: (19) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname1/proxy/: foo (200; 29.006734ms)
Oct 13 11:16:27.284: INFO: (19) /api/v1/namespaces/proxy-6488/services/proxy-service-t7ssz:portname1/proxy/: foo (200; 28.908798ms)
Oct 13 11:16:27.284: INFO: (19) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname2/proxy/: tls qux (200; 28.883626ms)
Oct 13 11:16:27.285: INFO: (19) /api/v1/namespaces/proxy-6488/services/https:proxy-service-t7ssz:tlsportname1/proxy/: tls baz (200; 29.202123ms)
Oct 13 11:16:27.287: INFO: (19) /api/v1/namespaces/proxy-6488/services/http:proxy-service-t7ssz:portname2/proxy/: bar (200; 31.177502ms)
STEP: deleting ReplicationController proxy-service-t7ssz in namespace proxy-6488, will wait for the garbage collector to delete the pods
Oct 13 11:16:27.368: INFO: Deleting ReplicationController proxy-service-t7ssz took: 20.016311ms
Oct 13 11:16:27.469: INFO: Terminating ReplicationController proxy-service-t7ssz pods took: 101.368865ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:16:37.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6488" for this suite.

• [SLOW TEST:22.206 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":67,"skipped":1035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:16:37.222: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-146ca356-f31e-4310-990a-1816e3f78af5 in namespace container-probe-6521
Oct 13 11:16:41.359: INFO: Started pod busybox-146ca356-f31e-4310-990a-1816e3f78af5 in namespace container-probe-6521
STEP: checking the pod's current state and verifying that restartCount is present
Oct 13 11:16:41.371: INFO: Initial restart count of pod busybox-146ca356-f31e-4310-990a-1816e3f78af5 is 0
Oct 13 11:17:29.841: INFO: Restart count of pod container-probe-6521/busybox-146ca356-f31e-4310-990a-1816e3f78af5 is now 1 (48.470294363s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:17:29.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6521" for this suite.

• [SLOW TEST:52.689 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":68,"skipped":1060,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:17:29.913: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Oct 13 11:19:30.620: INFO: Successfully updated pod "var-expansion-bc72801b-604e-4f72-a33a-895031c2d3a1"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Oct 13 11:19:32.652: INFO: Deleting pod "var-expansion-bc72801b-604e-4f72-a33a-895031c2d3a1" in namespace "var-expansion-7537"
Oct 13 11:19:32.678: INFO: Wait up to 5m0s for pod "var-expansion-bc72801b-604e-4f72-a33a-895031c2d3a1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:20:06.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7537" for this suite.

• [SLOW TEST:156.834 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":69,"skipped":1070,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:20:06.758: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:20:06.897: INFO: The status of Pod busybox-readonly-fs2356839b-d78c-4147-88d5-27cc880ea3d7 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:20:08.914: INFO: The status of Pod busybox-readonly-fs2356839b-d78c-4147-88d5-27cc880ea3d7 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:20:10.921: INFO: The status of Pod busybox-readonly-fs2356839b-d78c-4147-88d5-27cc880ea3d7 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:20:11.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7822" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":70,"skipped":1088,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:20:11.038: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-08e2f6f9-e2f0-4ab6-bd74-cf913bd595a7
STEP: Creating a pod to test consume configMaps
Oct 13 11:20:11.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859" in namespace "projected-1070" to be "Succeeded or Failed"
Oct 13 11:20:11.201: INFO: Pod "pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859": Phase="Pending", Reason="", readiness=false. Elapsed: 12.970971ms
Oct 13 11:20:13.220: INFO: Pod "pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032569252s
Oct 13 11:20:15.236: INFO: Pod "pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048085727s
STEP: Saw pod success
Oct 13 11:20:15.236: INFO: Pod "pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859" satisfied condition "Succeeded or Failed"
Oct 13 11:20:15.247: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 11:20:15.357: INFO: Waiting for pod pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859 to disappear
Oct 13 11:20:15.367: INFO: Pod pod-projected-configmaps-35c55454-d02b-4d3b-bd36-1178e995d859 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:20:15.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1070" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":71,"skipped":1097,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:20:15.404: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-22c2fdd1-7899-4c56-9e43-40745fcf6814
STEP: Creating a pod to test consume secrets
Oct 13 11:20:15.541: INFO: Waiting up to 5m0s for pod "pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0" in namespace "secrets-5456" to be "Succeeded or Failed"
Oct 13 11:20:15.554: INFO: Pod "pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.277045ms
Oct 13 11:20:17.581: INFO: Pod "pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039322458s
Oct 13 11:20:19.601: INFO: Pod "pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059156365s
STEP: Saw pod success
Oct 13 11:20:19.601: INFO: Pod "pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0" satisfied condition "Succeeded or Failed"
Oct 13 11:20:19.612: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0 container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:20:19.689: INFO: Waiting for pod pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0 to disappear
Oct 13 11:20:19.702: INFO: Pod pod-secrets-719d7cde-dfc9-4cd1-89e2-ee08b2447ff0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:20:19.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5456" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":72,"skipped":1104,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:20:19.734: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Oct 13 11:20:19.843: INFO: Waiting up to 5m0s for pod "var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db" in namespace "var-expansion-1365" to be "Succeeded or Failed"
Oct 13 11:20:19.852: INFO: Pod "var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db": Phase="Pending", Reason="", readiness=false. Elapsed: 9.075375ms
Oct 13 11:20:21.870: INFO: Pod "var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026824494s
Oct 13 11:20:23.888: INFO: Pod "var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044706549s
STEP: Saw pod success
Oct 13 11:20:23.889: INFO: Pod "var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db" satisfied condition "Succeeded or Failed"
Oct 13 11:20:23.899: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db container dapi-container: <nil>
STEP: delete the pod
Oct 13 11:20:23.960: INFO: Waiting for pod var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db to disappear
Oct 13 11:20:23.970: INFO: Pod var-expansion-ddcf1a9e-0c93-43f6-9fe1-df0f686784db no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:20:23.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1365" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":73,"skipped":1115,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:20:24.009: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 13 11:20:24.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7600 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Oct 13 11:20:25.544: INFO: stderr: ""
Oct 13 11:20:25.544: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Oct 13 11:20:30.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7600 get pod e2e-test-httpd-pod -o json'
Oct 13 11:20:30.706: INFO: stderr: ""
Oct 13 11:20:30.706: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.25.0.108/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.0.108/32\"\n        },\n        \"creationTimestamp\": \"2021-10-13T11:20:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7600\",\n        \"resourceVersion\": \"382935\",\n        \"uid\": \"3117d3b8-69d4-4e85-90d5-6daf6d115bb2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-6zm8b\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-6zm8b\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-13T11:20:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-13T11:20:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-13T11:20:27Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-10-13T11:20:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://04d8f455f9dcf6c14edb5e04d375436ff7c3994b045061e58b49b714db1d8012\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-10-13T11:20:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.1.246\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.0.108\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.0.108\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-10-13T11:20:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
Oct 13 11:20:30.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7600 replace -f -'
Oct 13 11:20:31.146: INFO: stderr: ""
Oct 13 11:20:31.146: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Oct 13 11:20:31.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7600 delete pods e2e-test-httpd-pod'
Oct 13 11:20:47.349: INFO: stderr: ""
Oct 13 11:20:47.349: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:20:47.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7600" for this suite.

• [SLOW TEST:23.393 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":74,"skipped":1118,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:20:47.402: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Oct 13 11:20:47.503: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:21:03.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4577" for this suite.

• [SLOW TEST:16.136 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":75,"skipped":1120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:21:03.539: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Oct 13 11:21:03.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-5705 cluster-info'
Oct 13 11:21:03.714: INFO: stderr: ""
Oct 13 11:21:03.714: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:21:03.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5705" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":76,"skipped":1153,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:21:03.743: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2701 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2701;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2701 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2701;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2701.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2701.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2701.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2701.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2701.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2701.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2701.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2701.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2701.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 245.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.245_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2701 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2701;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2701 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2701;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2701.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2701.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2701.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2701.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2701.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2701.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2701.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2701.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2701.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2701.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 245.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.26.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.26.245_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 11:21:19.962: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.020: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.085: INFO: Unable to read wheezy_udp@dns-test-service.dns-2701 from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.135: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2701 from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.153: INFO: Unable to read wheezy_udp@dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.175: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.195: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.213: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.340: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.357: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.374: INFO: Unable to read jessie_udp@dns-test-service.dns-2701 from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.391: INFO: Unable to read jessie_tcp@dns-test-service.dns-2701 from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.409: INFO: Unable to read jessie_udp@dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.426: INFO: Unable to read jessie_tcp@dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.446: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.464: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2701.svc from pod dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7: the server could not find the requested resource (get pods dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7)
Oct 13 11:21:20.583: INFO: Lookups using dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2701 wheezy_tcp@dns-test-service.dns-2701 wheezy_udp@dns-test-service.dns-2701.svc wheezy_tcp@dns-test-service.dns-2701.svc wheezy_udp@_http._tcp.dns-test-service.dns-2701.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2701.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2701 jessie_tcp@dns-test-service.dns-2701 jessie_udp@dns-test-service.dns-2701.svc jessie_tcp@dns-test-service.dns-2701.svc jessie_udp@_http._tcp.dns-test-service.dns-2701.svc jessie_tcp@_http._tcp.dns-test-service.dns-2701.svc]

Oct 13 11:21:26.110: INFO: DNS probes using dns-2701/dns-test-5025bd68-3e9e-4544-8095-c267ad5dc7b7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:21:26.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2701" for this suite.

• [SLOW TEST:22.480 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":77,"skipped":1158,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:21:26.223: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-5376
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-5376
Oct 13 11:21:26.340: INFO: Found 0 stateful pods, waiting for 1
Oct 13 11:21:36.353: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Oct 13 11:21:36.456: INFO: Deleting all statefulset in ns statefulset-5376
Oct 13 11:21:36.470: INFO: Scaling statefulset ss to 0
Oct 13 11:21:56.551: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 11:21:56.563: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:21:56.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5376" for this suite.

• [SLOW TEST:30.427 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":78,"skipped":1160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:21:56.653: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:22:12.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8626" for this suite.
STEP: Destroying namespace "nsdeletetest-8890" for this suite.
Oct 13 11:22:13.037: INFO: Namespace nsdeletetest-8890 was already deleted
STEP: Destroying namespace "nsdeletetest-9394" for this suite.

• [SLOW TEST:16.400 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":79,"skipped":1238,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:22:13.054: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Oct 13 11:22:13.139: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-6433 proxy --unix-socket=/tmp/kubectl-proxy-unix159924492/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:22:13.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6433" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":80,"skipped":1242,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:22:13.249: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-ad8a054e-b8f6-4c91-b2ee-9165fe5c27d7
STEP: Creating secret with name s-test-opt-upd-d7ee3b62-c018-45c8-8df6-2e7d26b1c31f
STEP: Creating the pod
Oct 13 11:22:13.411: INFO: The status of Pod pod-projected-secrets-6c65a2b0-7c9c-40c2-a2bb-5ad529e9de88 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:22:15.425: INFO: The status of Pod pod-projected-secrets-6c65a2b0-7c9c-40c2-a2bb-5ad529e9de88 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:22:17.431: INFO: The status of Pod pod-projected-secrets-6c65a2b0-7c9c-40c2-a2bb-5ad529e9de88 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:22:19.433: INFO: The status of Pod pod-projected-secrets-6c65a2b0-7c9c-40c2-a2bb-5ad529e9de88 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-ad8a054e-b8f6-4c91-b2ee-9165fe5c27d7
STEP: Updating secret s-test-opt-upd-d7ee3b62-c018-45c8-8df6-2e7d26b1c31f
STEP: Creating secret with name s-test-opt-create-e6ee2394-e62c-44d2-bdaa-c4a3c69915b9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:23:25.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1963" for this suite.

• [SLOW TEST:71.810 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":81,"skipped":1244,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:23:25.059: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Oct 13 11:23:25.170: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:23:27.186: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:23:29.193: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Oct 13 11:23:30.262: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:23:31.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2120" for this suite.

• [SLOW TEST:6.290 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":82,"skipped":1248,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:23:31.350: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff in namespace container-probe-5612
Oct 13 11:23:35.483: INFO: Started pod liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff in namespace container-probe-5612
STEP: checking the pod's current state and verifying that restartCount is present
Oct 13 11:23:35.495: INFO: Initial restart count of pod liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff is 0
Oct 13 11:23:53.660: INFO: Restart count of pod container-probe-5612/liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff is now 1 (18.164400204s elapsed)
Oct 13 11:24:13.829: INFO: Restart count of pod container-probe-5612/liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff is now 2 (38.33356755s elapsed)
Oct 13 11:24:34.067: INFO: Restart count of pod container-probe-5612/liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff is now 3 (58.571714651s elapsed)
Oct 13 11:24:54.277: INFO: Restart count of pod container-probe-5612/liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff is now 4 (1m18.781565601s elapsed)
Oct 13 11:25:54.859: INFO: Restart count of pod container-probe-5612/liveness-5a4a1a6d-7121-4cc0-b6d9-6b86089e99ff is now 5 (2m19.362910004s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:25:54.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5612" for this suite.

• [SLOW TEST:143.576 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":83,"skipped":1260,"failed":0}
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:25:54.928: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:25:55.037: INFO: Got root ca configmap in namespace "svcaccounts-7464"
Oct 13 11:25:55.050: INFO: Deleted root ca configmap in namespace "svcaccounts-7464"
STEP: waiting for a new root ca configmap created
Oct 13 11:25:55.563: INFO: Recreated root ca configmap in namespace "svcaccounts-7464"
Oct 13 11:25:55.580: INFO: Updated root ca configmap in namespace "svcaccounts-7464"
STEP: waiting for the root ca configmap reconciled
Oct 13 11:25:56.097: INFO: Reconciled root ca configmap in namespace "svcaccounts-7464"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:25:56.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7464" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":84,"skipped":1260,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:25:56.134: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:02.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5183" for this suite.
STEP: Destroying namespace "nsdeletetest-4743" for this suite.
Oct 13 11:26:02.564: INFO: Namespace nsdeletetest-4743 was already deleted
STEP: Destroying namespace "nsdeletetest-535" for this suite.

• [SLOW TEST:6.450 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":85,"skipped":1266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:02.586: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:30.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6119" for this suite.

• [SLOW TEST:28.273 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":86,"skipped":1301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:30.860: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Oct 13 11:26:30.959: INFO: Waiting up to 5m0s for pod "var-expansion-514a119c-31d7-4968-a424-f967e0429514" in namespace "var-expansion-5806" to be "Succeeded or Failed"
Oct 13 11:26:30.968: INFO: Pod "var-expansion-514a119c-31d7-4968-a424-f967e0429514": Phase="Pending", Reason="", readiness=false. Elapsed: 9.2515ms
Oct 13 11:26:32.985: INFO: Pod "var-expansion-514a119c-31d7-4968-a424-f967e0429514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025389442s
Oct 13 11:26:35.001: INFO: Pod "var-expansion-514a119c-31d7-4968-a424-f967e0429514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042214366s
STEP: Saw pod success
Oct 13 11:26:35.002: INFO: Pod "var-expansion-514a119c-31d7-4968-a424-f967e0429514" satisfied condition "Succeeded or Failed"
Oct 13 11:26:35.016: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod var-expansion-514a119c-31d7-4968-a424-f967e0429514 container dapi-container: <nil>
STEP: delete the pod
Oct 13 11:26:35.111: INFO: Waiting for pod var-expansion-514a119c-31d7-4968-a424-f967e0429514 to disappear
Oct 13 11:26:35.123: INFO: Pod var-expansion-514a119c-31d7-4968-a424-f967e0429514 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:35.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5806" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":87,"skipped":1324,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:35.160: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:35.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5134" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":88,"skipped":1328,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:35.315: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:26:36.100: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 11:26:38.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721196, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721196, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721196, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721196, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:26:41.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:41.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3917" for this suite.
STEP: Destroying namespace "webhook-3917-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.429 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":89,"skipped":1342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:41.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8119" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":90,"skipped":1374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:41.983: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Oct 13 11:26:42.085: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4436  6dae6f9b-65d6-401d-b5b1-b460afcab53d 385224 0 2021-10-13 11:26:42 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-10-13 11:26:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ds92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ds92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:26:42.097: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:26:44.115: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:26:46.120: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Oct 13 11:26:46.121: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4436 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:26:46.121: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Verifying customized DNS server is configured on pod...
Oct 13 11:26:46.486: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4436 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:26:46.486: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:26:46.861: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:46.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4436" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":91,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:46.930: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Oct 13 11:26:47.003: INFO: namespace kubectl-5356
Oct 13 11:26:47.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-5356 create -f -'
Oct 13 11:26:47.453: INFO: stderr: ""
Oct 13 11:26:47.453: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 13 11:26:48.473: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:26:48.473: INFO: Found 0 / 1
Oct 13 11:26:49.477: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:26:49.477: INFO: Found 0 / 1
Oct 13 11:26:50.471: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:26:50.476: INFO: Found 1 / 1
Oct 13 11:26:50.476: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 13 11:26:50.493: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:26:50.493: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 13 11:26:50.493: INFO: wait on agnhost-primary startup in kubectl-5356 
Oct 13 11:26:50.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-5356 logs agnhost-primary-6824z agnhost-primary'
Oct 13 11:26:50.634: INFO: stderr: ""
Oct 13 11:26:50.634: INFO: stdout: "Paused\n"
STEP: exposing RC
Oct 13 11:26:50.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-5356 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Oct 13 11:26:50.779: INFO: stderr: ""
Oct 13 11:26:50.780: INFO: stdout: "service/rm2 exposed\n"
Oct 13 11:26:50.790: INFO: Service rm2 in namespace kubectl-5356 found.
STEP: exposing service
Oct 13 11:26:52.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-5356 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Oct 13 11:26:52.961: INFO: stderr: ""
Oct 13 11:26:52.961: INFO: stdout: "service/rm3 exposed\n"
Oct 13 11:26:52.973: INFO: Service rm3 in namespace kubectl-5356 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:55.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5356" for this suite.

• [SLOW TEST:8.103 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":92,"skipped":1425,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:55.036: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:26:55.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6" in namespace "downward-api-8045" to be "Succeeded or Failed"
Oct 13 11:26:55.164: INFO: Pod "downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.746197ms
Oct 13 11:26:57.187: INFO: Pod "downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035834571s
Oct 13 11:26:59.206: INFO: Pod "downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054843255s
STEP: Saw pod success
Oct 13 11:26:59.206: INFO: Pod "downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6" satisfied condition "Succeeded or Failed"
Oct 13 11:26:59.216: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6 container client-container: <nil>
STEP: delete the pod
Oct 13 11:26:59.316: INFO: Waiting for pod downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6 to disappear
Oct 13 11:26:59.326: INFO: Pod downwardapi-volume-8e8cd115-d93b-47f0-a82b-5c86946716f6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:26:59.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8045" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":93,"skipped":1428,"failed":0}
S
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:26:59.364: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:26:59.491: INFO: The status of Pod server-envvars-85dca73c-6632-4d97-b1df-b61c17d3e465 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:27:01.513: INFO: The status of Pod server-envvars-85dca73c-6632-4d97-b1df-b61c17d3e465 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:27:03.508: INFO: The status of Pod server-envvars-85dca73c-6632-4d97-b1df-b61c17d3e465 is Running (Ready = true)
Oct 13 11:27:03.567: INFO: Waiting up to 5m0s for pod "client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e" in namespace "pods-5816" to be "Succeeded or Failed"
Oct 13 11:27:03.579: INFO: Pod "client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.485043ms
Oct 13 11:27:05.599: INFO: Pod "client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029943317s
Oct 13 11:27:07.623: INFO: Pod "client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053341666s
STEP: Saw pod success
Oct 13 11:27:07.623: INFO: Pod "client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e" satisfied condition "Succeeded or Failed"
Oct 13 11:27:07.632: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e container env3cont: <nil>
STEP: delete the pod
Oct 13 11:27:07.695: INFO: Waiting for pod client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e to disappear
Oct 13 11:27:07.705: INFO: Pod client-envvars-af8c5d65-68d8-422b-9d7f-18f619400b3e no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:27:07.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5816" for this suite.

• [SLOW TEST:8.372 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":94,"skipped":1429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:27:07.740: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:27:11.876: INFO: Deleting pod "var-expansion-88000abd-c4b0-4686-b160-d92d3bc1d563" in namespace "var-expansion-8808"
Oct 13 11:27:11.896: INFO: Wait up to 5m0s for pod "var-expansion-88000abd-c4b0-4686-b160-d92d3bc1d563" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:27:25.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8808" for this suite.

• [SLOW TEST:18.225 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":95,"skipped":1474,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:27:25.967: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Oct 13 11:27:26.088: INFO: Waiting up to 5m0s for pod "security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822" in namespace "security-context-8622" to be "Succeeded or Failed"
Oct 13 11:27:26.099: INFO: Pod "security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822": Phase="Pending", Reason="", readiness=false. Elapsed: 11.692343ms
Oct 13 11:27:28.123: INFO: Pod "security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035331726s
Oct 13 11:27:30.139: INFO: Pod "security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051764203s
STEP: Saw pod success
Oct 13 11:27:30.140: INFO: Pod "security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822" satisfied condition "Succeeded or Failed"
Oct 13 11:27:30.152: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822 container test-container: <nil>
STEP: delete the pod
Oct 13 11:27:30.256: INFO: Waiting for pod security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822 to disappear
Oct 13 11:27:30.266: INFO: Pod security-context-ab9d499f-bbdf-46e8-81aa-15ff68797822 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:27:30.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8622" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":96,"skipped":1481,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:27:30.302: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:27:30.406: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c" in namespace "downward-api-9310" to be "Succeeded or Failed"
Oct 13 11:27:30.418: INFO: Pod "downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011333ms
Oct 13 11:27:32.438: INFO: Pod "downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032198154s
Oct 13 11:27:34.453: INFO: Pod "downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046822322s
STEP: Saw pod success
Oct 13 11:27:34.453: INFO: Pod "downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c" satisfied condition "Succeeded or Failed"
Oct 13 11:27:34.466: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c container client-container: <nil>
STEP: delete the pod
Oct 13 11:27:34.567: INFO: Waiting for pod downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c to disappear
Oct 13 11:27:34.576: INFO: Pod downwardapi-volume-66321a58-2f2d-4511-88c5-7cff6e22963c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:27:34.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9310" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":97,"skipped":1482,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:27:34.611: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:27:34.712: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:27:35.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9676" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":98,"skipped":1486,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:27:35.827: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:27:35.906: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 13 11:27:39.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 create -f -'
Oct 13 11:27:41.559: INFO: stderr: ""
Oct 13 11:27:41.559: INFO: stdout: "e2e-test-crd-publish-openapi-3510-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 13 11:27:41.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 delete e2e-test-crd-publish-openapi-3510-crds test-cr'
Oct 13 11:27:41.741: INFO: stderr: ""
Oct 13 11:27:41.741: INFO: stdout: "e2e-test-crd-publish-openapi-3510-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Oct 13 11:27:41.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 apply -f -'
Oct 13 11:27:42.070: INFO: stderr: ""
Oct 13 11:27:42.070: INFO: stdout: "e2e-test-crd-publish-openapi-3510-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Oct 13 11:27:42.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-404 --namespace=crd-publish-openapi-404 delete e2e-test-crd-publish-openapi-3510-crds test-cr'
Oct 13 11:27:42.202: INFO: stderr: ""
Oct 13 11:27:42.202: INFO: stdout: "e2e-test-crd-publish-openapi-3510-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Oct 13 11:27:42.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-404 explain e2e-test-crd-publish-openapi-3510-crds'
Oct 13 11:27:42.505: INFO: stderr: ""
Oct 13 11:27:42.505: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3510-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:27:45.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-404" for this suite.

• [SLOW TEST:9.705 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":99,"skipped":1493,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:27:45.534: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:27:46.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 11:27:48.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721266, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721266, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721266, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721266, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:27:51.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Oct 13 11:27:55.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=webhook-2496 attach --namespace=webhook-2496 to-be-attached-pod -i -c=container1'
Oct 13 11:27:55.667: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:27:55.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2496" for this suite.
STEP: Destroying namespace "webhook-2496-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.253 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":100,"skipped":1501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:27:55.789: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 13 11:27:55.884: INFO: Waiting up to 5m0s for pod "pod-5ebdf0e1-026a-49af-8226-bb1803170ebd" in namespace "emptydir-4144" to be "Succeeded or Failed"
Oct 13 11:27:55.894: INFO: Pod "pod-5ebdf0e1-026a-49af-8226-bb1803170ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.397622ms
Oct 13 11:27:57.908: INFO: Pod "pod-5ebdf0e1-026a-49af-8226-bb1803170ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023870832s
Oct 13 11:27:59.928: INFO: Pod "pod-5ebdf0e1-026a-49af-8226-bb1803170ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043978086s
Oct 13 11:28:01.944: INFO: Pod "pod-5ebdf0e1-026a-49af-8226-bb1803170ebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06077975s
STEP: Saw pod success
Oct 13 11:28:01.945: INFO: Pod "pod-5ebdf0e1-026a-49af-8226-bb1803170ebd" satisfied condition "Succeeded or Failed"
Oct 13 11:28:01.956: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-5ebdf0e1-026a-49af-8226-bb1803170ebd container test-container: <nil>
STEP: delete the pod
Oct 13 11:28:02.104: INFO: Waiting for pod pod-5ebdf0e1-026a-49af-8226-bb1803170ebd to disappear
Oct 13 11:28:02.113: INFO: Pod pod-5ebdf0e1-026a-49af-8226-bb1803170ebd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:02.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4144" for this suite.

• [SLOW TEST:6.358 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":101,"skipped":1532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:02.154: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:28:02.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371" in namespace "projected-7904" to be "Succeeded or Failed"
Oct 13 11:28:02.283: INFO: Pod "downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371": Phase="Pending", Reason="", readiness=false. Elapsed: 11.707362ms
Oct 13 11:28:04.301: INFO: Pod "downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029308815s
Oct 13 11:28:06.318: INFO: Pod "downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045879175s
STEP: Saw pod success
Oct 13 11:28:06.318: INFO: Pod "downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371" satisfied condition "Succeeded or Failed"
Oct 13 11:28:06.329: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371 container client-container: <nil>
STEP: delete the pod
Oct 13 11:28:06.393: INFO: Waiting for pod downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371 to disappear
Oct 13 11:28:06.408: INFO: Pod downwardapi-volume-3dd21af7-8318-4e42-8845-d45a96be3371 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:06.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7904" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":102,"skipped":1585,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:06.447: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-374bf83d-f8db-4c90-ae40-aa2d3a95d4ff
STEP: Creating a pod to test consume secrets
Oct 13 11:28:06.556: INFO: Waiting up to 5m0s for pod "pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5" in namespace "secrets-5897" to be "Succeeded or Failed"
Oct 13 11:28:06.570: INFO: Pod "pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.063713ms
Oct 13 11:28:08.591: INFO: Pod "pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035055778s
Oct 13 11:28:10.613: INFO: Pod "pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056571337s
STEP: Saw pod success
Oct 13 11:28:10.613: INFO: Pod "pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5" satisfied condition "Succeeded or Failed"
Oct 13 11:28:10.623: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5 container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:28:10.851: INFO: Waiting for pod pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5 to disappear
Oct 13 11:28:10.865: INFO: Pod pod-secrets-80608fa2-f3fb-4a59-a671-36297e3ad2e5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:10.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5897" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":103,"skipped":1598,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:10.906: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 13 11:28:15.121: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:15.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3889" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":104,"skipped":1614,"failed":0}

------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:15.202: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-b853e78b-d3eb-4011-a7a1-70c95163490d
STEP: Creating a pod to test consume secrets
Oct 13 11:28:15.316: INFO: Waiting up to 5m0s for pod "pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837" in namespace "secrets-5334" to be "Succeeded or Failed"
Oct 13 11:28:15.326: INFO: Pod "pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837": Phase="Pending", Reason="", readiness=false. Elapsed: 10.607302ms
Oct 13 11:28:17.347: INFO: Pod "pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031264287s
Oct 13 11:28:19.362: INFO: Pod "pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046326002s
STEP: Saw pod success
Oct 13 11:28:19.362: INFO: Pod "pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837" satisfied condition "Succeeded or Failed"
Oct 13 11:28:19.371: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837 container secret-env-test: <nil>
STEP: delete the pod
Oct 13 11:28:19.425: INFO: Waiting for pod pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837 to disappear
Oct 13 11:28:19.434: INFO: Pod pod-secrets-9e04688b-4a20-42d6-96e0-ea54f22f4837 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5334" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":105,"skipped":1614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:19.463: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:28:19.611: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Oct 13 11:28:19.632: INFO: Number of nodes with available pods: 0
Oct 13 11:28:19.632: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Oct 13 11:28:19.714: INFO: Number of nodes with available pods: 0
Oct 13 11:28:19.714: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:20.730: INFO: Number of nodes with available pods: 0
Oct 13 11:28:20.730: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:21.741: INFO: Number of nodes with available pods: 0
Oct 13 11:28:21.741: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:22.728: INFO: Number of nodes with available pods: 1
Oct 13 11:28:22.729: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Oct 13 11:28:22.795: INFO: Number of nodes with available pods: 1
Oct 13 11:28:22.796: INFO: Number of running nodes: 0, number of available pods: 1
Oct 13 11:28:23.815: INFO: Number of nodes with available pods: 0
Oct 13 11:28:23.816: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Oct 13 11:28:23.853: INFO: Number of nodes with available pods: 0
Oct 13 11:28:23.853: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:24.881: INFO: Number of nodes with available pods: 0
Oct 13 11:28:24.881: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:25.867: INFO: Number of nodes with available pods: 0
Oct 13 11:28:25.867: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:26.873: INFO: Number of nodes with available pods: 0
Oct 13 11:28:26.873: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:27.871: INFO: Number of nodes with available pods: 0
Oct 13 11:28:27.871: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:28.874: INFO: Number of nodes with available pods: 0
Oct 13 11:28:28.874: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:29.864: INFO: Number of nodes with available pods: 0
Oct 13 11:28:29.864: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:30.888: INFO: Number of nodes with available pods: 0
Oct 13 11:28:30.888: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:31.871: INFO: Number of nodes with available pods: 0
Oct 13 11:28:31.871: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:28:32.869: INFO: Number of nodes with available pods: 1
Oct 13 11:28:32.869: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6066, will wait for the garbage collector to delete the pods
Oct 13 11:28:32.971: INFO: Deleting DaemonSet.extensions daemon-set took: 20.500543ms
Oct 13 11:28:33.071: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.481197ms
Oct 13 11:28:37.590: INFO: Number of nodes with available pods: 0
Oct 13 11:28:37.591: INFO: Number of running nodes: 0, number of available pods: 0
Oct 13 11:28:37.602: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"386396"},"items":null}

Oct 13 11:28:37.613: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"386396"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:37.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6066" for this suite.

• [SLOW TEST:18.252 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":106,"skipped":1637,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:37.716: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:28:37.871: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4e29aa43-50e2-4184-8c5e-09f10c3f4a52", Controller:(*bool)(0xc006a675e2), BlockOwnerDeletion:(*bool)(0xc006a675e3)}}
Oct 13 11:28:37.883: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8f52acbf-7dfe-4103-868f-49ca347d9027", Controller:(*bool)(0xc006a6783a), BlockOwnerDeletion:(*bool)(0xc006a6783b)}}
Oct 13 11:28:37.895: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ccb11a03-9ddb-4cd7-a288-b587fcf2a02e", Controller:(*bool)(0xc006a2ee1a), BlockOwnerDeletion:(*bool)(0xc006a2ee1b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:42.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7525" for this suite.

• [SLOW TEST:5.241 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":107,"skipped":1651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 13 11:28:43.064: INFO: Waiting up to 5m0s for pod "pod-875dd839-6480-4a2e-9aea-70769ff0ffe6" in namespace "emptydir-4614" to be "Succeeded or Failed"
Oct 13 11:28:43.074: INFO: Pod "pod-875dd839-6480-4a2e-9aea-70769ff0ffe6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.305439ms
Oct 13 11:28:45.090: INFO: Pod "pod-875dd839-6480-4a2e-9aea-70769ff0ffe6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025341686s
Oct 13 11:28:47.113: INFO: Pod "pod-875dd839-6480-4a2e-9aea-70769ff0ffe6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048485134s
STEP: Saw pod success
Oct 13 11:28:47.113: INFO: Pod "pod-875dd839-6480-4a2e-9aea-70769ff0ffe6" satisfied condition "Succeeded or Failed"
Oct 13 11:28:47.127: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-875dd839-6480-4a2e-9aea-70769ff0ffe6 container test-container: <nil>
STEP: delete the pod
Oct 13 11:28:47.230: INFO: Waiting for pod pod-875dd839-6480-4a2e-9aea-70769ff0ffe6 to disappear
Oct 13 11:28:47.241: INFO: Pod pod-875dd839-6480-4a2e-9aea-70769ff0ffe6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:47.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4614" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":108,"skipped":1675,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:47.276: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:47.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9128" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":109,"skipped":1689,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:47.486: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Oct 13 11:28:47.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-6724 api-versions'
Oct 13 11:28:47.736: INFO: stderr: ""
Oct 13 11:28:47.736: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetakube.syseleven.de/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:47.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6724" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":110,"skipped":1691,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:47.765: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:53.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7817" for this suite.

• [SLOW TEST:5.420 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":111,"skipped":1693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:28:53.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88" in namespace "projected-3568" to be "Succeeded or Failed"
Oct 13 11:28:53.296: INFO: Pod "downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88": Phase="Pending", Reason="", readiness=false. Elapsed: 8.323986ms
Oct 13 11:28:55.316: INFO: Pod "downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027914086s
Oct 13 11:28:57.338: INFO: Pod "downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049546432s
STEP: Saw pod success
Oct 13 11:28:57.338: INFO: Pod "downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88" satisfied condition "Succeeded or Failed"
Oct 13 11:28:57.349: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88 container client-container: <nil>
STEP: delete the pod
Oct 13 11:28:57.410: INFO: Waiting for pod downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88 to disappear
Oct 13 11:28:57.418: INFO: Pod downwardapi-volume-8569f7f8-be92-4a93-843d-cfb23f64df88 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:28:57.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3568" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":112,"skipped":1724,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:28:57.447: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:29:13.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2468" for this suite.

• [SLOW TEST:16.443 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":113,"skipped":1729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:29:13.890: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Oct 13 11:29:14.056: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9294  1b7f96e2-f05a-4fd3-9ed0-35a902a53d05 386884 0 2021-10-13 11:29:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-13 11:29:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:29:14.056: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9294  1b7f96e2-f05a-4fd3-9ed0-35a902a53d05 386885 0 2021-10-13 11:29:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-13 11:29:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:29:14.057: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9294  1b7f96e2-f05a-4fd3-9ed0-35a902a53d05 386886 0 2021-10-13 11:29:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-13 11:29:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Oct 13 11:29:24.156: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9294  1b7f96e2-f05a-4fd3-9ed0-35a902a53d05 386945 0 2021-10-13 11:29:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-13 11:29:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:29:24.157: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9294  1b7f96e2-f05a-4fd3-9ed0-35a902a53d05 386946 0 2021-10-13 11:29:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-13 11:29:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 11:29:24.157: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9294  1b7f96e2-f05a-4fd3-9ed0-35a902a53d05 386947 0 2021-10-13 11:29:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-10-13 11:29:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:29:24.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9294" for this suite.

• [SLOW TEST:10.298 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":114,"skipped":1757,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:29:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6791
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-6791
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6791
Oct 13 11:29:24.326: INFO: Found 0 stateful pods, waiting for 1
Oct 13 11:29:34.355: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Oct 13 11:29:34.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6791 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 11:29:35.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 11:29:35.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 11:29:35.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 11:29:35.025: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 13 11:29:45.040: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 11:29:45.040: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 11:29:45.119: INFO: POD   NODE                                              PHASE    GRACE  CONDITIONS
Oct 13 11:29:45.119: INFO: ss-0  flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  }]
Oct 13 11:29:45.119: INFO: 
Oct 13 11:29:45.119: INFO: StatefulSet ss has not reached scale 3, at 1
Oct 13 11:29:46.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988813353s
Oct 13 11:29:47.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975409256s
Oct 13 11:29:48.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956133266s
Oct 13 11:29:49.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.933854911s
Oct 13 11:29:50.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.913565792s
Oct 13 11:29:51.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.896661717s
Oct 13 11:29:52.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.876096712s
Oct 13 11:29:53.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.855254538s
Oct 13 11:29:54.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 830.738944ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6791
Oct 13 11:29:55.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6791 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 11:29:55.916: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 13 11:29:55.916: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 11:29:55.916: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 11:29:55.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6791 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 11:29:56.523: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 13 11:29:56.523: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 11:29:56.523: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 11:29:56.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6791 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 11:29:57.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct 13 11:29:57.006: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 11:29:57.006: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 11:29:57.021: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 11:29:57.021: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 11:29:57.021: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Oct 13 11:29:57.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6791 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 11:29:57.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 11:29:57.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 11:29:57.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 11:29:57.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6791 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 11:29:58.077: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 11:29:58.077: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 11:29:58.077: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 11:29:58.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6791 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 11:29:58.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 11:29:58.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 11:29:58.669: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 11:29:58.669: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 11:29:58.681: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Oct 13 11:30:08.710: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 11:30:08.710: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 11:30:08.710: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 11:30:08.746: INFO: POD   NODE                                              PHASE    GRACE  CONDITIONS
Oct 13 11:30:08.746: INFO: ss-0  flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  }]
Oct 13 11:30:08.747: INFO: ss-1  flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  }]
Oct 13 11:30:08.747: INFO: ss-2  flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  }]
Oct 13 11:30:08.747: INFO: 
Oct 13 11:30:08.747: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 13 11:30:09.764: INFO: POD   NODE                                              PHASE    GRACE  CONDITIONS
Oct 13 11:30:09.764: INFO: ss-0  flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  }]
Oct 13 11:30:09.764: INFO: ss-1  flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  }]
Oct 13 11:30:09.764: INFO: ss-2  flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  }]
Oct 13 11:30:09.764: INFO: 
Oct 13 11:30:09.764: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 13 11:30:10.778: INFO: POD   NODE                                              PHASE    GRACE  CONDITIONS
Oct 13 11:30:10.778: INFO: ss-0  flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:24 +0000 UTC  }]
Oct 13 11:30:10.779: INFO: ss-1  flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  }]
Oct 13 11:30:10.779: INFO: ss-2  flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:29:45 +0000 UTC  }]
Oct 13 11:30:10.779: INFO: 
Oct 13 11:30:10.779: INFO: StatefulSet ss has not reached scale 0, at 3
Oct 13 11:30:11.793: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.953687925s
Oct 13 11:30:12.807: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.940277378s
Oct 13 11:30:13.826: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.925971183s
Oct 13 11:30:14.841: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.905970734s
Oct 13 11:30:15.859: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.892267893s
Oct 13 11:30:16.873: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.873253849s
Oct 13 11:30:17.889: INFO: Verifying statefulset ss doesn't scale past 0 for another 860.241664ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6791
Oct 13 11:30:18.907: INFO: Scaling statefulset ss to 0
Oct 13 11:30:18.945: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Oct 13 11:30:18.955: INFO: Deleting all statefulset in ns statefulset-6791
Oct 13 11:30:18.977: INFO: Scaling statefulset ss to 0
Oct 13 11:30:19.014: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 11:30:19.025: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:30:19.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6791" for this suite.

• [SLOW TEST:54.912 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":115,"skipped":1774,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:30:19.108: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 13 11:30:19.222: INFO: Waiting up to 5m0s for pod "pod-f52464b3-c68b-4d55-a54c-22cb129070ef" in namespace "emptydir-482" to be "Succeeded or Failed"
Oct 13 11:30:19.238: INFO: Pod "pod-f52464b3-c68b-4d55-a54c-22cb129070ef": Phase="Pending", Reason="", readiness=false. Elapsed: 15.67926ms
Oct 13 11:30:21.254: INFO: Pod "pod-f52464b3-c68b-4d55-a54c-22cb129070ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031954354s
Oct 13 11:30:23.275: INFO: Pod "pod-f52464b3-c68b-4d55-a54c-22cb129070ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052625241s
STEP: Saw pod success
Oct 13 11:30:23.277: INFO: Pod "pod-f52464b3-c68b-4d55-a54c-22cb129070ef" satisfied condition "Succeeded or Failed"
Oct 13 11:30:23.290: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-f52464b3-c68b-4d55-a54c-22cb129070ef container test-container: <nil>
STEP: delete the pod
Oct 13 11:30:23.392: INFO: Waiting for pod pod-f52464b3-c68b-4d55-a54c-22cb129070ef to disappear
Oct 13 11:30:23.406: INFO: Pod pod-f52464b3-c68b-4d55-a54c-22cb129070ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:30:23.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-482" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":116,"skipped":1788,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:30:23.439: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Oct 13 11:30:23.573: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:30:25.588: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:30:27.599: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:30:29.597: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Oct 13 11:30:29.642: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:30:31.659: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:30:33.661: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Oct 13 11:30:33.672: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:33.672: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:34.141: INFO: Exec stderr: ""
Oct 13 11:30:34.142: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:34.142: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:34.570: INFO: Exec stderr: ""
Oct 13 11:30:34.570: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:34.571: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:34.961: INFO: Exec stderr: ""
Oct 13 11:30:34.961: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:34.961: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:35.376: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Oct 13 11:30:35.377: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:35.377: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:35.697: INFO: Exec stderr: ""
Oct 13 11:30:35.697: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:35.698: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:36.139: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Oct 13 11:30:36.140: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:36.141: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:36.582: INFO: Exec stderr: ""
Oct 13 11:30:36.582: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:36.582: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:37.029: INFO: Exec stderr: ""
Oct 13 11:30:37.029: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:37.029: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:37.380: INFO: Exec stderr: ""
Oct 13 11:30:37.380: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6199 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:30:37.380: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:30:37.845: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:30:37.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6199" for this suite.

• [SLOW TEST:14.449 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":117,"skipped":1806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:30:37.901: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Oct 13 11:30:38.071: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Oct 13 11:30:38.151: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:30:38.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3752" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":118,"skipped":1846,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:30:38.262: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:30:38.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94" in namespace "downward-api-8987" to be "Succeeded or Failed"
Oct 13 11:30:38.397: INFO: Pod "downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94": Phase="Pending", Reason="", readiness=false. Elapsed: 19.24363ms
Oct 13 11:30:40.421: INFO: Pod "downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0436775s
Oct 13 11:30:42.442: INFO: Pod "downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063807276s
STEP: Saw pod success
Oct 13 11:30:42.442: INFO: Pod "downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94" satisfied condition "Succeeded or Failed"
Oct 13 11:30:42.455: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94 container client-container: <nil>
STEP: delete the pod
Oct 13 11:30:42.558: INFO: Waiting for pod downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94 to disappear
Oct 13 11:30:42.568: INFO: Pod downwardapi-volume-e49dfefa-9b31-4c77-afad-075bdb30ff94 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:30:42.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8987" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":119,"skipped":1861,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:30:42.604: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Oct 13 11:30:42.744: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:30:42.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5627" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":120,"skipped":1873,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:30:42.841: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Oct 13 11:30:42.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Oct 13 11:30:43.059: INFO: stderr: ""
Oct 13 11:30:43.059: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Oct 13 11:30:43.059: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Oct 13 11:30:43.060: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7818" to be "running and ready, or succeeded"
Oct 13 11:30:43.069: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.243496ms
Oct 13 11:30:45.093: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033002781s
Oct 13 11:30:47.109: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049761861s
Oct 13 11:30:49.133: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 6.07302634s
Oct 13 11:30:49.133: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Oct 13 11:30:49.133: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Oct 13 11:30:49.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 logs logs-generator logs-generator'
Oct 13 11:30:49.283: INFO: stderr: ""
Oct 13 11:30:49.283: INFO: stdout: "I1013 11:30:47.393913       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/j5pv 238\nI1013 11:30:47.594283       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6htj 203\nI1013 11:30:47.794030       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/rpwt 289\nI1013 11:30:47.995014       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/926x 479\nI1013 11:30:48.194334       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kg8 229\nI1013 11:30:48.394736       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/2gj 483\nI1013 11:30:48.594027       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/8cx 242\nI1013 11:30:48.794458       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/62s5 329\nI1013 11:30:48.994166       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/56bm 479\nI1013 11:30:49.195252       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/79b 572\n"
STEP: limiting log lines
Oct 13 11:30:49.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 logs logs-generator logs-generator --tail=1'
Oct 13 11:30:49.455: INFO: stderr: ""
Oct 13 11:30:49.455: INFO: stdout: "I1013 11:30:49.394670       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/2b2 462\n"
Oct 13 11:30:49.456: INFO: got output "I1013 11:30:49.394670       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/2b2 462\n"
STEP: limiting log bytes
Oct 13 11:30:49.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 logs logs-generator logs-generator --limit-bytes=1'
Oct 13 11:30:49.580: INFO: stderr: ""
Oct 13 11:30:49.580: INFO: stdout: "I"
Oct 13 11:30:49.580: INFO: got output "I"
STEP: exposing timestamps
Oct 13 11:30:49.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 logs logs-generator logs-generator --tail=1 --timestamps'
Oct 13 11:30:49.722: INFO: stderr: ""
Oct 13 11:30:49.722: INFO: stdout: "2021-10-13T11:30:49.594265703Z I1013 11:30:49.593980       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/hhw 540\n"
Oct 13 11:30:49.723: INFO: got output "2021-10-13T11:30:49.594265703Z I1013 11:30:49.593980       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/hhw 540\n"
STEP: restricting to a time range
Oct 13 11:30:52.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 logs logs-generator logs-generator --since=1s'
Oct 13 11:30:52.360: INFO: stderr: ""
Oct 13 11:30:52.360: INFO: stdout: "I1013 11:30:51.395010       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/tlh2 517\nI1013 11:30:51.594437       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/62wn 416\nI1013 11:30:51.794903       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/lmv6 471\nI1013 11:30:51.994311       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/zl9 249\nI1013 11:30:52.194817       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/bspw 421\n"
Oct 13 11:30:52.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 logs logs-generator logs-generator --since=24h'
Oct 13 11:30:53.544: INFO: stderr: ""
Oct 13 11:30:53.544: INFO: stdout: "I1013 11:30:47.393913       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/j5pv 238\nI1013 11:30:47.594283       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6htj 203\nI1013 11:30:47.794030       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/rpwt 289\nI1013 11:30:47.995014       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/926x 479\nI1013 11:30:48.194334       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kg8 229\nI1013 11:30:48.394736       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/2gj 483\nI1013 11:30:48.594027       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/8cx 242\nI1013 11:30:48.794458       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/62s5 329\nI1013 11:30:48.994166       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/56bm 479\nI1013 11:30:49.195252       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/79b 572\nI1013 11:30:49.394670       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/2b2 462\nI1013 11:30:49.593980       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/hhw 540\nI1013 11:30:49.794421       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/kjpn 308\nI1013 11:30:49.994869       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/k9bq 237\nI1013 11:30:50.197629       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/v7s2 235\nI1013 11:30:50.394081       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/bnnt 293\nI1013 11:30:50.595303       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/nr5x 209\nI1013 11:30:50.794721       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/84k5 264\nI1013 11:30:50.994079       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/p8tv 467\nI1013 11:30:51.194505       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/r99n 438\nI1013 11:30:51.395010       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/tlh2 517\nI1013 11:30:51.594437       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/62wn 416\nI1013 11:30:51.794903       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/lmv6 471\nI1013 11:30:51.994311       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/zl9 249\nI1013 11:30:52.194817       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/bspw 421\nI1013 11:30:52.394343       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/g6j9 397\nI1013 11:30:52.594687       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/hll 254\nI1013 11:30:52.794131       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/fzzl 463\nI1013 11:30:52.994599       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/59m 256\nI1013 11:30:53.193992       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/dvj 578\nI1013 11:30:53.394493       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/2qkb 445\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Oct 13 11:30:53.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7818 delete pod logs-generator'
Oct 13 11:31:07.098: INFO: stderr: ""
Oct 13 11:31:07.098: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:07.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7818" for this suite.

• [SLOW TEST:24.300 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":121,"skipped":1884,"failed":0}
SS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:07.141: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Oct 13 11:31:07.249: INFO: created test-event-1
Oct 13 11:31:07.259: INFO: created test-event-2
Oct 13 11:31:07.269: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Oct 13 11:31:07.282: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Oct 13 11:31:07.336: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:07.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8326" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":122,"skipped":1886,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:07.381: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f
Oct 13 11:31:07.489: INFO: Pod name my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f: Found 0 pods out of 1
Oct 13 11:31:12.510: INFO: Pod name my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f: Found 1 pods out of 1
Oct 13 11:31:12.510: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f" are running
Oct 13 11:31:12.520: INFO: Pod "my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f-qqkzs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 11:31:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 11:31:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 11:31:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 11:31:07 +0000 UTC Reason: Message:}])
Oct 13 11:31:12.521: INFO: Trying to dial the pod
Oct 13 11:31:17.682: INFO: Controller my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f: Got expected result from replica 1 [my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f-qqkzs]: "my-hostname-basic-3c5db007-6c95-443e-8eb7-11460470484f-qqkzs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:17.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-367" for this suite.

• [SLOW TEST:10.347 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":123,"skipped":1897,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:17.728: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:31:17.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50" in namespace "projected-2216" to be "Succeeded or Failed"
Oct 13 11:31:17.860: INFO: Pod "downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50": Phase="Pending", Reason="", readiness=false. Elapsed: 11.955661ms
Oct 13 11:31:19.885: INFO: Pod "downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036747744s
Oct 13 11:31:21.909: INFO: Pod "downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061275028s
STEP: Saw pod success
Oct 13 11:31:21.909: INFO: Pod "downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50" satisfied condition "Succeeded or Failed"
Oct 13 11:31:21.923: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50 container client-container: <nil>
STEP: delete the pod
Oct 13 11:31:22.046: INFO: Waiting for pod downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50 to disappear
Oct 13 11:31:22.057: INFO: Pod downwardapi-volume-a8b8bc73-606a-4739-a243-074241205f50 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:22.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2216" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":124,"skipped":1900,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:22.098: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-d66057c4-1809-4146-b163-63fa2b16b205
STEP: Creating a pod to test consume configMaps
Oct 13 11:31:22.230: INFO: Waiting up to 5m0s for pod "pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d" in namespace "configmap-1885" to be "Succeeded or Failed"
Oct 13 11:31:22.241: INFO: Pod "pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.263693ms
Oct 13 11:31:24.265: INFO: Pod "pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034715179s
Oct 13 11:31:26.277: INFO: Pod "pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047233763s
Oct 13 11:31:28.295: INFO: Pod "pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065081844s
STEP: Saw pod success
Oct 13 11:31:28.295: INFO: Pod "pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d" satisfied condition "Succeeded or Failed"
Oct 13 11:31:28.304: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d container agnhost-container: <nil>
STEP: delete the pod
Oct 13 11:31:28.397: INFO: Waiting for pod pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d to disappear
Oct 13 11:31:28.406: INFO: Pod pod-configmaps-28ea3016-e6c0-4bfa-9fc8-a6a7e06c365d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:28.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1885" for this suite.

• [SLOW TEST:6.332 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":125,"skipped":1917,"failed":0}
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:28.431: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:42.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2364" for this suite.

• [SLOW TEST:14.146 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":126,"skipped":1917,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:42.585: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:42.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2745" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":127,"skipped":1920,"failed":0}
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:42.732: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:46.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9412" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":128,"skipped":1924,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:46.978: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Oct 13 11:31:47.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-6919 create -f -'
Oct 13 11:31:47.504: INFO: stderr: ""
Oct 13 11:31:47.504: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 13 11:31:48.519: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:31:48.519: INFO: Found 0 / 1
Oct 13 11:31:49.522: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:31:49.522: INFO: Found 0 / 1
Oct 13 11:31:50.524: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:31:50.524: INFO: Found 0 / 1
Oct 13 11:31:51.523: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:31:51.523: INFO: Found 1 / 1
Oct 13 11:31:51.523: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Oct 13 11:31:51.533: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:31:51.533: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 13 11:31:51.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-6919 patch pod agnhost-primary-gcjsj -p {"metadata":{"annotations":{"x":"y"}}}'
Oct 13 11:31:51.669: INFO: stderr: ""
Oct 13 11:31:51.669: INFO: stdout: "pod/agnhost-primary-gcjsj patched\n"
STEP: checking annotations
Oct 13 11:31:51.679: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 11:31:51.679: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:51.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6919" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":129,"skipped":1930,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:51.710: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct 13 11:31:51.822: INFO: Waiting up to 5m0s for pod "pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f" in namespace "emptydir-8124" to be "Succeeded or Failed"
Oct 13 11:31:51.834: INFO: Pod "pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.082241ms
Oct 13 11:31:53.848: INFO: Pod "pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026897955s
Oct 13 11:31:55.863: INFO: Pod "pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041524296s
STEP: Saw pod success
Oct 13 11:31:55.863: INFO: Pod "pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f" satisfied condition "Succeeded or Failed"
Oct 13 11:31:55.875: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f container test-container: <nil>
STEP: delete the pod
Oct 13 11:31:55.959: INFO: Waiting for pod pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f to disappear
Oct 13 11:31:55.970: INFO: Pod pod-cf55a7c4-f3d7-40fb-9ebe-4505dfe6ba0f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:31:55.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8124" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":130,"skipped":1944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:31:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5193
STEP: creating service affinity-nodeport-transition in namespace services-5193
STEP: creating replication controller affinity-nodeport-transition in namespace services-5193
I1013 11:31:56.149703      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5193, replica count: 3
I1013 11:31:59.201050      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:32:02.201401      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 11:32:02.239: INFO: Creating new exec pod
Oct 13 11:32:07.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-5193 exec execpod-affinityvp28n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Oct 13 11:32:07.790: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Oct 13 11:32:07.790: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:32:07.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-5193 exec execpod-affinityvp28n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.157 80'
Oct 13 11:32:08.285: INFO: stderr: "+ nc -v -t -w 2 10.240.26.157 80\n+ echo hostName\nConnection to 10.240.26.157 80 port [tcp/http] succeeded!\n"
Oct 13 11:32:08.286: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:32:08.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-5193 exec execpod-affinityvp28n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.102 31778'
Oct 13 11:32:08.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.102 31778\nConnection to 192.168.1.102 31778 port [tcp/*] succeeded!\n"
Oct 13 11:32:08.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:32:08.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-5193 exec execpod-affinityvp28n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.246 31778'
Oct 13 11:32:09.302: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.246 31778\nConnection to 192.168.1.246 31778 port [tcp/*] succeeded!\n"
Oct 13 11:32:09.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:32:09.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-5193 exec execpod-affinityvp28n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.102:31778/ ; done'
Oct 13 11:32:09.893: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n"
Oct 13 11:32:09.894: INFO: stdout: "\naffinity-nodeport-transition-kpqx8\naffinity-nodeport-transition-t7qbr\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-kpqx8\naffinity-nodeport-transition-t7qbr\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-kpqx8\naffinity-nodeport-transition-t7qbr\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-kpqx8\naffinity-nodeport-transition-t7qbr\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-kpqx8\naffinity-nodeport-transition-t7qbr\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-kpqx8"
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-kpqx8
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-t7qbr
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-kpqx8
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-t7qbr
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-kpqx8
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-t7qbr
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-kpqx8
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-t7qbr
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-kpqx8
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-t7qbr
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:09.894: INFO: Received response from host: affinity-nodeport-transition-kpqx8
Oct 13 11:32:09.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-5193 exec execpod-affinityvp28n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.1.102:31778/ ; done'
Oct 13 11:32:10.420: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.1.102:31778/\n"
Oct 13 11:32:10.421: INFO: stdout: "\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9\naffinity-nodeport-transition-nfpj9"
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Received response from host: affinity-nodeport-transition-nfpj9
Oct 13 11:32:10.421: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5193, will wait for the garbage collector to delete the pods
Oct 13 11:32:10.534: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.428532ms
Oct 13 11:32:10.635: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.655632ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:32:27.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5193" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:31.209 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":131,"skipped":2025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:32:27.224: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 13 11:32:27.345: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 13 11:33:27.450: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:33:27.462: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Oct 13 11:33:31.621: INFO: found a healthy node: flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:33:55.861: INFO: pods created so far: [1 1 1]
Oct 13 11:33:55.861: INFO: length of pods created so far: 3
Oct 13 11:34:11.898: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:34:18.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9413" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:34:19.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5053" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:111.950 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":132,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:34:19.175: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 13 11:34:19.271: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 13 11:34:19.305: INFO: Waiting for terminating namespaces to be deleted...
Oct 13 11:34:19.317: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf before test
Oct 13 11:34:19.344: INFO: canal-b5jhz from kube-system started at 2021-10-12 14:23:37 +0000 UTC (2 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 11:34:19.344: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 11:34:19.344: INFO: csi-cinder-nodeplugin-2p26p from kube-system started at 2021-10-12 14:23:37 +0000 UTC (3 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:34:19.344: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:34:19.344: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 11:34:19.344: INFO: kube-proxy-7p6hr from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 11:34:19.344: INFO: node-exporter-qjdpc from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 11:34:19.344: INFO: node-local-dns-tsflg from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 11:34:19.344: INFO: syseleven-node-problem-detector-rqc8m from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 11:34:19.344: INFO: user-ssh-keys-agent-mnrhx from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 11:34:19.344: INFO: sonobuoy from sonobuoy started at 2021-10-13 10:42:23 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 13 11:34:19.344: INFO: sonobuoy-e2e-job-c24ca988246e4377 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container e2e ready: true, restart count 0
Oct 13 11:34:19.344: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:34:19.344: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-qvznk from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:34:19.344: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 13 11:34:19.344: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-57pxk from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container controller ready: true, restart count 0
Oct 13 11:34:19.344: INFO: syseleven-ingress-nginx-ingress-defaultbackend-7b7489f769-xc4lv from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.344: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Oct 13 11:34:19.344: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr before test
Oct 13 11:34:19.379: INFO: canal-zmkkq from kube-system started at 2021-10-12 14:17:59 +0000 UTC (2 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 11:34:19.379: INFO: cluster-autoscaler-74b76d7f55-xqfpn from kube-system started at 2021-10-12 14:19:42 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Oct 13 11:34:19.379: INFO: coredns-7df5db5d6-qm65b from kube-system started at 2021-10-12 14:18:40 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container coredns ready: true, restart count 0
Oct 13 11:34:19.379: INFO: coredns-7df5db5d6-rztzx from kube-system started at 2021-10-12 14:18:40 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container coredns ready: true, restart count 0
Oct 13 11:34:19.379: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-10-12 14:18:40 +0000 UTC (6 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container csi-attacher ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container csi-provisioner ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container csi-resizer ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container csi-snapshotter ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:34:19.379: INFO: csi-cinder-nodeplugin-2wxbc from kube-system started at 2021-10-12 14:18:00 +0000 UTC (3 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 11:34:19.379: INFO: dns-autoscaler-6cc9cd7f9f-p7cpb from kube-system started at 2021-10-12 14:19:42 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container autoscaler ready: true, restart count 0
Oct 13 11:34:19.379: INFO: kube-proxy-ft8xt from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 11:34:19.379: INFO: node-exporter-jxsls from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 11:34:19.379: INFO: node-local-dns-ct8kv from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 11:34:19.379: INFO: openvpn-client-58d7dddf79-7nhns from kube-system started at 2021-10-12 14:18:40 +0000 UTC (2 container statuses recorded)
Oct 13 11:34:19.379: INFO: 	Container dnat-controller ready: true, restart count 0
Oct 13 11:34:19.379: INFO: 	Container openvpn-client ready: true, restart count 0
Oct 13 11:34:19.379: INFO: syseleven-node-problem-detector-2dvkn from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 11:34:19.380: INFO: user-ssh-keys-agent-2kfhw from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 11:34:19.380: INFO: pod4 from sched-preemption-path-9413 started at 2021-10-13 11:34:11 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container pod4 ready: true, restart count 0
Oct 13 11:34:19.380: INFO: rs-pod3-gf9ck from sched-preemption-path-9413 started at 2021-10-13 11:33:51 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container pod3 ready: true, restart count 0
Oct 13 11:34:19.380: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-fpst9 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:34:19.380: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 13 11:34:19.380: INFO: helm-operator-6b9895c4c5-nbgvj from syseleven-helm-operator started at 2021-10-12 14:21:27 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container helm-operator ready: true, restart count 0
Oct 13 11:34:19.380: INFO: syseleven-helm-exporter-66c559868f-v79j4 from syseleven-helm-operator started at 2021-10-12 14:21:43 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container helm-exporter ready: true, restart count 0
Oct 13 11:34:19.380: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-2d4dv from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:34:19.380: INFO: 	Container controller ready: true, restart count 0
Oct 13 11:34:19.380: INFO: kubernetes-dashboard-6b6956d96c-sl4th from syseleven-kubernetes-dashboard started at 2021-10-12 14:21:43 +0000 UTC (2 container statuses recorded)
Oct 13 11:34:19.381: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 13 11:34:19.381: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
STEP: verifying the node has the label node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod canal-b5jhz requesting resource cpu=350m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod canal-zmkkq requesting resource cpu=350m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod cluster-autoscaler-74b76d7f55-xqfpn requesting resource cpu=100m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod coredns-7df5db5d6-qm65b requesting resource cpu=50m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod coredns-7df5db5d6-rztzx requesting resource cpu=50m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod csi-cinder-controllerplugin-0 requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod csi-cinder-nodeplugin-2p26p requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod csi-cinder-nodeplugin-2wxbc requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod dns-autoscaler-6cc9cd7f9f-p7cpb requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod kube-proxy-7p6hr requesting resource cpu=75m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod kube-proxy-ft8xt requesting resource cpu=75m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod node-exporter-jxsls requesting resource cpu=3m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod node-exporter-qjdpc requesting resource cpu=3m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod node-local-dns-ct8kv requesting resource cpu=50m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod node-local-dns-tsflg requesting resource cpu=50m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod openvpn-client-58d7dddf79-7nhns requesting resource cpu=30m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod syseleven-node-problem-detector-2dvkn requesting resource cpu=10m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod syseleven-node-problem-detector-rqc8m requesting resource cpu=10m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod user-ssh-keys-agent-2kfhw requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod user-ssh-keys-agent-mnrhx requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod pod4 requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod rs-pod3-gf9ck requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod sonobuoy requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod sonobuoy-e2e-job-c24ca988246e4377 requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-fpst9 requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-qvznk requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod helm-operator-6b9895c4c5-nbgvj requesting resource cpu=50m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod syseleven-helm-exporter-66c559868f-v79j4 requesting resource cpu=50m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod syseleven-ingress-nginx-ingress-controller-846d65cdcd-2d4dv requesting resource cpu=100m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
Oct 13 11:34:25.666: INFO: Pod syseleven-ingress-nginx-ingress-controller-846d65cdcd-57pxk requesting resource cpu=100m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod syseleven-ingress-nginx-ingress-defaultbackend-7b7489f769-xc4lv requesting resource cpu=0m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.666: INFO: Pod kubernetes-dashboard-6b6956d96c-sl4th requesting resource cpu=100m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
STEP: Starting Pods to consume most of the cluster CPU.
Oct 13 11:34:25.666: INFO: Creating a pod which consumes cpu=708m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
Oct 13 11:34:25.686: INFO: Creating a pod which consumes cpu=407m on Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40234190-7bb9-426a-b1b5-d85897cfa50f.16ad93e84a5519de], Reason = [Scheduled], Message = [Successfully assigned sched-pred-131/filler-pod-40234190-7bb9-426a-b1b5-d85897cfa50f to flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40234190-7bb9-426a-b1b5-d85897cfa50f.16ad93e8abfd7a94], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40234190-7bb9-426a-b1b5-d85897cfa50f.16ad93e8d744e77a], Reason = [Created], Message = [Created container filler-pod-40234190-7bb9-426a-b1b5-d85897cfa50f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40234190-7bb9-426a-b1b5-d85897cfa50f.16ad93e8e2c9870e], Reason = [Started], Message = [Started container filler-pod-40234190-7bb9-426a-b1b5-d85897cfa50f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8c02-2364-43de-ac89-44b4e8b33a0e.16ad93e84b049772], Reason = [Scheduled], Message = [Successfully assigned sched-pred-131/filler-pod-e58c8c02-2364-43de-ac89-44b4e8b33a0e to flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8c02-2364-43de-ac89-44b4e8b33a0e.16ad93e8ad27c24d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8c02-2364-43de-ac89-44b4e8b33a0e.16ad93e8d2a2a24e], Reason = [Created], Message = [Created container filler-pod-e58c8c02-2364-43de-ac89-44b4e8b33a0e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e58c8c02-2364-43de-ac89-44b4e8b33a0e.16ad93e8dee86dfd], Reason = [Started], Message = [Started container filler-pod-e58c8c02-2364-43de-ac89-44b4e8b33a0e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16ad93e93e02835b], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:34:30.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-131" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:11.755 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":133,"skipped":2099,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:34:30.938: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-fr9h
STEP: Creating a pod to test atomic-volume-subpath
Oct 13 11:34:31.082: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fr9h" in namespace "subpath-8237" to be "Succeeded or Failed"
Oct 13 11:34:31.095: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Pending", Reason="", readiness=false. Elapsed: 12.759816ms
Oct 13 11:34:33.121: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03845651s
Oct 13 11:34:35.139: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 4.056267159s
Oct 13 11:34:37.156: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 6.073783894s
Oct 13 11:34:39.178: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 8.095502713s
Oct 13 11:34:41.197: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 10.114389689s
Oct 13 11:34:43.221: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 12.138287521s
Oct 13 11:34:45.237: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 14.154512101s
Oct 13 11:34:47.254: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 16.171534244s
Oct 13 11:34:49.269: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 18.186680733s
Oct 13 11:34:51.290: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 20.207287711s
Oct 13 11:34:53.312: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Running", Reason="", readiness=true. Elapsed: 22.229264531s
Oct 13 11:34:55.326: INFO: Pod "pod-subpath-test-downwardapi-fr9h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.243224689s
STEP: Saw pod success
Oct 13 11:34:55.326: INFO: Pod "pod-subpath-test-downwardapi-fr9h" satisfied condition "Succeeded or Failed"
Oct 13 11:34:55.337: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-subpath-test-downwardapi-fr9h container test-container-subpath-downwardapi-fr9h: <nil>
STEP: delete the pod
Oct 13 11:34:55.393: INFO: Waiting for pod pod-subpath-test-downwardapi-fr9h to disappear
Oct 13 11:34:55.403: INFO: Pod pod-subpath-test-downwardapi-fr9h no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fr9h
Oct 13 11:34:55.403: INFO: Deleting pod "pod-subpath-test-downwardapi-fr9h" in namespace "subpath-8237"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:34:55.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8237" for this suite.

• [SLOW TEST:24.510 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":134,"skipped":2118,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:34:55.450: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:34:56.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 11:34:58.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721696, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721696, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721696, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769721696, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:35:01.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:35:01.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6579" for this suite.
STEP: Destroying namespace "webhook-6579-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.416 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":135,"skipped":2121,"failed":0}
SSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:35:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-2165
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2165
STEP: Deleting pre-stop pod
Oct 13 11:35:17.211: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:35:17.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2165" for this suite.

• [SLOW TEST:15.412 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":136,"skipped":2128,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:35:17.289: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Oct 13 11:35:57.526: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Oct 13 11:35:57.526: INFO: Deleting pod "simpletest.rc-2dtl5" in namespace "gc-759"
W1013 11:35:57.526408      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1013 11:35:57.526457      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1013 11:35:57.526469      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 13 11:35:57.559: INFO: Deleting pod "simpletest.rc-4lf7f" in namespace "gc-759"
Oct 13 11:35:57.586: INFO: Deleting pod "simpletest.rc-hgnzw" in namespace "gc-759"
Oct 13 11:35:57.614: INFO: Deleting pod "simpletest.rc-k6kn8" in namespace "gc-759"
Oct 13 11:35:57.637: INFO: Deleting pod "simpletest.rc-llmbt" in namespace "gc-759"
Oct 13 11:35:57.670: INFO: Deleting pod "simpletest.rc-mmg7q" in namespace "gc-759"
Oct 13 11:35:57.696: INFO: Deleting pod "simpletest.rc-ncwbh" in namespace "gc-759"
Oct 13 11:35:57.741: INFO: Deleting pod "simpletest.rc-q5rph" in namespace "gc-759"
Oct 13 11:35:57.791: INFO: Deleting pod "simpletest.rc-tl9t9" in namespace "gc-759"
Oct 13 11:35:57.841: INFO: Deleting pod "simpletest.rc-xnwmr" in namespace "gc-759"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:35:57.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-759" for this suite.

• [SLOW TEST:40.613 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":137,"skipped":2131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:35:57.903: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:36:06.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8939" for this suite.

• [SLOW TEST:8.190 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":138,"skipped":2184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:36:06.096: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:36:06.187: INFO: Creating deployment "webserver-deployment"
Oct 13 11:36:06.200: INFO: Waiting for observed generation 1
Oct 13 11:36:08.251: INFO: Waiting for all required pods to come up
Oct 13 11:36:08.271: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Oct 13 11:36:16.298: INFO: Waiting for deployment "webserver-deployment" to complete
Oct 13 11:36:16.317: INFO: Updating deployment "webserver-deployment" with a non-existent image
Oct 13 11:36:16.340: INFO: Updating deployment webserver-deployment
Oct 13 11:36:16.340: INFO: Waiting for observed generation 2
Oct 13 11:36:18.367: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Oct 13 11:36:18.377: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Oct 13 11:36:18.386: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 13 11:36:18.411: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Oct 13 11:36:18.411: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Oct 13 11:36:18.419: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Oct 13 11:36:18.434: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Oct 13 11:36:18.434: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Oct 13 11:36:18.453: INFO: Updating deployment webserver-deployment
Oct 13 11:36:18.453: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Oct 13 11:36:18.473: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Oct 13 11:36:20.504: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Oct 13 11:36:20.529: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8566  70379770-bc70-42e5-bdca-c2d817f20804 390558 3 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037a3688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-10-13 11:36:18 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-10-13 11:36:18 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Oct 13 11:36:20.542: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8566  37ab857b-2561-45f4-ba11-371f638f3103 390550 3 2021-10-13 11:36:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 70379770-bc70-42e5-bdca-c2d817f20804 0xc0032e74d7 0xc0032e74d8}] []  [{kube-controller-manager Update apps/v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70379770-bc70-42e5-bdca-c2d817f20804\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032e7568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 13 11:36:20.542: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Oct 13 11:36:20.543: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-8566  6cdcc911-14a5-4d46-afef-e79261b990c9 390556 3 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 70379770-bc70-42e5-bdca-c2d817f20804 0xc0032e75d7 0xc0032e75d8}] []  [{kube-controller-manager Update apps/v1 2021-10-13 11:36:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70379770-bc70-42e5-bdca-c2d817f20804\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032e7648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Oct 13 11:36:20.579: INFO: Pod "webserver-deployment-795d758f88-92lkb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-92lkb webserver-deployment-795d758f88- deployment-8566  2c06ab89-5a62-467a-9199-f64a6975baee 390526 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc0037a3a67 0xc0037a3a68}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kw4dn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kw4dn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.580: INFO: Pod "webserver-deployment-795d758f88-9b4gf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9b4gf webserver-deployment-795d758f88- deployment-8566  6d11248a-e278-44ca-831a-f70ceec028cb 390522 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc0037a3bd0 0xc0037a3bd1}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vphdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vphdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.580: INFO: Pod "webserver-deployment-795d758f88-bfv64" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bfv64 webserver-deployment-795d758f88- deployment-8566  0eebcaa1-6776-4714-9e81-68a4011c6c79 390473 0 2021-10-13 11:36:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.1.46/32 cni.projectcalico.org/podIPs:172.25.1.46/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc0037a3d50 0xc0037a3d51}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xwnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xwnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:36:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.581: INFO: Pod "webserver-deployment-795d758f88-cwlxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cwlxq webserver-deployment-795d758f88- deployment-8566  dda4b9a4-e014-4912-8b84-34607996c496 390543 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc0037a3f37 0xc0037a3f38}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vzrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vzrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.581: INFO: Pod "webserver-deployment-795d758f88-fwk8t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fwk8t webserver-deployment-795d758f88- deployment-8566  028d3bc2-7bf1-46a6-a3c9-571a4d838af8 390533 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf60c0 0xc002bf60c1}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s79s7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s79s7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.582: INFO: Pod "webserver-deployment-795d758f88-kpfdl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kpfdl webserver-deployment-795d758f88- deployment-8566  07c2560e-be0c-4be7-a565-acce281fc820 390581 0 2021-10-13 11:36:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.0.178/32 cni.projectcalico.org/podIPs:172.25.0.178/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf62b7 0xc002bf62b8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-10-13 11:36:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5475d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5475d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.582: INFO: Pod "webserver-deployment-795d758f88-l9s5d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l9s5d webserver-deployment-795d758f88- deployment-8566  d67f6fc1-7378-445a-9832-cf698b5e1ebf 390523 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf64d7 0xc002bf64d8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jv7r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jv7r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.583: INFO: Pod "webserver-deployment-795d758f88-qbx5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qbx5s webserver-deployment-795d758f88- deployment-8566  2fbc380b-fbfd-4a57-acee-0780a71447d5 390560 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf6640 0xc002bf6641}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qb58,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qb58,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.583: INFO: Pod "webserver-deployment-795d758f88-qql4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qql4z webserver-deployment-795d758f88- deployment-8566  d809e088-4e9a-4d7f-b96d-1fcb6f8ab579 390565 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf6817 0xc002bf6818}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6hh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6hh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.584: INFO: Pod "webserver-deployment-795d758f88-rm9ls" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rm9ls webserver-deployment-795d758f88- deployment-8566  fd31b604-d5ee-4ec0-87d2-d3bca119c707 390596 0 2021-10-13 11:36:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.0.180/32 cni.projectcalico.org/podIPs:172.25.0.180/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf6a07 0xc002bf6a08}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-10-13 11:36:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzgmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzgmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.584: INFO: Pod "webserver-deployment-795d758f88-sz44h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sz44h webserver-deployment-795d758f88- deployment-8566  fe0f2896-ce4a-454b-9ffb-5e2e826588a7 390474 0 2021-10-13 11:36:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.1.47/32 cni.projectcalico.org/podIPs:172.25.1.47/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf6c17 0xc002bf6c18}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qh7lb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qh7lb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:36:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.585: INFO: Pod "webserver-deployment-795d758f88-x7vlx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x7vlx webserver-deployment-795d758f88- deployment-8566  be6bca21-2133-40af-8f55-e06bb088c9fe 390591 0 2021-10-13 11:36:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.25.0.179/32 cni.projectcalico.org/podIPs:172.25.0.179/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf6e37 0xc002bf6e38}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-10-13 11:36:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hgws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hgws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.585: INFO: Pod "webserver-deployment-795d758f88-zqk2z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zqk2z webserver-deployment-795d758f88- deployment-8566  3ff25780-1e52-4359-bc13-77b483a11e3e 390525 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 37ab857b-2561-45f4-ba11-371f638f3103 0xc002bf7037 0xc002bf7038}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37ab857b-2561-45f4-ba11-371f638f3103\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vzw5v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vzw5v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.586: INFO: Pod "webserver-deployment-847dcfb7fb-2pf8f" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2pf8f webserver-deployment-847dcfb7fb- deployment-8566  dc7924fe-fb7b-4de3-bbe2-5809490f0db5 390333 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.1.42/32 cni.projectcalico.org/podIPs:172.25.1.42/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf71c0 0xc002bf71c1}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5lxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5lxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:172.25.1.42,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://90d8fd4bad99438a6a8d69306df5efdc604e042b7680d87017cb01dc7a1a3001,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.586: INFO: Pod "webserver-deployment-847dcfb7fb-2xs59" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2xs59 webserver-deployment-847dcfb7fb- deployment-8566  35b932e2-1a32-4b46-a35b-14bdbee91e4b 390337 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.1.44/32 cni.projectcalico.org/podIPs:172.25.1.44/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf73c7 0xc002bf73c8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wxv5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wxv5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:172.25.1.44,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://1eaccea2b912f231abb6c5fa39b21aec2d027a1de9837077d26e2e92352faf92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.587: INFO: Pod "webserver-deployment-847dcfb7fb-4bs4g" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-4bs4g webserver-deployment-847dcfb7fb- deployment-8566  c51840a3-14e0-409a-b798-e2106f2b8b5f 390411 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.0.175/32 cni.projectcalico.org/podIPs:172.25.0.175/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf75d7 0xc002bf75d8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99lq6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99lq6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.175,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://a3622ba746b806a49dc74438886ffbca2566fa61116c978c4a9aff315f8174df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.587: INFO: Pod "webserver-deployment-847dcfb7fb-5qgp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5qgp6 webserver-deployment-847dcfb7fb- deployment-8566  01a04baa-c6c4-4ffc-9257-5f4020d228d6 390532 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf77c7 0xc002bf77c8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrbxb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrbxb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.588: INFO: Pod "webserver-deployment-847dcfb7fb-62hmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-62hmc webserver-deployment-847dcfb7fb- deployment-8566  707e8d2a-5061-48dc-826d-1ea53925f028 390541 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf7920 0xc002bf7921}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5phg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5phg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.588: INFO: Pod "webserver-deployment-847dcfb7fb-6ljd6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6ljd6 webserver-deployment-847dcfb7fb- deployment-8566  72bc125f-1173-46e8-b2f6-fc867be392c3 390386 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.0.172/32 cni.projectcalico.org/podIPs:172.25.0.172/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf7a90 0xc002bf7a91}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g9qxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g9qxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.172,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://1ce3ec98f913b8d96c2def92ba7b91b69842f30b2d260099aa9442fe5e528c63,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.588: INFO: Pod "webserver-deployment-847dcfb7fb-752sf" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-752sf webserver-deployment-847dcfb7fb- deployment-8566  74c4a79f-15ae-4c62-aac4-32e544b9254e 390398 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.0.177/32 cni.projectcalico.org/podIPs:172.25.0.177/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf7c97 0xc002bf7c98}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rx9dg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rx9dg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.177,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://3dd687051e4df89d9cbb75e1343ed0e29c0b7d5d1af96f123cee11dbabeb0ca8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.589: INFO: Pod "webserver-deployment-847dcfb7fb-8nmmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8nmmt webserver-deployment-847dcfb7fb- deployment-8566  c1661891-73cf-4c75-b484-dc6c1d57d355 390562 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002bf7e87 0xc002bf7e88}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fmx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fmx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.589: INFO: Pod "webserver-deployment-847dcfb7fb-8tppc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8tppc webserver-deployment-847dcfb7fb- deployment-8566  fad7e4e8-4ece-4074-a742-078bc5c382a7 390365 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.1.45/32 cni.projectcalico.org/podIPs:172.25.1.45/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f18147 0xc002f18148}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g96rx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g96rx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:172.25.1.45,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://31ccbcc972bae36f86b2d5ada9f2de330f481298be9cf27264b536ed2ef38532,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.589: INFO: Pod "webserver-deployment-847dcfb7fb-9cmdl" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9cmdl webserver-deployment-847dcfb7fb- deployment-8566  d14cd0d6-d94a-4b7c-be8c-1a09195c319e 390564 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f18607 0xc002f18608}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74zdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74zdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.590: INFO: Pod "webserver-deployment-847dcfb7fb-clksd" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-clksd webserver-deployment-847dcfb7fb- deployment-8566  f40bd3e2-86d2-4b8b-ab9e-4406e0097273 390563 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f18a37 0xc002f18a38}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7pmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7pmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.590: INFO: Pod "webserver-deployment-847dcfb7fb-gd4ss" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gd4ss webserver-deployment-847dcfb7fb- deployment-8566  ff0cb9fb-f389-4880-81cb-8ca226a855e5 390557 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f18db7 0xc002f18db8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26pxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26pxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.591: INFO: Pod "webserver-deployment-847dcfb7fb-jvbfw" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jvbfw webserver-deployment-847dcfb7fb- deployment-8566  3837a200-b450-4305-bd6c-daadbeddcac6 390363 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.1.43/32 cni.projectcalico.org/podIPs:172.25.1.43/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f191b7 0xc002f191b8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sr4b2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sr4b2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:172.25.1.43,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://725929e1f1d8ac682070b691930dc623558c718b38d6ca4eea06949385affc33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.591: INFO: Pod "webserver-deployment-847dcfb7fb-kkbs4" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-kkbs4 webserver-deployment-847dcfb7fb- deployment-8566  625ca236-f884-4b6a-82f6-bfdb46862293 390572 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f19657 0xc002f19658}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nc5fd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nc5fd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.591: INFO: Pod "webserver-deployment-847dcfb7fb-lfshb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lfshb webserver-deployment-847dcfb7fb- deployment-8566  2dbd567a-3c4c-4ee5-9001-6d55313fb8b4 390534 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f199d7 0xc002f199d8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppl8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppl8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.592: INFO: Pod "webserver-deployment-847dcfb7fb-lhchw" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lhchw webserver-deployment-847dcfb7fb- deployment-8566  f35f3c2d-b94a-47ab-9b92-1d969d1fc75d 390559 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc002f19cc0 0xc002f19cc1}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d94k8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d94k8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.592: INFO: Pod "webserver-deployment-847dcfb7fb-mdvf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mdvf2 webserver-deployment-847dcfb7fb- deployment-8566  b766a8e8-9ced-42c7-81fc-d9ebbef8302e 390567 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc003d76717 0xc003d76718}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lc5t9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lc5t9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 11:36:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.593: INFO: Pod "webserver-deployment-847dcfb7fb-nc9rh" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nc9rh webserver-deployment-847dcfb7fb- deployment-8566  52640b22-0bdd-4caa-a2c2-9dc93342c26d 390404 0 2021-10-13 11:36:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.0.174/32 cni.projectcalico.org/podIPs:172.25.0.174/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc003d76f07 0xc003d76f08}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8f687,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8f687,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.174,StartTime:2021-10-13 11:36:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:36:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://120ca9df60a0e2eab799b8f02e42c4e2f144373c35837439a4e00f5d278f66c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.593: INFO: Pod "webserver-deployment-847dcfb7fb-tsfvm" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tsfvm webserver-deployment-847dcfb7fb- deployment-8566  6979d3bd-b662-4037-9b9f-95fc541f7150 390535 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc003d775d7 0xc003d775d8}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kfgqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kfgqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:36:20.593: INFO: Pod "webserver-deployment-847dcfb7fb-wnxqk" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wnxqk webserver-deployment-847dcfb7fb- deployment-8566  def41f25-95ac-4458-9eb4-8f05ea3a8c7c 390527 0 2021-10-13 11:36:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 6cdcc911-14a5-4d46-afef-e79261b990c9 0xc003d77d80 0xc003d77d81}] []  [{kube-controller-manager Update v1 2021-10-13 11:36:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cdcc911-14a5-4d46-afef-e79261b990c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsj2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsj2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:36:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:36:20.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8566" for this suite.

• [SLOW TEST:14.534 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":139,"skipped":2228,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:36:20.637: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:36:20.781: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:36:22.803: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:36:24.809: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:36:26.796: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:36:28.799: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:36:30.801: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:36:32.799: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:36:34.796: INFO: The status of Pod busybox-scheduling-c926b85f-918d-4fdb-903b-edb540636213 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:36:34.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7116" for this suite.

• [SLOW TEST:14.270 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":140,"skipped":2231,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:36:34.914: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 13 11:36:35.026: INFO: Waiting up to 5m0s for pod "downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2" in namespace "downward-api-2483" to be "Succeeded or Failed"
Oct 13 11:36:35.037: INFO: Pod "downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.736742ms
Oct 13 11:36:37.051: INFO: Pod "downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024811932s
Oct 13 11:36:39.077: INFO: Pod "downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050971896s
Oct 13 11:36:41.095: INFO: Pod "downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069189156s
STEP: Saw pod success
Oct 13 11:36:41.096: INFO: Pod "downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2" satisfied condition "Succeeded or Failed"
Oct 13 11:36:41.106: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2 container dapi-container: <nil>
STEP: delete the pod
Oct 13 11:36:41.170: INFO: Waiting for pod downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2 to disappear
Oct 13 11:36:41.183: INFO: Pod downward-api-37eb52b8-4206-4179-aed0-37f3feb9e5e2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:36:41.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2483" for this suite.

• [SLOW TEST:6.300 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":141,"skipped":2233,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:36:41.214: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Oct 13 11:36:43.391: INFO: running pods: 0 < 1
Oct 13 11:36:45.406: INFO: running pods: 0 < 1
Oct 13 11:36:47.410: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:36:49.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8845" for this suite.

• [SLOW TEST:8.324 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":142,"skipped":2250,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:36:49.539: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-112.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-112.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 11:36:55.972: INFO: DNS probes using dns-112/dns-test-c1eec042-252e-4658-8240-3a342c02bb01 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:36:56.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-112" for this suite.

• [SLOW TEST:6.497 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":143,"skipped":2257,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:36:56.037: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:36:56.147: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 13 11:37:01.172: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Oct 13 11:37:01.211: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Oct 13 11:37:01.256: INFO: observed ReplicaSet test-rs in namespace replicaset-3380 with ReadyReplicas 1, AvailableReplicas 1
Oct 13 11:37:01.256: INFO: observed ReplicaSet test-rs in namespace replicaset-3380 with ReadyReplicas 1, AvailableReplicas 1
Oct 13 11:37:01.283: INFO: observed ReplicaSet test-rs in namespace replicaset-3380 with ReadyReplicas 1, AvailableReplicas 1
Oct 13 11:37:01.307: INFO: observed ReplicaSet test-rs in namespace replicaset-3380 with ReadyReplicas 1, AvailableReplicas 1
Oct 13 11:37:04.957: INFO: observed ReplicaSet test-rs in namespace replicaset-3380 with ReadyReplicas 2, AvailableReplicas 2
Oct 13 11:37:04.981: INFO: observed Replicaset test-rs in namespace replicaset-3380 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:37:04.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3380" for this suite.

• [SLOW TEST:8.976 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":144,"skipped":2274,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:37:05.014: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6439
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Oct 13 11:37:05.149: INFO: Found 0 stateful pods, waiting for 3
Oct 13 11:37:15.167: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 11:37:15.167: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 11:37:15.167: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Oct 13 11:37:25.183: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 11:37:25.183: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 11:37:25.183: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 11:37:25.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6439 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 11:37:25.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 11:37:25.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 11:37:25.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Oct 13 11:37:35.843: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Oct 13 11:37:45.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6439 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 11:37:48.267: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 13 11:37:48.267: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 11:37:48.267: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 11:37:58.346: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
Oct 13 11:37:58.347: INFO: Waiting for Pod statefulset-6439/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 11:37:58.347: INFO: Waiting for Pod statefulset-6439/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 11:38:08.382: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
Oct 13 11:38:08.382: INFO: Waiting for Pod statefulset-6439/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 11:38:08.382: INFO: Waiting for Pod statefulset-6439/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 11:38:18.378: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
Oct 13 11:38:18.378: INFO: Waiting for Pod statefulset-6439/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 11:38:18.378: INFO: Waiting for Pod statefulset-6439/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 11:38:28.381: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
Oct 13 11:38:28.381: INFO: Waiting for Pod statefulset-6439/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 11:38:38.376: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
STEP: Rolling back to a previous revision
Oct 13 11:38:48.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6439 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 11:38:49.068: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 11:38:49.068: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 11:38:49.068: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 11:38:59.177: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Oct 13 11:39:09.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-6439 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 11:39:09.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 13 11:39:09.847: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 11:39:09.847: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 11:39:19.935: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
Oct 13 11:39:19.935: INFO: Waiting for Pod statefulset-6439/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Oct 13 11:39:19.935: INFO: Waiting for Pod statefulset-6439/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Oct 13 11:39:29.966: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
Oct 13 11:39:29.966: INFO: Waiting for Pod statefulset-6439/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Oct 13 11:39:39.963: INFO: Waiting for StatefulSet statefulset-6439/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Oct 13 11:39:49.970: INFO: Deleting all statefulset in ns statefulset-6439
Oct 13 11:39:49.983: INFO: Scaling statefulset ss2 to 0
Oct 13 11:40:10.054: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 11:40:10.066: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:10.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6439" for this suite.

• [SLOW TEST:185.133 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":145,"skipped":2279,"failed":0}
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:10.150: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:40:10.258: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-978b7150-d944-4af9-852e-cbfca3c34adc" in namespace "security-context-test-5547" to be "Succeeded or Failed"
Oct 13 11:40:10.267: INFO: Pod "busybox-readonly-false-978b7150-d944-4af9-852e-cbfca3c34adc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.197272ms
Oct 13 11:40:12.283: INFO: Pod "busybox-readonly-false-978b7150-d944-4af9-852e-cbfca3c34adc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025102055s
Oct 13 11:40:14.305: INFO: Pod "busybox-readonly-false-978b7150-d944-4af9-852e-cbfca3c34adc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047328602s
Oct 13 11:40:14.305: INFO: Pod "busybox-readonly-false-978b7150-d944-4af9-852e-cbfca3c34adc" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:14.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5547" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":146,"skipped":2279,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:14.344: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-83d849fa-4fa6-4da6-817c-9542c19d4ea7
STEP: Creating a pod to test consume secrets
Oct 13 11:40:14.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc" in namespace "projected-9298" to be "Succeeded or Failed"
Oct 13 11:40:14.504: INFO: Pod "pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.758636ms
Oct 13 11:40:16.522: INFO: Pod "pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032801836s
Oct 13 11:40:18.541: INFO: Pod "pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051535721s
STEP: Saw pod success
Oct 13 11:40:18.541: INFO: Pod "pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc" satisfied condition "Succeeded or Failed"
Oct 13 11:40:18.553: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:40:18.634: INFO: Waiting for pod pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc to disappear
Oct 13 11:40:18.644: INFO: Pod pod-projected-secrets-5008785d-58d1-4762-8cb7-ee5b6e014ffc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:18.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9298" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":147,"skipped":2293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:18.684: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:29.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1696" for this suite.

• [SLOW TEST:11.261 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":148,"skipped":2316,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:29.947: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:30.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5419" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":149,"skipped":2319,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:30.103: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-add5a1cb-8d0a-407a-90a0-8c475e88bb28
STEP: Creating a pod to test consume secrets
Oct 13 11:40:30.218: INFO: Waiting up to 5m0s for pod "pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d" in namespace "secrets-3598" to be "Succeeded or Failed"
Oct 13 11:40:30.229: INFO: Pod "pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.334159ms
Oct 13 11:40:32.252: INFO: Pod "pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033060741s
Oct 13 11:40:34.276: INFO: Pod "pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057196023s
STEP: Saw pod success
Oct 13 11:40:34.276: INFO: Pod "pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d" satisfied condition "Succeeded or Failed"
Oct 13 11:40:34.285: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:40:34.390: INFO: Waiting for pod pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d to disappear
Oct 13 11:40:34.401: INFO: Pod pod-secrets-d6957e62-d548-4418-b4ca-7f5f7ded361d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:34.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3598" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":150,"skipped":2334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:34.436: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:40:34.589: INFO: The status of Pod pod-secrets-fac46cd4-f014-4ff0-b881-d9789054aab8 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:40:36.606: INFO: The status of Pod pod-secrets-fac46cd4-f014-4ff0-b881-d9789054aab8 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:40:38.601: INFO: The status of Pod pod-secrets-fac46cd4-f014-4ff0-b881-d9789054aab8 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:38.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2290" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":151,"skipped":2367,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:38.691: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Oct 13 11:40:38.768: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Oct 13 11:40:39.145: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Oct 13 11:40:41.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:40:43.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:40:45.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:40:47.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:40:49.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:40:51.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722039, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:40:54.502: INFO: Waited 1.202599127s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Oct 13 11:40:55.022: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:40:55.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2057" for this suite.

• [SLOW TEST:17.033 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":152,"skipped":2372,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:40:55.730: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct 13 11:40:55.836: INFO: Waiting up to 5m0s for pod "pod-046008f7-5723-41e3-a304-7fc0b66ea326" in namespace "emptydir-167" to be "Succeeded or Failed"
Oct 13 11:40:55.845: INFO: Pod "pod-046008f7-5723-41e3-a304-7fc0b66ea326": Phase="Pending", Reason="", readiness=false. Elapsed: 9.141299ms
Oct 13 11:40:57.861: INFO: Pod "pod-046008f7-5723-41e3-a304-7fc0b66ea326": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024958325s
Oct 13 11:40:59.883: INFO: Pod "pod-046008f7-5723-41e3-a304-7fc0b66ea326": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047153508s
STEP: Saw pod success
Oct 13 11:40:59.883: INFO: Pod "pod-046008f7-5723-41e3-a304-7fc0b66ea326" satisfied condition "Succeeded or Failed"
Oct 13 11:40:59.895: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-046008f7-5723-41e3-a304-7fc0b66ea326 container test-container: <nil>
STEP: delete the pod
Oct 13 11:40:59.992: INFO: Waiting for pod pod-046008f7-5723-41e3-a304-7fc0b66ea326 to disappear
Oct 13 11:41:00.005: INFO: Pod pod-046008f7-5723-41e3-a304-7fc0b66ea326 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:00.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-167" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":153,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:00.047: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6237.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6237.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6237.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6237.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6237.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6237.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 11:41:06.492: INFO: DNS probes using dns-6237/dns-test-f4429811-9507-425e-8bc9-12807a6915f8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:06.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6237" for this suite.

• [SLOW TEST:6.514 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":154,"skipped":2402,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:06.561: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 13 11:41:06.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-8588 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Oct 13 11:41:06.790: INFO: stderr: ""
Oct 13 11:41:06.790: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Oct 13 11:41:06.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-8588 delete pods e2e-test-httpd-pod'
Oct 13 11:41:11.914: INFO: stderr: ""
Oct 13 11:41:11.914: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:11.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8588" for this suite.

• [SLOW TEST:5.390 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":155,"skipped":2414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:11.952: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 13 11:41:12.076: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:41:14.091: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:41:16.092: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 13 11:41:16.138: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:41:18.162: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:41:20.158: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 13 11:41:20.257: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 13 11:41:20.269: INFO: Pod pod-with-poststart-http-hook still exists
Oct 13 11:41:22.271: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 13 11:41:22.287: INFO: Pod pod-with-poststart-http-hook still exists
Oct 13 11:41:24.271: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 13 11:41:24.293: INFO: Pod pod-with-poststart-http-hook still exists
Oct 13 11:41:26.279: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 13 11:41:26.307: INFO: Pod pod-with-poststart-http-hook still exists
Oct 13 11:41:28.271: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct 13 11:41:28.286: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:28.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7781" for this suite.

• [SLOW TEST:16.369 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":156,"skipped":2443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:28.332: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Oct 13 11:41:28.445: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:41:31.484: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:43.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6673" for this suite.

• [SLOW TEST:15.159 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":157,"skipped":2472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:43.494: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Oct 13 11:41:43.636: INFO: observed Pod pod-test in namespace pods-4089 in phase Pending with labels: map[test-pod-static:true] & conditions []
Oct 13 11:41:43.639: INFO: observed Pod pod-test in namespace pods-4089 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC  }]
Oct 13 11:41:43.672: INFO: observed Pod pod-test in namespace pods-4089 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC  }]
Oct 13 11:41:45.269: INFO: observed Pod pod-test in namespace pods-4089 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC  }]
Oct 13 11:41:46.741: INFO: Found Pod pod-test in namespace pods-4089 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-10-13 11:41:43 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Oct 13 11:41:46.775: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Oct 13 11:41:46.869: INFO: observed event type ADDED
Oct 13 11:41:46.869: INFO: observed event type MODIFIED
Oct 13 11:41:46.869: INFO: observed event type MODIFIED
Oct 13 11:41:46.870: INFO: observed event type MODIFIED
Oct 13 11:41:46.870: INFO: observed event type MODIFIED
Oct 13 11:41:46.870: INFO: observed event type MODIFIED
Oct 13 11:41:46.871: INFO: observed event type MODIFIED
Oct 13 11:41:46.871: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:46.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4089" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":158,"skipped":2504,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:46.909: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:41:47.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df" in namespace "downward-api-832" to be "Succeeded or Failed"
Oct 13 11:41:47.037: INFO: Pod "downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.256452ms
Oct 13 11:41:49.059: INFO: Pod "downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032147324s
Oct 13 11:41:51.078: INFO: Pod "downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051667293s
STEP: Saw pod success
Oct 13 11:41:51.078: INFO: Pod "downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df" satisfied condition "Succeeded or Failed"
Oct 13 11:41:51.090: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df container client-container: <nil>
STEP: delete the pod
Oct 13 11:41:51.189: INFO: Waiting for pod downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df to disappear
Oct 13 11:41:51.199: INFO: Pod downwardapi-volume-9852d8d4-de30-4da9-97fa-67fd3d0c53df no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:51.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-832" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":159,"skipped":2509,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:51.234: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:41:51.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-3745 version'
Oct 13 11:41:51.489: INFO: stderr: ""
Oct 13 11:41:51.489: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.3\", GitCommit:\"ca643a4d1f7bfe34773c74f79527be4afd95bf39\", GitTreeState:\"clean\", BuildDate:\"2021-07-15T21:04:39Z\", GoVersion:\"go1.16.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.3\", GitCommit:\"ca643a4d1f7bfe34773c74f79527be4afd95bf39\", GitTreeState:\"clean\", BuildDate:\"2021-07-15T20:59:07Z\", GoVersion:\"go1.16.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:51.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3745" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":160,"skipped":2526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:51.521: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:53.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3008" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":161,"skipped":2580,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:53.755: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Oct 13 11:41:53.861: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 13 11:41:58.876: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:41:58.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9606" for this suite.

• [SLOW TEST:5.198 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":162,"skipped":2583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:41:58.953: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1737
STEP: creating service affinity-clusterip-transition in namespace services-1737
STEP: creating replication controller affinity-clusterip-transition in namespace services-1737
I1013 11:41:59.045361      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1737, replica count: 3
I1013 11:42:02.097467      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:42:05.097902      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 11:42:05.123: INFO: Creating new exec pod
Oct 13 11:42:10.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1737 exec execpod-affinitylzqrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Oct 13 11:42:10.704: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Oct 13 11:42:10.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:42:10.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1737 exec execpod-affinitylzqrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.126 80'
Oct 13 11:42:11.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.17.126 80\nConnection to 10.240.17.126 80 port [tcp/http] succeeded!\n"
Oct 13 11:42:11.228: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:42:11.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1737 exec execpod-affinitylzqrl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.17.126:80/ ; done'
Oct 13 11:42:11.823: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n"
Oct 13 11:42:11.823: INFO: stdout: "\naffinity-clusterip-transition-px72k\naffinity-clusterip-transition-rxsth\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-px72k\naffinity-clusterip-transition-rxsth\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-px72k\naffinity-clusterip-transition-rxsth\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-px72k\naffinity-clusterip-transition-rxsth\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-px72k\naffinity-clusterip-transition-rxsth\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-px72k"
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-px72k
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-rxsth
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-px72k
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-rxsth
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-px72k
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-rxsth
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-px72k
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-rxsth
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-px72k
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-rxsth
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:11.823: INFO: Received response from host: affinity-clusterip-transition-px72k
Oct 13 11:42:11.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1737 exec execpod-affinitylzqrl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.17.126:80/ ; done'
Oct 13 11:42:12.354: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.17.126:80/\n"
Oct 13 11:42:12.354: INFO: stdout: "\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq\naffinity-clusterip-transition-hh8vq"
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Received response from host: affinity-clusterip-transition-hh8vq
Oct 13 11:42:12.355: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1737, will wait for the garbage collector to delete the pods
Oct 13 11:42:12.466: INFO: Deleting ReplicationController affinity-clusterip-transition took: 20.204552ms
Oct 13 11:42:12.566: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.345253ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:42:28.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1737" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:29.984 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":163,"skipped":2615,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:42:28.941: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Oct 13 11:42:49.284: INFO: EndpointSlice for Service endpointslice-9523/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:42:59.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9523" for this suite.

• [SLOW TEST:30.427 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":164,"skipped":2625,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:42:59.370: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Oct 13 11:42:59.461: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Oct 13 11:42:59.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 create -f -'
Oct 13 11:42:59.914: INFO: stderr: ""
Oct 13 11:42:59.914: INFO: stdout: "service/agnhost-replica created\n"
Oct 13 11:42:59.914: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Oct 13 11:42:59.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 create -f -'
Oct 13 11:43:00.328: INFO: stderr: ""
Oct 13 11:43:00.328: INFO: stdout: "service/agnhost-primary created\n"
Oct 13 11:43:00.329: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Oct 13 11:43:00.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 create -f -'
Oct 13 11:43:00.716: INFO: stderr: ""
Oct 13 11:43:00.716: INFO: stdout: "service/frontend created\n"
Oct 13 11:43:00.716: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Oct 13 11:43:00.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 create -f -'
Oct 13 11:43:01.048: INFO: stderr: ""
Oct 13 11:43:01.048: INFO: stdout: "deployment.apps/frontend created\n"
Oct 13 11:43:01.049: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 13 11:43:01.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 create -f -'
Oct 13 11:43:01.378: INFO: stderr: ""
Oct 13 11:43:01.378: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Oct 13 11:43:01.379: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct 13 11:43:01.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 create -f -'
Oct 13 11:43:01.951: INFO: stderr: ""
Oct 13 11:43:01.951: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Oct 13 11:43:01.951: INFO: Waiting for all frontend pods to be Running.
Oct 13 11:43:12.002: INFO: Waiting for frontend to serve content.
Oct 13 11:43:12.112: INFO: Trying to add a new entry to the guestbook.
Oct 13 11:43:12.140: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Oct 13 11:43:12.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 delete --grace-period=0 --force -f -'
Oct 13 11:43:12.394: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:43:12.394: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Oct 13 11:43:12.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 delete --grace-period=0 --force -f -'
Oct 13 11:43:12.545: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:43:12.546: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 13 11:43:12.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 delete --grace-period=0 --force -f -'
Oct 13 11:43:12.680: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:43:12.680: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 13 11:43:12.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 delete --grace-period=0 --force -f -'
Oct 13 11:43:12.794: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:43:12.794: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct 13 11:43:12.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 delete --grace-period=0 --force -f -'
Oct 13 11:43:13.014: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:43:13.014: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Oct 13 11:43:13.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2492 delete --grace-period=0 --force -f -'
Oct 13 11:43:13.156: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:43:13.157: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:43:13.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2492" for this suite.

• [SLOW TEST:13.832 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":165,"skipped":2642,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:43:13.202: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Oct 13 11:43:13.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 create -f -'
Oct 13 11:43:13.877: INFO: stderr: ""
Oct 13 11:43:13.877: INFO: stdout: "pod/pause created\n"
Oct 13 11:43:13.877: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Oct 13 11:43:13.877: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7645" to be "running and ready"
Oct 13 11:43:13.887: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.649255ms
Oct 13 11:43:15.913: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03523053s
Oct 13 11:43:17.927: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049807174s
Oct 13 11:43:19.943: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.065745046s
Oct 13 11:43:19.944: INFO: Pod "pause" satisfied condition "running and ready"
Oct 13 11:43:19.944: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Oct 13 11:43:19.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 label pods pause testing-label=testing-label-value'
Oct 13 11:43:20.071: INFO: stderr: ""
Oct 13 11:43:20.071: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Oct 13 11:43:20.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 get pod pause -L testing-label'
Oct 13 11:43:20.175: INFO: stderr: ""
Oct 13 11:43:20.175: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Oct 13 11:43:20.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 label pods pause testing-label-'
Oct 13 11:43:20.302: INFO: stderr: ""
Oct 13 11:43:20.302: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Oct 13 11:43:20.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 get pod pause -L testing-label'
Oct 13 11:43:20.397: INFO: stderr: ""
Oct 13 11:43:20.397: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Oct 13 11:43:20.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 delete --grace-period=0 --force -f -'
Oct 13 11:43:20.514: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:43:20.514: INFO: stdout: "pod \"pause\" force deleted\n"
Oct 13 11:43:20.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 get rc,svc -l name=pause --no-headers'
Oct 13 11:43:20.633: INFO: stderr: "No resources found in kubectl-7645 namespace.\n"
Oct 13 11:43:20.633: INFO: stdout: ""
Oct 13 11:43:20.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7645 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 13 11:43:20.757: INFO: stderr: ""
Oct 13 11:43:20.757: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:43:20.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7645" for this suite.

• [SLOW TEST:7.586 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":166,"skipped":2645,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:43:20.789: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Oct 13 11:43:20.863: INFO: Major version: 1
STEP: Confirm minor version
Oct 13 11:43:20.863: INFO: cleanMinorVersion: 21
Oct 13 11:43:20.863: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:43:20.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3126" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":167,"skipped":2657,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:43:20.887: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:43:20.955: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:43:21.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2647" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":168,"skipped":2665,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:43:21.593: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:43:22.148: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 11:43:24.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722202, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722202, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722202, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722202, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:43:27.232: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:43:27.243: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8976-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:43:30.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-387" for this suite.
STEP: Destroying namespace "webhook-387-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.159 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":169,"skipped":2670,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:43:30.754: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9811
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9811
STEP: creating replication controller externalsvc in namespace services-9811
I1013 11:43:30.887787      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9811, replica count: 2
I1013 11:43:33.942785      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:43:36.944030      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Oct 13 11:43:37.011: INFO: Creating new exec pod
Oct 13 11:43:41.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-9811 exec execpodzpc8m -- /bin/sh -x -c nslookup clusterip-service.services-9811.svc.cluster.local'
Oct 13 11:43:41.554: INFO: stderr: "+ nslookup clusterip-service.services-9811.svc.cluster.local\n"
Oct 13 11:43:41.554: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-9811.svc.cluster.local\tcanonical name = externalsvc.services-9811.svc.cluster.local.\nName:\texternalsvc.services-9811.svc.cluster.local\nAddress: 10.240.17.130\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9811, will wait for the garbage collector to delete the pods
Oct 13 11:43:41.631: INFO: Deleting ReplicationController externalsvc took: 14.33907ms
Oct 13 11:43:41.732: INFO: Terminating ReplicationController externalsvc pods took: 101.010107ms
Oct 13 11:43:47.278: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:43:47.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9811" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:16.578 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":170,"skipped":2675,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:43:47.334: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W1013 11:43:47.412873      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:48:47.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7112" for this suite.

• [SLOW TEST:300.177 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":171,"skipped":2685,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:48:47.512: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Oct 13 11:48:47.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 create -f -'
Oct 13 11:48:49.580: INFO: stderr: ""
Oct 13 11:48:49.580: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 13 11:48:49.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 11:48:49.861: INFO: stderr: ""
Oct 13 11:48:49.861: INFO: stdout: "update-demo-nautilus-dmg7z update-demo-nautilus-rc4qd "
Oct 13 11:48:49.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods update-demo-nautilus-dmg7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 11:48:49.945: INFO: stderr: ""
Oct 13 11:48:49.945: INFO: stdout: ""
Oct 13 11:48:49.945: INFO: update-demo-nautilus-dmg7z is created but not running
Oct 13 11:48:54.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 11:48:55.083: INFO: stderr: ""
Oct 13 11:48:55.083: INFO: stdout: "update-demo-nautilus-dmg7z update-demo-nautilus-rc4qd "
Oct 13 11:48:55.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods update-demo-nautilus-dmg7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 11:48:55.197: INFO: stderr: ""
Oct 13 11:48:55.197: INFO: stdout: ""
Oct 13 11:48:55.197: INFO: update-demo-nautilus-dmg7z is created but not running
Oct 13 11:49:00.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 11:49:00.323: INFO: stderr: ""
Oct 13 11:49:00.323: INFO: stdout: "update-demo-nautilus-dmg7z update-demo-nautilus-rc4qd "
Oct 13 11:49:00.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods update-demo-nautilus-dmg7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 11:49:00.426: INFO: stderr: ""
Oct 13 11:49:00.426: INFO: stdout: "true"
Oct 13 11:49:00.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods update-demo-nautilus-dmg7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 13 11:49:00.543: INFO: stderr: ""
Oct 13 11:49:00.543: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 13 11:49:00.543: INFO: validating pod update-demo-nautilus-dmg7z
Oct 13 11:49:00.622: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 13 11:49:00.622: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 13 11:49:00.622: INFO: update-demo-nautilus-dmg7z is verified up and running
Oct 13 11:49:00.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods update-demo-nautilus-rc4qd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 11:49:00.723: INFO: stderr: ""
Oct 13 11:49:00.723: INFO: stdout: "true"
Oct 13 11:49:00.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods update-demo-nautilus-rc4qd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 13 11:49:00.824: INFO: stderr: ""
Oct 13 11:49:00.824: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 13 11:49:00.824: INFO: validating pod update-demo-nautilus-rc4qd
Oct 13 11:49:00.848: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 13 11:49:00.848: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 13 11:49:00.848: INFO: update-demo-nautilus-rc4qd is verified up and running
STEP: using delete to clean up resources
Oct 13 11:49:00.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 delete --grace-period=0 --force -f -'
Oct 13 11:49:01.003: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 11:49:01.003: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 13 11:49:01.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get rc,svc -l name=update-demo --no-headers'
Oct 13 11:49:01.150: INFO: stderr: "No resources found in kubectl-2229 namespace.\n"
Oct 13 11:49:01.150: INFO: stdout: ""
Oct 13 11:49:01.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-2229 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 13 11:49:01.252: INFO: stderr: ""
Oct 13 11:49:01.252: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:01.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2229" for this suite.

• [SLOW TEST:13.782 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":172,"skipped":2685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:01.295: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Oct 13 11:49:01.437: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Oct 13 11:49:03.496: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Oct 13 11:49:05.554: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:07.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6898" for this suite.

• [SLOW TEST:6.317 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":173,"skipped":2713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:07.612: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:49:07.716: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:49:09.735: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:49:11.739: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:13.734: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:15.742: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:17.734: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:19.735: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:21.738: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:23.738: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:25.738: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:27.733: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = false)
Oct 13 11:49:29.739: INFO: The status of Pod test-webserver-080d0542-f93d-4b15-b73d-6c4ffd192972 is Running (Ready = true)
Oct 13 11:49:29.751: INFO: Container started at 2021-10-13 11:49:10 +0000 UTC, pod became ready at 2021-10-13 11:49:27 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:29.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6299" for this suite.

• [SLOW TEST:22.175 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":174,"skipped":2737,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:29.792: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-7de34221-955a-4230-80d9-78fd22424f43
STEP: Creating a pod to test consume secrets
Oct 13 11:49:29.919: INFO: Waiting up to 5m0s for pod "pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673" in namespace "secrets-4117" to be "Succeeded or Failed"
Oct 13 11:49:29.929: INFO: Pod "pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673": Phase="Pending", Reason="", readiness=false. Elapsed: 10.497251ms
Oct 13 11:49:31.953: INFO: Pod "pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033640344s
Oct 13 11:49:33.977: INFO: Pod "pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058461384s
Oct 13 11:49:36.001: INFO: Pod "pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082440509s
STEP: Saw pod success
Oct 13 11:49:36.001: INFO: Pod "pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673" satisfied condition "Succeeded or Failed"
Oct 13 11:49:36.013: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673 container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:49:36.110: INFO: Waiting for pod pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673 to disappear
Oct 13 11:49:36.120: INFO: Pod pod-secrets-7353e5df-de32-47b4-8b4f-6edc19a28673 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:36.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4117" for this suite.

• [SLOW TEST:6.359 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":175,"skipped":2744,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:36.155: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-df058e7d-b469-4d26-9729-455719f7979e
STEP: Creating a pod to test consume secrets
Oct 13 11:49:36.275: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292" in namespace "projected-8038" to be "Succeeded or Failed"
Oct 13 11:49:36.288: INFO: Pod "pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292": Phase="Pending", Reason="", readiness=false. Elapsed: 12.711338ms
Oct 13 11:49:38.305: INFO: Pod "pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030053915s
Oct 13 11:49:40.327: INFO: Pod "pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051647886s
STEP: Saw pod success
Oct 13 11:49:40.327: INFO: Pod "pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292" satisfied condition "Succeeded or Failed"
Oct 13 11:49:40.338: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:49:40.593: INFO: Waiting for pod pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292 to disappear
Oct 13 11:49:40.603: INFO: Pod pod-projected-secrets-5a7488a0-9073-4d70-9446-18cc367e0292 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:40.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8038" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":176,"skipped":2749,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:40.644: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-7712/configmap-test-783992ed-c1cf-40da-ac0d-91832483e1a2
STEP: Creating a pod to test consume configMaps
Oct 13 11:49:40.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-78be9402-db64-464c-850c-a69789740732" in namespace "configmap-7712" to be "Succeeded or Failed"
Oct 13 11:49:40.777: INFO: Pod "pod-configmaps-78be9402-db64-464c-850c-a69789740732": Phase="Pending", Reason="", readiness=false. Elapsed: 11.672255ms
Oct 13 11:49:42.801: INFO: Pod "pod-configmaps-78be9402-db64-464c-850c-a69789740732": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035488615s
Oct 13 11:49:44.821: INFO: Pod "pod-configmaps-78be9402-db64-464c-850c-a69789740732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056265852s
STEP: Saw pod success
Oct 13 11:49:44.822: INFO: Pod "pod-configmaps-78be9402-db64-464c-850c-a69789740732" satisfied condition "Succeeded or Failed"
Oct 13 11:49:44.833: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-78be9402-db64-464c-850c-a69789740732 container env-test: <nil>
STEP: delete the pod
Oct 13 11:49:44.995: INFO: Waiting for pod pod-configmaps-78be9402-db64-464c-850c-a69789740732 to disappear
Oct 13 11:49:45.006: INFO: Pod pod-configmaps-78be9402-db64-464c-850c-a69789740732 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:45.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7712" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":177,"skipped":2765,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:45.048: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 13 11:49:45.163: INFO: Waiting up to 5m0s for pod "pod-2ff3858c-abc5-4d72-9286-85fa281378e6" in namespace "emptydir-7332" to be "Succeeded or Failed"
Oct 13 11:49:45.180: INFO: Pod "pod-2ff3858c-abc5-4d72-9286-85fa281378e6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.648794ms
Oct 13 11:49:47.197: INFO: Pod "pod-2ff3858c-abc5-4d72-9286-85fa281378e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034359971s
Oct 13 11:49:49.213: INFO: Pod "pod-2ff3858c-abc5-4d72-9286-85fa281378e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049432658s
STEP: Saw pod success
Oct 13 11:49:49.213: INFO: Pod "pod-2ff3858c-abc5-4d72-9286-85fa281378e6" satisfied condition "Succeeded or Failed"
Oct 13 11:49:49.226: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-2ff3858c-abc5-4d72-9286-85fa281378e6 container test-container: <nil>
STEP: delete the pod
Oct 13 11:49:49.293: INFO: Waiting for pod pod-2ff3858c-abc5-4d72-9286-85fa281378e6 to disappear
Oct 13 11:49:49.304: INFO: Pod pod-2ff3858c-abc5-4d72-9286-85fa281378e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:49.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7332" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":178,"skipped":2776,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:49.337: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Oct 13 11:49:50.003: INFO: created pod pod-service-account-defaultsa
Oct 13 11:49:50.003: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Oct 13 11:49:50.017: INFO: created pod pod-service-account-mountsa
Oct 13 11:49:50.018: INFO: pod pod-service-account-mountsa service account token volume mount: true
Oct 13 11:49:50.032: INFO: created pod pod-service-account-nomountsa
Oct 13 11:49:50.032: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Oct 13 11:49:50.043: INFO: created pod pod-service-account-defaultsa-mountspec
Oct 13 11:49:50.043: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Oct 13 11:49:50.055: INFO: created pod pod-service-account-mountsa-mountspec
Oct 13 11:49:50.055: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Oct 13 11:49:50.065: INFO: created pod pod-service-account-nomountsa-mountspec
Oct 13 11:49:50.065: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Oct 13 11:49:50.077: INFO: created pod pod-service-account-defaultsa-nomountspec
Oct 13 11:49:50.077: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Oct 13 11:49:50.088: INFO: created pod pod-service-account-mountsa-nomountspec
Oct 13 11:49:50.088: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Oct 13 11:49:50.097: INFO: created pod pod-service-account-nomountsa-nomountspec
Oct 13 11:49:50.098: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:49:50.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4873" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":179,"skipped":2779,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:49:50.126: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:50:07.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9564" for this suite.

• [SLOW TEST:17.248 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":180,"skipped":2787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:50:07.375: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:50:07.869: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Oct 13 11:50:09.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722607, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722607, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722607, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722607, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:50:12.958: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:50:12.971: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:50:16.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1355" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.306 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":181,"skipped":2824,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:50:16.683: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 13 11:50:16.790: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:50:18.808: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:50:20.814: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:50:22.816: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 13 11:50:22.862: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:50:24.885: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:50:26.886: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct 13 11:50:26.956: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 13 11:50:26.970: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 13 11:50:28.972: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 13 11:50:28.982: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 13 11:50:30.970: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 13 11:50:30.983: INFO: Pod pod-with-poststart-exec-hook still exists
Oct 13 11:50:32.971: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct 13 11:50:32.992: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:50:32.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9257" for this suite.

• [SLOW TEST:16.339 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":182,"skipped":2833,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:50:33.027: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:50:33.131: INFO: Pod name rollover-pod: Found 0 pods out of 1
Oct 13 11:50:38.156: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 13 11:50:38.157: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Oct 13 11:50:40.169: INFO: Creating deployment "test-rollover-deployment"
Oct 13 11:50:40.195: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Oct 13 11:50:42.224: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Oct 13 11:50:42.250: INFO: Ensure that both replica sets have 1 created replica
Oct 13 11:50:42.283: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Oct 13 11:50:42.305: INFO: Updating deployment test-rollover-deployment
Oct 13 11:50:42.305: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Oct 13 11:50:44.342: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Oct 13 11:50:44.368: INFO: Make sure deployment "test-rollover-deployment" is complete
Oct 13 11:50:44.396: INFO: all replica sets need to contain the pod-template-hash label
Oct 13 11:50:44.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722642, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:50:46.426: INFO: all replica sets need to contain the pod-template-hash label
Oct 13 11:50:46.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722642, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:50:48.423: INFO: all replica sets need to contain the pod-template-hash label
Oct 13 11:50:48.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722646, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:50:50.422: INFO: all replica sets need to contain the pod-template-hash label
Oct 13 11:50:50.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722646, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:50:52.436: INFO: all replica sets need to contain the pod-template-hash label
Oct 13 11:50:52.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722646, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:50:54.434: INFO: all replica sets need to contain the pod-template-hash label
Oct 13 11:50:54.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722646, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:50:56.444: INFO: all replica sets need to contain the pod-template-hash label
Oct 13 11:50:56.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722646, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722640, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 11:50:58.424: INFO: 
Oct 13 11:50:58.424: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Oct 13 11:50:58.458: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2605  a3c2fb84-fdcf-479b-afea-1a71f019be46 397111 2 2021-10-13 11:50:40 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-10-13 11:50:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-10-13 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bf7cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-10-13 11:50:40 +0000 UTC,LastTransitionTime:2021-10-13 11:50:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-10-13 11:50:56 +0000 UTC,LastTransitionTime:2021-10-13 11:50:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 13 11:50:58.469: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-2605  0f807788-93bb-461f-80f4-e89429a8f9ac 397101 2 2021-10-13 11:50:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a3c2fb84-fdcf-479b-afea-1a71f019be46 0xc0037ad6e0 0xc0037ad6e1}] []  [{kube-controller-manager Update apps/v1 2021-10-13 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3c2fb84-fdcf-479b-afea-1a71f019be46\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037ad758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 13 11:50:58.469: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Oct 13 11:50:58.469: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2605  fd8104fa-6e5a-4e98-9e9e-84a4bbaa18fb 397110 2 2021-10-13 11:50:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a3c2fb84-fdcf-479b-afea-1a71f019be46 0xc0037ad4d7 0xc0037ad4d8}] []  [{e2e.test Update apps/v1 2021-10-13 11:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-10-13 11:50:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3c2fb84-fdcf-479b-afea-1a71f019be46\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037ad578 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 13 11:50:58.470: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-2605  242e14cb-0072-42cc-bef1-12c250efdf4f 397033 2 2021-10-13 11:50:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a3c2fb84-fdcf-479b-afea-1a71f019be46 0xc0037ad5e7 0xc0037ad5e8}] []  [{kube-controller-manager Update apps/v1 2021-10-13 11:50:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3c2fb84-fdcf-479b-afea-1a71f019be46\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037ad678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 13 11:50:58.482: INFO: Pod "test-rollover-deployment-98c5f4599-zwf69" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-zwf69 test-rollover-deployment-98c5f4599- deployment-2605  aca6d66d-5614-458a-acde-3631615f9227 397058 0 2021-10-13 11:50:42 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/podIP:172.25.0.241/32 cni.projectcalico.org/podIPs:172.25.0.241/32] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 0f807788-93bb-461f-80f4-e89429a8f9ac 0xc0037adcb0 0xc0037adcb1}] []  [{kube-controller-manager Update v1 2021-10-13 11:50:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f807788-93bb-461f-80f4-e89429a8f9ac\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:50:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wxn4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wxn4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:50:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:50:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:50:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:50:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.241,StartTime:2021-10-13 11:50:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:50:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://6b4fe2950c48b994ad76b84b04c4a1742489abd5509384d1b49af88d17642c40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:50:58.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2605" for this suite.

• [SLOW TEST:25.488 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":183,"skipped":2836,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:50:58.517: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 13 11:50:58.627: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 13 11:50:58.658: INFO: Waiting for terminating namespaces to be deleted...
Oct 13 11:50:58.668: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf before test
Oct 13 11:50:58.695: INFO: canal-b5jhz from kube-system started at 2021-10-12 14:23:37 +0000 UTC (2 container statuses recorded)
Oct 13 11:50:58.696: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 11:50:58.696: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 11:50:58.696: INFO: csi-cinder-nodeplugin-2p26p from kube-system started at 2021-10-12 14:23:37 +0000 UTC (3 container statuses recorded)
Oct 13 11:50:58.696: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:50:58.697: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:50:58.697: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 11:50:58.697: INFO: kube-proxy-7p6hr from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.697: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 11:50:58.697: INFO: node-exporter-qjdpc from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.698: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 11:50:58.698: INFO: node-local-dns-tsflg from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.698: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 11:50:58.698: INFO: syseleven-node-problem-detector-rqc8m from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.698: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 11:50:58.698: INFO: user-ssh-keys-agent-mnrhx from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.699: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 11:50:58.699: INFO: sonobuoy from sonobuoy started at 2021-10-13 10:42:23 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.699: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 13 11:50:58.699: INFO: sonobuoy-e2e-job-c24ca988246e4377 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:50:58.699: INFO: 	Container e2e ready: true, restart count 0
Oct 13 11:50:58.700: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:50:58.700: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-qvznk from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:50:58.700: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:50:58.700: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 13 11:50:58.700: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-57pxk from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.700: INFO: 	Container controller ready: true, restart count 0
Oct 13 11:50:58.701: INFO: syseleven-ingress-nginx-ingress-defaultbackend-7b7489f769-xc4lv from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.701: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Oct 13 11:50:58.701: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr before test
Oct 13 11:50:58.747: INFO: test-rollover-deployment-98c5f4599-zwf69 from deployment-2605 started at 2021-10-13 11:50:42 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.747: INFO: 	Container agnhost ready: true, restart count 0
Oct 13 11:50:58.748: INFO: canal-zmkkq from kube-system started at 2021-10-12 14:17:59 +0000 UTC (2 container statuses recorded)
Oct 13 11:50:58.748: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 11:50:58.748: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 11:50:58.748: INFO: cluster-autoscaler-74b76d7f55-xqfpn from kube-system started at 2021-10-12 14:19:42 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.749: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Oct 13 11:50:58.749: INFO: coredns-7df5db5d6-qm65b from kube-system started at 2021-10-12 14:18:40 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.749: INFO: 	Container coredns ready: true, restart count 0
Oct 13 11:50:58.749: INFO: coredns-7df5db5d6-rztzx from kube-system started at 2021-10-12 14:18:40 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.749: INFO: 	Container coredns ready: true, restart count 0
Oct 13 11:50:58.750: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-10-12 14:18:40 +0000 UTC (6 container statuses recorded)
Oct 13 11:50:58.750: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:50:58.750: INFO: 	Container csi-attacher ready: true, restart count 0
Oct 13 11:50:58.750: INFO: 	Container csi-provisioner ready: true, restart count 0
Oct 13 11:50:58.750: INFO: 	Container csi-resizer ready: true, restart count 0
Oct 13 11:50:58.751: INFO: 	Container csi-snapshotter ready: true, restart count 0
Oct 13 11:50:58.751: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:50:58.751: INFO: csi-cinder-nodeplugin-2wxbc from kube-system started at 2021-10-12 14:18:00 +0000 UTC (3 container statuses recorded)
Oct 13 11:50:58.751: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 11:50:58.751: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 11:50:58.751: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 11:50:58.752: INFO: dns-autoscaler-6cc9cd7f9f-p7cpb from kube-system started at 2021-10-12 14:19:42 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.752: INFO: 	Container autoscaler ready: true, restart count 0
Oct 13 11:50:58.752: INFO: kube-proxy-ft8xt from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.752: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 11:50:58.752: INFO: node-exporter-jxsls from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.753: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 11:50:58.753: INFO: node-local-dns-ct8kv from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.753: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 11:50:58.753: INFO: openvpn-client-58d7dddf79-7nhns from kube-system started at 2021-10-12 14:18:40 +0000 UTC (2 container statuses recorded)
Oct 13 11:50:58.753: INFO: 	Container dnat-controller ready: true, restart count 0
Oct 13 11:50:58.753: INFO: 	Container openvpn-client ready: true, restart count 0
Oct 13 11:50:58.754: INFO: syseleven-node-problem-detector-2dvkn from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.754: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 11:50:58.754: INFO: user-ssh-keys-agent-2kfhw from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.754: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 11:50:58.754: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-fpst9 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 11:50:58.754: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 11:50:58.754: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 13 11:50:58.755: INFO: pod-service-account-mountsa-nomountspec from svcaccounts-4873 started at 2021-10-13 11:49:50 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.755: INFO: 	Container token-test ready: true, restart count 0
Oct 13 11:50:58.755: INFO: pod-service-account-nomountsa from svcaccounts-4873 started at 2021-10-13 11:49:50 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.755: INFO: 	Container token-test ready: true, restart count 0
Oct 13 11:50:58.755: INFO: helm-operator-6b9895c4c5-nbgvj from syseleven-helm-operator started at 2021-10-12 14:21:27 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.755: INFO: 	Container helm-operator ready: true, restart count 0
Oct 13 11:50:58.755: INFO: syseleven-helm-exporter-66c559868f-v79j4 from syseleven-helm-operator started at 2021-10-12 14:21:43 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.755: INFO: 	Container helm-exporter ready: true, restart count 0
Oct 13 11:50:58.755: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-2d4dv from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 11:50:58.755: INFO: 	Container controller ready: true, restart count 0
Oct 13 11:50:58.755: INFO: kubernetes-dashboard-6b6956d96c-sl4th from syseleven-kubernetes-dashboard started at 2021-10-12 14:21:43 +0000 UTC (2 container statuses recorded)
Oct 13 11:50:58.755: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 13 11:50:58.755: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a61d5afb-3d41-40e7-b281-5ed07d0daaf3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a61d5afb-3d41-40e7-b281-5ed07d0daaf3 off the node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a61d5afb-3d41-40e7-b281-5ed07d0daaf3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:51:09.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-975" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:10.537 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":184,"skipped":2836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:51:09.056: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:51:09.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676" in namespace "projected-773" to be "Succeeded or Failed"
Oct 13 11:51:09.158: INFO: Pod "downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038285ms
Oct 13 11:51:11.178: INFO: Pod "downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032768418s
Oct 13 11:51:13.200: INFO: Pod "downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054853701s
STEP: Saw pod success
Oct 13 11:51:13.201: INFO: Pod "downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676" satisfied condition "Succeeded or Failed"
Oct 13 11:51:13.214: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676 container client-container: <nil>
STEP: delete the pod
Oct 13 11:51:13.317: INFO: Waiting for pod downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676 to disappear
Oct 13 11:51:13.327: INFO: Pod downwardapi-volume-babb0260-c277-4c3e-9520-88b857fc7676 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:51:13.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-773" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":185,"skipped":2859,"failed":0}
SS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:51:13.374: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:51:17.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9950" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":186,"skipped":2861,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:51:17.636: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:51:17.763: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Oct 13 11:51:17.799: INFO: Number of nodes with available pods: 0
Oct 13 11:51:17.799: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:51:18.829: INFO: Number of nodes with available pods: 0
Oct 13 11:51:18.829: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:51:19.833: INFO: Number of nodes with available pods: 0
Oct 13 11:51:19.833: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:51:20.836: INFO: Number of nodes with available pods: 0
Oct 13 11:51:20.836: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:51:21.833: INFO: Number of nodes with available pods: 2
Oct 13 11:51:21.833: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Oct 13 11:51:21.928: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:21.928: INFO: Wrong image for pod: daemon-set-kft56. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:22.958: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:23.962: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:24.962: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:25.962: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:26.963: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:27.962: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:28.959: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:29.964: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:30.957: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:31.961: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:32.965: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:33.959: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:34.959: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:35.962: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:36.964: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:37.961: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:37.961: INFO: Pod daemon-set-swqgm is not available
Oct 13 11:51:38.963: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:38.963: INFO: Pod daemon-set-swqgm is not available
Oct 13 11:51:39.957: INFO: Wrong image for pod: daemon-set-4nrnt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Oct 13 11:51:39.958: INFO: Pod daemon-set-swqgm is not available
Oct 13 11:51:48.960: INFO: Pod daemon-set-tk8mb is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Oct 13 11:51:48.998: INFO: Number of nodes with available pods: 1
Oct 13 11:51:48.998: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:51:50.036: INFO: Number of nodes with available pods: 1
Oct 13 11:51:50.036: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:51:51.032: INFO: Number of nodes with available pods: 1
Oct 13 11:51:51.032: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 11:51:52.035: INFO: Number of nodes with available pods: 2
Oct 13 11:51:52.035: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-750, will wait for the garbage collector to delete the pods
Oct 13 11:51:52.181: INFO: Deleting DaemonSet.extensions daemon-set took: 21.239254ms
Oct 13 11:51:52.281: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.12731ms
Oct 13 11:51:58.902: INFO: Number of nodes with available pods: 0
Oct 13 11:51:58.902: INFO: Number of running nodes: 0, number of available pods: 0
Oct 13 11:51:58.913: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"397621"},"items":null}

Oct 13 11:51:58.922: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"397622"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:51:58.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-750" for this suite.

• [SLOW TEST:41.361 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":187,"skipped":2886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:51:59.003: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct 13 11:51:59.138: INFO: Waiting up to 5m0s for pod "pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a" in namespace "emptydir-2888" to be "Succeeded or Failed"
Oct 13 11:51:59.150: INFO: Pod "pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.973819ms
Oct 13 11:52:01.171: INFO: Pod "pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033755005s
Oct 13 11:52:03.192: INFO: Pod "pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054259525s
Oct 13 11:52:05.213: INFO: Pod "pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075301327s
STEP: Saw pod success
Oct 13 11:52:05.213: INFO: Pod "pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a" satisfied condition "Succeeded or Failed"
Oct 13 11:52:05.225: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a container test-container: <nil>
STEP: delete the pod
Oct 13 11:52:05.338: INFO: Waiting for pod pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a to disappear
Oct 13 11:52:05.349: INFO: Pod pod-eed638f7-73db-4b9f-a686-a92aa5f4b30a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:05.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2888" for this suite.

• [SLOW TEST:6.379 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":188,"skipped":2920,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:05.386: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 13 11:52:05.524: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:52:07.534: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:52:09.540: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 13 11:52:09.584: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:52:11.599: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:52:13.603: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Oct 13 11:52:13.633: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 13 11:52:13.648: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 13 11:52:15.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 13 11:52:15.677: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 13 11:52:17.648: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 13 11:52:17.664: INFO: Pod pod-with-prestop-exec-hook still exists
Oct 13 11:52:19.648: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct 13 11:52:19.665: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:19.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-116" for this suite.

• [SLOW TEST:14.354 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":189,"skipped":2924,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:19.742: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Oct 13 11:52:19.860: INFO: Waiting up to 5m0s for pod "client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3" in namespace "containers-7016" to be "Succeeded or Failed"
Oct 13 11:52:19.872: INFO: Pod "client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.586066ms
Oct 13 11:52:21.895: INFO: Pod "client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03522007s
Oct 13 11:52:23.915: INFO: Pod "client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054565339s
STEP: Saw pod success
Oct 13 11:52:23.915: INFO: Pod "client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3" satisfied condition "Succeeded or Failed"
Oct 13 11:52:23.926: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 11:52:24.026: INFO: Waiting for pod client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3 to disappear
Oct 13 11:52:24.037: INFO: Pod client-containers-5739615e-53b0-4164-b5a6-59f53e939ca3 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:24.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7016" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":190,"skipped":2935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:24.073: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 11:52:24.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e" in namespace "projected-2996" to be "Succeeded or Failed"
Oct 13 11:52:24.187: INFO: Pod "downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.967194ms
Oct 13 11:52:26.198: INFO: Pod "downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02370755s
Oct 13 11:52:28.219: INFO: Pod "downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045149838s
STEP: Saw pod success
Oct 13 11:52:28.219: INFO: Pod "downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e" satisfied condition "Succeeded or Failed"
Oct 13 11:52:28.230: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e container client-container: <nil>
STEP: delete the pod
Oct 13 11:52:28.290: INFO: Waiting for pod downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e to disappear
Oct 13 11:52:28.300: INFO: Pod downwardapi-volume-67a5c28d-f7a5-4f0b-a06b-8aa7f98ab90e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:28.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2996" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":191,"skipped":2959,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:28.340: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-2ae04451-3a3d-405e-b243-5531b4daf627
STEP: Creating a pod to test consume configMaps
Oct 13 11:52:28.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29" in namespace "configmap-2750" to be "Succeeded or Failed"
Oct 13 11:52:28.478: INFO: Pod "pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29": Phase="Pending", Reason="", readiness=false. Elapsed: 9.895146ms
Oct 13 11:52:30.498: INFO: Pod "pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029779448s
Oct 13 11:52:32.514: INFO: Pod "pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045411832s
STEP: Saw pod success
Oct 13 11:52:32.514: INFO: Pod "pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29" satisfied condition "Succeeded or Failed"
Oct 13 11:52:32.525: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 11:52:32.590: INFO: Waiting for pod pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29 to disappear
Oct 13 11:52:32.599: INFO: Pod pod-configmaps-b3c13d8a-f430-4932-8476-ee0049f3ce29 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:32.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2750" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":192,"skipped":2967,"failed":0}

------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:32.630: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:36.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3352" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":193,"skipped":2967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:36.841: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Oct 13 11:52:36.932: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:54.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2083" for this suite.

• [SLOW TEST:17.564 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":194,"skipped":3030,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:54.411: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 13 11:52:54.534: INFO: Waiting up to 5m0s for pod "downward-api-a376c325-6a4b-4182-b466-9066cc3646b8" in namespace "downward-api-5589" to be "Succeeded or Failed"
Oct 13 11:52:54.546: INFO: Pod "downward-api-a376c325-6a4b-4182-b466-9066cc3646b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.590337ms
Oct 13 11:52:56.562: INFO: Pod "downward-api-a376c325-6a4b-4182-b466-9066cc3646b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027274477s
Oct 13 11:52:58.589: INFO: Pod "downward-api-a376c325-6a4b-4182-b466-9066cc3646b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053974262s
STEP: Saw pod success
Oct 13 11:52:58.589: INFO: Pod "downward-api-a376c325-6a4b-4182-b466-9066cc3646b8" satisfied condition "Succeeded or Failed"
Oct 13 11:52:58.599: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downward-api-a376c325-6a4b-4182-b466-9066cc3646b8 container dapi-container: <nil>
STEP: delete the pod
Oct 13 11:52:58.705: INFO: Waiting for pod downward-api-a376c325-6a4b-4182-b466-9066cc3646b8 to disappear
Oct 13 11:52:58.715: INFO: Pod downward-api-a376c325-6a4b-4182-b466-9066cc3646b8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:52:58.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5589" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":195,"skipped":3040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:52:58.749: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:52:59.358: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 11:53:01.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722779, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722779, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722779, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722779, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:53:04.447: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:53:04.459: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3358-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:53:07.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9629" for this suite.
STEP: Destroying namespace "webhook-9629-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.297 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":196,"skipped":3072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:53:08.050: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:53:08.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3113" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":197,"skipped":3094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:53:08.189: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2670
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 13 11:53:08.273: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 13 11:53:08.350: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:53:10.365: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:53:12.370: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:14.374: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:16.372: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:18.372: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:20.366: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:22.369: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:24.371: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:26.382: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:28.372: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:53:30.368: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 13 11:53:30.389: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 13 11:53:34.441: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 13 11:53:34.441: INFO: Breadth first check of 172.25.1.68 on host 192.168.1.102...
Oct 13 11:53:34.454: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.17:9080/dial?request=hostname&protocol=http&host=172.25.1.68&port=8080&tries=1'] Namespace:pod-network-test-2670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:53:34.454: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:53:34.887: INFO: Waiting for responses: map[]
Oct 13 11:53:34.887: INFO: reached 172.25.1.68 after 0/1 tries
Oct 13 11:53:34.887: INFO: Breadth first check of 172.25.0.16 on host 192.168.1.246...
Oct 13 11:53:34.902: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.0.17:9080/dial?request=hostname&protocol=http&host=172.25.0.16&port=8080&tries=1'] Namespace:pod-network-test-2670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:53:34.902: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:53:35.197: INFO: Waiting for responses: map[]
Oct 13 11:53:35.198: INFO: reached 172.25.0.16 after 0/1 tries
Oct 13 11:53:35.198: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:53:35.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2670" for this suite.

• [SLOW TEST:27.053 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":198,"skipped":3120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:53:35.242: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Oct 13 11:53:35.402: INFO: Waiting up to 5m0s for pod "var-expansion-6692055d-411f-4cee-b787-1981244a2a1b" in namespace "var-expansion-2807" to be "Succeeded or Failed"
Oct 13 11:53:35.413: INFO: Pod "var-expansion-6692055d-411f-4cee-b787-1981244a2a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.884272ms
Oct 13 11:53:37.435: INFO: Pod "var-expansion-6692055d-411f-4cee-b787-1981244a2a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032265065s
Oct 13 11:53:39.446: INFO: Pod "var-expansion-6692055d-411f-4cee-b787-1981244a2a1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04351916s
STEP: Saw pod success
Oct 13 11:53:39.446: INFO: Pod "var-expansion-6692055d-411f-4cee-b787-1981244a2a1b" satisfied condition "Succeeded or Failed"
Oct 13 11:53:39.457: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod var-expansion-6692055d-411f-4cee-b787-1981244a2a1b container dapi-container: <nil>
STEP: delete the pod
Oct 13 11:53:39.528: INFO: Waiting for pod var-expansion-6692055d-411f-4cee-b787-1981244a2a1b to disappear
Oct 13 11:53:39.535: INFO: Pod var-expansion-6692055d-411f-4cee-b787-1981244a2a1b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:53:39.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2807" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":199,"skipped":3165,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:53:39.561: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:53:40.117: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 11:53:42.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722820, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722820, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722820, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722820, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:53:45.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:53:45.213: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:53:49.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-854" for this suite.
STEP: Destroying namespace "webhook-854-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.837 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":200,"skipped":3171,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:53:49.399: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3939.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3939.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3939.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3939.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 11:53:55.691: INFO: DNS probes using dns-test-41df06fd-f39a-4ff9-8290-a30ce56bf0d9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3939.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3939.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3939.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3939.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 11:54:05.949: INFO: DNS probes using dns-test-6497af53-a38d-4b03-9817-0e4363afdf2d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3939.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3939.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3939.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3939.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 11:54:14.232: INFO: File jessie_udp@dns-test-service-3.dns-3939.svc.cluster.local from pod  dns-3939/dns-test-02717cbe-aeea-4360-a5fe-b0e2262811dc contains '' instead of '10.240.20.20'
Oct 13 11:54:14.232: INFO: Lookups using dns-3939/dns-test-02717cbe-aeea-4360-a5fe-b0e2262811dc failed for: [jessie_udp@dns-test-service-3.dns-3939.svc.cluster.local]

Oct 13 11:54:19.343: INFO: DNS probes using dns-test-02717cbe-aeea-4360-a5fe-b0e2262811dc succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:54:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3939" for this suite.

• [SLOW TEST:30.024 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":201,"skipped":3173,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:54:19.425: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:54:19.505: INFO: Creating simple deployment test-new-deployment
Oct 13 11:54:19.542: INFO: deployment "test-new-deployment" doesn't have the required revision set
Oct 13 11:54:21.588: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722859, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722859, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722859, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722859, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Oct 13 11:54:23.686: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9255  4c4c0211-e0e2-4c1e-86b6-82ea19a366cd 398953 3 2021-10-13 11:54:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-10-13 11:54:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-10-13 11:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045e2f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-10-13 11:54:23 +0000 UTC,LastTransitionTime:2021-10-13 11:54:19 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-10-13 11:54:23 +0000 UTC,LastTransitionTime:2021-10-13 11:54:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 13 11:54:23.694: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-9255  4095d0e5-e560-4111-bbfc-5cd9a5809863 398957 3 2021-10-13 11:54:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4c4c0211-e0e2-4c1e-86b6-82ea19a366cd 0xc0046179b7 0xc0046179b8}] []  [{kube-controller-manager Update apps/v1 2021-10-13 11:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c4c0211-e0e2-4c1e-86b6-82ea19a366cd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004617a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 13 11:54:23.703: INFO: Pod "test-new-deployment-847dcfb7fb-2ckcz" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-2ckcz test-new-deployment-847dcfb7fb- deployment-9255  48b502c4-2e3e-4fb2-9832-435b12bded02 398959 0 2021-10-13 11:54:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4095d0e5-e560-4111-bbfc-5cd9a5809863 0xc004617e47 0xc004617e48}] []  [{kube-controller-manager Update v1 2021-10-13 11:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4095d0e5-e560-4111-bbfc-5cd9a5809863\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ksxvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ksxvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:54:23.704: INFO: Pod "test-new-deployment-847dcfb7fb-pk7f4" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-pk7f4 test-new-deployment-847dcfb7fb- deployment-9255  d27d3f82-2737-4d6c-922a-11891f70bc63 398958 0 2021-10-13 11:54:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4095d0e5-e560-4111-bbfc-5cd9a5809863 0xc004617f90 0xc004617f91}] []  [{kube-controller-manager Update v1 2021-10-13 11:54:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4095d0e5-e560-4111-bbfc-5cd9a5809863\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 11:54:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqlkr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqlkr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.102,PodIP:,StartTime:2021-10-13 11:54:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Oct 13 11:54:23.704: INFO: Pod "test-new-deployment-847dcfb7fb-r896m" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-r896m test-new-deployment-847dcfb7fb- deployment-9255  89d1bdfd-e754-412f-ba1f-3c3825fc079a 398937 0 2021-10-13 11:54:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:172.25.0.23/32 cni.projectcalico.org/podIPs:172.25.0.23/32] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 4095d0e5-e560-4111-bbfc-5cd9a5809863 0xc00463e197 0xc00463e198}] []  [{kube-controller-manager Update v1 2021-10-13 11:54:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4095d0e5-e560-4111-bbfc-5cd9a5809863\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 11:54:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 11:54:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpzs6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpzs6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 11:54:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.23,StartTime:2021-10-13 11:54:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 11:54:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://a559cff2ac78d1fe8b6a2036f15dd1d7d2fdb27deecbb6c143fd1bf012981ffd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:54:23.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9255" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":202,"skipped":3181,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:54:23.732: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:54:34.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-441" for this suite.

• [SLOW TEST:11.269 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":203,"skipped":3186,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:54:35.004: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1355
STEP: creating service affinity-clusterip in namespace services-1355
STEP: creating replication controller affinity-clusterip in namespace services-1355
I1013 11:54:35.142087      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-1355, replica count: 3
I1013 11:54:38.193977      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:54:41.195569      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 11:54:41.224: INFO: Creating new exec pod
Oct 13 11:54:48.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1355 exec execpod-affinitys25n2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Oct 13 11:54:48.916: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80+ \necho hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Oct 13 11:54:48.916: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:54:48.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1355 exec execpod-affinitys25n2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.27.170 80'
Oct 13 11:54:49.456: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.27.170 80\nConnection to 10.240.27.170 80 port [tcp/http] succeeded!\n"
Oct 13 11:54:49.456: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 11:54:49.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1355 exec execpod-affinitys25n2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.27.170:80/ ; done'
Oct 13 11:54:49.982: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.27.170:80/\n"
Oct 13 11:54:49.982: INFO: stdout: "\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7\naffinity-clusterip-9jvn7"
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Received response from host: affinity-clusterip-9jvn7
Oct 13 11:54:49.982: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1355, will wait for the garbage collector to delete the pods
Oct 13 11:54:50.089: INFO: Deleting ReplicationController affinity-clusterip took: 16.831395ms
Oct 13 11:54:50.193: INFO: Terminating ReplicationController affinity-clusterip pods took: 103.954672ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:07.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1355" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:32.177 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":204,"skipped":3197,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:07.182: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:07.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8771" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":205,"skipped":3204,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:07.317: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:07.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7195" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":206,"skipped":3229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5492
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 13 11:55:07.524: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 13 11:55:07.598: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:55:09.616: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:55:11.620: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:13.619: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:15.617: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:17.616: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:19.622: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:21.619: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:23.616: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:25.620: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:27.618: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 11:55:29.617: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 13 11:55:29.642: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 13 11:55:33.787: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 13 11:55:33.787: INFO: Going to poll 172.25.1.70 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Oct 13 11:55:33.797: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.70:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5492 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:55:33.797: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:55:34.151: INFO: Found all 1 expected endpoints: [netserver-0]
Oct 13 11:55:34.151: INFO: Going to poll 172.25.0.27 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Oct 13 11:55:34.163: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.27:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5492 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 11:55:34.163: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 11:55:34.518: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:34.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5492" for this suite.

• [SLOW TEST:27.111 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":207,"skipped":3255,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:34.556: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Oct 13 11:55:34.667: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:55:36.682: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:55:38.689: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:39.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2447" for this suite.

• [SLOW TEST:5.220 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":208,"skipped":3259,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-a8108fb1-4795-41cf-98de-3724de7e94ca
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:39.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9300" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":209,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:39.884: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Oct 13 11:55:40.368: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Oct 13 11:55:42.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722940, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722940, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722940, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769722940, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 11:55:45.457: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:55:45.474: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:49.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4953" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.582 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":210,"skipped":3283,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:49.466: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
W1013 11:55:49.553178      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: getting
STEP: listing
STEP: watching
Oct 13 11:55:49.591: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 13 11:55:49.606: INFO: starting watch
STEP: patching
STEP: updating
Oct 13 11:55:49.662: INFO: waiting for watch events with expected annotations
Oct 13 11:55:49.662: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:49.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9940" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":211,"skipped":3302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:49.827: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Oct 13 11:55:49.899: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:55:58.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2779" for this suite.

• [SLOW TEST:8.591 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":212,"skipped":3338,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:55:58.420: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-4952
STEP: creating replication controller nodeport-test in namespace services-4952
I1013 11:55:58.569873      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4952, replica count: 2
I1013 11:56:01.629657      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:56:04.629970      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 11:56:07.631021      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 11:56:07.631: INFO: Creating new exec pod
Oct 13 11:56:12.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-4952 exec execpodppwtc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Oct 13 11:56:13.186: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Oct 13 11:56:13.186: INFO: stdout: "nodeport-test-ch64w"
Oct 13 11:56:13.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-4952 exec execpodppwtc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.21.193 80'
Oct 13 11:56:13.686: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.21.193 80\nConnection to 10.240.21.193 80 port [tcp/http] succeeded!\n"
Oct 13 11:56:13.686: INFO: stdout: "nodeport-test-sdwvl"
Oct 13 11:56:13.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-4952 exec execpodppwtc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.102 30649'
Oct 13 11:56:14.225: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.102 30649\nConnection to 192.168.1.102 30649 port [tcp/*] succeeded!\n"
Oct 13 11:56:14.226: INFO: stdout: "nodeport-test-ch64w"
Oct 13 11:56:14.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-4952 exec execpodppwtc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.246 30649'
Oct 13 11:56:14.742: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.1.246 30649\nConnection to 192.168.1.246 30649 port [tcp/*] succeeded!\n"
Oct 13 11:56:14.743: INFO: stdout: ""
Oct 13 11:56:15.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-4952 exec execpodppwtc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.1.246 30649'
Oct 13 11:56:16.192: INFO: stderr: "+ nc -v -t -w 2 192.168.1.246 30649\n+ echo hostName\nConnection to 192.168.1.246 30649 port [tcp/*] succeeded!\n"
Oct 13 11:56:16.192: INFO: stdout: "nodeport-test-sdwvl"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:56:16.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4952" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:17.806 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":213,"skipped":3343,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:56:16.227: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6530.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6530.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6530.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6530.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6530.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6530.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6530.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 11:56:22.500: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.514: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.573: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.617: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.656: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.669: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.682: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.696: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6530.svc.cluster.local from pod dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff: the server could not find the requested resource (get pods dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff)
Oct 13 11:56:22.723: INFO: Lookups using dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6530.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6530.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6530.svc.cluster.local jessie_udp@dns-test-service-2.dns-6530.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6530.svc.cluster.local]

Oct 13 11:56:28.007: INFO: DNS probes using dns-6530/dns-test-e2416a32-0dfe-4f1a-8c02-2fb0d5056fff succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:56:28.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6530" for this suite.

• [SLOW TEST:11.879 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":214,"skipped":3345,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:56:28.109: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:56:28.497: INFO: Checking APIGroup: apiregistration.k8s.io
Oct 13 11:56:28.501: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Oct 13 11:56:28.501: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.501: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Oct 13 11:56:28.501: INFO: Checking APIGroup: apps
Oct 13 11:56:28.505: INFO: PreferredVersion.GroupVersion: apps/v1
Oct 13 11:56:28.505: INFO: Versions found [{apps/v1 v1}]
Oct 13 11:56:28.505: INFO: apps/v1 matches apps/v1
Oct 13 11:56:28.505: INFO: Checking APIGroup: events.k8s.io
Oct 13 11:56:28.509: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Oct 13 11:56:28.509: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.509: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Oct 13 11:56:28.509: INFO: Checking APIGroup: authentication.k8s.io
Oct 13 11:56:28.514: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Oct 13 11:56:28.514: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.514: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Oct 13 11:56:28.514: INFO: Checking APIGroup: authorization.k8s.io
Oct 13 11:56:28.518: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Oct 13 11:56:28.518: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.518: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Oct 13 11:56:28.518: INFO: Checking APIGroup: autoscaling
Oct 13 11:56:28.523: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Oct 13 11:56:28.523: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Oct 13 11:56:28.523: INFO: autoscaling/v1 matches autoscaling/v1
Oct 13 11:56:28.523: INFO: Checking APIGroup: batch
Oct 13 11:56:28.527: INFO: PreferredVersion.GroupVersion: batch/v1
Oct 13 11:56:28.527: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Oct 13 11:56:28.527: INFO: batch/v1 matches batch/v1
Oct 13 11:56:28.527: INFO: Checking APIGroup: certificates.k8s.io
Oct 13 11:56:28.531: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Oct 13 11:56:28.531: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.531: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Oct 13 11:56:28.531: INFO: Checking APIGroup: networking.k8s.io
Oct 13 11:56:28.535: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Oct 13 11:56:28.535: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.535: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Oct 13 11:56:28.535: INFO: Checking APIGroup: extensions
Oct 13 11:56:28.539: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Oct 13 11:56:28.539: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Oct 13 11:56:28.539: INFO: extensions/v1beta1 matches extensions/v1beta1
Oct 13 11:56:28.539: INFO: Checking APIGroup: policy
Oct 13 11:56:28.543: INFO: PreferredVersion.GroupVersion: policy/v1
Oct 13 11:56:28.543: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Oct 13 11:56:28.543: INFO: policy/v1 matches policy/v1
Oct 13 11:56:28.543: INFO: Checking APIGroup: rbac.authorization.k8s.io
Oct 13 11:56:28.548: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Oct 13 11:56:28.548: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.548: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Oct 13 11:56:28.548: INFO: Checking APIGroup: storage.k8s.io
Oct 13 11:56:28.554: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Oct 13 11:56:28.555: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.555: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Oct 13 11:56:28.555: INFO: Checking APIGroup: admissionregistration.k8s.io
Oct 13 11:56:28.560: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Oct 13 11:56:28.560: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.561: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Oct 13 11:56:28.561: INFO: Checking APIGroup: apiextensions.k8s.io
Oct 13 11:56:28.568: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Oct 13 11:56:28.568: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.568: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Oct 13 11:56:28.568: INFO: Checking APIGroup: scheduling.k8s.io
Oct 13 11:56:28.574: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Oct 13 11:56:28.574: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.574: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Oct 13 11:56:28.575: INFO: Checking APIGroup: coordination.k8s.io
Oct 13 11:56:28.580: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Oct 13 11:56:28.580: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.580: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Oct 13 11:56:28.580: INFO: Checking APIGroup: node.k8s.io
Oct 13 11:56:28.585: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Oct 13 11:56:28.585: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.585: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Oct 13 11:56:28.585: INFO: Checking APIGroup: discovery.k8s.io
Oct 13 11:56:28.590: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Oct 13 11:56:28.590: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.590: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Oct 13 11:56:28.591: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Oct 13 11:56:28.596: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Oct 13 11:56:28.596: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.596: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Oct 13 11:56:28.596: INFO: Checking APIGroup: crd.projectcalico.org
Oct 13 11:56:28.601: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Oct 13 11:56:28.601: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Oct 13 11:56:28.601: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Oct 13 11:56:28.601: INFO: Checking APIGroup: metakube.syseleven.de
Oct 13 11:56:28.606: INFO: PreferredVersion.GroupVersion: metakube.syseleven.de/v1
Oct 13 11:56:28.606: INFO: Versions found [{metakube.syseleven.de/v1 v1}]
Oct 13 11:56:28.606: INFO: metakube.syseleven.de/v1 matches metakube.syseleven.de/v1
Oct 13 11:56:28.607: INFO: Checking APIGroup: cluster.k8s.io
Oct 13 11:56:28.613: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Oct 13 11:56:28.613: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Oct 13 11:56:28.613: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Oct 13 11:56:28.614: INFO: Checking APIGroup: metrics.k8s.io
Oct 13 11:56:28.619: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Oct 13 11:56:28.619: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Oct 13 11:56:28.619: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:56:28.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3124" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":215,"skipped":3366,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:56:28.654: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct 13 11:56:28.767: INFO: Waiting up to 5m0s for pod "pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7" in namespace "emptydir-1608" to be "Succeeded or Failed"
Oct 13 11:56:28.777: INFO: Pod "pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.471723ms
Oct 13 11:56:30.789: INFO: Pod "pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021849654s
Oct 13 11:56:32.806: INFO: Pod "pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038122041s
Oct 13 11:56:34.830: INFO: Pod "pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06201903s
STEP: Saw pod success
Oct 13 11:56:34.830: INFO: Pod "pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7" satisfied condition "Succeeded or Failed"
Oct 13 11:56:34.841: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7 container test-container: <nil>
STEP: delete the pod
Oct 13 11:56:34.943: INFO: Waiting for pod pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7 to disappear
Oct 13 11:56:34.955: INFO: Pod pod-5bcb9a23-6fb2-4f36-b9d9-2e71c6e353d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:56:34.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1608" for this suite.

• [SLOW TEST:6.336 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":216,"skipped":3375,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:56:34.991: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-861fc82e-fa14-402e-bb4b-700168548042
STEP: Creating a pod to test consume secrets
Oct 13 11:56:35.128: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777" in namespace "projected-6244" to be "Succeeded or Failed"
Oct 13 11:56:35.144: INFO: Pod "pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777": Phase="Pending", Reason="", readiness=false. Elapsed: 15.968431ms
Oct 13 11:56:37.165: INFO: Pod "pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037335567s
Oct 13 11:56:39.180: INFO: Pod "pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052209727s
Oct 13 11:56:41.202: INFO: Pod "pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073765254s
STEP: Saw pod success
Oct 13 11:56:41.202: INFO: Pod "pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777" satisfied condition "Succeeded or Failed"
Oct 13 11:56:41.211: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 13 11:56:41.312: INFO: Waiting for pod pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777 to disappear
Oct 13 11:56:41.328: INFO: Pod pod-projected-secrets-5aa04c20-a45e-4e8f-bedc-61c5b0e83777 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:56:41.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6244" for this suite.

• [SLOW TEST:6.371 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":217,"skipped":3392,"failed":0}
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:56:41.364: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Oct 13 11:56:41.453: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 13 11:57:41.533: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 11:57:41.544: INFO: Starting informer...
STEP: Starting pods...
Oct 13 11:57:41.805: INFO: Pod1 is running on flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr. Tainting Node
Oct 13 11:57:48.074: INFO: Pod2 is running on flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Oct 13 11:57:59.857: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Oct 13 11:58:27.086: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:58:27.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7056" for this suite.

• [SLOW TEST:105.852 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":218,"skipped":3396,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:58:27.217: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Oct 13 11:58:33.925: INFO: Successfully updated pod "adopt-release-fk9b7"
STEP: Checking that the Job readopts the Pod
Oct 13 11:58:33.925: INFO: Waiting up to 15m0s for pod "adopt-release-fk9b7" in namespace "job-9542" to be "adopted"
Oct 13 11:58:33.939: INFO: Pod "adopt-release-fk9b7": Phase="Running", Reason="", readiness=true. Elapsed: 13.022761ms
Oct 13 11:58:35.959: INFO: Pod "adopt-release-fk9b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.033086751s
Oct 13 11:58:35.959: INFO: Pod "adopt-release-fk9b7" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Oct 13 11:58:36.493: INFO: Successfully updated pod "adopt-release-fk9b7"
STEP: Checking that the Job releases the Pod
Oct 13 11:58:36.493: INFO: Waiting up to 15m0s for pod "adopt-release-fk9b7" in namespace "job-9542" to be "released"
Oct 13 11:58:36.504: INFO: Pod "adopt-release-fk9b7": Phase="Running", Reason="", readiness=true. Elapsed: 10.756732ms
Oct 13 11:58:38.521: INFO: Pod "adopt-release-fk9b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02761057s
Oct 13 11:58:38.521: INFO: Pod "adopt-release-fk9b7" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:58:38.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9542" for this suite.

• [SLOW TEST:11.340 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":219,"skipped":3409,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:58:38.559: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:58:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4395" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":220,"skipped":3427,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:58:38.809: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 13 11:58:38.909: INFO: Waiting up to 5m0s for pod "downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2" in namespace "downward-api-814" to be "Succeeded or Failed"
Oct 13 11:58:38.920: INFO: Pod "downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.747501ms
Oct 13 11:58:40.936: INFO: Pod "downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027405333s
Oct 13 11:58:42.951: INFO: Pod "downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042623429s
Oct 13 11:58:44.971: INFO: Pod "downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061979547s
STEP: Saw pod success
Oct 13 11:58:44.972: INFO: Pod "downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2" satisfied condition "Succeeded or Failed"
Oct 13 11:58:44.986: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf pod downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2 container dapi-container: <nil>
STEP: delete the pod
Oct 13 11:58:45.052: INFO: Waiting for pod downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2 to disappear
Oct 13 11:58:45.061: INFO: Pod downward-api-4a87ab14-a6bd-4d9c-b3e4-ac197cdd07a2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:58:45.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-814" for this suite.

• [SLOW TEST:6.288 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":221,"skipped":3438,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:58:45.098: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Oct 13 11:58:45.224: INFO: Waiting up to 5m0s for pod "var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7" in namespace "var-expansion-3590" to be "Succeeded or Failed"
Oct 13 11:58:45.244: INFO: Pod "var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.019498ms
Oct 13 11:58:47.260: INFO: Pod "var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035494907s
Oct 13 11:58:49.297: INFO: Pod "var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072016743s
Oct 13 11:58:51.313: INFO: Pod "var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088667371s
STEP: Saw pod success
Oct 13 11:58:51.313: INFO: Pod "var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7" satisfied condition "Succeeded or Failed"
Oct 13 11:58:51.324: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf pod var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7 container dapi-container: <nil>
STEP: delete the pod
Oct 13 11:58:51.503: INFO: Waiting for pod var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7 to disappear
Oct 13 11:58:51.514: INFO: Pod var-expansion-b2b8af44-57de-4123-8d7c-8e99e94d78f7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:58:51.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3590" for this suite.

• [SLOW TEST:6.444 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":222,"skipped":3458,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:58:51.547: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-556e376f-3f73-4cfe-8d03-cfadbbab26c6
STEP: Creating the pod
Oct 13 11:58:51.712: INFO: The status of Pod pod-projected-configmaps-f57ead82-fdb6-4c6b-b002-f9e8f6ae1c59 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:58:53.738: INFO: The status of Pod pod-projected-configmaps-f57ead82-fdb6-4c6b-b002-f9e8f6ae1c59 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 11:58:55.734: INFO: The status of Pod pod-projected-configmaps-f57ead82-fdb6-4c6b-b002-f9e8f6ae1c59 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-556e376f-3f73-4cfe-8d03-cfadbbab26c6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 11:59:59.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8564" for this suite.

• [SLOW TEST:67.605 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":223,"skipped":3462,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 11:59:59.153: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Oct 13 11:59:59.283: INFO: Waiting up to 5m0s for pod "pod-96698064-0a47-4017-a01f-4ebce48ee0f0" in namespace "emptydir-421" to be "Succeeded or Failed"
Oct 13 11:59:59.298: INFO: Pod "pod-96698064-0a47-4017-a01f-4ebce48ee0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.330128ms
Oct 13 12:00:01.318: INFO: Pod "pod-96698064-0a47-4017-a01f-4ebce48ee0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034951871s
Oct 13 12:00:03.337: INFO: Pod "pod-96698064-0a47-4017-a01f-4ebce48ee0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053991085s
STEP: Saw pod success
Oct 13 12:00:03.337: INFO: Pod "pod-96698064-0a47-4017-a01f-4ebce48ee0f0" satisfied condition "Succeeded or Failed"
Oct 13 12:00:03.350: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-96698064-0a47-4017-a01f-4ebce48ee0f0 container test-container: <nil>
STEP: delete the pod
Oct 13 12:00:03.539: INFO: Waiting for pod pod-96698064-0a47-4017-a01f-4ebce48ee0f0 to disappear
Oct 13 12:00:03.553: INFO: Pod pod-96698064-0a47-4017-a01f-4ebce48ee0f0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:03.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-421" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":224,"skipped":3468,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:03.584: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:00:04.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:00:06.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723204, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723204, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723204, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723204, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:00:09.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:09.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3147" for this suite.
STEP: Destroying namespace "webhook-3147-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.138 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":225,"skipped":3468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:09.734: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Oct 13 12:00:09.832: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 12:00:12.886: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:25.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-535" for this suite.

• [SLOW TEST:15.639 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":226,"skipped":3535,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:25.375: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Oct 13 12:00:27.557: INFO: running pods: 0 < 3
Oct 13 12:00:29.578: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:31.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-264" for this suite.

• [SLOW TEST:6.232 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":227,"skipped":3542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:31.615: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:00:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:34.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1084" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":228,"skipped":3568,"failed":0}

------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:34.998: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Oct 13 12:00:35.100: INFO: Waiting up to 5m0s for pod "security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7" in namespace "security-context-4100" to be "Succeeded or Failed"
Oct 13 12:00:35.110: INFO: Pod "security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.622028ms
Oct 13 12:00:37.138: INFO: Pod "security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038480175s
Oct 13 12:00:39.161: INFO: Pod "security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060883804s
Oct 13 12:00:41.179: INFO: Pod "security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078939705s
STEP: Saw pod success
Oct 13 12:00:41.179: INFO: Pod "security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7" satisfied condition "Succeeded or Failed"
Oct 13 12:00:41.191: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf pod security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7 container test-container: <nil>
STEP: delete the pod
Oct 13 12:00:41.296: INFO: Waiting for pod security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7 to disappear
Oct 13 12:00:41.308: INFO: Pod security-context-fd1eda8d-b487-4ccc-a791-35b7af21d0d7 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:41.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4100" for this suite.

• [SLOW TEST:6.346 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":229,"skipped":3568,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:41.344: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:00:41.435: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:48.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8039" for this suite.

• [SLOW TEST:6.761 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":230,"skipped":3572,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:48.111: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 13 12:00:52.351: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:00:52.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7882" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":231,"skipped":3577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:00:52.427: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3935
Oct 13 12:00:52.540: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:00:54.561: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:00:56.571: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Oct 13 12:00:56.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3935 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Oct 13 12:00:58.776: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Oct 13 12:00:58.776: INFO: stdout: "ipvs"
Oct 13 12:00:58.776: INFO: proxyMode: ipvs
Oct 13 12:00:58.812: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Oct 13 12:00:58.825: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3935
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3935
I1013 12:00:58.865197      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3935, replica count: 3
I1013 12:01:01.916524      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 12:01:04.917770      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 12:01:04.941: INFO: Creating new exec pod
Oct 13 12:01:09.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3935 exec execpod-affinityzf5nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Oct 13 12:01:10.510: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Oct 13 12:01:10.510: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 12:01:10.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3935 exec execpod-affinityzf5nq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.213 80'
Oct 13 12:01:11.069: INFO: stderr: "+ nc -v -t -w 2 10.240.18.213 80\n+ echo hostName\nConnection to 10.240.18.213 80 port [tcp/http] succeeded!\n"
Oct 13 12:01:11.069: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Oct 13 12:01:11.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3935 exec execpod-affinityzf5nq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.18.213:80/ ; done'
Oct 13 12:01:11.593: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n"
Oct 13 12:01:11.593: INFO: stdout: "\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc\naffinity-clusterip-timeout-nzcmc"
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Received response from host: affinity-clusterip-timeout-nzcmc
Oct 13 12:01:11.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3935 exec execpod-affinityzf5nq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.18.213:80/'
Oct 13 12:01:12.071: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n"
Oct 13 12:01:12.071: INFO: stdout: "affinity-clusterip-timeout-nzcmc"
Oct 13 12:03:22.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-3935 exec execpod-affinityzf5nq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.18.213:80/'
Oct 13 12:03:22.571: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.18.213:80/\n"
Oct 13 12:03:22.572: INFO: stdout: "affinity-clusterip-timeout-mg6kn"
Oct 13 12:03:22.572: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3935, will wait for the garbage collector to delete the pods
Oct 13 12:03:22.700: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 20.072507ms
Oct 13 12:03:22.800: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.66461ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:03:38.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3935" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:166.560 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":232,"skipped":3627,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:03:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1135
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1135
STEP: creating replication controller externalsvc in namespace services-1135
I1013 12:03:39.144550      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1135, replica count: 2
I1013 12:03:42.195540      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 12:03:45.197110      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Oct 13 12:03:45.270: INFO: Creating new exec pod
Oct 13 12:03:49.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-1135 exec execpodrsrhd -- /bin/sh -x -c nslookup nodeport-service.services-1135.svc.cluster.local'
Oct 13 12:03:49.795: INFO: stderr: "+ nslookup nodeport-service.services-1135.svc.cluster.local\n"
Oct 13 12:03:49.795: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-1135.svc.cluster.local\tcanonical name = externalsvc.services-1135.svc.cluster.local.\nName:\texternalsvc.services-1135.svc.cluster.local\nAddress: 10.240.25.17\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1135, will wait for the garbage collector to delete the pods
Oct 13 12:03:49.883: INFO: Deleting ReplicationController externalsvc took: 21.350432ms
Oct 13 12:03:49.984: INFO: Terminating ReplicationController externalsvc pods took: 100.923437ms
Oct 13 12:03:58.934: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:03:58.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1135" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:20.003 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":233,"skipped":3629,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:03:58.992: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 13 12:03:59.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-4944 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Oct 13 12:03:59.220: INFO: stderr: ""
Oct 13 12:03:59.220: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Oct 13 12:03:59.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-4944 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Oct 13 12:03:59.627: INFO: stderr: ""
Oct 13 12:03:59.627: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Oct 13 12:03:59.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-4944 delete pods e2e-test-httpd-pod'
Oct 13 12:04:07.090: INFO: stderr: ""
Oct 13 12:04:07.090: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:04:07.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4944" for this suite.

• [SLOW TEST:8.128 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:903
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":234,"skipped":3633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:04:07.121: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-e361c9b6-3a92-4546-9d1a-d22244cd829c
STEP: Creating a pod to test consume configMaps
Oct 13 12:04:07.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95" in namespace "projected-5569" to be "Succeeded or Failed"
Oct 13 12:04:07.255: INFO: Pod "pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04861ms
Oct 13 12:04:09.275: INFO: Pod "pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032578173s
Oct 13 12:04:11.292: INFO: Pod "pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048712254s
STEP: Saw pod success
Oct 13 12:04:11.292: INFO: Pod "pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95" satisfied condition "Succeeded or Failed"
Oct 13 12:04:11.305: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:04:11.516: INFO: Waiting for pod pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95 to disappear
Oct 13 12:04:11.525: INFO: Pod pod-projected-configmaps-a2feea72-2175-4cdf-b283-3d2c4f5fbc95 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:04:11.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5569" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":235,"skipped":3662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:04:11.565: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:04:11.666: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Oct 13 12:04:11.692: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct 13 12:04:16.715: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 13 12:04:16.715: INFO: Creating deployment "test-rolling-update-deployment"
Oct 13 12:04:16.729: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Oct 13 12:04:16.751: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Oct 13 12:04:18.788: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Oct 13 12:04:18.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723456, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723456, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723456, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723456, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 12:04:20.821: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Oct 13 12:04:20.862: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9519  5a486d03-21d9-43a3-9e4a-3b2f68859247 403597 1 2021-10-13 12:04:16 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-10-13 12:04:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-10-13 12:04:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002853a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-10-13 12:04:16 +0000 UTC,LastTransitionTime:2021-10-13 12:04:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-10-13 12:04:20 +0000 UTC,LastTransitionTime:2021-10-13 12:04:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Oct 13 12:04:20.872: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-9519  c8969bac-6af4-42da-a043-cb75b9088b4b 403586 1 2021-10-13 12:04:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5a486d03-21d9-43a3-9e4a-3b2f68859247 0xc002853f37 0xc002853f38}] []  [{kube-controller-manager Update apps/v1 2021-10-13 12:04:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a486d03-21d9-43a3-9e4a-3b2f68859247\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002853fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Oct 13 12:04:20.872: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Oct 13 12:04:20.872: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9519  8c21877a-a4a4-42aa-998d-76dc29cf0892 403596 2 2021-10-13 12:04:11 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5a486d03-21d9-43a3-9e4a-3b2f68859247 0xc002853e27 0xc002853e28}] []  [{e2e.test Update apps/v1 2021-10-13 12:04:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-10-13 12:04:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a486d03-21d9-43a3-9e4a-3b2f68859247\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002853ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 13 12:04:20.881: INFO: Pod "test-rolling-update-deployment-585b757574-rmkqd" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-rmkqd test-rolling-update-deployment-585b757574- deployment-9519  6e06c971-b8b7-43ee-b570-327749e49b8b 403585 0 2021-10-13 12:04:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/podIP:172.25.0.53/32 cni.projectcalico.org/podIPs:172.25.0.53/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 c8969bac-6af4-42da-a043-cb75b9088b4b 0xc005686427 0xc005686428}] []  [{kube-controller-manager Update v1 2021-10-13 12:04:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8969bac-6af4-42da-a043-cb75b9088b4b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 12:04:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 12:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fs29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fs29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:04:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:04:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.53,StartTime:2021-10-13 12:04:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 12:04:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://cbeae8b45d7eb873d64e19af7cf27c503bc553905814e114629acb29f086825f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:04:20.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9519" for this suite.

• [SLOW TEST:9.344 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":236,"skipped":3688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:04:20.914: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 13 12:04:21.055: INFO: The status of Pod annotationupdateda985f5d-afa4-4824-8f93-65dbb0200071 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:04:23.077: INFO: The status of Pod annotationupdateda985f5d-afa4-4824-8f93-65dbb0200071 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:04:25.075: INFO: The status of Pod annotationupdateda985f5d-afa4-4824-8f93-65dbb0200071 is Running (Ready = true)
Oct 13 12:04:25.683: INFO: Successfully updated pod "annotationupdateda985f5d-afa4-4824-8f93-65dbb0200071"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:04:27.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-431" for this suite.

• [SLOW TEST:6.875 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":237,"skipped":3735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:04:27.791: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 12:04:27.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f" in namespace "downward-api-9985" to be "Succeeded or Failed"
Oct 13 12:04:27.909: INFO: Pod "downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.243535ms
Oct 13 12:04:29.924: INFO: Pod "downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025249077s
Oct 13 12:04:31.947: INFO: Pod "downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048191103s
Oct 13 12:04:33.961: INFO: Pod "downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062245584s
STEP: Saw pod success
Oct 13 12:04:33.961: INFO: Pod "downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f" satisfied condition "Succeeded or Failed"
Oct 13 12:04:33.974: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f container client-container: <nil>
STEP: delete the pod
Oct 13 12:04:34.076: INFO: Waiting for pod downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f to disappear
Oct 13 12:04:34.086: INFO: Pod downwardapi-volume-928a838b-2f3d-49a9-b94c-f9e3379b000f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:04:34.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9985" for this suite.

• [SLOW TEST:6.323 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":238,"skipped":3780,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:04:34.131: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:04:38.280: INFO: Deleting pod "var-expansion-5cd71a83-1efc-4c2f-99f5-d5612f58512e" in namespace "var-expansion-6897"
Oct 13 12:04:38.305: INFO: Wait up to 5m0s for pod "var-expansion-5cd71a83-1efc-4c2f-99f5-d5612f58512e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:04:42.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6897" for this suite.

• [SLOW TEST:8.245 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":239,"skipped":3800,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:04:42.378: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:04:43.099: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:04:45.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 12:04:47.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723483, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:04:50.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:04:51.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9080" for this suite.
STEP: Destroying namespace "webhook-9080-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.942 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":240,"skipped":3804,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:04:51.321: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2029
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Oct 13 12:04:51.435: INFO: Found 0 stateful pods, waiting for 3
Oct 13 12:05:01.454: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:05:01.454: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:05:01.455: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Oct 13 12:05:01.524: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Oct 13 12:05:11.609: INFO: Updating stateful set ss2
Oct 13 12:05:11.637: INFO: Waiting for Pod statefulset-2029/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Oct 13 12:05:21.737: INFO: Found 2 stateful pods, waiting for 3
Oct 13 12:05:31.763: INFO: Found 2 stateful pods, waiting for 3
Oct 13 12:05:41.760: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:05:41.761: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:05:41.761: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Oct 13 12:05:51.768: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:05:51.769: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:05:51.769: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Oct 13 12:05:51.835: INFO: Updating stateful set ss2
Oct 13 12:05:51.882: INFO: Waiting for Pod statefulset-2029/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Oct 13 12:06:01.967: INFO: Updating stateful set ss2
Oct 13 12:06:01.994: INFO: Waiting for StatefulSet statefulset-2029/ss2 to complete update
Oct 13 12:06:01.995: INFO: Waiting for Pod statefulset-2029/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Oct 13 12:06:12.035: INFO: Deleting all statefulset in ns statefulset-2029
Oct 13 12:06:12.046: INFO: Scaling statefulset ss2 to 0
Oct 13 12:06:42.114: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 12:06:42.127: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:06:42.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2029" for this suite.

• [SLOW TEST:110.897 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":241,"skipped":3812,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:06:42.227: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 13 12:06:42.348: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 13 12:07:42.445: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Oct 13 12:07:42.523: INFO: Created pod: pod0-sched-preemption-low-priority
Oct 13 12:07:42.571: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:08:06.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6035" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.719 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":242,"skipped":3833,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:08:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
W1013 12:08:07.083481      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:10:01.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7652" for this suite.

• [SLOW TEST:114.264 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":243,"skipped":3852,"failed":0}
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:10:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-e05c6200-662d-453f-81a0-5d51749c3da7
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:10:01.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9463" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":244,"skipped":3852,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:10:01.357: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Oct 13 12:10:01.448: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-3533 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:10:01.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3533" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":245,"skipped":3861,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:10:01.567: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 13 12:10:01.699: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 13 12:11:01.811: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Oct 13 12:11:01.881: INFO: Created pod: pod0-sched-preemption-low-priority
Oct 13 12:11:01.924: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:11:22.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7637" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:80.642 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":246,"skipped":3863,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:11:22.211: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:11:53.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2423" for this suite.

• [SLOW TEST:31.035 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":247,"skipped":3868,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:11:53.252: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:11:53.385: INFO: created pod
Oct 13 12:11:53.385: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1604" to be "Succeeded or Failed"
Oct 13 12:11:53.397: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.183636ms
Oct 13 12:11:55.417: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031410693s
Oct 13 12:11:57.441: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055515298s
STEP: Saw pod success
Oct 13 12:11:57.441: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Oct 13 12:12:27.442: INFO: polling logs
Oct 13 12:12:27.496: INFO: Pod logs: 
2021/10/13 12:11:55 OK: Got token
2021/10/13 12:11:55 validating with in-cluster discovery
2021/10/13 12:11:55 OK: got issuer https://9xsx4x6wwl.fes1.metakube.syseleven.de:32176
2021/10/13 12:11:55 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://9xsx4x6wwl.fes1.metakube.syseleven.de:32176", Subject:"system:serviceaccount:svcaccounts-1604:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1634127713, NotBefore:1634127113, IssuedAt:1634127113, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1604", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ec96c3ec-c815-44c6-9b04-0f0bcc08b57a"}}}
2021/10/13 12:11:56 OK: Constructed OIDC provider for issuer https://9xsx4x6wwl.fes1.metakube.syseleven.de:32176
2021/10/13 12:11:56 OK: Validated signature on JWT
2021/10/13 12:11:56 OK: Got valid claims from token!
2021/10/13 12:11:56 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://9xsx4x6wwl.fes1.metakube.syseleven.de:32176", Subject:"system:serviceaccount:svcaccounts-1604:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1634127713, NotBefore:1634127113, IssuedAt:1634127113, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1604", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ec96c3ec-c815-44c6-9b04-0f0bcc08b57a"}}}

Oct 13 12:12:27.496: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:12:27.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1604" for this suite.

• [SLOW TEST:34.301 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":248,"skipped":3873,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:12:27.558: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:12:43.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-27" for this suite.

• [SLOW TEST:16.427 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":249,"skipped":3878,"failed":0}
SSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:12:43.986: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 13 12:12:44.184: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Oct 13 12:12:44.207: INFO: starting watch
STEP: patching
STEP: updating
Oct 13 12:12:44.256: INFO: waiting for watch events with expected annotations
Oct 13 12:12:44.256: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:12:44.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5036" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":250,"skipped":3885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:12:44.395: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:12:44.497: INFO: Waiting up to 5m0s for pod "busybox-user-65534-099136b2-780a-4e3f-a729-14c15a6575ee" in namespace "security-context-test-2814" to be "Succeeded or Failed"
Oct 13 12:12:44.512: INFO: Pod "busybox-user-65534-099136b2-780a-4e3f-a729-14c15a6575ee": Phase="Pending", Reason="", readiness=false. Elapsed: 15.186646ms
Oct 13 12:12:46.524: INFO: Pod "busybox-user-65534-099136b2-780a-4e3f-a729-14c15a6575ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027774292s
Oct 13 12:12:48.545: INFO: Pod "busybox-user-65534-099136b2-780a-4e3f-a729-14c15a6575ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048164773s
Oct 13 12:12:48.545: INFO: Pod "busybox-user-65534-099136b2-780a-4e3f-a729-14c15a6575ee" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:12:48.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2814" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":251,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:12:48.579: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Oct 13 12:12:48.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab" in namespace "downward-api-1766" to be "Succeeded or Failed"
Oct 13 12:12:48.702: INFO: Pod "downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab": Phase="Pending", Reason="", readiness=false. Elapsed: 11.24884ms
Oct 13 12:12:50.723: INFO: Pod "downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032579403s
Oct 13 12:12:52.740: INFO: Pod "downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049625627s
Oct 13 12:12:54.759: INFO: Pod "downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06816753s
STEP: Saw pod success
Oct 13 12:12:54.759: INFO: Pod "downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab" satisfied condition "Succeeded or Failed"
Oct 13 12:12:54.777: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab container client-container: <nil>
STEP: delete the pod
Oct 13 12:12:54.874: INFO: Waiting for pod downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab to disappear
Oct 13 12:12:54.882: INFO: Pod downwardapi-volume-821acfa8-9e2d-4cf6-a7be-531f774d17ab no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:12:54.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1766" for this suite.

• [SLOW TEST:6.331 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":252,"skipped":3959,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:12:54.910: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Oct 13 12:12:56.093: INFO: starting watch
STEP: patching
STEP: updating
Oct 13 12:12:56.125: INFO: waiting for watch events with expected annotations
Oct 13 12:12:56.125: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:12:56.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3011" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":253,"skipped":3972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:12:56.320: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Oct 13 12:12:56.460: INFO: Found Service test-service-rtnnv in namespace services-6038 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Oct 13 12:12:56.460: INFO: Service test-service-rtnnv created
STEP: Getting /status
Oct 13 12:12:56.472: INFO: Service test-service-rtnnv has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Oct 13 12:12:56.497: INFO: observed Service test-service-rtnnv in namespace services-6038 with annotations: map[] & LoadBalancer: {[]}
Oct 13 12:12:56.497: INFO: Found Service test-service-rtnnv in namespace services-6038 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Oct 13 12:12:56.497: INFO: Service test-service-rtnnv has service status patched
STEP: updating the ServiceStatus
Oct 13 12:12:56.522: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Oct 13 12:12:56.528: INFO: Observed Service test-service-rtnnv in namespace services-6038 with annotations: map[] & Conditions: {[]}
Oct 13 12:12:56.529: INFO: Observed event: &Service{ObjectMeta:{test-service-rtnnv  services-6038  4dbb72e6-659b-4ebc-8b13-21d5c78a3ded 406822 0 2021-10-13 12:12:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-10-13 12:12:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.28.67,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.28.67],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Oct 13 12:12:56.529: INFO: Found Service test-service-rtnnv in namespace services-6038 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Oct 13 12:12:56.530: INFO: Service test-service-rtnnv has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Oct 13 12:12:56.564: INFO: observed Service test-service-rtnnv in namespace services-6038 with labels: map[test-service-static:true]
Oct 13 12:12:56.564: INFO: observed Service test-service-rtnnv in namespace services-6038 with labels: map[test-service-static:true]
Oct 13 12:12:56.564: INFO: observed Service test-service-rtnnv in namespace services-6038 with labels: map[test-service-static:true]
Oct 13 12:12:56.564: INFO: Found Service test-service-rtnnv in namespace services-6038 with labels: map[test-service:patched test-service-static:true]
Oct 13 12:12:56.564: INFO: Service test-service-rtnnv patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Oct 13 12:12:56.603: INFO: Observed event: ADDED
Oct 13 12:12:56.603: INFO: Observed event: MODIFIED
Oct 13 12:12:56.604: INFO: Observed event: MODIFIED
Oct 13 12:12:56.604: INFO: Observed event: MODIFIED
Oct 13 12:12:56.604: INFO: Found Service test-service-rtnnv in namespace services-6038 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Oct 13 12:12:56.604: INFO: Service test-service-rtnnv deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:12:56.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6038" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":254,"skipped":4003,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:12:56.641: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:12:57.109: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:12:59.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723977, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723977, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723977, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769723977, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:13:02.213: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:13:02.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-971" for this suite.
STEP: Destroying namespace "webhook-971-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.146 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":255,"skipped":4004,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:13:02.787: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-ee0e17a2-7a1d-4c5a-96ec-e164d18dfd14
STEP: Creating a pod to test consume configMaps
Oct 13 12:13:02.898: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe" in namespace "projected-7700" to be "Succeeded or Failed"
Oct 13 12:13:02.910: INFO: Pod "pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.797524ms
Oct 13 12:13:04.927: INFO: Pod "pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028479759s
Oct 13 12:13:06.942: INFO: Pod "pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043108527s
STEP: Saw pod success
Oct 13 12:13:06.942: INFO: Pod "pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe" satisfied condition "Succeeded or Failed"
Oct 13 12:13:06.954: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:13:07.065: INFO: Waiting for pod pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe to disappear
Oct 13 12:13:07.079: INFO: Pod pod-projected-configmaps-c3817b00-828f-490b-be99-62b4b8d871fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:13:07.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7700" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":256,"skipped":4009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:13:07.119: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:13:20.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5210" for this suite.

• [SLOW TEST:13.302 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":257,"skipped":4034,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:13:20.422: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-9eb968d1-c08c-4713-beb0-6daa79e538d8
STEP: Creating a pod to test consume secrets
Oct 13 12:13:20.552: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411" in namespace "projected-6182" to be "Succeeded or Failed"
Oct 13 12:13:20.564: INFO: Pod "pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411": Phase="Pending", Reason="", readiness=false. Elapsed: 12.234942ms
Oct 13 12:13:22.586: INFO: Pod "pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03366532s
Oct 13 12:13:24.609: INFO: Pod "pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05641855s
STEP: Saw pod success
Oct 13 12:13:24.609: INFO: Pod "pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411" satisfied condition "Succeeded or Failed"
Oct 13 12:13:24.621: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 13 12:13:24.722: INFO: Waiting for pod pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411 to disappear
Oct 13 12:13:24.733: INFO: Pod pod-projected-secrets-afe44db3-0adf-4faa-9f65-65fc7f042411 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:13:24.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6182" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":258,"skipped":4036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:13:24.769: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:13:25.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6820" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":259,"skipped":4062,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:13:25.044: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 13 12:13:25.160: INFO: The status of Pod labelsupdate8c7eab63-489d-4070-9c01-e1d9f528418d is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:13:27.181: INFO: The status of Pod labelsupdate8c7eab63-489d-4070-9c01-e1d9f528418d is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:13:29.180: INFO: The status of Pod labelsupdate8c7eab63-489d-4070-9c01-e1d9f528418d is Running (Ready = true)
Oct 13 12:13:29.804: INFO: Successfully updated pod "labelsupdate8c7eab63-489d-4070-9c01-e1d9f528418d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:13:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4364" for this suite.

• [SLOW TEST:8.904 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":260,"skipped":4071,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:13:33.959: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-11e44897-597e-43b8-a7af-fbdda3d22fa1 in namespace container-probe-7939
Oct 13 12:13:38.116: INFO: Started pod test-webserver-11e44897-597e-43b8-a7af-fbdda3d22fa1 in namespace container-probe-7939
STEP: checking the pod's current state and verifying that restartCount is present
Oct 13 12:13:38.126: INFO: Initial restart count of pod test-webserver-11e44897-597e-43b8-a7af-fbdda3d22fa1 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:17:38.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7939" for this suite.

• [SLOW TEST:244.621 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":261,"skipped":4101,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:17:38.583: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-x7np
STEP: Creating a pod to test atomic-volume-subpath
Oct 13 12:17:38.732: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-x7np" in namespace "subpath-7618" to be "Succeeded or Failed"
Oct 13 12:17:38.745: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Pending", Reason="", readiness=false. Elapsed: 13.120553ms
Oct 13 12:17:40.774: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04231412s
Oct 13 12:17:42.798: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06602708s
Oct 13 12:17:44.820: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 6.087866228s
Oct 13 12:17:46.843: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 8.111363671s
Oct 13 12:17:48.864: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 10.132283131s
Oct 13 12:17:50.881: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 12.148636786s
Oct 13 12:17:52.901: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 14.168763822s
Oct 13 12:17:54.921: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 16.188907269s
Oct 13 12:17:56.951: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 18.218779814s
Oct 13 12:17:58.973: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 20.240982546s
Oct 13 12:18:00.990: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 22.258439987s
Oct 13 12:18:03.012: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Running", Reason="", readiness=true. Elapsed: 24.280254829s
Oct 13 12:18:05.033: INFO: Pod "pod-subpath-test-configmap-x7np": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.300826379s
STEP: Saw pod success
Oct 13 12:18:05.033: INFO: Pod "pod-subpath-test-configmap-x7np" satisfied condition "Succeeded or Failed"
Oct 13 12:18:05.047: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-subpath-test-configmap-x7np container test-container-subpath-configmap-x7np: <nil>
STEP: delete the pod
Oct 13 12:18:05.135: INFO: Waiting for pod pod-subpath-test-configmap-x7np to disappear
Oct 13 12:18:05.144: INFO: Pod pod-subpath-test-configmap-x7np no longer exists
STEP: Deleting pod pod-subpath-test-configmap-x7np
Oct 13 12:18:05.144: INFO: Deleting pod "pod-subpath-test-configmap-x7np" in namespace "subpath-7618"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:18:05.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7618" for this suite.

• [SLOW TEST:26.608 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":262,"skipped":4104,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:18:05.200: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:18:12.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4709" for this suite.

• [SLOW TEST:7.143 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":263,"skipped":4119,"failed":0}
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:18:12.344: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:18:12.424: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8533
I1013 12:18:12.451273      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8533, replica count: 1
I1013 12:18:13.504425      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 12:18:14.504790      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 12:18:15.505017      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 12:18:15.631: INFO: Created: latency-svc-47mjp
Oct 13 12:18:15.648: INFO: Got endpoints: latency-svc-47mjp [43.216733ms]
Oct 13 12:18:15.670: INFO: Created: latency-svc-nfxb4
Oct 13 12:18:15.677: INFO: Got endpoints: latency-svc-nfxb4 [28.72314ms]
Oct 13 12:18:15.685: INFO: Created: latency-svc-jwm8t
Oct 13 12:18:15.702: INFO: Created: latency-svc-dfwc9
Oct 13 12:18:15.703: INFO: Got endpoints: latency-svc-dfwc9 [54.466753ms]
Oct 13 12:18:15.703: INFO: Got endpoints: latency-svc-jwm8t [54.321883ms]
Oct 13 12:18:15.704: INFO: Created: latency-svc-jdgfk
Oct 13 12:18:15.712: INFO: Got endpoints: latency-svc-jdgfk [62.776947ms]
Oct 13 12:18:15.714: INFO: Created: latency-svc-6ch85
Oct 13 12:18:15.717: INFO: Created: latency-svc-94vz4
Oct 13 12:18:15.721: INFO: Got endpoints: latency-svc-6ch85 [71.478703ms]
Oct 13 12:18:15.722: INFO: Got endpoints: latency-svc-94vz4 [73.019428ms]
Oct 13 12:18:15.731: INFO: Created: latency-svc-7fd6w
Oct 13 12:18:15.741: INFO: Got endpoints: latency-svc-7fd6w [91.515692ms]
Oct 13 12:18:15.742: INFO: Created: latency-svc-dzl6m
Oct 13 12:18:15.745: INFO: Created: latency-svc-svqjd
Oct 13 12:18:15.749: INFO: Got endpoints: latency-svc-dzl6m [98.920304ms]
Oct 13 12:18:15.751: INFO: Got endpoints: latency-svc-svqjd [100.726713ms]
Oct 13 12:18:15.755: INFO: Created: latency-svc-xtfrl
Oct 13 12:18:15.761: INFO: Got endpoints: latency-svc-xtfrl [111.072039ms]
Oct 13 12:18:15.764: INFO: Created: latency-svc-2mvsj
Oct 13 12:18:15.768: INFO: Got endpoints: latency-svc-2mvsj [116.928292ms]
Oct 13 12:18:15.769: INFO: Created: latency-svc-kws95
Oct 13 12:18:15.774: INFO: Got endpoints: latency-svc-kws95 [123.101769ms]
Oct 13 12:18:15.782: INFO: Created: latency-svc-d5thf
Oct 13 12:18:15.791: INFO: Created: latency-svc-xcr5v
Oct 13 12:18:15.791: INFO: Got endpoints: latency-svc-d5thf [140.983453ms]
Oct 13 12:18:15.795: INFO: Got endpoints: latency-svc-xcr5v [143.968211ms]
Oct 13 12:18:15.798: INFO: Created: latency-svc-cnfw4
Oct 13 12:18:15.803: INFO: Got endpoints: latency-svc-cnfw4 [151.927272ms]
Oct 13 12:18:15.807: INFO: Created: latency-svc-v6xx4
Oct 13 12:18:15.812: INFO: Got endpoints: latency-svc-v6xx4 [127.302459ms]
Oct 13 12:18:15.812: INFO: Created: latency-svc-bz4cj
Oct 13 12:18:15.819: INFO: Created: latency-svc-grpmg
Oct 13 12:18:15.821: INFO: Got endpoints: latency-svc-bz4cj [117.900393ms]
Oct 13 12:18:15.823: INFO: Got endpoints: latency-svc-grpmg [119.869125ms]
Oct 13 12:18:15.826: INFO: Created: latency-svc-bj2wq
Oct 13 12:18:15.832: INFO: Got endpoints: latency-svc-bj2wq [119.140438ms]
Oct 13 12:18:15.834: INFO: Created: latency-svc-pvgtf
Oct 13 12:18:15.855: INFO: Created: latency-svc-nlhqj
Oct 13 12:18:15.856: INFO: Created: latency-svc-bwh57
Oct 13 12:18:15.856: INFO: Created: latency-svc-hd7xd
Oct 13 12:18:15.857: INFO: Got endpoints: latency-svc-bwh57 [134.519181ms]
Oct 13 12:18:15.859: INFO: Got endpoints: latency-svc-hd7xd [117.711093ms]
Oct 13 12:18:15.857: INFO: Got endpoints: latency-svc-pvgtf [135.868324ms]
Oct 13 12:18:15.861: INFO: Created: latency-svc-b9zr9
Oct 13 12:18:15.861: INFO: Got endpoints: latency-svc-nlhqj [112.0649ms]
Oct 13 12:18:15.863: INFO: Got endpoints: latency-svc-b9zr9 [112.63382ms]
Oct 13 12:18:15.864: INFO: Created: latency-svc-2jd7s
Oct 13 12:18:15.872: INFO: Created: latency-svc-xwsld
Oct 13 12:18:15.875: INFO: Got endpoints: latency-svc-2jd7s [113.883198ms]
Oct 13 12:18:15.882: INFO: Created: latency-svc-vpkc9
Oct 13 12:18:15.883: INFO: Got endpoints: latency-svc-xwsld [115.011261ms]
Oct 13 12:18:15.888: INFO: Got endpoints: latency-svc-vpkc9 [113.483959ms]
Oct 13 12:18:15.889: INFO: Created: latency-svc-tjbdj
Oct 13 12:18:15.898: INFO: Created: latency-svc-8q97r
Oct 13 12:18:15.904: INFO: Got endpoints: latency-svc-tjbdj [111.96821ms]
Oct 13 12:18:15.904: INFO: Got endpoints: latency-svc-8q97r [109.735729ms]
Oct 13 12:18:15.908: INFO: Created: latency-svc-8z7l5
Oct 13 12:18:15.908: INFO: Got endpoints: latency-svc-8z7l5 [105.538409ms]
Oct 13 12:18:15.909: INFO: Created: latency-svc-dftlg
Oct 13 12:18:15.912: INFO: Got endpoints: latency-svc-dftlg [99.977065ms]
Oct 13 12:18:15.912: INFO: Created: latency-svc-nn6g2
Oct 13 12:18:15.921: INFO: Got endpoints: latency-svc-nn6g2 [99.640299ms]
Oct 13 12:18:15.921: INFO: Created: latency-svc-bpm98
Oct 13 12:18:15.928: INFO: Created: latency-svc-8tzsv
Oct 13 12:18:15.929: INFO: Got endpoints: latency-svc-bpm98 [105.697433ms]
Oct 13 12:18:15.935: INFO: Created: latency-svc-ll8k7
Oct 13 12:18:15.939: INFO: Got endpoints: latency-svc-8tzsv [106.811614ms]
Oct 13 12:18:15.949: INFO: Created: latency-svc-59dck
Oct 13 12:18:15.949: INFO: Created: latency-svc-7rdcl
Oct 13 12:18:15.950: INFO: Got endpoints: latency-svc-ll8k7 [90.912878ms]
Oct 13 12:18:15.955: INFO: Created: latency-svc-nx79t
Oct 13 12:18:15.964: INFO: Created: latency-svc-zb2sz
Oct 13 12:18:15.985: INFO: Created: latency-svc-khd6g
Oct 13 12:18:15.986: INFO: Created: latency-svc-6kqrn
Oct 13 12:18:15.986: INFO: Created: latency-svc-kr77h
Oct 13 12:18:15.991: INFO: Got endpoints: latency-svc-7rdcl [130.91648ms]
Oct 13 12:18:15.995: INFO: Created: latency-svc-n52p6
Oct 13 12:18:16.000: INFO: Created: latency-svc-42rtg
Oct 13 12:18:16.009: INFO: Created: latency-svc-q8gnp
Oct 13 12:18:16.016: INFO: Created: latency-svc-wxr9t
Oct 13 12:18:16.025: INFO: Created: latency-svc-nsjp6
Oct 13 12:18:16.033: INFO: Created: latency-svc-klj7c
Oct 13 12:18:16.043: INFO: Created: latency-svc-vrbsk
Oct 13 12:18:16.044: INFO: Got endpoints: latency-svc-59dck [182.777841ms]
Oct 13 12:18:16.050: INFO: Created: latency-svc-r8xhm
Oct 13 12:18:16.058: INFO: Created: latency-svc-jhkgl
Oct 13 12:18:16.067: INFO: Created: latency-svc-zbj98
Oct 13 12:18:16.089: INFO: Got endpoints: latency-svc-nx79t [230.5512ms]
Oct 13 12:18:16.111: INFO: Created: latency-svc-m7b7j
Oct 13 12:18:16.142: INFO: Got endpoints: latency-svc-zb2sz [278.919381ms]
Oct 13 12:18:16.166: INFO: Created: latency-svc-bl6td
Oct 13 12:18:16.195: INFO: Got endpoints: latency-svc-6kqrn [319.260613ms]
Oct 13 12:18:16.217: INFO: Created: latency-svc-bclww
Oct 13 12:18:16.245: INFO: Got endpoints: latency-svc-khd6g [361.490155ms]
Oct 13 12:18:16.274: INFO: Created: latency-svc-2x9mp
Oct 13 12:18:16.292: INFO: Got endpoints: latency-svc-kr77h [403.805466ms]
Oct 13 12:18:16.314: INFO: Created: latency-svc-pvd2s
Oct 13 12:18:16.347: INFO: Got endpoints: latency-svc-n52p6 [443.289172ms]
Oct 13 12:18:16.369: INFO: Created: latency-svc-zgd58
Oct 13 12:18:16.392: INFO: Got endpoints: latency-svc-42rtg [484.921954ms]
Oct 13 12:18:16.414: INFO: Created: latency-svc-6qz8p
Oct 13 12:18:16.442: INFO: Got endpoints: latency-svc-q8gnp [533.82475ms]
Oct 13 12:18:16.468: INFO: Created: latency-svc-7lf4t
Oct 13 12:18:16.495: INFO: Got endpoints: latency-svc-wxr9t [582.497607ms]
Oct 13 12:18:16.522: INFO: Created: latency-svc-h2ssj
Oct 13 12:18:16.547: INFO: Got endpoints: latency-svc-nsjp6 [626.629959ms]
Oct 13 12:18:16.575: INFO: Created: latency-svc-z4qsq
Oct 13 12:18:16.593: INFO: Got endpoints: latency-svc-klj7c [663.446467ms]
Oct 13 12:18:16.626: INFO: Created: latency-svc-kkh8b
Oct 13 12:18:16.643: INFO: Got endpoints: latency-svc-vrbsk [704.141292ms]
Oct 13 12:18:16.672: INFO: Created: latency-svc-mxm26
Oct 13 12:18:16.692: INFO: Got endpoints: latency-svc-r8xhm [742.330471ms]
Oct 13 12:18:16.716: INFO: Created: latency-svc-nvfbj
Oct 13 12:18:16.746: INFO: Got endpoints: latency-svc-jhkgl [754.587749ms]
Oct 13 12:18:16.766: INFO: Created: latency-svc-sxxbf
Oct 13 12:18:16.793: INFO: Got endpoints: latency-svc-zbj98 [748.959395ms]
Oct 13 12:18:16.815: INFO: Created: latency-svc-b8cnv
Oct 13 12:18:16.845: INFO: Got endpoints: latency-svc-m7b7j [755.523762ms]
Oct 13 12:18:16.871: INFO: Created: latency-svc-nfgtd
Oct 13 12:18:16.890: INFO: Got endpoints: latency-svc-bl6td [746.670841ms]
Oct 13 12:18:16.915: INFO: Created: latency-svc-hbvqt
Oct 13 12:18:16.954: INFO: Got endpoints: latency-svc-bclww [759.161403ms]
Oct 13 12:18:16.978: INFO: Created: latency-svc-79m4t
Oct 13 12:18:16.996: INFO: Got endpoints: latency-svc-2x9mp [747.480479ms]
Oct 13 12:18:17.017: INFO: Created: latency-svc-2nvkx
Oct 13 12:18:17.043: INFO: Got endpoints: latency-svc-pvd2s [751.048948ms]
Oct 13 12:18:17.067: INFO: Created: latency-svc-z5zh8
Oct 13 12:18:17.089: INFO: Got endpoints: latency-svc-zgd58 [742.040796ms]
Oct 13 12:18:17.110: INFO: Created: latency-svc-zcfsx
Oct 13 12:18:17.153: INFO: Got endpoints: latency-svc-6qz8p [760.106779ms]
Oct 13 12:18:17.181: INFO: Created: latency-svc-57kvv
Oct 13 12:18:17.191: INFO: Got endpoints: latency-svc-7lf4t [748.512423ms]
Oct 13 12:18:17.212: INFO: Created: latency-svc-tjsm9
Oct 13 12:18:17.244: INFO: Got endpoints: latency-svc-h2ssj [749.819112ms]
Oct 13 12:18:17.268: INFO: Created: latency-svc-ltqmn
Oct 13 12:18:17.292: INFO: Got endpoints: latency-svc-z4qsq [744.337274ms]
Oct 13 12:18:17.316: INFO: Created: latency-svc-dn8vq
Oct 13 12:18:17.344: INFO: Got endpoints: latency-svc-kkh8b [750.871649ms]
Oct 13 12:18:17.368: INFO: Created: latency-svc-ttss4
Oct 13 12:18:17.391: INFO: Got endpoints: latency-svc-mxm26 [747.807093ms]
Oct 13 12:18:17.413: INFO: Created: latency-svc-6tbmp
Oct 13 12:18:17.440: INFO: Got endpoints: latency-svc-nvfbj [747.300077ms]
Oct 13 12:18:17.462: INFO: Created: latency-svc-knhqc
Oct 13 12:18:17.489: INFO: Got endpoints: latency-svc-sxxbf [743.302497ms]
Oct 13 12:18:17.507: INFO: Created: latency-svc-ktn4d
Oct 13 12:18:17.541: INFO: Got endpoints: latency-svc-b8cnv [747.777217ms]
Oct 13 12:18:17.561: INFO: Created: latency-svc-x7g8z
Oct 13 12:18:17.590: INFO: Got endpoints: latency-svc-nfgtd [745.461182ms]
Oct 13 12:18:17.610: INFO: Created: latency-svc-rbslk
Oct 13 12:18:17.642: INFO: Got endpoints: latency-svc-hbvqt [752.218864ms]
Oct 13 12:18:17.664: INFO: Created: latency-svc-wch8s
Oct 13 12:18:17.689: INFO: Got endpoints: latency-svc-79m4t [734.814051ms]
Oct 13 12:18:17.709: INFO: Created: latency-svc-8nstj
Oct 13 12:18:17.741: INFO: Got endpoints: latency-svc-2nvkx [744.914093ms]
Oct 13 12:18:17.762: INFO: Created: latency-svc-nrtkp
Oct 13 12:18:17.792: INFO: Got endpoints: latency-svc-z5zh8 [748.676862ms]
Oct 13 12:18:17.817: INFO: Created: latency-svc-2nx9v
Oct 13 12:18:17.840: INFO: Got endpoints: latency-svc-zcfsx [750.006921ms]
Oct 13 12:18:17.861: INFO: Created: latency-svc-x7mlf
Oct 13 12:18:17.894: INFO: Got endpoints: latency-svc-57kvv [741.236809ms]
Oct 13 12:18:17.916: INFO: Created: latency-svc-swdjz
Oct 13 12:18:17.942: INFO: Got endpoints: latency-svc-tjsm9 [750.161398ms]
Oct 13 12:18:17.961: INFO: Created: latency-svc-vg49p
Oct 13 12:18:17.993: INFO: Got endpoints: latency-svc-ltqmn [747.896528ms]
Oct 13 12:18:18.014: INFO: Created: latency-svc-zknm2
Oct 13 12:18:18.040: INFO: Got endpoints: latency-svc-dn8vq [747.398882ms]
Oct 13 12:18:18.064: INFO: Created: latency-svc-qf67k
Oct 13 12:18:18.091: INFO: Got endpoints: latency-svc-ttss4 [747.332348ms]
Oct 13 12:18:18.114: INFO: Created: latency-svc-b9hqp
Oct 13 12:18:18.141: INFO: Got endpoints: latency-svc-6tbmp [749.742638ms]
Oct 13 12:18:18.165: INFO: Created: latency-svc-8vxs5
Oct 13 12:18:18.189: INFO: Got endpoints: latency-svc-knhqc [749.536864ms]
Oct 13 12:18:18.213: INFO: Created: latency-svc-glhvq
Oct 13 12:18:18.243: INFO: Got endpoints: latency-svc-ktn4d [754.208161ms]
Oct 13 12:18:18.262: INFO: Created: latency-svc-qgc9z
Oct 13 12:18:18.300: INFO: Got endpoints: latency-svc-x7g8z [759.361419ms]
Oct 13 12:18:18.319: INFO: Created: latency-svc-bp7lv
Oct 13 12:18:18.359: INFO: Got endpoints: latency-svc-rbslk [768.397932ms]
Oct 13 12:18:18.389: INFO: Created: latency-svc-cmwdt
Oct 13 12:18:18.391: INFO: Got endpoints: latency-svc-wch8s [748.134301ms]
Oct 13 12:18:18.412: INFO: Created: latency-svc-kmtmc
Oct 13 12:18:18.446: INFO: Got endpoints: latency-svc-8nstj [756.17753ms]
Oct 13 12:18:18.469: INFO: Created: latency-svc-ks9vc
Oct 13 12:18:18.498: INFO: Got endpoints: latency-svc-nrtkp [756.632616ms]
Oct 13 12:18:18.524: INFO: Created: latency-svc-dgvvx
Oct 13 12:18:18.569: INFO: Got endpoints: latency-svc-2nx9v [776.223613ms]
Oct 13 12:18:18.593: INFO: Got endpoints: latency-svc-x7mlf [752.743275ms]
Oct 13 12:18:18.598: INFO: Created: latency-svc-9s9l2
Oct 13 12:18:18.615: INFO: Created: latency-svc-fkr7t
Oct 13 12:18:18.645: INFO: Got endpoints: latency-svc-swdjz [750.958237ms]
Oct 13 12:18:18.678: INFO: Created: latency-svc-tvmnl
Oct 13 12:18:18.695: INFO: Got endpoints: latency-svc-vg49p [752.867069ms]
Oct 13 12:18:18.714: INFO: Created: latency-svc-lsj5r
Oct 13 12:18:18.755: INFO: Got endpoints: latency-svc-zknm2 [761.95671ms]
Oct 13 12:18:18.777: INFO: Created: latency-svc-vxvbk
Oct 13 12:18:18.791: INFO: Got endpoints: latency-svc-qf67k [750.980257ms]
Oct 13 12:18:18.814: INFO: Created: latency-svc-jgzq2
Oct 13 12:18:18.842: INFO: Got endpoints: latency-svc-b9hqp [750.565378ms]
Oct 13 12:18:18.864: INFO: Created: latency-svc-8jn2z
Oct 13 12:18:18.891: INFO: Got endpoints: latency-svc-8vxs5 [750.112448ms]
Oct 13 12:18:18.912: INFO: Created: latency-svc-cp5qf
Oct 13 12:18:18.955: INFO: Got endpoints: latency-svc-glhvq [765.439035ms]
Oct 13 12:18:18.981: INFO: Created: latency-svc-rwmt7
Oct 13 12:18:18.992: INFO: Got endpoints: latency-svc-qgc9z [748.810441ms]
Oct 13 12:18:19.012: INFO: Created: latency-svc-gwfhv
Oct 13 12:18:19.043: INFO: Got endpoints: latency-svc-bp7lv [742.62783ms]
Oct 13 12:18:19.063: INFO: Created: latency-svc-5p9sg
Oct 13 12:18:19.091: INFO: Got endpoints: latency-svc-cmwdt [730.297508ms]
Oct 13 12:18:19.111: INFO: Created: latency-svc-9ds2j
Oct 13 12:18:19.140: INFO: Got endpoints: latency-svc-kmtmc [749.461675ms]
Oct 13 12:18:19.161: INFO: Created: latency-svc-52bct
Oct 13 12:18:19.189: INFO: Got endpoints: latency-svc-ks9vc [743.493632ms]
Oct 13 12:18:19.213: INFO: Created: latency-svc-2tjdl
Oct 13 12:18:19.239: INFO: Got endpoints: latency-svc-dgvvx [741.589293ms]
Oct 13 12:18:19.278: INFO: Created: latency-svc-m5t4g
Oct 13 12:18:19.292: INFO: Got endpoints: latency-svc-9s9l2 [722.946135ms]
Oct 13 12:18:19.311: INFO: Created: latency-svc-wdtc6
Oct 13 12:18:19.365: INFO: Got endpoints: latency-svc-fkr7t [772.022371ms]
Oct 13 12:18:19.385: INFO: Created: latency-svc-gqx2z
Oct 13 12:18:19.389: INFO: Got endpoints: latency-svc-tvmnl [741.849056ms]
Oct 13 12:18:19.417: INFO: Created: latency-svc-2x85g
Oct 13 12:18:19.461: INFO: Got endpoints: latency-svc-lsj5r [766.490876ms]
Oct 13 12:18:19.483: INFO: Created: latency-svc-gfqns
Oct 13 12:18:19.488: INFO: Got endpoints: latency-svc-vxvbk [733.572226ms]
Oct 13 12:18:19.508: INFO: Created: latency-svc-gw4g2
Oct 13 12:18:19.557: INFO: Got endpoints: latency-svc-jgzq2 [765.763628ms]
Oct 13 12:18:19.577: INFO: Created: latency-svc-rbffq
Oct 13 12:18:19.589: INFO: Got endpoints: latency-svc-8jn2z [746.872627ms]
Oct 13 12:18:19.608: INFO: Created: latency-svc-79cht
Oct 13 12:18:19.643: INFO: Got endpoints: latency-svc-cp5qf [751.551851ms]
Oct 13 12:18:19.658: INFO: Created: latency-svc-hn62f
Oct 13 12:18:19.698: INFO: Got endpoints: latency-svc-rwmt7 [742.464646ms]
Oct 13 12:18:19.722: INFO: Created: latency-svc-h9vdd
Oct 13 12:18:19.750: INFO: Got endpoints: latency-svc-gwfhv [757.404698ms]
Oct 13 12:18:19.780: INFO: Created: latency-svc-tdfpl
Oct 13 12:18:19.791: INFO: Got endpoints: latency-svc-5p9sg [747.88708ms]
Oct 13 12:18:19.810: INFO: Created: latency-svc-jbjnd
Oct 13 12:18:19.858: INFO: Got endpoints: latency-svc-9ds2j [767.173985ms]
Oct 13 12:18:19.884: INFO: Created: latency-svc-zq2ws
Oct 13 12:18:19.889: INFO: Got endpoints: latency-svc-52bct [749.1378ms]
Oct 13 12:18:19.912: INFO: Created: latency-svc-xfmd4
Oct 13 12:18:19.942: INFO: Got endpoints: latency-svc-2tjdl [752.903486ms]
Oct 13 12:18:19.969: INFO: Created: latency-svc-52kd4
Oct 13 12:18:19.992: INFO: Got endpoints: latency-svc-m5t4g [753.102147ms]
Oct 13 12:18:20.017: INFO: Created: latency-svc-hjrkn
Oct 13 12:18:20.045: INFO: Got endpoints: latency-svc-wdtc6 [753.751756ms]
Oct 13 12:18:20.077: INFO: Created: latency-svc-t78gb
Oct 13 12:18:20.090: INFO: Got endpoints: latency-svc-gqx2z [725.240151ms]
Oct 13 12:18:20.111: INFO: Created: latency-svc-hnqfv
Oct 13 12:18:20.143: INFO: Got endpoints: latency-svc-2x85g [754.094787ms]
Oct 13 12:18:20.163: INFO: Created: latency-svc-rmphg
Oct 13 12:18:20.195: INFO: Got endpoints: latency-svc-gfqns [733.993879ms]
Oct 13 12:18:20.218: INFO: Created: latency-svc-6hqhv
Oct 13 12:18:20.249: INFO: Got endpoints: latency-svc-gw4g2 [758.557888ms]
Oct 13 12:18:20.282: INFO: Created: latency-svc-9q7tq
Oct 13 12:18:20.290: INFO: Got endpoints: latency-svc-rbffq [733.421054ms]
Oct 13 12:18:20.318: INFO: Created: latency-svc-wq4j8
Oct 13 12:18:20.352: INFO: Got endpoints: latency-svc-79cht [763.18767ms]
Oct 13 12:18:20.376: INFO: Created: latency-svc-plsvb
Oct 13 12:18:20.391: INFO: Got endpoints: latency-svc-hn62f [747.732173ms]
Oct 13 12:18:20.411: INFO: Created: latency-svc-bnmfd
Oct 13 12:18:20.442: INFO: Got endpoints: latency-svc-h9vdd [743.419769ms]
Oct 13 12:18:20.466: INFO: Created: latency-svc-s4hmz
Oct 13 12:18:20.495: INFO: Got endpoints: latency-svc-tdfpl [741.299886ms]
Oct 13 12:18:20.520: INFO: Created: latency-svc-gtb4x
Oct 13 12:18:20.545: INFO: Got endpoints: latency-svc-jbjnd [753.810073ms]
Oct 13 12:18:20.568: INFO: Created: latency-svc-4k7cv
Oct 13 12:18:20.593: INFO: Got endpoints: latency-svc-zq2ws [734.46455ms]
Oct 13 12:18:20.619: INFO: Created: latency-svc-p88tj
Oct 13 12:18:20.644: INFO: Got endpoints: latency-svc-xfmd4 [754.364101ms]
Oct 13 12:18:20.684: INFO: Created: latency-svc-6t264
Oct 13 12:18:20.689: INFO: Got endpoints: latency-svc-52kd4 [745.91767ms]
Oct 13 12:18:20.709: INFO: Created: latency-svc-mkfbl
Oct 13 12:18:20.741: INFO: Got endpoints: latency-svc-hjrkn [748.20492ms]
Oct 13 12:18:20.760: INFO: Created: latency-svc-btqvq
Oct 13 12:18:20.792: INFO: Got endpoints: latency-svc-t78gb [740.309318ms]
Oct 13 12:18:20.816: INFO: Created: latency-svc-kh2kh
Oct 13 12:18:20.843: INFO: Got endpoints: latency-svc-hnqfv [753.234477ms]
Oct 13 12:18:20.865: INFO: Created: latency-svc-2wcbv
Oct 13 12:18:20.889: INFO: Got endpoints: latency-svc-rmphg [745.867449ms]
Oct 13 12:18:20.912: INFO: Created: latency-svc-85fv9
Oct 13 12:18:20.944: INFO: Got endpoints: latency-svc-6hqhv [748.056606ms]
Oct 13 12:18:20.965: INFO: Created: latency-svc-htm7q
Oct 13 12:18:20.994: INFO: Got endpoints: latency-svc-9q7tq [736.504142ms]
Oct 13 12:18:21.013: INFO: Created: latency-svc-xfxn8
Oct 13 12:18:21.041: INFO: Got endpoints: latency-svc-wq4j8 [750.568019ms]
Oct 13 12:18:21.065: INFO: Created: latency-svc-spwfr
Oct 13 12:18:21.091: INFO: Got endpoints: latency-svc-plsvb [737.971175ms]
Oct 13 12:18:21.115: INFO: Created: latency-svc-7wrmv
Oct 13 12:18:21.142: INFO: Got endpoints: latency-svc-bnmfd [751.533914ms]
Oct 13 12:18:21.166: INFO: Created: latency-svc-99wf9
Oct 13 12:18:21.190: INFO: Got endpoints: latency-svc-s4hmz [748.565403ms]
Oct 13 12:18:21.222: INFO: Created: latency-svc-pvnsv
Oct 13 12:18:21.251: INFO: Got endpoints: latency-svc-gtb4x [756.267971ms]
Oct 13 12:18:21.276: INFO: Created: latency-svc-tcfwv
Oct 13 12:18:21.292: INFO: Got endpoints: latency-svc-4k7cv [747.072184ms]
Oct 13 12:18:21.319: INFO: Created: latency-svc-zzx94
Oct 13 12:18:21.357: INFO: Got endpoints: latency-svc-p88tj [764.248412ms]
Oct 13 12:18:21.382: INFO: Created: latency-svc-ljf7t
Oct 13 12:18:21.393: INFO: Got endpoints: latency-svc-6t264 [731.578156ms]
Oct 13 12:18:21.416: INFO: Created: latency-svc-jzbs4
Oct 13 12:18:21.444: INFO: Got endpoints: latency-svc-mkfbl [754.226561ms]
Oct 13 12:18:21.470: INFO: Created: latency-svc-h82rs
Oct 13 12:18:21.492: INFO: Got endpoints: latency-svc-btqvq [751.45065ms]
Oct 13 12:18:21.519: INFO: Created: latency-svc-gsx4g
Oct 13 12:18:21.545: INFO: Got endpoints: latency-svc-kh2kh [752.3301ms]
Oct 13 12:18:21.570: INFO: Created: latency-svc-npfp8
Oct 13 12:18:21.591: INFO: Got endpoints: latency-svc-2wcbv [747.271195ms]
Oct 13 12:18:21.612: INFO: Created: latency-svc-5t79l
Oct 13 12:18:21.645: INFO: Got endpoints: latency-svc-85fv9 [756.086115ms]
Oct 13 12:18:21.668: INFO: Created: latency-svc-5b4ch
Oct 13 12:18:21.693: INFO: Got endpoints: latency-svc-htm7q [748.139207ms]
Oct 13 12:18:21.718: INFO: Created: latency-svc-5w84q
Oct 13 12:18:21.744: INFO: Got endpoints: latency-svc-xfxn8 [750.362289ms]
Oct 13 12:18:21.763: INFO: Created: latency-svc-r446t
Oct 13 12:18:21.790: INFO: Got endpoints: latency-svc-spwfr [749.187179ms]
Oct 13 12:18:21.806: INFO: Created: latency-svc-22z7r
Oct 13 12:18:21.838: INFO: Got endpoints: latency-svc-7wrmv [746.080326ms]
Oct 13 12:18:21.855: INFO: Created: latency-svc-fc5wq
Oct 13 12:18:21.894: INFO: Got endpoints: latency-svc-99wf9 [751.652714ms]
Oct 13 12:18:21.916: INFO: Created: latency-svc-ps9jc
Oct 13 12:18:21.942: INFO: Got endpoints: latency-svc-pvnsv [751.53649ms]
Oct 13 12:18:21.963: INFO: Created: latency-svc-fj5w9
Oct 13 12:18:21.991: INFO: Got endpoints: latency-svc-tcfwv [739.578655ms]
Oct 13 12:18:22.014: INFO: Created: latency-svc-t5blj
Oct 13 12:18:22.045: INFO: Got endpoints: latency-svc-zzx94 [752.738656ms]
Oct 13 12:18:22.065: INFO: Created: latency-svc-z2vmm
Oct 13 12:18:22.090: INFO: Got endpoints: latency-svc-ljf7t [732.546696ms]
Oct 13 12:18:22.112: INFO: Created: latency-svc-ckwc4
Oct 13 12:18:22.147: INFO: Got endpoints: latency-svc-jzbs4 [753.350989ms]
Oct 13 12:18:22.167: INFO: Created: latency-svc-kxtxc
Oct 13 12:18:22.195: INFO: Got endpoints: latency-svc-h82rs [750.518042ms]
Oct 13 12:18:22.215: INFO: Created: latency-svc-nkqt7
Oct 13 12:18:22.239: INFO: Got endpoints: latency-svc-gsx4g [746.972503ms]
Oct 13 12:18:22.261: INFO: Created: latency-svc-5jvkf
Oct 13 12:18:22.291: INFO: Got endpoints: latency-svc-npfp8 [746.461521ms]
Oct 13 12:18:22.312: INFO: Created: latency-svc-mjm9j
Oct 13 12:18:22.343: INFO: Got endpoints: latency-svc-5t79l [751.843308ms]
Oct 13 12:18:22.363: INFO: Created: latency-svc-h789c
Oct 13 12:18:22.390: INFO: Got endpoints: latency-svc-5b4ch [744.592853ms]
Oct 13 12:18:22.409: INFO: Created: latency-svc-5zhwg
Oct 13 12:18:22.444: INFO: Got endpoints: latency-svc-5w84q [751.053709ms]
Oct 13 12:18:22.463: INFO: Created: latency-svc-zfkt9
Oct 13 12:18:22.492: INFO: Got endpoints: latency-svc-r446t [747.840438ms]
Oct 13 12:18:22.512: INFO: Created: latency-svc-bwtkv
Oct 13 12:18:22.546: INFO: Got endpoints: latency-svc-22z7r [755.984083ms]
Oct 13 12:18:22.571: INFO: Created: latency-svc-fx4t6
Oct 13 12:18:22.592: INFO: Got endpoints: latency-svc-fc5wq [753.779116ms]
Oct 13 12:18:22.611: INFO: Created: latency-svc-gp4lf
Oct 13 12:18:22.640: INFO: Got endpoints: latency-svc-ps9jc [746.226057ms]
Oct 13 12:18:22.660: INFO: Created: latency-svc-4skxz
Oct 13 12:18:22.689: INFO: Got endpoints: latency-svc-fj5w9 [747.146803ms]
Oct 13 12:18:22.710: INFO: Created: latency-svc-thhqw
Oct 13 12:18:22.742: INFO: Got endpoints: latency-svc-t5blj [750.527771ms]
Oct 13 12:18:22.767: INFO: Created: latency-svc-pb4hz
Oct 13 12:18:22.793: INFO: Got endpoints: latency-svc-z2vmm [747.603003ms]
Oct 13 12:18:22.814: INFO: Created: latency-svc-dx4vh
Oct 13 12:18:22.848: INFO: Got endpoints: latency-svc-ckwc4 [757.646549ms]
Oct 13 12:18:22.874: INFO: Created: latency-svc-lvgsw
Oct 13 12:18:22.898: INFO: Got endpoints: latency-svc-kxtxc [750.700036ms]
Oct 13 12:18:22.919: INFO: Created: latency-svc-nz6t8
Oct 13 12:18:22.941: INFO: Got endpoints: latency-svc-nkqt7 [746.109555ms]
Oct 13 12:18:22.963: INFO: Created: latency-svc-4k49k
Oct 13 12:18:22.990: INFO: Got endpoints: latency-svc-5jvkf [750.83307ms]
Oct 13 12:18:23.016: INFO: Created: latency-svc-f8j2q
Oct 13 12:18:23.042: INFO: Got endpoints: latency-svc-mjm9j [749.898038ms]
Oct 13 12:18:23.062: INFO: Created: latency-svc-kdfv6
Oct 13 12:18:23.095: INFO: Got endpoints: latency-svc-h789c [751.733643ms]
Oct 13 12:18:23.116: INFO: Created: latency-svc-dhvqw
Oct 13 12:18:23.139: INFO: Got endpoints: latency-svc-5zhwg [748.987375ms]
Oct 13 12:18:23.164: INFO: Created: latency-svc-wq22l
Oct 13 12:18:23.191: INFO: Got endpoints: latency-svc-zfkt9 [746.8537ms]
Oct 13 12:18:23.212: INFO: Created: latency-svc-dxdsq
Oct 13 12:18:23.247: INFO: Got endpoints: latency-svc-bwtkv [754.678157ms]
Oct 13 12:18:23.279: INFO: Created: latency-svc-v26hd
Oct 13 12:18:23.289: INFO: Got endpoints: latency-svc-fx4t6 [741.702219ms]
Oct 13 12:18:23.305: INFO: Created: latency-svc-dhdn8
Oct 13 12:18:23.342: INFO: Got endpoints: latency-svc-gp4lf [750.350899ms]
Oct 13 12:18:23.361: INFO: Created: latency-svc-rlrvl
Oct 13 12:18:23.391: INFO: Got endpoints: latency-svc-4skxz [751.102252ms]
Oct 13 12:18:23.414: INFO: Created: latency-svc-hsnmv
Oct 13 12:18:23.441: INFO: Got endpoints: latency-svc-thhqw [751.79826ms]
Oct 13 12:18:23.473: INFO: Created: latency-svc-5ps8g
Oct 13 12:18:23.505: INFO: Got endpoints: latency-svc-pb4hz [762.653394ms]
Oct 13 12:18:23.545: INFO: Got endpoints: latency-svc-dx4vh [752.220919ms]
Oct 13 12:18:23.590: INFO: Got endpoints: latency-svc-lvgsw [739.231895ms]
Oct 13 12:18:23.639: INFO: Got endpoints: latency-svc-nz6t8 [741.431484ms]
Oct 13 12:18:23.691: INFO: Got endpoints: latency-svc-4k49k [748.86181ms]
Oct 13 12:18:23.742: INFO: Got endpoints: latency-svc-f8j2q [751.800954ms]
Oct 13 12:18:23.791: INFO: Got endpoints: latency-svc-kdfv6 [748.881566ms]
Oct 13 12:18:23.845: INFO: Got endpoints: latency-svc-dhvqw [750.112018ms]
Oct 13 12:18:23.896: INFO: Got endpoints: latency-svc-wq22l [756.448286ms]
Oct 13 12:18:23.943: INFO: Got endpoints: latency-svc-dxdsq [752.014129ms]
Oct 13 12:18:23.992: INFO: Got endpoints: latency-svc-v26hd [743.6059ms]
Oct 13 12:18:24.044: INFO: Got endpoints: latency-svc-dhdn8 [754.741112ms]
Oct 13 12:18:24.121: INFO: Got endpoints: latency-svc-rlrvl [778.175531ms]
Oct 13 12:18:24.143: INFO: Got endpoints: latency-svc-hsnmv [751.790055ms]
Oct 13 12:18:24.192: INFO: Got endpoints: latency-svc-5ps8g [750.265088ms]
Oct 13 12:18:24.192: INFO: Latencies: [28.72314ms 54.321883ms 54.466753ms 62.776947ms 71.478703ms 73.019428ms 90.912878ms 91.515692ms 98.920304ms 99.640299ms 99.977065ms 100.726713ms 105.538409ms 105.697433ms 106.811614ms 109.735729ms 111.072039ms 111.96821ms 112.0649ms 112.63382ms 113.483959ms 113.883198ms 115.011261ms 116.928292ms 117.711093ms 117.900393ms 119.140438ms 119.869125ms 123.101769ms 127.302459ms 130.91648ms 134.519181ms 135.868324ms 140.983453ms 143.968211ms 151.927272ms 182.777841ms 230.5512ms 278.919381ms 319.260613ms 361.490155ms 403.805466ms 443.289172ms 484.921954ms 533.82475ms 582.497607ms 626.629959ms 663.446467ms 704.141292ms 722.946135ms 725.240151ms 730.297508ms 731.578156ms 732.546696ms 733.421054ms 733.572226ms 733.993879ms 734.46455ms 734.814051ms 736.504142ms 737.971175ms 739.231895ms 739.578655ms 740.309318ms 741.236809ms 741.299886ms 741.431484ms 741.589293ms 741.702219ms 741.849056ms 742.040796ms 742.330471ms 742.464646ms 742.62783ms 743.302497ms 743.419769ms 743.493632ms 743.6059ms 744.337274ms 744.592853ms 744.914093ms 745.461182ms 745.867449ms 745.91767ms 746.080326ms 746.109555ms 746.226057ms 746.461521ms 746.670841ms 746.8537ms 746.872627ms 746.972503ms 747.072184ms 747.146803ms 747.271195ms 747.300077ms 747.332348ms 747.398882ms 747.480479ms 747.603003ms 747.732173ms 747.777217ms 747.807093ms 747.840438ms 747.88708ms 747.896528ms 748.056606ms 748.134301ms 748.139207ms 748.20492ms 748.512423ms 748.565403ms 748.676862ms 748.810441ms 748.86181ms 748.881566ms 748.959395ms 748.987375ms 749.1378ms 749.187179ms 749.461675ms 749.536864ms 749.742638ms 749.819112ms 749.898038ms 750.006921ms 750.112018ms 750.112448ms 750.161398ms 750.265088ms 750.350899ms 750.362289ms 750.518042ms 750.527771ms 750.565378ms 750.568019ms 750.700036ms 750.83307ms 750.871649ms 750.958237ms 750.980257ms 751.048948ms 751.053709ms 751.102252ms 751.45065ms 751.533914ms 751.53649ms 751.551851ms 751.652714ms 751.733643ms 751.790055ms 751.79826ms 751.800954ms 751.843308ms 752.014129ms 752.218864ms 752.220919ms 752.3301ms 752.738656ms 752.743275ms 752.867069ms 752.903486ms 753.102147ms 753.234477ms 753.350989ms 753.751756ms 753.779116ms 753.810073ms 754.094787ms 754.208161ms 754.226561ms 754.364101ms 754.587749ms 754.678157ms 754.741112ms 755.523762ms 755.984083ms 756.086115ms 756.17753ms 756.267971ms 756.448286ms 756.632616ms 757.404698ms 757.646549ms 758.557888ms 759.161403ms 759.361419ms 760.106779ms 761.95671ms 762.653394ms 763.18767ms 764.248412ms 765.439035ms 765.763628ms 766.490876ms 767.173985ms 768.397932ms 772.022371ms 776.223613ms 778.175531ms]
Oct 13 12:18:24.193: INFO: 50 %ile: 747.732173ms
Oct 13 12:18:24.193: INFO: 90 %ile: 756.448286ms
Oct 13 12:18:24.193: INFO: 99 %ile: 776.223613ms
Oct 13 12:18:24.193: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:18:24.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8533" for this suite.

• [SLOW TEST:11.887 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":264,"skipped":4121,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:18:24.234: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-ab10d6c7-51b0-4aac-b59a-8d96a5c5eddf
STEP: Creating a pod to test consume configMaps
Oct 13 12:18:24.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975" in namespace "configmap-6225" to be "Succeeded or Failed"
Oct 13 12:18:24.413: INFO: Pod "pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975": Phase="Pending", Reason="", readiness=false. Elapsed: 12.81328ms
Oct 13 12:18:26.428: INFO: Pod "pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027959806s
Oct 13 12:18:28.449: INFO: Pod "pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048121805s
STEP: Saw pod success
Oct 13 12:18:28.449: INFO: Pod "pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975" satisfied condition "Succeeded or Failed"
Oct 13 12:18:28.457: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf pod pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:18:28.527: INFO: Waiting for pod pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975 to disappear
Oct 13 12:18:28.538: INFO: Pod pod-configmaps-cf02a45b-6ac3-47c1-b482-85d1ee47a975 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:18:28.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6225" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":265,"skipped":4122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:18:28.571: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Oct 13 12:18:32.714: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9288 PodName:var-expansion-392f66b9-1a37-4791-9172-ad78563f977d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 12:18:32.714: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: test for file in mounted path
Oct 13 12:18:33.219: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9288 PodName:var-expansion-392f66b9-1a37-4791-9172-ad78563f977d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 12:18:33.219: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: updating the annotation value
Oct 13 12:18:34.122: INFO: Successfully updated pod "var-expansion-392f66b9-1a37-4791-9172-ad78563f977d"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Oct 13 12:18:34.132: INFO: Deleting pod "var-expansion-392f66b9-1a37-4791-9172-ad78563f977d" in namespace "var-expansion-9288"
Oct 13 12:18:34.145: INFO: Wait up to 5m0s for pod "var-expansion-392f66b9-1a37-4791-9172-ad78563f977d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:19:08.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9288" for this suite.

• [SLOW TEST:39.643 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":266,"skipped":4149,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:19:08.217: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Oct 13 12:19:08.361: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:10.377: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:12.377: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:14.379: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.1.102 on the node which pod1 resides and expect scheduled
Oct 13 12:19:14.408: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:16.432: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:18.427: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.1.102 but use UDP protocol on the node which pod2 resides
Oct 13 12:19:18.452: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:20.471: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:22.475: INFO: The status of Pod pod3 is Running (Ready = true)
Oct 13 12:19:22.516: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:24.535: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:19:26.544: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Oct 13 12:19:26.554: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.1.102 http://127.0.0.1:54323/hostname] Namespace:hostport-6725 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 12:19:26.554: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.102, port: 54323
Oct 13 12:19:27.067: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.1.102:54323/hostname] Namespace:hostport-6725 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 12:19:27.067: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.1.102, port: 54323 UDP
Oct 13 12:19:27.497: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.1.102 54323] Namespace:hostport-6725 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 12:19:27.497: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:19:32.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6725" for this suite.

• [SLOW TEST:24.744 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":267,"skipped":4169,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:19:32.962: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 13 12:19:33.082: INFO: Waiting up to 5m0s for pod "downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a" in namespace "downward-api-6214" to be "Succeeded or Failed"
Oct 13 12:19:33.096: INFO: Pod "downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.427834ms
Oct 13 12:19:35.115: INFO: Pod "downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032957056s
Oct 13 12:19:37.136: INFO: Pod "downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054040755s
STEP: Saw pod success
Oct 13 12:19:37.136: INFO: Pod "downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a" satisfied condition "Succeeded or Failed"
Oct 13 12:19:37.147: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf pod downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a container dapi-container: <nil>
STEP: delete the pod
Oct 13 12:19:37.202: INFO: Waiting for pod downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a to disappear
Oct 13 12:19:37.215: INFO: Pod downward-api-109fcbf3-2f26-43a8-9004-d564c630df9a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:19:37.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6214" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":268,"skipped":4171,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:19:37.247: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2572
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2572
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2572
Oct 13 12:19:37.385: INFO: Found 0 stateful pods, waiting for 1
Oct 13 12:19:47.413: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Oct 13 12:19:47.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 12:19:49.882: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 12:19:49.882: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 12:19:49.882: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 12:19:49.904: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct 13 12:19:59.939: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 12:19:59.939: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 12:19:59.993: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999007s
Oct 13 12:20:01.016: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983400342s
Oct 13 12:20:02.050: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.959693523s
Oct 13 12:20:03.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.926396903s
Oct 13 12:20:04.076: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.912145728s
Oct 13 12:20:05.093: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.89944446s
Oct 13 12:20:06.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.883456404s
Oct 13 12:20:07.121: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.870741242s
Oct 13 12:20:08.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.854366862s
Oct 13 12:20:09.167: INFO: Verifying statefulset ss doesn't scale past 1 for another 825.029579ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2572
Oct 13 12:20:10.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 12:20:10.600: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 13 12:20:10.600: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 12:20:10.600: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 12:20:10.616: INFO: Found 1 stateful pods, waiting for 3
Oct 13 12:20:20.647: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:20:20.647: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct 13 12:20:20.647: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Oct 13 12:20:20.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 12:20:21.189: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 12:20:21.189: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 12:20:21.189: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 12:20:21.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 12:20:21.719: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 12:20:21.719: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 12:20:21.719: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 12:20:21.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Oct 13 12:20:22.254: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Oct 13 12:20:22.254: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Oct 13 12:20:22.254: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Oct 13 12:20:22.254: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 12:20:22.272: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Oct 13 12:20:32.302: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 12:20:32.303: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 12:20:32.303: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct 13 12:20:32.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999786s
Oct 13 12:20:33.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988623092s
Oct 13 12:20:34.378: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.969627636s
Oct 13 12:20:35.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952174587s
Oct 13 12:20:36.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931271252s
Oct 13 12:20:37.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.900003808s
Oct 13 12:20:38.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.882435938s
Oct 13 12:20:39.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.860426999s
Oct 13 12:20:40.508: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.841034537s
Oct 13 12:20:41.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 821.330208ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2572
Oct 13 12:20:42.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 12:20:43.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 13 12:20:43.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 12:20:43.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 12:20:43.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 12:20:43.570: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 13 12:20:43.570: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 12:20:43.570: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 12:20:43.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=statefulset-2572 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Oct 13 12:20:44.080: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Oct 13 12:20:44.080: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Oct 13 12:20:44.080: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Oct 13 12:20:44.080: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Oct 13 12:21:04.129: INFO: Deleting all statefulset in ns statefulset-2572
Oct 13 12:21:04.140: INFO: Scaling statefulset ss to 0
Oct 13 12:21:04.179: INFO: Waiting for statefulset status.replicas updated to 0
Oct 13 12:21:04.186: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:21:04.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2572" for this suite.

• [SLOW TEST:87.010 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":269,"skipped":4175,"failed":0}
SS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:21:04.260: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:21:04.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-108" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":270,"skipped":4177,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:21:04.489: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-00342129-1912-4fbc-ba3b-f28f11e90a20
STEP: Creating configMap with name cm-test-opt-upd-a2343369-babc-4097-a70e-c467078fe8af
STEP: Creating the pod
Oct 13 12:21:04.645: INFO: The status of Pod pod-projected-configmaps-d3e68bb8-dccf-4db7-a509-413139a383c7 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:21:06.673: INFO: The status of Pod pod-projected-configmaps-d3e68bb8-dccf-4db7-a509-413139a383c7 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:21:08.679: INFO: The status of Pod pod-projected-configmaps-d3e68bb8-dccf-4db7-a509-413139a383c7 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:21:10.677: INFO: The status of Pod pod-projected-configmaps-d3e68bb8-dccf-4db7-a509-413139a383c7 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-00342129-1912-4fbc-ba3b-f28f11e90a20
STEP: Updating configmap cm-test-opt-upd-a2343369-babc-4097-a70e-c467078fe8af
STEP: Creating configMap with name cm-test-opt-create-3b70614b-7c45-46d0-87a2-757e508bbe6e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:22:18.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4179" for this suite.

• [SLOW TEST:74.120 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":271,"skipped":4196,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:22:18.614: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-efe1d7ae-2b56-4113-a45f-31a96b6f977b in namespace container-probe-1212
Oct 13 12:22:22.765: INFO: Started pod liveness-efe1d7ae-2b56-4113-a45f-31a96b6f977b in namespace container-probe-1212
STEP: checking the pod's current state and verifying that restartCount is present
Oct 13 12:22:22.776: INFO: Initial restart count of pod liveness-efe1d7ae-2b56-4113-a45f-31a96b6f977b is 0
Oct 13 12:22:40.975: INFO: Restart count of pod container-probe-1212/liveness-efe1d7ae-2b56-4113-a45f-31a96b6f977b is now 1 (18.199251283s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:22:41.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1212" for this suite.

• [SLOW TEST:22.416 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":272,"skipped":4209,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:22:41.035: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5991
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct 13 12:22:41.133: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Oct 13 12:22:41.200: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:22:43.227: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:22:45.212: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:22:47.220: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:22:49.220: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:22:51.220: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:22:53.220: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:22:55.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:22:57.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:22:59.218: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:23:01.221: INFO: The status of Pod netserver-0 is Running (Ready = false)
Oct 13 12:23:03.228: INFO: The status of Pod netserver-0 is Running (Ready = true)
Oct 13 12:23:03.251: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Oct 13 12:23:07.579: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Oct 13 12:23:07.580: INFO: Going to poll 172.25.1.105 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Oct 13 12:23:07.589: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.105 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5991 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 12:23:07.590: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 12:23:08.928: INFO: Found all 1 expected endpoints: [netserver-0]
Oct 13 12:23:08.928: INFO: Going to poll 172.25.0.86 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Oct 13 12:23:08.951: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.86 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5991 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Oct 13 12:23:08.951: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 12:23:10.325: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:23:10.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5991" for this suite.

• [SLOW TEST:29.331 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":273,"skipped":4212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:23:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:23:10.544: INFO: Create a RollingUpdate DaemonSet
Oct 13 12:23:10.561: INFO: Check that daemon pods launch on every node of the cluster
Oct 13 12:23:10.583: INFO: Number of nodes with available pods: 0
Oct 13 12:23:10.583: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:23:11.618: INFO: Number of nodes with available pods: 0
Oct 13 12:23:11.618: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:23:12.611: INFO: Number of nodes with available pods: 0
Oct 13 12:23:12.611: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:23:13.619: INFO: Number of nodes with available pods: 1
Oct 13 12:23:13.619: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr is running more than one daemon pod
Oct 13 12:23:14.616: INFO: Number of nodes with available pods: 2
Oct 13 12:23:14.616: INFO: Number of running nodes: 2, number of available pods: 2
Oct 13 12:23:14.617: INFO: Update the DaemonSet to trigger a rollout
Oct 13 12:23:14.646: INFO: Updating DaemonSet daemon-set
Oct 13 12:23:19.703: INFO: Roll back the DaemonSet before rollout is complete
Oct 13 12:23:19.743: INFO: Updating DaemonSet daemon-set
Oct 13 12:23:19.743: INFO: Make sure DaemonSet rollback is complete
Oct 13 12:23:19.754: INFO: Wrong image for pod: daemon-set-jcgk6. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Oct 13 12:23:19.754: INFO: Pod daemon-set-jcgk6 is not available
Oct 13 12:23:21.783: INFO: Pod daemon-set-rb9c6 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8435, will wait for the garbage collector to delete the pods
Oct 13 12:23:21.910: INFO: Deleting DaemonSet.extensions daemon-set took: 15.549595ms
Oct 13 12:23:22.012: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.257713ms
Oct 13 12:23:25.228: INFO: Number of nodes with available pods: 0
Oct 13 12:23:25.229: INFO: Number of running nodes: 0, number of available pods: 0
Oct 13 12:23:25.240: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"412141"},"items":null}

Oct 13 12:23:25.255: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"412141"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:23:25.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8435" for this suite.

• [SLOW TEST:14.965 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":274,"skipped":4261,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:23:25.338: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:23:25.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4343" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":275,"skipped":4275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:23:25.546: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-1327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1327 to expose endpoints map[]
Oct 13 12:23:25.676: INFO: successfully validated that service endpoint-test2 in namespace services-1327 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1327
Oct 13 12:23:25.712: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:23:27.747: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:23:29.739: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1327 to expose endpoints map[pod1:[80]]
Oct 13 12:23:29.777: INFO: successfully validated that service endpoint-test2 in namespace services-1327 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1327
Oct 13 12:23:29.802: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:23:31.817: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:23:33.820: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1327 to expose endpoints map[pod1:[80] pod2:[80]]
Oct 13 12:23:33.881: INFO: successfully validated that service endpoint-test2 in namespace services-1327 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1327 to expose endpoints map[pod2:[80]]
Oct 13 12:23:33.973: INFO: successfully validated that service endpoint-test2 in namespace services-1327 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1327 to expose endpoints map[]
Oct 13 12:23:34.055: INFO: successfully validated that service endpoint-test2 in namespace services-1327 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:23:34.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1327" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:8.584 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":276,"skipped":4367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:23:34.131: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:23:34.899: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:23:36.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724614, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724614, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724614, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724614, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:23:40.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:23:52.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1799" for this suite.
STEP: Destroying namespace "webhook-1799-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.611 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":277,"skipped":4398,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:23:52.742: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:23:53.455: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:23:55.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724633, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724633, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724633, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724633, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:23:58.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Oct 13 12:23:58.706: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:23:58.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-497" for this suite.
STEP: Destroying namespace "webhook-497-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.230 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":278,"skipped":4402,"failed":0}
S
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:23:58.972: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:23:59.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8739" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":279,"skipped":4403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:23:59.193: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:23:59.278: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 13 12:23:59.306: INFO: The status of Pod pod-exec-websocket-a48ba972-a554-4c5d-a191-3a576d473bb9 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:24:01.329: INFO: The status of Pod pod-exec-websocket-a48ba972-a554-4c5d-a191-3a576d473bb9 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:24:03.323: INFO: The status of Pod pod-exec-websocket-a48ba972-a554-4c5d-a191-3a576d473bb9 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:24:03.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8306" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":280,"skipped":4429,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:24:03.572: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct 13 12:24:07.773: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:24:07.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2172" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":281,"skipped":4439,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:24:07.861: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:24:07.959: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Oct 13 12:24:11.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 create -f -'
Oct 13 12:24:13.383: INFO: stderr: ""
Oct 13 12:24:13.383: INFO: stdout: "e2e-test-crd-publish-openapi-1307-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 13 12:24:13.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 delete e2e-test-crd-publish-openapi-1307-crds test-cr'
Oct 13 12:24:13.561: INFO: stderr: ""
Oct 13 12:24:13.561: INFO: stdout: "e2e-test-crd-publish-openapi-1307-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Oct 13 12:24:13.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 apply -f -'
Oct 13 12:24:13.991: INFO: stderr: ""
Oct 13 12:24:13.991: INFO: stdout: "e2e-test-crd-publish-openapi-1307-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Oct 13 12:24:13.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 delete e2e-test-crd-publish-openapi-1307-crds test-cr'
Oct 13 12:24:14.103: INFO: stderr: ""
Oct 13 12:24:14.103: INFO: stdout: "e2e-test-crd-publish-openapi-1307-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Oct 13 12:24:14.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-8719 explain e2e-test-crd-publish-openapi-1307-crds'
Oct 13 12:24:14.517: INFO: stderr: ""
Oct 13 12:24:14.517: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1307-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:24:18.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8719" for this suite.

• [SLOW TEST:10.271 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":282,"skipped":4440,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:24:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Oct 13 12:24:18.220: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Oct 13 12:24:30.685: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
Oct 13 12:24:33.722: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:24:45.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5631" for this suite.

• [SLOW TEST:27.655 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":283,"skipped":4444,"failed":0}
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:24:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Oct 13 12:24:45.947: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9915  612ea091-7709-483c-aa6c-6a114ea3e6d9 412938 0 2021-10-13 12:24:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-10-13 12:24:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Oct 13 12:24:45.948: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9915  612ea091-7709-483c-aa6c-6a114ea3e6d9 412939 0 2021-10-13 12:24:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-10-13 12:24:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:24:45.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9915" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":284,"skipped":4444,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:24:45.980: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-10aa72ad-c6c2-40cc-a4a7-c706dfec268b
STEP: Creating a pod to test consume secrets
Oct 13 12:24:46.085: INFO: Waiting up to 5m0s for pod "pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9" in namespace "secrets-174" to be "Succeeded or Failed"
Oct 13 12:24:46.098: INFO: Pod "pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.196375ms
Oct 13 12:24:48.118: INFO: Pod "pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033177685s
Oct 13 12:24:50.138: INFO: Pod "pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053372454s
STEP: Saw pod success
Oct 13 12:24:50.138: INFO: Pod "pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9" satisfied condition "Succeeded or Failed"
Oct 13 12:24:50.149: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9 container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 12:24:50.241: INFO: Waiting for pod pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9 to disappear
Oct 13 12:24:50.249: INFO: Pod pod-secrets-008d239d-36c1-47bc-a13a-23796114efb9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:24:50.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-174" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":285,"skipped":4451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:24:50.282: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-943509bb-c85d-428c-8d28-2f854d5d3cf5
STEP: Creating a pod to test consume configMaps
Oct 13 12:24:50.391: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828" in namespace "projected-9619" to be "Succeeded or Failed"
Oct 13 12:24:50.402: INFO: Pod "pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828": Phase="Pending", Reason="", readiness=false. Elapsed: 10.292514ms
Oct 13 12:24:52.422: INFO: Pod "pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030592757s
Oct 13 12:24:54.445: INFO: Pod "pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053521054s
STEP: Saw pod success
Oct 13 12:24:54.445: INFO: Pod "pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828" satisfied condition "Succeeded or Failed"
Oct 13 12:24:54.459: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:24:54.518: INFO: Waiting for pod pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828 to disappear
Oct 13 12:24:54.526: INFO: Pod pod-projected-configmaps-5c8968ae-9316-49fe-8127-33db55ce3828 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:24:54.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9619" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":286,"skipped":4511,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:24:54.585: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Oct 13 12:25:00.750: INFO: &Pod{ObjectMeta:{send-events-6c060105-2390-4c5c-a046-4fa2cf807e66  events-3849  c8fd53a6-5733-4357-86be-f88d475f24da 413083 0 2021-10-13 12:24:54 +0000 UTC <nil> <nil> map[name:foo time:668344118] map[cni.projectcalico.org/podIP:172.25.0.96/32 cni.projectcalico.org/podIPs:172.25.0.96/32] [] []  [{e2e.test Update v1 2021-10-13 12:24:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-10-13 12:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-10-13 12:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9msx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9msx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:24:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:24:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:172.25.0.96,StartTime:2021-10-13 12:24:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-10-13 12:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://b404afd2747165cefee734dfd9f2a113c5c2282cf14ac0929417f53474e5ea78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Oct 13 12:25:02.769: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Oct 13 12:25:04.782: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:25:04.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3849" for this suite.

• [SLOW TEST:10.262 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":287,"skipped":4522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:25:04.856: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:25:04.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7254 create -f -'
Oct 13 12:25:05.362: INFO: stderr: ""
Oct 13 12:25:05.362: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Oct 13 12:25:05.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7254 create -f -'
Oct 13 12:25:05.735: INFO: stderr: ""
Oct 13 12:25:05.735: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Oct 13 12:25:06.753: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 12:25:06.754: INFO: Found 0 / 1
Oct 13 12:25:07.752: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 12:25:07.753: INFO: Found 0 / 1
Oct 13 12:25:08.760: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 12:25:08.760: INFO: Found 0 / 1
Oct 13 12:25:09.753: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 12:25:09.753: INFO: Found 1 / 1
Oct 13 12:25:09.753: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct 13 12:25:09.772: INFO: Selector matched 1 pods for map[app:agnhost]
Oct 13 12:25:09.772: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct 13 12:25:09.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7254 describe pod agnhost-primary-gb7sq'
Oct 13 12:25:10.190: INFO: stderr: ""
Oct 13 12:25:10.190: INFO: stdout: "Name:         agnhost-primary-gb7sq\nNamespace:    kubectl-7254\nPriority:     0\nNode:         flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr/192.168.1.246\nStart Time:   Wed, 13 Oct 2021 12:25:05 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.25.0.97/32\n              cni.projectcalico.org/podIPs: 172.25.0.97/32\nStatus:       Running\nIP:           172.25.0.97\nIPs:\n  IP:           172.25.0.97\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://434025e6bc124d87da7c28077bef7e7c0e7bc35d6a720be766166cc5aba57c11\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 13 Oct 2021 12:25:08 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5wfh2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5wfh2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-7254/agnhost-primary-gb7sq to flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr\n  Normal  Pulled     3s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    3s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Oct 13 12:25:10.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7254 describe rc agnhost-primary'
Oct 13 12:25:10.334: INFO: stderr: ""
Oct 13 12:25:10.334: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7254\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: agnhost-primary-gb7sq\n"
Oct 13 12:25:10.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7254 describe service agnhost-primary'
Oct 13 12:25:10.468: INFO: stderr: ""
Oct 13 12:25:10.468: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7254\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.31.194\nIPs:               10.240.31.194\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.0.97:6379\nSession Affinity:  None\nEvents:            <none>\n"
Oct 13 12:25:10.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7254 describe node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf'
Oct 13 12:25:10.704: INFO: stderr: ""
Oct 13 12:25:10.704: INFO: stdout: "Name:               flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m1.small\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=fes\n                    failure-domain.beta.kubernetes.io/zone=fes1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf\n                    kubernetes.io/os=linux\n                    machine-controller/host-id=04c72622dc645ea9a46e771a8dea932b19f48718dac326f1eee553f9\n                    machine-controller/owned-by=e45b5cc7-d489-4c69-adb6-6bfcecfd6eec\n                    machine-controller/physical-host-id=04c72622dc645ea9a46e771a8dea932b19f48718dac326f1eee553f9\n                    node.kubernetes.io/instance-type=m1.small\n                    system/cluster=9xsx4x6wwl\n                    system/project=7rq7pnhncm\n                    topology.cinder.csi.openstack.org/zone=fes1\n                    topology.kubernetes.io/region=fes\n                    topology.kubernetes.io/zone=fes1\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.1.102\n                    cluster.k8s.io/machine: kube-system/flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf\n                    csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"cad2685a-a6de-4a43-a08d-885843fc4b73\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"46:4b:b9:39:41:38\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.1.102\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 12 Oct 2021 14:23:37 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 13 Oct 2021 12:25:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                    Message\n  ----                 ------  -----------------                 ------------------                ------                    -------\n  KernelDeadlock       False   Wed, 13 Oct 2021 12:23:02 +0000   Tue, 12 Oct 2021 14:25:14 +0000   KernelHasNoDeadlock       kernel has no deadlock\n  ReadonlyFilesystem   False   Wed, 13 Oct 2021 12:23:02 +0000   Tue, 12 Oct 2021 14:25:14 +0000   FilesystemIsNotReadOnly   Filesystem is not read-only\n  MetakubeNodeReady    True    Wed, 13 Oct 2021 12:23:02 +0000   Tue, 12 Oct 2021 14:25:26 +0000   MetakubeNodeUp            Server:    169.254.20.10\nAddress:               169.254.20.10#53\n\nName:                  kubernetes.default.svc.c\n  NetworkUnavailable   False   Tue, 12 Oct 2021 14:24:57 +0000   Tue, 12 Oct 2021 14:24:57 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 13 Oct 2021 12:21:33 +0000   Tue, 12 Oct 2021 14:23:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 13 Oct 2021 12:21:33 +0000   Tue, 12 Oct 2021 14:23:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 13 Oct 2021 12:21:33 +0000   Tue, 12 Oct 2021 14:23:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 13 Oct 2021 12:21:33 +0000   Tue, 12 Oct 2021 14:24:17 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.1.102\n  ExternalIP:  195.192.155.185\n  Hostname:    flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50633164Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8153152Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  44516040218\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7231552Ki\n  pods:               110\nSystem Info:\n  Machine ID:                     cad2685aa6de4a43a08d885843fc4b73\n  System UUID:                    cad2685a-a6de-4a43-a08d-885843fc4b73\n  Boot ID:                        71658490-8579-4b1e-b446-178215550a6d\n  Kernel Version:                 5.4.0-88-generic\n  OS Image:                       Ubuntu 20.04.3 LTS\n  Operating System:               linux\n  Architecture:                   amd64\n  Container Runtime Version:      docker://19.3.15\n  Kubelet Version:                v1.21.3\n  Kube-Proxy Version:             v1.21.3\nPodCIDR:                          172.25.1.0/24\nPodCIDRs:                         172.25.1.0/24\nProviderID:                       openstack:///cad2685a-a6de-4a43-a08d-885843fc4b73\nNon-terminated Pods:              (22 in total)\n  Namespace                       Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                       ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                     canal-b5jhz                                                        350m (21%)    200m (12%)  50Mi (0%)        256Mi (3%)     22h\n  kube-system                     cluster-autoscaler-74b76d7f55-9xh55                                100m (6%)     100m (6%)   300Mi (4%)       300Mi (4%)     27m\n  kube-system                     coredns-7df5db5d6-4b95z                                            50m (3%)      100m (6%)   32Mi (0%)        64Mi (0%)      27m\n  kube-system                     coredns-7df5db5d6-5sk8m                                            50m (3%)      100m (6%)   32Mi (0%)        64Mi (0%)      27m\n  kube-system                     csi-cinder-controllerplugin-0                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  kube-system                     csi-cinder-nodeplugin-2p26p                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         22h\n  kube-system                     dns-autoscaler-6cc9cd7f9f-rj6x7                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\n  kube-system                     kube-proxy-7p6hr                                                   75m (4%)      250m (15%)  50Mi (0%)        250Mi (3%)     22h\n  kube-system                     node-exporter-qjdpc                                                3m (0%)       300m (18%)  16Mi (0%)        50Mi (0%)      22h\n  kube-system                     node-local-dns-tsflg                                               50m (3%)      0 (0%)      20Mi (0%)        100Mi (1%)     22h\n  kube-system                     openvpn-client-58d7dddf79-9vcj9                                    30m (1%)      500m (31%)  30Mi (0%)        232Mi (3%)     27m\n  kube-system                     syseleven-node-problem-detector-rqc8m                              10m (0%)      20m (1%)    80Mi (1%)        80Mi (1%)      22h\n  kube-system                     user-ssh-keys-agent-mnrhx                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         22h\n  sonobuoy                        sonobuoy                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  sonobuoy                        sonobuoy-e2e-job-c24ca988246e4377                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  sonobuoy                        sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-qvznk            0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  syseleven-helm-operator         helm-operator-6b9895c4c5-fjxwj                                     50m (3%)      1 (62%)     64Mi (0%)        0 (0%)         27m\n  syseleven-helm-operator         syseleven-helm-exporter-66c559868f-pl4xv                           50m (3%)      250m (15%)  32Mi (0%)        250Mi (3%)     27m\n  syseleven-ingress               syseleven-ingress-nginx-ingress-controller-846d65cdcd-57pxk        100m (6%)     200m (12%)  128Mi (1%)       512Mi (7%)     21h\n  syseleven-ingress               syseleven-ingress-nginx-ingress-controller-846d65cdcd-m2s59        100m (6%)     200m (12%)  128Mi (1%)       512Mi (7%)     27m\n  syseleven-ingress               syseleven-ingress-nginx-ingress-defaultbackend-7b7489f769-xc4lv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         21h\n  syseleven-kubernetes-dashboard  kubernetes-dashboard-6b6956d96c-9r9cn                              100m (6%)     100m (6%)   100Mi (1%)       100Mi (1%)     27m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1118m (69%)   3320m (207%)\n  memory             1062Mi (15%)  2770Mi (39%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
Oct 13 12:25:10.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-7254 describe namespace kubectl-7254'
Oct 13 12:25:10.848: INFO: stderr: ""
Oct 13 12:25:10.848: INFO: stdout: "Name:         kubectl-7254\nLabels:       e2e-framework=kubectl\n              e2e-run=82f6921c-2ad0-4123-a70f-7a2a5853eec8\n              kubernetes.io/metadata.name=kubectl-7254\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:25:10.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7254" for this suite.

• [SLOW TEST:6.027 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1084
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":288,"skipped":4559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:25:10.884: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Oct 13 12:25:10.998: INFO: The status of Pod labelsupdate5bac6518-a9ca-4f7c-bc59-c51732af7c9c is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:25:13.008: INFO: The status of Pod labelsupdate5bac6518-a9ca-4f7c-bc59-c51732af7c9c is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:25:15.011: INFO: The status of Pod labelsupdate5bac6518-a9ca-4f7c-bc59-c51732af7c9c is Running (Ready = true)
Oct 13 12:25:15.642: INFO: Successfully updated pod "labelsupdate5bac6518-a9ca-4f7c-bc59-c51732af7c9c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:25:19.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1851" for this suite.

• [SLOW TEST:8.916 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":289,"skipped":4593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:25:19.802: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Oct 13 12:25:19.904: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 13 12:26:20.003: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:26:20.015: INFO: Starting informer...
STEP: Starting pod...
Oct 13 12:26:20.271: INFO: Pod is running on flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Oct 13 12:26:20.313: INFO: Pod wasn't evicted. Proceeding
Oct 13 12:26:20.313: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Oct 13 12:27:35.367: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:27:35.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7803" for this suite.

• [SLOW TEST:135.608 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":290,"skipped":4620,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:27:35.415: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 13 12:27:35.529: INFO: Waiting up to 5m0s for pod "pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c" in namespace "emptydir-3185" to be "Succeeded or Failed"
Oct 13 12:27:35.540: INFO: Pod "pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.80393ms
Oct 13 12:27:37.561: INFO: Pod "pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032109505s
Oct 13 12:27:39.577: INFO: Pod "pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048060454s
STEP: Saw pod success
Oct 13 12:27:39.577: INFO: Pod "pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c" satisfied condition "Succeeded or Failed"
Oct 13 12:27:39.590: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c container test-container: <nil>
STEP: delete the pod
Oct 13 12:27:39.657: INFO: Waiting for pod pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c to disappear
Oct 13 12:27:39.666: INFO: Pod pod-b546f0df-d2f8-4631-b1a2-c18d7a87326c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:27:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3185" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":291,"skipped":4628,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:27:39.695: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Oct 13 12:27:49.917: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W1013 12:27:49.917505      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1013 12:27:49.917543      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1013 12:27:49.917551      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 13 12:27:49.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7384" for this suite.

• [SLOW TEST:10.254 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":292,"skipped":4637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:27:49.957: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:27:50.083: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Oct 13 12:27:55.119: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct 13 12:27:55.120: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Oct 13 12:27:55.188: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4219  da208c48-7f83-4d5f-89bc-f9dacad7a736 414067 1 2021-10-13 12:27:55 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-10-13 12:27:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037ac888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Oct 13 12:27:55.197: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:27:55.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4219" for this suite.

• [SLOW TEST:5.283 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":293,"skipped":4659,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:27:55.241: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:27:55.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:27:57.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724875, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724875, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724875, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769724875, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:28:00.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:28:00.867: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8271-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:28:04.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1005" for this suite.
STEP: Destroying namespace "webhook-1005-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.407 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":294,"skipped":4666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:28:04.649: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-20fab087-a087-4697-af36-657c5691c09c
STEP: Creating a pod to test consume secrets
Oct 13 12:28:04.816: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d" in namespace "projected-6380" to be "Succeeded or Failed"
Oct 13 12:28:04.824: INFO: Pod "pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.27707ms
Oct 13 12:28:06.832: INFO: Pod "pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016656339s
Oct 13 12:28:08.853: INFO: Pod "pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037081094s
STEP: Saw pod success
Oct 13 12:28:08.853: INFO: Pod "pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d" satisfied condition "Succeeded or Failed"
Oct 13 12:28:08.864: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct 13 12:28:08.970: INFO: Waiting for pod pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d to disappear
Oct 13 12:28:08.980: INFO: Pod pod-projected-secrets-27984ba7-4e2a-4b69-bbd9-f8e999e63c3d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:28:08.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6380" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":295,"skipped":4729,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:28:09.021: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Oct 13 12:28:09.149: INFO: Waiting up to 5m0s for pod "test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23" in namespace "svcaccounts-1917" to be "Succeeded or Failed"
Oct 13 12:28:09.167: INFO: Pod "test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23": Phase="Pending", Reason="", readiness=false. Elapsed: 17.337292ms
Oct 13 12:28:11.183: INFO: Pod "test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033615378s
Oct 13 12:28:13.199: INFO: Pod "test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050035348s
STEP: Saw pod success
Oct 13 12:28:13.199: INFO: Pod "test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23" satisfied condition "Succeeded or Failed"
Oct 13 12:28:13.211: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:28:13.273: INFO: Waiting for pod test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23 to disappear
Oct 13 12:28:13.282: INFO: Pod test-pod-a3d67ecd-b2fa-4aa3-92cd-552373c4fb23 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:28:13.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1917" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":296,"skipped":4735,"failed":0}
SSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:28:13.318: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-3857/secret-test-9a1a864c-068a-4c1e-9a24-a889d92d4489
STEP: Creating a pod to test consume secrets
Oct 13 12:28:13.424: INFO: Waiting up to 5m0s for pod "pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599" in namespace "secrets-3857" to be "Succeeded or Failed"
Oct 13 12:28:13.436: INFO: Pod "pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599": Phase="Pending", Reason="", readiness=false. Elapsed: 11.393852ms
Oct 13 12:28:15.450: INFO: Pod "pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025967168s
Oct 13 12:28:17.464: INFO: Pod "pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039567572s
STEP: Saw pod success
Oct 13 12:28:17.464: INFO: Pod "pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599" satisfied condition "Succeeded or Failed"
Oct 13 12:28:17.475: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599 container env-test: <nil>
STEP: delete the pod
Oct 13 12:28:17.527: INFO: Waiting for pod pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599 to disappear
Oct 13 12:28:17.542: INFO: Pod pod-configmaps-5fd5012c-bc63-49f6-bbf0-4ee895c37599 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:28:17.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3857" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":297,"skipped":4738,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:28:17.569: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7329.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7329.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7329.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7329.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7329.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7329.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 12:28:39.970: INFO: DNS probes using dns-7329/dns-test-9a9e17d7-2179-4bd5-90c7-a55417147948 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:28:40.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7329" for this suite.

• [SLOW TEST:22.510 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":298,"skipped":4740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:28:40.080: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
W1013 12:28:40.155349      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:00.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5195" for this suite.

• [SLOW TEST:320.220 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":299,"skipped":4794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:00.308: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-3a44ef1b-48f5-484f-a9a2-dc24017bb328
STEP: Creating a pod to test consume secrets
Oct 13 12:34:00.430: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d" in namespace "projected-6765" to be "Succeeded or Failed"
Oct 13 12:34:00.443: INFO: Pod "pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.927389ms
Oct 13 12:34:02.461: INFO: Pod "pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030962907s
Oct 13 12:34:04.485: INFO: Pod "pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055582359s
STEP: Saw pod success
Oct 13 12:34:04.485: INFO: Pod "pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d" satisfied condition "Succeeded or Failed"
Oct 13 12:34:04.497: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d container secret-volume-test: <nil>
STEP: delete the pod
Oct 13 12:34:04.605: INFO: Waiting for pod pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d to disappear
Oct 13 12:34:04.619: INFO: Pod pod-projected-secrets-4f498781-0157-40a2-8170-72bbc678382d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:04.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6765" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":300,"skipped":4823,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:04.654: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:04.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1864" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":301,"skipped":4836,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:04.822: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Oct 13 12:34:04.929: INFO: The status of Pod pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:34:06.942: INFO: The status of Pod pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:34:08.942: INFO: The status of Pod pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:34:10.951: INFO: The status of Pod pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct 13 12:34:11.513: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006"
Oct 13 12:34:11.514: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006" in namespace "pods-5966" to be "terminated due to deadline exceeded"
Oct 13 12:34:11.531: INFO: Pod "pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006": Phase="Running", Reason="", readiness=true. Elapsed: 17.413694ms
Oct 13 12:34:13.546: INFO: Pod "pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.032145062s
Oct 13 12:34:13.546: INFO: Pod "pod-update-activedeadlineseconds-bc319800-1f27-4819-8fc2-4257a844c006" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:13.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5966" for this suite.

• [SLOW TEST:8.761 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":302,"skipped":4839,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:13.596: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Oct 13 12:34:13.713: INFO: Waiting up to 5m0s for pod "client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179" in namespace "containers-9467" to be "Succeeded or Failed"
Oct 13 12:34:13.726: INFO: Pod "client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179": Phase="Pending", Reason="", readiness=false. Elapsed: 12.756172ms
Oct 13 12:34:15.749: INFO: Pod "client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035614538s
Oct 13 12:34:17.768: INFO: Pod "client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054676419s
Oct 13 12:34:19.784: INFO: Pod "client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07087915s
STEP: Saw pod success
Oct 13 12:34:19.784: INFO: Pod "client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179" satisfied condition "Succeeded or Failed"
Oct 13 12:34:19.795: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf pod client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:34:19.891: INFO: Waiting for pod client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179 to disappear
Oct 13 12:34:19.908: INFO: Pod client-containers-1c55df81-c918-4dd0-bc20-f49f8b738179 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9467" for this suite.

• [SLOW TEST:6.347 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":303,"skipped":4854,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Oct 13 12:34:20.030: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct 13 12:34:20.058: INFO: Waiting for terminating namespaces to be deleted...
Oct 13 12:34:20.069: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf before test
Oct 13 12:34:20.105: INFO: canal-b5jhz from kube-system started at 2021-10-12 14:23:37 +0000 UTC (2 container statuses recorded)
Oct 13 12:34:20.106: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 12:34:20.106: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 12:34:20.106: INFO: cluster-autoscaler-74b76d7f55-9xh55 from kube-system started at 2021-10-13 11:57:48 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.106: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Oct 13 12:34:20.106: INFO: coredns-7df5db5d6-4b95z from kube-system started at 2021-10-13 11:57:48 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.107: INFO: 	Container coredns ready: true, restart count 0
Oct 13 12:34:20.107: INFO: coredns-7df5db5d6-5sk8m from kube-system started at 2021-10-13 11:57:48 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.107: INFO: 	Container coredns ready: true, restart count 0
Oct 13 12:34:20.107: INFO: csi-cinder-controllerplugin-0 from kube-system started at 2021-10-13 11:58:07 +0000 UTC (6 container statuses recorded)
Oct 13 12:34:20.107: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 12:34:20.107: INFO: 	Container csi-attacher ready: true, restart count 0
Oct 13 12:34:20.107: INFO: 	Container csi-provisioner ready: true, restart count 0
Oct 13 12:34:20.107: INFO: 	Container csi-resizer ready: true, restart count 0
Oct 13 12:34:20.107: INFO: 	Container csi-snapshotter ready: true, restart count 0
Oct 13 12:34:20.108: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 12:34:20.108: INFO: csi-cinder-nodeplugin-2p26p from kube-system started at 2021-10-12 14:23:37 +0000 UTC (3 container statuses recorded)
Oct 13 12:34:20.108: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 12:34:20.108: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 12:34:20.108: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 12:34:20.108: INFO: dns-autoscaler-6cc9cd7f9f-rj6x7 from kube-system started at 2021-10-13 11:57:48 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.108: INFO: 	Container autoscaler ready: true, restart count 0
Oct 13 12:34:20.108: INFO: kube-proxy-7p6hr from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.108: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 12:34:20.108: INFO: node-exporter-qjdpc from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.109: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 12:34:20.109: INFO: node-local-dns-tsflg from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.109: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 12:34:20.109: INFO: openvpn-client-58d7dddf79-9vcj9 from kube-system started at 2021-10-13 11:57:48 +0000 UTC (2 container statuses recorded)
Oct 13 12:34:20.109: INFO: 	Container dnat-controller ready: true, restart count 0
Oct 13 12:34:20.109: INFO: 	Container openvpn-client ready: true, restart count 0
Oct 13 12:34:20.109: INFO: syseleven-node-problem-detector-rqc8m from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.109: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 12:34:20.109: INFO: user-ssh-keys-agent-mnrhx from kube-system started at 2021-10-12 14:23:37 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.109: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 12:34:20.110: INFO: sonobuoy from sonobuoy started at 2021-10-13 10:42:23 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.110: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct 13 12:34:20.110: INFO: sonobuoy-e2e-job-c24ca988246e4377 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 12:34:20.110: INFO: 	Container e2e ready: true, restart count 0
Oct 13 12:34:20.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 12:34:20.110: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-qvznk from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 12:34:20.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 12:34:20.110: INFO: 	Container systemd-logs ready: true, restart count 0
Oct 13 12:34:20.110: INFO: helm-operator-6b9895c4c5-fjxwj from syseleven-helm-operator started at 2021-10-13 11:57:48 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.110: INFO: 	Container helm-operator ready: true, restart count 0
Oct 13 12:34:20.111: INFO: syseleven-helm-exporter-66c559868f-pl4xv from syseleven-helm-operator started at 2021-10-13 11:57:48 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.111: INFO: 	Container helm-exporter ready: true, restart count 0
Oct 13 12:34:20.111: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-57pxk from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.111: INFO: 	Container controller ready: true, restart count 0
Oct 13 12:34:20.111: INFO: syseleven-ingress-nginx-ingress-controller-846d65cdcd-m2s59 from syseleven-ingress started at 2021-10-13 11:57:48 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.111: INFO: 	Container controller ready: true, restart count 0
Oct 13 12:34:20.111: INFO: syseleven-ingress-nginx-ingress-defaultbackend-7b7489f769-xc4lv from syseleven-ingress started at 2021-10-12 14:30:46 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.111: INFO: 	Container ingress-nginx-default-backend ready: true, restart count 0
Oct 13 12:34:20.111: INFO: kubernetes-dashboard-6b6956d96c-9r9cn from syseleven-kubernetes-dashboard started at 2021-10-13 11:57:48 +0000 UTC (2 container statuses recorded)
Oct 13 12:34:20.111: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Oct 13 12:34:20.112: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Oct 13 12:34:20.112: INFO: 
Logging pods the apiserver thinks is on node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr before test
Oct 13 12:34:20.142: INFO: canal-zmkkq from kube-system started at 2021-10-12 14:17:59 +0000 UTC (2 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container calico-node ready: true, restart count 0
Oct 13 12:34:20.143: INFO: 	Container kube-flannel ready: true, restart count 0
Oct 13 12:34:20.143: INFO: csi-cinder-nodeplugin-2wxbc from kube-system started at 2021-10-12 14:18:00 +0000 UTC (3 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Oct 13 12:34:20.143: INFO: 	Container liveness-probe ready: true, restart count 0
Oct 13 12:34:20.143: INFO: 	Container node-driver-registrar ready: true, restart count 0
Oct 13 12:34:20.143: INFO: kube-proxy-ft8xt from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container kube-proxy ready: true, restart count 0
Oct 13 12:34:20.143: INFO: node-exporter-jxsls from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container node-exporter ready: true, restart count 0
Oct 13 12:34:20.143: INFO: node-local-dns-ct8kv from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container node-cache ready: true, restart count 0
Oct 13 12:34:20.143: INFO: syseleven-node-problem-detector-2dvkn from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container node-problem-detector ready: true, restart count 0
Oct 13 12:34:20.143: INFO: user-ssh-keys-agent-2kfhw from kube-system started at 2021-10-12 14:18:00 +0000 UTC (1 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Oct 13 12:34:20.143: INFO: sonobuoy-systemd-logs-daemon-set-b14197eee0304bfb-fpst9 from sonobuoy started at 2021-10-13 10:42:29 +0000 UTC (2 container statuses recorded)
Oct 13 12:34:20.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct 13 12:34:20.143: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16ad972d340e5f80], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:21.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4145" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":304,"skipped":4872,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct 13 12:34:21.430: INFO: Number of nodes with available pods: 0
Oct 13 12:34:21.430: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:22.460: INFO: Number of nodes with available pods: 0
Oct 13 12:34:22.460: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:23.459: INFO: Number of nodes with available pods: 0
Oct 13 12:34:23.459: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:24.467: INFO: Number of nodes with available pods: 0
Oct 13 12:34:24.467: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:25.456: INFO: Number of nodes with available pods: 2
Oct 13 12:34:25.456: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Oct 13 12:34:25.534: INFO: Number of nodes with available pods: 1
Oct 13 12:34:25.534: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:26.566: INFO: Number of nodes with available pods: 1
Oct 13 12:34:26.566: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:27.570: INFO: Number of nodes with available pods: 1
Oct 13 12:34:27.571: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:28.569: INFO: Number of nodes with available pods: 1
Oct 13 12:34:28.569: INFO: Node flamboyant-colden-worker-54ds2p-6fc95f445d-6vnxf is running more than one daemon pod
Oct 13 12:34:29.572: INFO: Number of nodes with available pods: 2
Oct 13 12:34:29.572: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4662, will wait for the garbage collector to delete the pods
Oct 13 12:34:29.682: INFO: Deleting DaemonSet.extensions daemon-set took: 19.646659ms
Oct 13 12:34:29.782: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.615051ms
Oct 13 12:34:38.901: INFO: Number of nodes with available pods: 0
Oct 13 12:34:38.901: INFO: Number of running nodes: 0, number of available pods: 0
Oct 13 12:34:38.911: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"416293"},"items":null}

Oct 13 12:34:38.920: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"416293"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4662" for this suite.

• [SLOW TEST:17.741 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":305,"skipped":4872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:39.002: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-23c13db6-a3dd-41eb-bfd2-5d563f52f946
STEP: Creating a pod to test consume configMaps
Oct 13 12:34:39.136: INFO: Waiting up to 5m0s for pod "pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35" in namespace "configmap-4697" to be "Succeeded or Failed"
Oct 13 12:34:39.148: INFO: Pod "pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35": Phase="Pending", Reason="", readiness=false. Elapsed: 11.844735ms
Oct 13 12:34:41.169: INFO: Pod "pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032451904s
Oct 13 12:34:43.186: INFO: Pod "pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05005694s
STEP: Saw pod success
Oct 13 12:34:43.187: INFO: Pod "pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35" satisfied condition "Succeeded or Failed"
Oct 13 12:34:43.197: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:34:43.302: INFO: Waiting for pod pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35 to disappear
Oct 13 12:34:43.311: INFO: Pod pod-configmaps-06db1da8-c242-4205-8f66-303c05684c35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:43.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4697" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":306,"skipped":4899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:43.344: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:54.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3148" for this suite.

• [SLOW TEST:11.431 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":307,"skipped":4922,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:54.779: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:34:54.912: INFO: Creating deployment "test-recreate-deployment"
Oct 13 12:34:54.927: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Oct 13 12:34:54.953: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Oct 13 12:34:56.985: INFO: Waiting deployment "test-recreate-deployment" to complete
Oct 13 12:34:56.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725294, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725294, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725294, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725294, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct 13 12:34:59.014: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Oct 13 12:34:59.052: INFO: Updating deployment test-recreate-deployment
Oct 13 12:34:59.052: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Oct 13 12:34:59.164: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7719  223960e7-62b4-483c-a222-e9c052d1ae5f 416518 2 2021-10-13 12:34:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-10-13 12:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-10-13 12:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d11688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-10-13 12:34:59 +0000 UTC,LastTransitionTime:2021-10-13 12:34:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-10-13 12:34:59 +0000 UTC,LastTransitionTime:2021-10-13 12:34:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Oct 13 12:34:59.170: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-7719  77735d9f-96b0-4096-9d7a-a449485acdee 416516 1 2021-10-13 12:34:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 223960e7-62b4-483c-a222-e9c052d1ae5f 0xc006d11b20 0xc006d11b21}] []  [{kube-controller-manager Update apps/v1 2021-10-13 12:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"223960e7-62b4-483c-a222-e9c052d1ae5f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d11b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 13 12:34:59.170: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Oct 13 12:34:59.170: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-7719  22429f4e-7bf4-4a95-8298-8fa4b560c054 416506 2 2021-10-13 12:34:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 223960e7-62b4-483c-a222-e9c052d1ae5f 0xc006d11a27 0xc006d11a28}] []  [{kube-controller-manager Update apps/v1 2021-10-13 12:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"223960e7-62b4-483c-a222-e9c052d1ae5f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d11ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Oct 13 12:34:59.179: INFO: Pod "test-recreate-deployment-85d47dcb4-hx9qf" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-hx9qf test-recreate-deployment-85d47dcb4- deployment-7719  1cfb97bf-0460-4d18-bb91-627dd29516bd 416517 0 2021-10-13 12:34:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 77735d9f-96b0-4096-9d7a-a449485acdee 0xc0036add80 0xc0036add81}] []  [{kube-controller-manager Update v1 2021-10-13 12:34:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77735d9f-96b0-4096-9d7a-a449485acdee\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-10-13 12:34:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vg5h7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vg5h7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:34:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:34:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:34:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-10-13 12:34:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.1.246,PodIP:,StartTime:2021-10-13 12:34:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:34:59.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7719" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":308,"skipped":4934,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:34:59.207: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Oct 13 12:34:59.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 create -f -'
Oct 13 12:35:01.325: INFO: stderr: ""
Oct 13 12:35:01.325: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 13 12:35:01.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 12:35:01.449: INFO: stderr: ""
Oct 13 12:35:01.449: INFO: stdout: "update-demo-nautilus-9xkcg update-demo-nautilus-hrsd2 "
Oct 13 12:35:01.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-9xkcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 12:35:01.540: INFO: stderr: ""
Oct 13 12:35:01.540: INFO: stdout: ""
Oct 13 12:35:01.540: INFO: update-demo-nautilus-9xkcg is created but not running
Oct 13 12:35:06.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 12:35:06.656: INFO: stderr: ""
Oct 13 12:35:06.656: INFO: stdout: "update-demo-nautilus-9xkcg update-demo-nautilus-hrsd2 "
Oct 13 12:35:06.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-9xkcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 12:35:06.754: INFO: stderr: ""
Oct 13 12:35:06.754: INFO: stdout: "true"
Oct 13 12:35:06.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-9xkcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 13 12:35:06.860: INFO: stderr: ""
Oct 13 12:35:06.860: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 13 12:35:06.860: INFO: validating pod update-demo-nautilus-9xkcg
Oct 13 12:35:06.927: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 13 12:35:06.927: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 13 12:35:06.927: INFO: update-demo-nautilus-9xkcg is verified up and running
Oct 13 12:35:06.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-hrsd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 12:35:07.028: INFO: stderr: ""
Oct 13 12:35:07.028: INFO: stdout: "true"
Oct 13 12:35:07.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-hrsd2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 13 12:35:07.134: INFO: stderr: ""
Oct 13 12:35:07.134: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 13 12:35:07.134: INFO: validating pod update-demo-nautilus-hrsd2
Oct 13 12:35:07.245: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 13 12:35:07.245: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 13 12:35:07.245: INFO: update-demo-nautilus-hrsd2 is verified up and running
STEP: scaling down the replication controller
Oct 13 12:35:07.249: INFO: scanned /root for discovery docs: <nil>
Oct 13 12:35:07.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Oct 13 12:35:08.448: INFO: stderr: ""
Oct 13 12:35:08.448: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 13 12:35:08.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 12:35:08.574: INFO: stderr: ""
Oct 13 12:35:08.574: INFO: stdout: "update-demo-nautilus-9xkcg update-demo-nautilus-hrsd2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 13 12:35:13.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 12:35:13.708: INFO: stderr: ""
Oct 13 12:35:13.708: INFO: stdout: "update-demo-nautilus-9xkcg update-demo-nautilus-hrsd2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct 13 12:35:18.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 12:35:18.840: INFO: stderr: ""
Oct 13 12:35:18.840: INFO: stdout: "update-demo-nautilus-hrsd2 "
Oct 13 12:35:18.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-hrsd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 12:35:18.930: INFO: stderr: ""
Oct 13 12:35:18.930: INFO: stdout: "true"
Oct 13 12:35:18.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-hrsd2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 13 12:35:19.036: INFO: stderr: ""
Oct 13 12:35:19.037: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 13 12:35:19.037: INFO: validating pod update-demo-nautilus-hrsd2
Oct 13 12:35:19.098: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 13 12:35:19.098: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 13 12:35:19.098: INFO: update-demo-nautilus-hrsd2 is verified up and running
STEP: scaling up the replication controller
Oct 13 12:35:19.104: INFO: scanned /root for discovery docs: <nil>
Oct 13 12:35:19.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Oct 13 12:35:20.272: INFO: stderr: ""
Oct 13 12:35:20.272: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct 13 12:35:20.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 12:35:20.404: INFO: stderr: ""
Oct 13 12:35:20.404: INFO: stdout: "update-demo-nautilus-7zht6 update-demo-nautilus-hrsd2 "
Oct 13 12:35:20.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-7zht6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 12:35:20.503: INFO: stderr: ""
Oct 13 12:35:20.503: INFO: stdout: ""
Oct 13 12:35:20.503: INFO: update-demo-nautilus-7zht6 is created but not running
Oct 13 12:35:25.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Oct 13 12:35:25.646: INFO: stderr: ""
Oct 13 12:35:25.646: INFO: stdout: "update-demo-nautilus-7zht6 update-demo-nautilus-hrsd2 "
Oct 13 12:35:25.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-7zht6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 12:35:25.753: INFO: stderr: ""
Oct 13 12:35:25.753: INFO: stdout: "true"
Oct 13 12:35:25.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-7zht6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 13 12:35:25.853: INFO: stderr: ""
Oct 13 12:35:25.853: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 13 12:35:25.853: INFO: validating pod update-demo-nautilus-7zht6
Oct 13 12:35:25.971: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 13 12:35:25.972: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 13 12:35:25.972: INFO: update-demo-nautilus-7zht6 is verified up and running
Oct 13 12:35:25.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-hrsd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Oct 13 12:35:26.075: INFO: stderr: ""
Oct 13 12:35:26.075: INFO: stdout: "true"
Oct 13 12:35:26.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods update-demo-nautilus-hrsd2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Oct 13 12:35:26.198: INFO: stderr: ""
Oct 13 12:35:26.198: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Oct 13 12:35:26.198: INFO: validating pod update-demo-nautilus-hrsd2
Oct 13 12:35:26.220: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct 13 12:35:26.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct 13 12:35:26.220: INFO: update-demo-nautilus-hrsd2 is verified up and running
STEP: using delete to clean up resources
Oct 13 12:35:26.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 delete --grace-period=0 --force -f -'
Oct 13 12:35:26.346: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct 13 12:35:26.346: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct 13 12:35:26.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get rc,svc -l name=update-demo --no-headers'
Oct 13 12:35:26.485: INFO: stderr: "No resources found in kubectl-9188 namespace.\n"
Oct 13 12:35:26.485: INFO: stdout: ""
Oct 13 12:35:26.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-9188 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct 13 12:35:26.606: INFO: stderr: ""
Oct 13 12:35:26.606: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:35:26.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9188" for this suite.

• [SLOW TEST:27.434 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":309,"skipped":4936,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:35:26.641: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Oct 13 12:35:26.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-1207 create -f -'
Oct 13 12:35:27.173: INFO: stderr: ""
Oct 13 12:35:27.173: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Oct 13 12:35:27.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-1207 diff -f -'
Oct 13 12:35:27.865: INFO: rc: 1
Oct 13 12:35:27.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=kubectl-1207 delete -f -'
Oct 13 12:35:27.967: INFO: stderr: ""
Oct 13 12:35:27.967: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:35:27.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1207" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":310,"skipped":4937,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:35:28.007: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1013 12:35:29.268217      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W1013 12:35:29.268579      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W1013 12:35:29.268601      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Oct 13 12:35:29.268: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:35:29.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1375" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":311,"skipped":4941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:35:29.309: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:35:29.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:35:31.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725329, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725329, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725329, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725329, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:35:34.849: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:35:45.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6199" for this suite.
STEP: Destroying namespace "webhook-6199-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.535 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":312,"skipped":4976,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:35:45.846: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct 13 12:35:45.942: INFO: Waiting up to 5m0s for pod "pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15" in namespace "emptydir-4800" to be "Succeeded or Failed"
Oct 13 12:35:45.948: INFO: Pod "pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15": Phase="Pending", Reason="", readiness=false. Elapsed: 6.306889ms
Oct 13 12:35:47.983: INFO: Pod "pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040948723s
Oct 13 12:35:50.009: INFO: Pod "pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066831371s
STEP: Saw pod success
Oct 13 12:35:50.009: INFO: Pod "pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15" satisfied condition "Succeeded or Failed"
Oct 13 12:35:50.020: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15 container test-container: <nil>
STEP: delete the pod
Oct 13 12:35:50.131: INFO: Waiting for pod pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15 to disappear
Oct 13 12:35:50.144: INFO: Pod pod-09bc7b0a-b051-4366-ae9f-e0db8e49eb15 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:35:50.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4800" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":313,"skipped":4984,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:35:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Oct 13 12:35:50.307: INFO: Waiting up to 5m0s for pod "client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b" in namespace "containers-1257" to be "Succeeded or Failed"
Oct 13 12:35:50.318: INFO: Pod "client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.417195ms
Oct 13 12:35:52.335: INFO: Pod "client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027113543s
Oct 13 12:35:54.358: INFO: Pod "client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050129225s
Oct 13 12:35:56.377: INFO: Pod "client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069746536s
STEP: Saw pod success
Oct 13 12:35:56.378: INFO: Pod "client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b" satisfied condition "Succeeded or Failed"
Oct 13 12:35:56.389: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:35:56.486: INFO: Waiting for pod client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b to disappear
Oct 13 12:35:56.497: INFO: Pod client-containers-7dd96fdd-1133-452e-99b4-7ee6ae7a0d6b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:35:56.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1257" for this suite.

• [SLOW TEST:6.347 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":314,"skipped":4999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:35:56.532: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:35:56.613: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Creating first CR 
Oct 13 12:35:59.259: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-13T12:35:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-13T12:35:59Z]] name:name1 resourceVersion:417150 uid:b32cadc1-04bc-43a0-9196-31908b16ae12] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Oct 13 12:36:09.285: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-13T12:36:09Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-13T12:36:09Z]] name:name2 resourceVersion:417206 uid:4715c971-ce5f-4cc6-9aad-1378ae0f5b14] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Oct 13 12:36:19.308: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-13T12:35:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-13T12:36:19Z]] name:name1 resourceVersion:417249 uid:b32cadc1-04bc-43a0-9196-31908b16ae12] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Oct 13 12:36:29.329: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-13T12:36:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-13T12:36:29Z]] name:name2 resourceVersion:417290 uid:4715c971-ce5f-4cc6-9aad-1378ae0f5b14] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Oct 13 12:36:39.356: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-13T12:35:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-13T12:36:19Z]] name:name1 resourceVersion:417331 uid:b32cadc1-04bc-43a0-9196-31908b16ae12] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Oct 13 12:36:49.382: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-10-13T12:36:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-10-13T12:36:29Z]] name:name2 resourceVersion:417371 uid:4715c971-ce5f-4cc6-9aad-1378ae0f5b14] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:36:59.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7888" for this suite.

• [SLOW TEST:63.447 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":315,"skipped":5024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:36:59.991: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Oct 13 12:37:00.574: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Oct 13 12:37:02.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725420, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725420, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725420, loc:(*time.Location)(0x9ddf5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63769725420, loc:(*time.Location)(0x9ddf5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Oct 13 12:37:05.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:37:05.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9882" for this suite.
STEP: Destroying namespace "webhook-9882-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.075 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":316,"skipped":5062,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:37:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9803
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9803
I1013 12:37:06.268882      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9803, replica count: 2
I1013 12:37:09.326398      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1013 12:37:12.329004      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct 13 12:37:12.329: INFO: Creating new exec pod
Oct 13 12:37:17.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-9803 exec execpodvdph5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Oct 13 12:37:17.935: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Oct 13 12:37:17.935: INFO: stdout: "externalname-service-pd6vf"
Oct 13 12:37:17.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=services-9803 exec execpodvdph5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.210 80'
Oct 13 12:37:18.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.210 80\nConnection to 10.240.19.210 80 port [tcp/http] succeeded!\n"
Oct 13 12:37:18.426: INFO: stdout: "externalname-service-682kf"
Oct 13 12:37:18.426: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:37:18.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9803" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:12.433 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":317,"skipped":5079,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:37:18.500: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:37:18.577: INFO: Creating ReplicaSet my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c
Oct 13 12:37:18.600: INFO: Pod name my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c: Found 0 pods out of 1
Oct 13 12:37:23.628: INFO: Pod name my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c: Found 1 pods out of 1
Oct 13 12:37:23.628: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c" is running
Oct 13 12:37:23.638: INFO: Pod "my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c-sfdkw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 12:37:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 12:37:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 12:37:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-10-13 12:37:18 +0000 UTC Reason: Message:}])
Oct 13 12:37:23.639: INFO: Trying to dial the pod
Oct 13 12:37:28.743: INFO: Controller my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c: Got expected result from replica 1 [my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c-sfdkw]: "my-hostname-basic-0b4a9527-b212-4eed-a449-c9576a36a69c-sfdkw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:37:28.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8091" for this suite.

• [SLOW TEST:10.282 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":318,"skipped":5086,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:37:28.783: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Oct 13 12:37:28.894: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:37:47.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1119" for this suite.

• [SLOW TEST:18.356 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":319,"skipped":5096,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:37:47.151: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:37:47.267: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4b765f7b-e377-45e4-8ce9-14ff3bf0e2cc" in namespace "security-context-test-8367" to be "Succeeded or Failed"
Oct 13 12:37:47.287: INFO: Pod "alpine-nnp-false-4b765f7b-e377-45e4-8ce9-14ff3bf0e2cc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.159263ms
Oct 13 12:37:49.303: INFO: Pod "alpine-nnp-false-4b765f7b-e377-45e4-8ce9-14ff3bf0e2cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035599804s
Oct 13 12:37:51.323: INFO: Pod "alpine-nnp-false-4b765f7b-e377-45e4-8ce9-14ff3bf0e2cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055802231s
Oct 13 12:37:53.342: INFO: Pod "alpine-nnp-false-4b765f7b-e377-45e4-8ce9-14ff3bf0e2cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074284416s
Oct 13 12:37:53.342: INFO: Pod "alpine-nnp-false-4b765f7b-e377-45e4-8ce9-14ff3bf0e2cc" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:37:53.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8367" for this suite.

• [SLOW TEST:6.294 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":320,"skipped":5109,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:37:53.445: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Oct 13 12:37:53.579: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:37:55.600: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:37:57.604: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Oct 13 12:37:57.649: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:37:59.671: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:38:01.659: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Oct 13 12:38:01.705: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 13 12:38:01.718: INFO: Pod pod-with-prestop-http-hook still exists
Oct 13 12:38:03.719: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 13 12:38:03.738: INFO: Pod pod-with-prestop-http-hook still exists
Oct 13 12:38:05.719: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct 13 12:38:05.742: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:05.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5287" for this suite.

• [SLOW TEST:12.402 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":321,"skipped":5115,"failed":0}
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:05.849: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:06.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7504" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":322,"skipped":5115,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:06.091: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Oct 13 12:38:06.182: INFO: created test-podtemplate-1
Oct 13 12:38:06.193: INFO: created test-podtemplate-2
Oct 13 12:38:06.204: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Oct 13 12:38:06.211: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Oct 13 12:38:06.251: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:06.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5123" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":323,"skipped":5127,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:06.295: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-549c417c-7b7f-435e-ba33-1a0574da5c82
STEP: Creating a pod to test consume configMaps
Oct 13 12:38:06.410: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd" in namespace "projected-9739" to be "Succeeded or Failed"
Oct 13 12:38:06.424: INFO: Pod "pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.763576ms
Oct 13 12:38:08.447: INFO: Pod "pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035359272s
Oct 13 12:38:10.468: INFO: Pod "pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057007202s
STEP: Saw pod success
Oct 13 12:38:10.468: INFO: Pod "pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd" satisfied condition "Succeeded or Failed"
Oct 13 12:38:10.478: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:38:10.537: INFO: Waiting for pod pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd to disappear
Oct 13 12:38:10.547: INFO: Pod pod-projected-configmaps-c7b9ca82-3e9f-4035-8dc4-8054d766a1dd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:10.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9739" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":324,"skipped":5145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:10.593: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Oct 13 12:38:10.722: INFO: The status of Pod pod-hostip-edddcaa4-d6af-42e8-bbae-5986c06199e7 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:38:12.749: INFO: The status of Pod pod-hostip-edddcaa4-d6af-42e8-bbae-5986c06199e7 is Pending, waiting for it to be Running (with Ready = true)
Oct 13 12:38:14.745: INFO: The status of Pod pod-hostip-edddcaa4-d6af-42e8-bbae-5986c06199e7 is Running (Ready = true)
Oct 13 12:38:14.769: INFO: Pod pod-hostip-edddcaa4-d6af-42e8-bbae-5986c06199e7 has hostIP: 192.168.1.246
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:14.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3187" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":325,"skipped":5190,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:14.814: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:38:14.900: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Oct 13 12:38:17.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 create -f -'
Oct 13 12:38:20.311: INFO: stderr: ""
Oct 13 12:38:20.311: INFO: stdout: "e2e-test-crd-publish-openapi-9008-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 13 12:38:20.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 delete e2e-test-crd-publish-openapi-9008-crds test-foo'
Oct 13 12:38:20.438: INFO: stderr: ""
Oct 13 12:38:20.438: INFO: stdout: "e2e-test-crd-publish-openapi-9008-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Oct 13 12:38:20.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 apply -f -'
Oct 13 12:38:20.797: INFO: stderr: ""
Oct 13 12:38:20.798: INFO: stdout: "e2e-test-crd-publish-openapi-9008-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Oct 13 12:38:20.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 delete e2e-test-crd-publish-openapi-9008-crds test-foo'
Oct 13 12:38:20.939: INFO: stderr: ""
Oct 13 12:38:20.939: INFO: stdout: "e2e-test-crd-publish-openapi-9008-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Oct 13 12:38:20.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 create -f -'
Oct 13 12:38:21.299: INFO: rc: 1
Oct 13 12:38:21.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 apply -f -'
Oct 13 12:38:21.638: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Oct 13 12:38:21.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 create -f -'
Oct 13 12:38:22.045: INFO: rc: 1
Oct 13 12:38:22.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 --namespace=crd-publish-openapi-6089 apply -f -'
Oct 13 12:38:22.344: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Oct 13 12:38:22.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 explain e2e-test-crd-publish-openapi-9008-crds'
Oct 13 12:38:22.660: INFO: stderr: ""
Oct 13 12:38:22.661: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9008-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Oct 13 12:38:22.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 explain e2e-test-crd-publish-openapi-9008-crds.metadata'
Oct 13 12:38:22.936: INFO: stderr: ""
Oct 13 12:38:22.936: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9008-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Oct 13 12:38:22.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 explain e2e-test-crd-publish-openapi-9008-crds.spec'
Oct 13 12:38:23.309: INFO: stderr: ""
Oct 13 12:38:23.309: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9008-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Oct 13 12:38:23.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 explain e2e-test-crd-publish-openapi-9008-crds.spec.bars'
Oct 13 12:38:23.802: INFO: stderr: ""
Oct 13 12:38:23.802: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9008-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Oct 13 12:38:23.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371874133 --namespace=crd-publish-openapi-6089 explain e2e-test-crd-publish-openapi-9008-crds.spec.bars2'
Oct 13 12:38:24.124: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:27.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6089" for this suite.

• [SLOW TEST:12.337 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":326,"skipped":5193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:27.157: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-4488c409-8cd9-4075-af78-820206701ad6
STEP: Creating a pod to test consume configMaps
Oct 13 12:38:27.294: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a" in namespace "projected-3930" to be "Succeeded or Failed"
Oct 13 12:38:27.304: INFO: Pod "pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.499486ms
Oct 13 12:38:29.322: INFO: Pod "pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028269004s
Oct 13 12:38:31.344: INFO: Pod "pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049495923s
STEP: Saw pod success
Oct 13 12:38:31.344: INFO: Pod "pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a" satisfied condition "Succeeded or Failed"
Oct 13 12:38:31.355: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct 13 12:38:31.459: INFO: Waiting for pod pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a to disappear
Oct 13 12:38:31.470: INFO: Pod pod-projected-configmaps-f1704150-e48a-4d00-8d5f-dfdb8af8930a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:31.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3930" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":327,"skipped":5217,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:31.511: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Oct 13 12:38:31.631: INFO: Waiting up to 5m0s for pod "downward-api-7f676c0f-a126-4500-a80b-df62419473c1" in namespace "downward-api-5392" to be "Succeeded or Failed"
Oct 13 12:38:31.647: INFO: Pod "downward-api-7f676c0f-a126-4500-a80b-df62419473c1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.613292ms
Oct 13 12:38:33.667: INFO: Pod "downward-api-7f676c0f-a126-4500-a80b-df62419473c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034832706s
Oct 13 12:38:35.681: INFO: Pod "downward-api-7f676c0f-a126-4500-a80b-df62419473c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049181892s
STEP: Saw pod success
Oct 13 12:38:35.681: INFO: Pod "downward-api-7f676c0f-a126-4500-a80b-df62419473c1" satisfied condition "Succeeded or Failed"
Oct 13 12:38:35.693: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod downward-api-7f676c0f-a126-4500-a80b-df62419473c1 container dapi-container: <nil>
STEP: delete the pod
Oct 13 12:38:35.759: INFO: Waiting for pod downward-api-7f676c0f-a126-4500-a80b-df62419473c1 to disappear
Oct 13 12:38:35.766: INFO: Pod downward-api-7f676c0f-a126-4500-a80b-df62419473c1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:35.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5392" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":328,"skipped":5230,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:35.795: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Oct 13 12:38:35.900: INFO: Waiting up to 5m0s for pod "pod-008d9dd0-357f-4e9b-834f-dab87999e02d" in namespace "emptydir-1669" to be "Succeeded or Failed"
Oct 13 12:38:35.912: INFO: Pod "pod-008d9dd0-357f-4e9b-834f-dab87999e02d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.756296ms
Oct 13 12:38:37.925: INFO: Pod "pod-008d9dd0-357f-4e9b-834f-dab87999e02d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023539363s
Oct 13 12:38:39.940: INFO: Pod "pod-008d9dd0-357f-4e9b-834f-dab87999e02d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03841693s
STEP: Saw pod success
Oct 13 12:38:39.940: INFO: Pod "pod-008d9dd0-357f-4e9b-834f-dab87999e02d" satisfied condition "Succeeded or Failed"
Oct 13 12:38:39.948: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-008d9dd0-357f-4e9b-834f-dab87999e02d container test-container: <nil>
STEP: delete the pod
Oct 13 12:38:40.049: INFO: Waiting for pod pod-008d9dd0-357f-4e9b-834f-dab87999e02d to disappear
Oct 13 12:38:40.056: INFO: Pod pod-008d9dd0-357f-4e9b-834f-dab87999e02d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:40.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1669" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":329,"skipped":5245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:40.083: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-a26f670e-8667-430f-8a8b-5f2e924f3934
STEP: Creating a pod to test consume configMaps
Oct 13 12:38:40.193: INFO: Waiting up to 5m0s for pod "pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848" in namespace "configmap-8433" to be "Succeeded or Failed"
Oct 13 12:38:40.205: INFO: Pod "pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848": Phase="Pending", Reason="", readiness=false. Elapsed: 11.370555ms
Oct 13 12:38:42.225: INFO: Pod "pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031992089s
Oct 13 12:38:44.249: INFO: Pod "pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055925654s
STEP: Saw pod success
Oct 13 12:38:44.249: INFO: Pod "pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848" satisfied condition "Succeeded or Failed"
Oct 13 12:38:44.261: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848 container agnhost-container: <nil>
STEP: delete the pod
Oct 13 12:38:44.342: INFO: Waiting for pod pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848 to disappear
Oct 13 12:38:44.354: INFO: Pod pod-configmaps-b62f56c0-75a6-4446-8963-d0d1c33db848 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:44.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8433" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":330,"skipped":5269,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:44.390: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:38:48.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2445" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":331,"skipped":5282,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:38:48.686: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Oct 13 12:38:48.822: INFO: Waiting up to 1m0s for all nodes to be ready
Oct 13 12:39:48.927: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:39:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:39:49.087: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Oct 13 12:39:49.098: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:39:49.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1054" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:39:49.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2455" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.639 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":332,"skipped":5286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:39:49.325: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-ss5k
STEP: Creating a pod to test atomic-volume-subpath
Oct 13 12:39:49.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ss5k" in namespace "subpath-6175" to be "Succeeded or Failed"
Oct 13 12:39:49.458: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.477149ms
Oct 13 12:39:51.477: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027047744s
Oct 13 12:39:53.492: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 4.042596115s
Oct 13 12:39:55.511: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 6.062082608s
Oct 13 12:39:57.527: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 8.07736927s
Oct 13 12:39:59.549: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 10.099414392s
Oct 13 12:40:01.574: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 12.124483014s
Oct 13 12:40:03.590: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 14.14035205s
Oct 13 12:40:05.609: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 16.159768504s
Oct 13 12:40:07.621: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 18.172063176s
Oct 13 12:40:09.635: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 20.185377972s
Oct 13 12:40:11.649: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Running", Reason="", readiness=true. Elapsed: 22.200008697s
Oct 13 12:40:13.665: INFO: Pod "pod-subpath-test-configmap-ss5k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.215989653s
STEP: Saw pod success
Oct 13 12:40:13.666: INFO: Pod "pod-subpath-test-configmap-ss5k" satisfied condition "Succeeded or Failed"
Oct 13 12:40:13.678: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-subpath-test-configmap-ss5k container test-container-subpath-configmap-ss5k: <nil>
STEP: delete the pod
Oct 13 12:40:13.758: INFO: Waiting for pod pod-subpath-test-configmap-ss5k to disappear
Oct 13 12:40:13.768: INFO: Pod pod-subpath-test-configmap-ss5k no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ss5k
Oct 13 12:40:13.768: INFO: Deleting pod "pod-subpath-test-configmap-ss5k" in namespace "subpath-6175"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:40:13.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6175" for this suite.

• [SLOW TEST:24.485 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":333,"skipped":5331,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:40:13.811: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-zd6n
STEP: Creating a pod to test atomic-volume-subpath
Oct 13 12:40:13.955: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zd6n" in namespace "subpath-6364" to be "Succeeded or Failed"
Oct 13 12:40:13.966: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Pending", Reason="", readiness=false. Elapsed: 11.515066ms
Oct 13 12:40:15.987: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032017354s
Oct 13 12:40:18.004: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 4.049609734s
Oct 13 12:40:20.027: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 6.071889002s
Oct 13 12:40:22.044: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 8.089011285s
Oct 13 12:40:24.070: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 10.11540323s
Oct 13 12:40:26.086: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 12.131700035s
Oct 13 12:40:28.103: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 14.147806s
Oct 13 12:40:30.119: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 16.16389999s
Oct 13 12:40:32.137: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 18.18239953s
Oct 13 12:40:34.157: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 20.202018177s
Oct 13 12:40:36.174: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Running", Reason="", readiness=true. Elapsed: 22.219355084s
Oct 13 12:40:38.194: INFO: Pod "pod-subpath-test-projected-zd6n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.238910016s
STEP: Saw pod success
Oct 13 12:40:38.194: INFO: Pod "pod-subpath-test-projected-zd6n" satisfied condition "Succeeded or Failed"
Oct 13 12:40:38.206: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-subpath-test-projected-zd6n container test-container-subpath-projected-zd6n: <nil>
STEP: delete the pod
Oct 13 12:40:38.280: INFO: Waiting for pod pod-subpath-test-projected-zd6n to disappear
Oct 13 12:40:38.292: INFO: Pod pod-subpath-test-projected-zd6n no longer exists
STEP: Deleting pod pod-subpath-test-projected-zd6n
Oct 13 12:40:38.292: INFO: Deleting pod "pod-subpath-test-projected-zd6n" in namespace "subpath-6364"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:40:38.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6364" for this suite.

• [SLOW TEST:24.529 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":334,"skipped":5349,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:40:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:40:38.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7686" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":335,"skipped":5367,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:40:38.553: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:40:38.631: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-2723
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:40:42.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6672" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:40:42.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2723" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":336,"skipped":5380,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:40:42.970: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8968.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8968.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 1.22.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.22.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.22.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.22.1_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8968.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8968.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8968.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8968.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8968.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 1.22.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.22.1_udp@PTR;check="$$(dig +tcp +noall +answer +search 1.22.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.22.1_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct 13 12:40:49.287: INFO: Unable to read wheezy_udp@dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.339: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.437: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.591: INFO: Unable to read jessie_udp@dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.607: INFO: Unable to read jessie_tcp@dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.627: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.651: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local from pod dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012: the server could not find the requested resource (get pods dns-test-79e39f15-d4f2-4997-aa16-61a67d686012)
Oct 13 12:40:49.781: INFO: Lookups using dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012 failed for: [wheezy_udp@dns-test-service.dns-8968.svc.cluster.local wheezy_tcp@dns-test-service.dns-8968.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local jessie_udp@dns-test-service.dns-8968.svc.cluster.local jessie_tcp@dns-test-service.dns-8968.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8968.svc.cluster.local]

Oct 13 12:40:55.190: INFO: DNS probes using dns-8968/dns-test-79e39f15-d4f2-4997-aa16-61a67d686012 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:40:55.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8968" for this suite.

• [SLOW TEST:12.398 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":337,"skipped":5381,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:40:55.368: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Oct 13 12:40:55.446: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Oct 13 12:40:56.535: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:40:57.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-769" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":338,"skipped":5398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Oct 13 12:40:57.595: INFO: >>> kubeConfig: /tmp/kubeconfig-371874133
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct 13 12:40:57.732: INFO: Waiting up to 5m0s for pod "pod-6187db04-63d7-487f-8441-225833a06ba8" in namespace "emptydir-7059" to be "Succeeded or Failed"
Oct 13 12:40:57.741: INFO: Pod "pod-6187db04-63d7-487f-8441-225833a06ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.321579ms
Oct 13 12:40:59.772: INFO: Pod "pod-6187db04-63d7-487f-8441-225833a06ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039884251s
Oct 13 12:41:01.798: INFO: Pod "pod-6187db04-63d7-487f-8441-225833a06ba8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065911254s
STEP: Saw pod success
Oct 13 12:41:01.799: INFO: Pod "pod-6187db04-63d7-487f-8441-225833a06ba8" satisfied condition "Succeeded or Failed"
Oct 13 12:41:01.811: INFO: Trying to get logs from node flamboyant-colden-worker-54ds2p-6fc95f445d-f89qr pod pod-6187db04-63d7-487f-8441-225833a06ba8 container test-container: <nil>
STEP: delete the pod
Oct 13 12:41:01.915: INFO: Waiting for pod pod-6187db04-63d7-487f-8441-225833a06ba8 to disappear
Oct 13 12:41:01.926: INFO: Pod pod-6187db04-63d7-487f-8441-225833a06ba8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Oct 13 12:41:01.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7059" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":339,"skipped":5420,"failed":0}
SSSSSSSSSSSSOct 13 12:41:01.966: INFO: Running AfterSuite actions on all nodes
Oct 13 12:41:01.966: INFO: Running AfterSuite actions on node 1
Oct 13 12:41:01.966: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5432,"failed":0}

Ran 339 of 5771 Specs in 7085.953 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5432 Skipped
PASS

Ginkgo ran 1 suite in 1h58m7.454439205s
Test Suite Passed
