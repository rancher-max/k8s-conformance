I0221 13:15:30.117026      22 e2e.go:132] Starting e2e run "c0328a2c-4c81-4b6e-a827-8f7fdb76bf2b" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1645449330 - Will randomize all specs
Will run 346 of 7042 specs

Feb 21 13:15:31.447: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:15:31.448: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 21 13:15:31.464: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 21 13:15:31.485: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 21 13:15:31.485: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 21 13:15:31.485: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 21 13:15:31.490: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Feb 21 13:15:31.490: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 21 13:15:31.490: INFO: e2e test version: v1.23.3
Feb 21 13:15:31.491: INFO: kube-apiserver version: v1.23.3
Feb 21 13:15:31.491: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:15:31.494: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:15:31.494: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename tables
W0221 13:15:31.525698      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Feb 21 13:15:31.525: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Feb 21 13:15:31.534: INFO: PSP annotation exists on dry run pod: "openebs-privileged"; assuming PodSecurityPolicy is enabled
W0221 13:15:31.537187      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0221 13:15:31.542639      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Feb 21 13:15:31.547: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:15:31.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8748" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":1,"skipped":35,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:15:31.672: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9561.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9561.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9561.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9561.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 13:15:45.837: INFO: DNS probes using dns-9561/dns-test-3911cd04-30c9-4c2b-b15b-1305f19b9252 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:15:45.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9561" for this suite.

• [SLOW TEST:14.184 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":2,"skipped":56,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:15:45.857: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 21 13:15:45.991: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 13:15:45.996: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 13:15:45.998: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-1 before test
Feb 21 13:15:46.003: INFO: calico-kube-controllers-6d9dfd554f-x2lk5 from kube-system started at 2022-02-21 12:49:56 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 21 13:15:46.003: INFO: canal-vnh8t from kube-system started at 2022-02-21 12:49:43 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:15:46.003: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:15:46.003: INFO: kube-apiserver-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:15:46.003: INFO: kube-controller-manager-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:15:46.003: INFO: kube-proxy-r42th from kube-system started at 2022-02-21 12:49:43 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:15:46.003: INFO: kube-scheduler-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:15:46.003: INFO: openebs-ndm-lqth6 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:15:46.003: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-6psfh from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.003: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:15:46.003: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:15:46.003: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-2 before test
Feb 21 13:15:46.009: INFO: canal-9jnkf from kube-system started at 2022-02-21 12:51:19 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:15:46.009: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:15:46.009: INFO: coredns-599b888cfc-hvh9p from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:15:46.009: INFO: kube-apiserver-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:15:46.009: INFO: kube-controller-manager-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:15:46.009: INFO: kube-proxy-9s7vr from kube-system started at 2022-02-21 12:51:19 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:15:46.009: INFO: kube-scheduler-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:15:46.009: INFO: openebs-ndm-operator-5788658f97-r6nkb from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container node-disk-operator ready: true, restart count 0
Feb 21 13:15:46.009: INFO: openebs-ndm-zsg6k from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:15:46.009: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-8t6r2 from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:15:46.009: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:15:46.009: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-3 before test
Feb 21 13:15:46.014: INFO: canal-l6tfl from kube-system started at 2022-02-21 12:51:15 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:15:46.014: INFO: 	Container kube-flannel ready: true, restart count 1
Feb 21 13:15:46.014: INFO: coredns-599b888cfc-n8dbh from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:15:46.014: INFO: kube-apiserver-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:15:46.014: INFO: kube-controller-manager-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:15:46.014: INFO: kube-proxy-mjljh from kube-system started at 2022-02-21 12:51:15 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:15:46.014: INFO: kube-scheduler-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:15:46.014: INFO: kubelet-rubber-stamp-7f44cf9f9-stx2z from kube-system started at 2022-02-21 12:51:35 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 0
Feb 21 13:15:46.014: INFO: openebs-ndm-v6zq8 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:15:46.014: INFO: rafay-connector-v3-5ff84d7bb-sqhgm from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container connector ready: true, restart count 0
Feb 21 13:15:46.014: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-66hxs from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.014: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:15:46.014: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:15:46.014: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-4 before test
Feb 21 13:15:46.019: INFO: canal-x2v7q from kube-system started at 2022-02-21 12:53:38 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:15:46.019: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:15:46.019: INFO: kube-proxy-xzzph from kube-system started at 2022-02-21 12:53:38 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:15:46.019: INFO: openebs-localpv-provisioner-6867454bf5-wvwjv from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 0
Feb 21 13:15:46.019: INFO: openebs-ndm-pm2fl from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:15:46.019: INFO: controller-manager-v3-6b6c76796b-gv5np from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container manager ready: true, restart count 0
Feb 21 13:15:46.019: INFO: edge-client-86d55698bf-sx7hx from rafay-system started at 2022-02-21 12:57:39 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container edge-client ready: true, restart count 0
Feb 21 13:15:46.019: INFO: relay-agent-7694556d-48tl4 from rafay-system started at 2022-02-21 12:57:42 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container relay-agent ready: true, restart count 0
Feb 21 13:15:46.019: INFO: sonobuoy from sonobuoy started at 2022-02-21 13:15:07 +0000 UTC (1 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 21 13:15:46.019: INFO: sonobuoy-e2e-job-575d91b12b2744fe from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container e2e ready: true, restart count 0
Feb 21 13:15:46.019: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:15:46.019: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-98wsf from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:15:46.019: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:15:46.019: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node aksh-cncf-1
STEP: verifying the node has the label node aksh-cncf-2
STEP: verifying the node has the label node aksh-cncf-3
STEP: verifying the node has the label node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod calico-kube-controllers-6d9dfd554f-x2lk5 requesting resource cpu=0m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod canal-9jnkf requesting resource cpu=250m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod canal-l6tfl requesting resource cpu=250m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod canal-vnh8t requesting resource cpu=250m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod canal-x2v7q requesting resource cpu=250m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod coredns-599b888cfc-hvh9p requesting resource cpu=50m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod coredns-599b888cfc-n8dbh requesting resource cpu=50m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod kube-apiserver-aksh-cncf-1 requesting resource cpu=150m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod kube-apiserver-aksh-cncf-2 requesting resource cpu=150m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod kube-apiserver-aksh-cncf-3 requesting resource cpu=150m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod kube-controller-manager-aksh-cncf-1 requesting resource cpu=100m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod kube-controller-manager-aksh-cncf-2 requesting resource cpu=100m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod kube-controller-manager-aksh-cncf-3 requesting resource cpu=100m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod kube-proxy-9s7vr requesting resource cpu=0m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod kube-proxy-mjljh requesting resource cpu=0m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod kube-proxy-r42th requesting resource cpu=0m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod kube-proxy-xzzph requesting resource cpu=0m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod kube-scheduler-aksh-cncf-1 requesting resource cpu=100m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod kube-scheduler-aksh-cncf-2 requesting resource cpu=100m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod kube-scheduler-aksh-cncf-3 requesting resource cpu=100m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod kubelet-rubber-stamp-7f44cf9f9-stx2z requesting resource cpu=2m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod openebs-localpv-provisioner-6867454bf5-wvwjv requesting resource cpu=0m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod openebs-ndm-lqth6 requesting resource cpu=0m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod openebs-ndm-operator-5788658f97-r6nkb requesting resource cpu=0m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod openebs-ndm-pm2fl requesting resource cpu=0m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod openebs-ndm-v6zq8 requesting resource cpu=0m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod openebs-ndm-zsg6k requesting resource cpu=0m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod controller-manager-v3-6b6c76796b-gv5np requesting resource cpu=100m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod edge-client-86d55698bf-sx7hx requesting resource cpu=50m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod rafay-connector-v3-5ff84d7bb-sqhgm requesting resource cpu=50m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod relay-agent-7694556d-48tl4 requesting resource cpu=100m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod sonobuoy requesting resource cpu=0m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod sonobuoy-e2e-job-575d91b12b2744fe requesting resource cpu=0m on Node aksh-cncf-4
Feb 21 13:15:46.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-66hxs requesting resource cpu=0m on Node aksh-cncf-3
Feb 21 13:15:46.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-6psfh requesting resource cpu=0m on Node aksh-cncf-1
Feb 21 13:15:46.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-8t6r2 requesting resource cpu=0m on Node aksh-cncf-2
Feb 21 13:15:46.085: INFO: Pod sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-98wsf requesting resource cpu=0m on Node aksh-cncf-4
STEP: Starting Pods to consume most of the cluster CPU.
Feb 21 13:15:46.085: INFO: Creating a pod which consumes cpu=5187m on Node aksh-cncf-4
Feb 21 13:15:46.091: INFO: Creating a pod which consumes cpu=5117m on Node aksh-cncf-1
Feb 21 13:15:46.099: INFO: Creating a pod which consumes cpu=5082m on Node aksh-cncf-2
Feb 21 13:15:46.105: INFO: Creating a pod which consumes cpu=5045m on Node aksh-cncf-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396.16d5cf765bba9462], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7964/filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396 to aksh-cncf-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396.16d5cf767c0f6f23], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396.16d5cf76b0c7ce59], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 884.486016ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396.16d5cf76b2288efd], Reason = [Created], Message = [Created container filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396.16d5cf76b5c0cace], Reason = [Started], Message = [Started container filler-pod-1eca4b17-780b-4960-960d-2c1ed06d0396]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-900fedae-4732-47d8-a4c0-22854959408a.16d5cf765c2b415c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7964/filler-pod-900fedae-4732-47d8-a4c0-22854959408a to aksh-cncf-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-900fedae-4732-47d8-a4c0-22854959408a.16d5cf767d46c30a], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-900fedae-4732-47d8-a4c0-22854959408a.16d5cf76a8880113], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 725.681782ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-900fedae-4732-47d8-a4c0-22854959408a.16d5cf76a9e8cdf5], Reason = [Created], Message = [Created container filler-pod-900fedae-4732-47d8-a4c0-22854959408a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-900fedae-4732-47d8-a4c0-22854959408a.16d5cf76aeb1e1ca], Reason = [Started], Message = [Started container filler-pod-900fedae-4732-47d8-a4c0-22854959408a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c.16d5cf765c95d0d1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7964/filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c to aksh-cncf-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c.16d5cf767b07bb1d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c.16d5cf769f9fe458], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 613.93952ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c.16d5cf76a18d6215], Reason = [Created], Message = [Created container filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c.16d5cf76a52f6d0c], Reason = [Started], Message = [Started container filler-pod-9b9e1332-2ec2-4c2d-9ebe-9c7773eb9f1c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce.16d5cf765b67de3a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7964/filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce to aksh-cncf-4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce.16d5cf7678588347], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce.16d5cf76b16260e9], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 956.930851ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce.16d5cf76b2d140ac], Reason = [Created], Message = [Created container filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce.16d5cf76b6e38ddd], Reason = [Started], Message = [Started container filler-pod-d83ff01c-50ab-4477-a126-abe7156833ce]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16d5cf76d52b8a34], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node aksh-cncf-4
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node aksh-cncf-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node aksh-cncf-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node aksh-cncf-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:15:49.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7964" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":3,"skipped":77,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:15:49.210: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:15:49.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3227" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":4,"skipped":106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:15:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Feb 21 13:15:49.518: INFO: created test-pod-1
Feb 21 13:15:51.527: INFO: running and ready test-pod-1
Feb 21 13:15:51.535: INFO: created test-pod-2
Feb 21 13:15:57.545: INFO: running and ready test-pod-2
Feb 21 13:15:57.553: INFO: created test-pod-3
Feb 21 13:15:59.559: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Feb 21 13:15:59.584: INFO: Pod quantity 3 is different from expected quantity 0
Feb 21 13:16:00.591: INFO: Pod quantity 2 is different from expected quantity 0
Feb 21 13:16:01.588: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:02.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6504" for this suite.

• [SLOW TEST:13.224 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":5,"skipped":130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:02.600: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Feb 21 13:16:02.742: INFO: Creating simple deployment test-deployment-wrps2
Feb 21 13:16:02.752: INFO: deployment "test-deployment-wrps2" doesn't have the required revision set
Feb 21 13:16:04.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-wrps2-764bc7c4b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 13:16:06.772: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-wrps2-764bc7c4b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Feb 21 13:16:08.781: INFO: Deployment test-deployment-wrps2 has Conditions: [{Available True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wrps2-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Feb 21 13:16:08.791: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 16, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 16, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 16, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 16, 2, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-wrps2-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Feb 21 13:16:08.793: INFO: Observed &Deployment event: ADDED
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wrps2-764bc7c4b7"}
Feb 21 13:16:08.793: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wrps2-764bc7c4b7"}
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 21 13:16:08.793: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-wrps2-764bc7c4b7" is progressing.}
Feb 21 13:16:08.793: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wrps2-764bc7c4b7" has successfully progressed.}
Feb 21 13:16:08.793: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 21 13:16:08.793: INFO: Observed Deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wrps2-764bc7c4b7" has successfully progressed.}
Feb 21 13:16:08.793: INFO: Found Deployment test-deployment-wrps2 in namespace deployment-4498 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 21 13:16:08.793: INFO: Deployment test-deployment-wrps2 has an updated status
STEP: patching the Statefulset Status
Feb 21 13:16:08.793: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 21 13:16:08.801: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Feb 21 13:16:08.802: INFO: Observed &Deployment event: ADDED
Feb 21 13:16:08.802: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wrps2-764bc7c4b7"}
Feb 21 13:16:08.802: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.802: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-wrps2-764bc7c4b7"}
Feb 21 13:16:08.802: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 21 13:16:08.802: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.802: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 21 13:16:08.802: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:02 +0000 UTC 2022-02-21 13:16:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-wrps2-764bc7c4b7" is progressing.}
Feb 21 13:16:08.802: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.802: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 21 13:16:08.802: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wrps2-764bc7c4b7" has successfully progressed.}
Feb 21 13:16:08.803: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.803: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 21 13:16:08.803: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-02-21 13:16:07 +0000 UTC 2022-02-21 13:16:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-wrps2-764bc7c4b7" has successfully progressed.}
Feb 21 13:16:08.803: INFO: Observed deployment test-deployment-wrps2 in namespace deployment-4498 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 21 13:16:08.803: INFO: Observed &Deployment event: MODIFIED
Feb 21 13:16:08.803: INFO: Found deployment test-deployment-wrps2 in namespace deployment-4498 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 21 13:16:08.803: INFO: Deployment test-deployment-wrps2 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 13:16:08.805: INFO: Deployment "test-deployment-wrps2":
&Deployment{ObjectMeta:{test-deployment-wrps2  deployment-4498  d4615ae2-1eb5-4dd5-8c75-58c59d1c7305 7551 1 2022-02-21 13:16:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-02-21 13:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:16:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-02-21 13:16:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ad70b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 13:16:08.808: INFO: New ReplicaSet "test-deployment-wrps2-764bc7c4b7" of Deployment "test-deployment-wrps2":
&ReplicaSet{ObjectMeta:{test-deployment-wrps2-764bc7c4b7  deployment-4498  531275c2-8ac8-4358-87ef-686b27adcfe6 7525 1 2022-02-21 13:16:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-wrps2 d4615ae2-1eb5-4dd5-8c75-58c59d1c7305 0xc003ad7467 0xc003ad7468}] []  [{kube-controller-manager Update apps/v1 2022-02-21 13:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4615ae2-1eb5-4dd5-8c75-58c59d1c7305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:16:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ad7518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 21 13:16:08.810: INFO: Pod "test-deployment-wrps2-764bc7c4b7-bzhxt" is available:
&Pod{ObjectMeta:{test-deployment-wrps2-764bc7c4b7-bzhxt test-deployment-wrps2-764bc7c4b7- deployment-4498  ebe8c001-f45b-4c88-b999-e9918f404961 7524 0 2022-02-21 13:16:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/podIP:10.244.1.11/32 cni.projectcalico.org/podIPs:10.244.1.11/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-wrps2-764bc7c4b7 531275c2-8ac8-4358-87ef-686b27adcfe6 0xc003ad78d7 0xc003ad78d8}] []  [{kube-controller-manager Update v1 2022-02-21 13:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"531275c2-8ac8-4358-87ef-686b27adcfe6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-21 13:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-02-21 13:16:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9l2pd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9l2pd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:16:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:16:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:16:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.131,PodIP:10.244.1.11,StartTime:2022-02-21 13:16:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 13:16:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://313a471fe1b8cadbe938a34f0e633bd61fe126bcb64626cb9755c0de68416a45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:08.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4498" for this suite.

• [SLOW TEST:6.218 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":6,"skipped":196,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:08.819: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:11.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-807" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":7,"skipped":210,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:11.778: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3855
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:16:11.914: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:18.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3855" for this suite.

• [SLOW TEST:6.408 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":8,"skipped":219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:18.186: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 21 13:16:19.376: INFO: The status of Pod kube-controller-manager-aksh-cncf-3 is Running (Ready = true)
Feb 21 13:16:19.410: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:19.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5002" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":9,"skipped":272,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:19.419: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Feb 21 13:16:19.563: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:19.563: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:19.570: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:19.570: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:19.580: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:19.580: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:19.602: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:19.602: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 21 13:16:20.512: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 21 13:16:20.512: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 21 13:16:21.068: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Feb 21 13:16:21.085: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 0
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.086: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.095: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.095: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:21.118: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:21.118: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:21.124: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:21.124: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:22.120: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:22.120: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:22.139: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
STEP: listing Deployments
Feb 21 13:16:22.143: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Feb 21 13:16:22.153: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Feb 21 13:16:22.158: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:22.164: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:22.176: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:22.188: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:22.194: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:23.045: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:23.549: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:23.569: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:23.579: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 21 13:16:28.084: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Feb 21 13:16:28.109: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 1
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 3
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 2
Feb 21 13:16:28.110: INFO: observed Deployment test-deployment in namespace deployment-5176 with ReadyReplicas 3
STEP: deleting the Deployment
Feb 21 13:16:28.120: INFO: observed event type MODIFIED
Feb 21 13:16:28.120: INFO: observed event type MODIFIED
Feb 21 13:16:28.120: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
Feb 21 13:16:28.121: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 13:16:28.124: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:28.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5176" for this suite.

• [SLOW TEST:8.719 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":10,"skipped":283,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:28.139: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6029
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6029
STEP: creating replication controller externalsvc in namespace services-6029
I0221 13:16:28.309859      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6029, replica count: 2
I0221 13:16:31.360408      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 21 13:16:31.407: INFO: Creating new exec pod
Feb 21 13:16:33.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-6029 exec execpod8vk94 -- /bin/sh -x -c nslookup nodeport-service.services-6029.svc.cluster.local'
Feb 21 13:16:33.651: INFO: stderr: "+ nslookup nodeport-service.services-6029.svc.cluster.local\n"
Feb 21 13:16:33.651: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-6029.svc.cluster.local\tcanonical name = externalsvc.services-6029.svc.cluster.local.\nName:\texternalsvc.services-6029.svc.cluster.local\nAddress: 10.101.137.83\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6029, will wait for the garbage collector to delete the pods
Feb 21 13:16:33.711: INFO: Deleting ReplicationController externalsvc took: 6.382413ms
Feb 21 13:16:33.812: INFO: Terminating ReplicationController externalsvc pods took: 100.367507ms
Feb 21 13:16:36.130: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:36.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6029" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.009 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":11,"skipped":293,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:36.147: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:38.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5766" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":12,"skipped":303,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:38.612: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-96
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:42.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-96" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":13,"skipped":330,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:42.795: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Feb 21 13:16:42.939: INFO: Waiting up to 5m0s for pod "var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b" in namespace "var-expansion-5095" to be "Succeeded or Failed"
Feb 21 13:16:42.941: INFO: Pod "var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.254977ms
Feb 21 13:16:44.949: INFO: Pod "var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010189285s
Feb 21 13:16:46.954: INFO: Pod "var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015326638s
STEP: Saw pod success
Feb 21 13:16:46.954: INFO: Pod "var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b" satisfied condition "Succeeded or Failed"
Feb 21 13:16:46.957: INFO: Trying to get logs from node aksh-cncf-3 pod var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b container dapi-container: <nil>
STEP: delete the pod
Feb 21 13:16:46.971: INFO: Waiting for pod var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b to disappear
Feb 21 13:16:46.973: INFO: Pod var-expansion-47d4cfd1-36f1-492c-aec9-e30fa0b57c4b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:46.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5095" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":14,"skipped":348,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:46.981: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-06e3459d-a07d-423c-857a-bf19a375ccfe
STEP: Creating a pod to test consume secrets
Feb 21 13:16:47.127: INFO: Waiting up to 5m0s for pod "pod-secrets-8f881cd4-fdaf-4704-9545-b718b36277be" in namespace "secrets-4509" to be "Succeeded or Failed"
Feb 21 13:16:47.129: INFO: Pod "pod-secrets-8f881cd4-fdaf-4704-9545-b718b36277be": Phase="Pending", Reason="", readiness=false. Elapsed: 1.995052ms
Feb 21 13:16:49.136: INFO: Pod "pod-secrets-8f881cd4-fdaf-4704-9545-b718b36277be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009334727s
STEP: Saw pod success
Feb 21 13:16:49.136: INFO: Pod "pod-secrets-8f881cd4-fdaf-4704-9545-b718b36277be" satisfied condition "Succeeded or Failed"
Feb 21 13:16:49.139: INFO: Trying to get logs from node aksh-cncf-3 pod pod-secrets-8f881cd4-fdaf-4704-9545-b718b36277be container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 13:16:49.161: INFO: Waiting for pod pod-secrets-8f881cd4-fdaf-4704-9545-b718b36277be to disappear
Feb 21 13:16:49.164: INFO: Pod pod-secrets-8f881cd4-fdaf-4704-9545-b718b36277be no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:16:49.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4509" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":353,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:16:49.171: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4701
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4701
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4701
Feb 21 13:16:49.340: INFO: Found 0 stateful pods, waiting for 1
Feb 21 13:16:59.347: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 21 13:16:59.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:16:59.458: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:16:59.458: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:16:59.458: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:16:59.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 21 13:17:09.469: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:17:09.469: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:17:09.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999829s
Feb 21 13:17:10.489: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996784203s
Feb 21 13:17:11.495: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992339265s
Feb 21 13:17:12.502: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986235997s
Feb 21 13:17:13.507: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978734247s
Feb 21 13:17:14.512: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973460147s
Feb 21 13:17:15.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.967940996s
Feb 21 13:17:16.524: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960964319s
Feb 21 13:17:17.531: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.956869442s
Feb 21 13:17:18.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 950.36084ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4701
Feb 21 13:17:19.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 13:17:19.651: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 21 13:17:19.651: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 13:17:19.651: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 13:17:19.654: INFO: Found 1 stateful pods, waiting for 3
Feb 21 13:17:29.663: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:17:29.663: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:17:29.663: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 21 13:17:29.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:17:29.772: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:17:29.772: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:17:29.772: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:17:29.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:17:29.890: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:17:29.890: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:17:29.890: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:17:29.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:17:29.996: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:17:29.996: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:17:29.996: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:17:29.996: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:17:29.998: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 21 13:17:40.006: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:17:40.006: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:17:40.006: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:17:40.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999659s
Feb 21 13:17:41.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997457033s
Feb 21 13:17:42.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992832755s
Feb 21 13:17:43.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989035037s
Feb 21 13:17:44.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98111784s
Feb 21 13:17:45.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977273332s
Feb 21 13:17:46.047: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970292028s
Feb 21 13:17:47.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966831222s
Feb 21 13:17:48.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961160568s
Feb 21 13:17:49.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.333479ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4701
Feb 21 13:17:50.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 13:17:50.163: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 21 13:17:50.163: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 13:17:50.163: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 13:17:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 13:17:50.261: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 21 13:17:50.261: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 13:17:50.261: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 13:17:50.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4701 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 13:17:50.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 21 13:17:50.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 13:17:50.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 13:17:50.362: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 13:18:00.381: INFO: Deleting all statefulset in ns statefulset-4701
Feb 21 13:18:00.383: INFO: Scaling statefulset ss to 0
Feb 21 13:18:00.391: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:18:00.393: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:18:00.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4701" for this suite.

• [SLOW TEST:71.243 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":16,"skipped":368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:18:00.415: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8989
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:20:00.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8989" for this suite.

• [SLOW TEST:120.167 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":17,"skipped":418,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:20:00.583: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 21 13:20:00.748: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 21 13:20:00.751: INFO: starting watch
STEP: patching
STEP: updating
Feb 21 13:20:00.762: INFO: waiting for watch events with expected annotations
Feb 21 13:20:00.762: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:20:00.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1159" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":18,"skipped":421,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:20:00.790: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2104
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9723
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:20:07.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5895" for this suite.
STEP: Destroying namespace "nsdeletetest-2104" for this suite.
Feb 21 13:20:07.227: INFO: Namespace nsdeletetest-2104 was already deleted
STEP: Destroying namespace "nsdeletetest-9723" for this suite.

• [SLOW TEST:6.442 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":19,"skipped":430,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:20:07.232: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:20:07.383: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 21 13:20:07.391: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:20:07.391: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Feb 21 13:20:07.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:20:07.415: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 13:20:08.421: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 21 13:20:08.421: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 21 13:20:08.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 21 13:20:08.438: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb 21 13:20:09.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:20:09.443: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 21 13:20:09.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:20:09.456: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 13:20:10.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:20:10.461: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 13:20:11.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:20:11.461: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 13:20:12.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 21 13:20:12.463: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1140, will wait for the garbage collector to delete the pods
Feb 21 13:20:12.526: INFO: Deleting DaemonSet.extensions daemon-set took: 5.836955ms
Feb 21 13:20:12.627: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.104746ms
Feb 21 13:20:15.433: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:20:15.434: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 21 13:20:15.436: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9671"},"items":null}

Feb 21 13:20:15.438: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9671"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:20:15.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1140" for this suite.

• [SLOW TEST:8.238 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":20,"skipped":451,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:20:15.470: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 21 13:20:15.607: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 13:20:15.613: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 13:20:15.615: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-1 before test
Feb 21 13:20:15.620: INFO: replace-27424160-b7cvd from cronjob-8989 started at 2022-02-21 13:20:00 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container c ready: true, restart count 0
Feb 21 13:20:15.620: INFO: calico-kube-controllers-6d9dfd554f-x2lk5 from kube-system started at 2022-02-21 12:49:56 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 21 13:20:15.620: INFO: canal-vnh8t from kube-system started at 2022-02-21 12:49:43 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:20:15.620: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:20:15.620: INFO: kube-apiserver-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:20:15.620: INFO: kube-controller-manager-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:20:15.620: INFO: kube-proxy-r42th from kube-system started at 2022-02-21 12:49:43 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:20:15.620: INFO: kube-scheduler-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:20:15.620: INFO: openebs-ndm-lqth6 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:20:15.620: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-6psfh from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.620: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:20:15.620: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:20:15.620: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-2 before test
Feb 21 13:20:15.626: INFO: canal-9jnkf from kube-system started at 2022-02-21 12:51:19 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:20:15.626: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:20:15.626: INFO: coredns-599b888cfc-hvh9p from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:20:15.626: INFO: kube-apiserver-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:20:15.626: INFO: kube-controller-manager-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:20:15.626: INFO: kube-proxy-9s7vr from kube-system started at 2022-02-21 12:51:19 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:20:15.626: INFO: kube-scheduler-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:20:15.626: INFO: openebs-ndm-operator-5788658f97-r6nkb from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container node-disk-operator ready: true, restart count 0
Feb 21 13:20:15.626: INFO: openebs-ndm-zsg6k from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:20:15.626: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-8t6r2 from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.626: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:20:15.626: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:20:15.626: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-3 before test
Feb 21 13:20:15.631: INFO: replace-27424159-4qtdb from cronjob-8989 started at 2022-02-21 13:19:00 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container c ready: true, restart count 0
Feb 21 13:20:15.631: INFO: canal-l6tfl from kube-system started at 2022-02-21 12:51:15 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:20:15.631: INFO: 	Container kube-flannel ready: true, restart count 1
Feb 21 13:20:15.631: INFO: coredns-599b888cfc-n8dbh from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:20:15.631: INFO: kube-apiserver-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:20:15.631: INFO: kube-controller-manager-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:20:15.631: INFO: kube-proxy-mjljh from kube-system started at 2022-02-21 12:51:15 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:20:15.631: INFO: kube-scheduler-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:20:15.631: INFO: kubelet-rubber-stamp-7f44cf9f9-stx2z from kube-system started at 2022-02-21 12:51:35 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 0
Feb 21 13:20:15.631: INFO: openebs-ndm-v6zq8 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:20:15.631: INFO: rafay-connector-v3-5ff84d7bb-sqhgm from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container connector ready: true, restart count 0
Feb 21 13:20:15.631: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-66hxs from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.631: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:20:15.631: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:20:15.631: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-4 before test
Feb 21 13:20:15.636: INFO: canal-x2v7q from kube-system started at 2022-02-21 12:53:38 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:20:15.636: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:20:15.636: INFO: kube-proxy-xzzph from kube-system started at 2022-02-21 12:53:38 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:20:15.636: INFO: openebs-localpv-provisioner-6867454bf5-wvwjv from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 0
Feb 21 13:20:15.636: INFO: openebs-ndm-pm2fl from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:20:15.636: INFO: controller-manager-v3-6b6c76796b-gv5np from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container manager ready: true, restart count 0
Feb 21 13:20:15.636: INFO: edge-client-86d55698bf-sx7hx from rafay-system started at 2022-02-21 12:57:39 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container edge-client ready: true, restart count 0
Feb 21 13:20:15.636: INFO: relay-agent-7694556d-48tl4 from rafay-system started at 2022-02-21 12:57:42 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container relay-agent ready: true, restart count 0
Feb 21 13:20:15.636: INFO: sonobuoy from sonobuoy started at 2022-02-21 13:15:07 +0000 UTC (1 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 21 13:20:15.636: INFO: sonobuoy-e2e-job-575d91b12b2744fe from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container e2e ready: true, restart count 0
Feb 21 13:20:15.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:20:15.636: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-98wsf from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:20:15.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:20:15.636: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16d5cfb51e6f857b], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:20:16.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-779" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":21,"skipped":455,"failed":0}
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:20:16.676: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:20:16.809: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1689
I0221 13:20:16.814082      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1689, replica count: 1
I0221 13:20:17.864799      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 13:20:17.978: INFO: Created: latency-svc-n45m9
Feb 21 13:20:17.983: INFO: Got endpoints: latency-svc-n45m9 [18.026415ms]
Feb 21 13:20:17.995: INFO: Created: latency-svc-7cn65
Feb 21 13:20:17.999: INFO: Got endpoints: latency-svc-7cn65 [16.551212ms]
Feb 21 13:20:18.001: INFO: Created: latency-svc-bzf9t
Feb 21 13:20:18.007: INFO: Got endpoints: latency-svc-bzf9t [23.765824ms]
Feb 21 13:20:18.008: INFO: Created: latency-svc-8m69d
Feb 21 13:20:18.014: INFO: Got endpoints: latency-svc-8m69d [31.35694ms]
Feb 21 13:20:18.017: INFO: Created: latency-svc-ltkvk
Feb 21 13:20:18.023: INFO: Got endpoints: latency-svc-ltkvk [39.416387ms]
Feb 21 13:20:18.028: INFO: Created: latency-svc-sh9p4
Feb 21 13:20:18.033: INFO: Got endpoints: latency-svc-sh9p4 [49.882904ms]
Feb 21 13:20:18.036: INFO: Created: latency-svc-xzf84
Feb 21 13:20:18.040: INFO: Got endpoints: latency-svc-xzf84 [56.859562ms]
Feb 21 13:20:18.044: INFO: Created: latency-svc-tmmrz
Feb 21 13:20:18.049: INFO: Got endpoints: latency-svc-tmmrz [65.527865ms]
Feb 21 13:20:18.056: INFO: Created: latency-svc-t9686
Feb 21 13:20:18.060: INFO: Got endpoints: latency-svc-t9686 [76.549337ms]
Feb 21 13:20:18.062: INFO: Created: latency-svc-ckqtv
Feb 21 13:20:18.067: INFO: Got endpoints: latency-svc-ckqtv [83.523522ms]
Feb 21 13:20:18.069: INFO: Created: latency-svc-jdwmn
Feb 21 13:20:18.075: INFO: Got endpoints: latency-svc-jdwmn [92.490992ms]
Feb 21 13:20:18.079: INFO: Created: latency-svc-v6fq8
Feb 21 13:20:18.083: INFO: Got endpoints: latency-svc-v6fq8 [100.149025ms]
Feb 21 13:20:18.089: INFO: Created: latency-svc-96swp
Feb 21 13:20:18.093: INFO: Got endpoints: latency-svc-96swp [110.387979ms]
Feb 21 13:20:18.097: INFO: Created: latency-svc-g7n9n
Feb 21 13:20:18.101: INFO: Got endpoints: latency-svc-g7n9n [118.284596ms]
Feb 21 13:20:18.106: INFO: Created: latency-svc-84dg5
Feb 21 13:20:18.111: INFO: Got endpoints: latency-svc-84dg5 [127.63334ms]
Feb 21 13:20:18.114: INFO: Created: latency-svc-cfxnk
Feb 21 13:20:18.119: INFO: Got endpoints: latency-svc-cfxnk [136.299088ms]
Feb 21 13:20:18.123: INFO: Created: latency-svc-2x9wq
Feb 21 13:20:18.128: INFO: Got endpoints: latency-svc-2x9wq [129.013643ms]
Feb 21 13:20:18.134: INFO: Created: latency-svc-6jmvk
Feb 21 13:20:18.139: INFO: Got endpoints: latency-svc-6jmvk [132.727505ms]
Feb 21 13:20:18.142: INFO: Created: latency-svc-2bnnw
Feb 21 13:20:18.152: INFO: Got endpoints: latency-svc-2bnnw [137.991852ms]
Feb 21 13:20:18.156: INFO: Created: latency-svc-9qpqk
Feb 21 13:20:18.161: INFO: Got endpoints: latency-svc-9qpqk [138.80654ms]
Feb 21 13:20:18.164: INFO: Created: latency-svc-st7bb
Feb 21 13:20:18.170: INFO: Got endpoints: latency-svc-st7bb [136.778919ms]
Feb 21 13:20:18.171: INFO: Created: latency-svc-q2rhh
Feb 21 13:20:18.175: INFO: Got endpoints: latency-svc-q2rhh [135.068019ms]
Feb 21 13:20:18.179: INFO: Created: latency-svc-ngj69
Feb 21 13:20:18.186: INFO: Got endpoints: latency-svc-ngj69 [137.782134ms]
Feb 21 13:20:18.189: INFO: Created: latency-svc-9sh6n
Feb 21 13:20:18.193: INFO: Got endpoints: latency-svc-9sh6n [133.499922ms]
Feb 21 13:20:18.203: INFO: Created: latency-svc-trbmj
Feb 21 13:20:18.209: INFO: Got endpoints: latency-svc-trbmj [142.221585ms]
Feb 21 13:20:18.212: INFO: Created: latency-svc-g4pmp
Feb 21 13:20:18.218: INFO: Got endpoints: latency-svc-g4pmp [142.616544ms]
Feb 21 13:20:18.220: INFO: Created: latency-svc-dclg6
Feb 21 13:20:18.226: INFO: Got endpoints: latency-svc-dclg6 [142.427716ms]
Feb 21 13:20:18.229: INFO: Created: latency-svc-mqcbf
Feb 21 13:20:18.235: INFO: Got endpoints: latency-svc-mqcbf [141.741281ms]
Feb 21 13:20:18.237: INFO: Created: latency-svc-jwl6c
Feb 21 13:20:18.244: INFO: Got endpoints: latency-svc-jwl6c [142.66678ms]
Feb 21 13:20:18.250: INFO: Created: latency-svc-588r4
Feb 21 13:20:18.254: INFO: Got endpoints: latency-svc-588r4 [143.402337ms]
Feb 21 13:20:18.258: INFO: Created: latency-svc-m2hvv
Feb 21 13:20:18.266: INFO: Got endpoints: latency-svc-m2hvv [146.338985ms]
Feb 21 13:20:18.269: INFO: Created: latency-svc-kzgdl
Feb 21 13:20:18.277: INFO: Created: latency-svc-4dcmq
Feb 21 13:20:18.281: INFO: Got endpoints: latency-svc-kzgdl [152.749087ms]
Feb 21 13:20:18.281: INFO: Got endpoints: latency-svc-4dcmq [141.954748ms]
Feb 21 13:20:18.285: INFO: Created: latency-svc-zbr4d
Feb 21 13:20:18.289: INFO: Got endpoints: latency-svc-zbr4d [137.286352ms]
Feb 21 13:20:18.292: INFO: Created: latency-svc-dc8f7
Feb 21 13:20:18.296: INFO: Got endpoints: latency-svc-dc8f7 [134.844594ms]
Feb 21 13:20:18.302: INFO: Created: latency-svc-9vsc5
Feb 21 13:20:18.310: INFO: Got endpoints: latency-svc-9vsc5 [140.460881ms]
Feb 21 13:20:18.312: INFO: Created: latency-svc-bfbc6
Feb 21 13:20:18.321: INFO: Created: latency-svc-nrcd5
Feb 21 13:20:18.330: INFO: Created: latency-svc-vbg4l
Feb 21 13:20:18.332: INFO: Got endpoints: latency-svc-bfbc6 [156.869752ms]
Feb 21 13:20:18.337: INFO: Created: latency-svc-ddq2s
Feb 21 13:20:18.351: INFO: Created: latency-svc-9985z
Feb 21 13:20:18.366: INFO: Created: latency-svc-p6pqm
Feb 21 13:20:18.371: INFO: Created: latency-svc-nxxjj
Feb 21 13:20:18.379: INFO: Created: latency-svc-xwmq7
Feb 21 13:20:18.384: INFO: Got endpoints: latency-svc-nrcd5 [197.799664ms]
Feb 21 13:20:18.389: INFO: Created: latency-svc-cjt6l
Feb 21 13:20:18.397: INFO: Created: latency-svc-7h7kw
Feb 21 13:20:18.403: INFO: Created: latency-svc-q8hv4
Feb 21 13:20:18.412: INFO: Created: latency-svc-jnnfh
Feb 21 13:20:18.417: INFO: Created: latency-svc-4hjb7
Feb 21 13:20:18.424: INFO: Created: latency-svc-m9r5z
Feb 21 13:20:18.434: INFO: Got endpoints: latency-svc-vbg4l [240.715065ms]
Feb 21 13:20:18.434: INFO: Created: latency-svc-vm8j5
Feb 21 13:20:18.442: INFO: Created: latency-svc-8wwlq
Feb 21 13:20:18.450: INFO: Created: latency-svc-8jp92
Feb 21 13:20:18.456: INFO: Created: latency-svc-vqxhl
Feb 21 13:20:18.504: INFO: Got endpoints: latency-svc-ddq2s [294.738271ms]
Feb 21 13:20:18.517: INFO: Created: latency-svc-4jrf8
Feb 21 13:20:18.533: INFO: Got endpoints: latency-svc-9985z [314.998077ms]
Feb 21 13:20:18.544: INFO: Created: latency-svc-ckctt
Feb 21 13:20:18.581: INFO: Got endpoints: latency-svc-p6pqm [355.786077ms]
Feb 21 13:20:18.594: INFO: Created: latency-svc-pj9cg
Feb 21 13:20:18.631: INFO: Got endpoints: latency-svc-nxxjj [396.215149ms]
Feb 21 13:20:18.643: INFO: Created: latency-svc-gdhmf
Feb 21 13:20:18.683: INFO: Got endpoints: latency-svc-xwmq7 [439.218966ms]
Feb 21 13:20:18.694: INFO: Created: latency-svc-brwt7
Feb 21 13:20:18.734: INFO: Got endpoints: latency-svc-cjt6l [479.62905ms]
Feb 21 13:20:18.745: INFO: Created: latency-svc-8rdph
Feb 21 13:20:18.783: INFO: Got endpoints: latency-svc-7h7kw [516.958332ms]
Feb 21 13:20:18.796: INFO: Created: latency-svc-z6x5q
Feb 21 13:20:18.832: INFO: Got endpoints: latency-svc-q8hv4 [551.054235ms]
Feb 21 13:20:18.843: INFO: Created: latency-svc-mzd7p
Feb 21 13:20:18.883: INFO: Got endpoints: latency-svc-jnnfh [601.162446ms]
Feb 21 13:20:18.894: INFO: Created: latency-svc-t9z8k
Feb 21 13:20:18.932: INFO: Got endpoints: latency-svc-4hjb7 [642.300212ms]
Feb 21 13:20:18.943: INFO: Created: latency-svc-fktjv
Feb 21 13:20:18.984: INFO: Got endpoints: latency-svc-m9r5z [687.493339ms]
Feb 21 13:20:18.995: INFO: Created: latency-svc-67m9w
Feb 21 13:20:19.035: INFO: Got endpoints: latency-svc-vm8j5 [724.745594ms]
Feb 21 13:20:19.047: INFO: Created: latency-svc-8kz6b
Feb 21 13:20:19.082: INFO: Got endpoints: latency-svc-8wwlq [749.514503ms]
Feb 21 13:20:19.093: INFO: Created: latency-svc-6sdds
Feb 21 13:20:19.133: INFO: Got endpoints: latency-svc-8jp92 [748.585228ms]
Feb 21 13:20:19.145: INFO: Created: latency-svc-6jsh5
Feb 21 13:20:19.183: INFO: Got endpoints: latency-svc-vqxhl [748.638188ms]
Feb 21 13:20:19.195: INFO: Created: latency-svc-g6d5t
Feb 21 13:20:19.231: INFO: Got endpoints: latency-svc-4jrf8 [727.920211ms]
Feb 21 13:20:19.243: INFO: Created: latency-svc-tblcd
Feb 21 13:20:19.283: INFO: Got endpoints: latency-svc-ckctt [749.766479ms]
Feb 21 13:20:19.294: INFO: Created: latency-svc-67kq5
Feb 21 13:20:19.332: INFO: Got endpoints: latency-svc-pj9cg [750.594492ms]
Feb 21 13:20:19.344: INFO: Created: latency-svc-2jzsd
Feb 21 13:20:19.383: INFO: Got endpoints: latency-svc-gdhmf [751.864282ms]
Feb 21 13:20:19.394: INFO: Created: latency-svc-l86ds
Feb 21 13:20:19.433: INFO: Got endpoints: latency-svc-brwt7 [749.355888ms]
Feb 21 13:20:19.448: INFO: Created: latency-svc-xvxks
Feb 21 13:20:19.483: INFO: Got endpoints: latency-svc-8rdph [749.455115ms]
Feb 21 13:20:19.496: INFO: Created: latency-svc-qwhjw
Feb 21 13:20:19.532: INFO: Got endpoints: latency-svc-z6x5q [749.154463ms]
Feb 21 13:20:19.545: INFO: Created: latency-svc-6gbww
Feb 21 13:20:19.584: INFO: Got endpoints: latency-svc-mzd7p [751.379479ms]
Feb 21 13:20:19.598: INFO: Created: latency-svc-tsbcp
Feb 21 13:20:19.632: INFO: Got endpoints: latency-svc-t9z8k [748.849723ms]
Feb 21 13:20:19.643: INFO: Created: latency-svc-vdxwq
Feb 21 13:20:19.683: INFO: Got endpoints: latency-svc-fktjv [751.328009ms]
Feb 21 13:20:19.700: INFO: Created: latency-svc-h5tcx
Feb 21 13:20:19.732: INFO: Got endpoints: latency-svc-67m9w [747.933232ms]
Feb 21 13:20:19.743: INFO: Created: latency-svc-69qrc
Feb 21 13:20:19.781: INFO: Got endpoints: latency-svc-8kz6b [746.108718ms]
Feb 21 13:20:19.797: INFO: Created: latency-svc-hz67s
Feb 21 13:20:19.833: INFO: Got endpoints: latency-svc-6sdds [751.057705ms]
Feb 21 13:20:19.844: INFO: Created: latency-svc-jwdd5
Feb 21 13:20:19.882: INFO: Got endpoints: latency-svc-6jsh5 [748.864891ms]
Feb 21 13:20:19.893: INFO: Created: latency-svc-75qzp
Feb 21 13:20:19.933: INFO: Got endpoints: latency-svc-g6d5t [750.210166ms]
Feb 21 13:20:19.944: INFO: Created: latency-svc-7hhdr
Feb 21 13:20:19.985: INFO: Got endpoints: latency-svc-tblcd [753.734939ms]
Feb 21 13:20:19.997: INFO: Created: latency-svc-dz5rq
Feb 21 13:20:20.032: INFO: Got endpoints: latency-svc-67kq5 [749.544071ms]
Feb 21 13:20:20.047: INFO: Created: latency-svc-p4dv5
Feb 21 13:20:20.083: INFO: Got endpoints: latency-svc-2jzsd [751.018641ms]
Feb 21 13:20:20.095: INFO: Created: latency-svc-dh62m
Feb 21 13:20:20.133: INFO: Got endpoints: latency-svc-l86ds [749.414643ms]
Feb 21 13:20:20.144: INFO: Created: latency-svc-vlnhv
Feb 21 13:20:20.182: INFO: Got endpoints: latency-svc-xvxks [749.114003ms]
Feb 21 13:20:20.196: INFO: Created: latency-svc-6n4fv
Feb 21 13:20:20.233: INFO: Got endpoints: latency-svc-qwhjw [749.822016ms]
Feb 21 13:20:20.246: INFO: Created: latency-svc-c225h
Feb 21 13:20:20.281: INFO: Got endpoints: latency-svc-6gbww [749.182031ms]
Feb 21 13:20:20.296: INFO: Created: latency-svc-tgls8
Feb 21 13:20:20.333: INFO: Got endpoints: latency-svc-tsbcp [748.740691ms]
Feb 21 13:20:20.350: INFO: Created: latency-svc-vq5b4
Feb 21 13:20:20.383: INFO: Got endpoints: latency-svc-vdxwq [751.659565ms]
Feb 21 13:20:20.395: INFO: Created: latency-svc-qddnr
Feb 21 13:20:20.432: INFO: Got endpoints: latency-svc-h5tcx [748.570958ms]
Feb 21 13:20:20.442: INFO: Created: latency-svc-cmx95
Feb 21 13:20:20.482: INFO: Got endpoints: latency-svc-69qrc [750.263662ms]
Feb 21 13:20:20.493: INFO: Created: latency-svc-8w29r
Feb 21 13:20:20.533: INFO: Got endpoints: latency-svc-hz67s [751.32236ms]
Feb 21 13:20:20.545: INFO: Created: latency-svc-xkqb4
Feb 21 13:20:20.583: INFO: Got endpoints: latency-svc-jwdd5 [750.083229ms]
Feb 21 13:20:20.593: INFO: Created: latency-svc-8cttx
Feb 21 13:20:20.631: INFO: Got endpoints: latency-svc-75qzp [749.061568ms]
Feb 21 13:20:20.642: INFO: Created: latency-svc-mksxg
Feb 21 13:20:20.685: INFO: Got endpoints: latency-svc-7hhdr [751.87502ms]
Feb 21 13:20:20.698: INFO: Created: latency-svc-bm92v
Feb 21 13:20:20.733: INFO: Got endpoints: latency-svc-dz5rq [747.930809ms]
Feb 21 13:20:20.744: INFO: Created: latency-svc-mpvqd
Feb 21 13:20:20.782: INFO: Got endpoints: latency-svc-p4dv5 [749.882887ms]
Feb 21 13:20:20.792: INFO: Created: latency-svc-5h9fh
Feb 21 13:20:20.833: INFO: Got endpoints: latency-svc-dh62m [749.817232ms]
Feb 21 13:20:20.845: INFO: Created: latency-svc-r6jsb
Feb 21 13:20:20.883: INFO: Got endpoints: latency-svc-vlnhv [750.715448ms]
Feb 21 13:20:20.897: INFO: Created: latency-svc-ldqng
Feb 21 13:20:20.933: INFO: Got endpoints: latency-svc-6n4fv [751.22253ms]
Feb 21 13:20:20.943: INFO: Created: latency-svc-wp4xr
Feb 21 13:20:20.983: INFO: Got endpoints: latency-svc-c225h [749.93228ms]
Feb 21 13:20:20.994: INFO: Created: latency-svc-7m2jq
Feb 21 13:20:21.032: INFO: Got endpoints: latency-svc-tgls8 [751.169149ms]
Feb 21 13:20:21.045: INFO: Created: latency-svc-7hkd2
Feb 21 13:20:21.082: INFO: Got endpoints: latency-svc-vq5b4 [748.957528ms]
Feb 21 13:20:21.093: INFO: Created: latency-svc-tkxfn
Feb 21 13:20:21.134: INFO: Got endpoints: latency-svc-qddnr [750.235123ms]
Feb 21 13:20:21.150: INFO: Created: latency-svc-fm695
Feb 21 13:20:21.182: INFO: Got endpoints: latency-svc-cmx95 [750.234341ms]
Feb 21 13:20:21.194: INFO: Created: latency-svc-rrm7k
Feb 21 13:20:21.233: INFO: Got endpoints: latency-svc-8w29r [751.026535ms]
Feb 21 13:20:21.244: INFO: Created: latency-svc-whr9m
Feb 21 13:20:21.283: INFO: Got endpoints: latency-svc-xkqb4 [750.539069ms]
Feb 21 13:20:21.294: INFO: Created: latency-svc-tb8zc
Feb 21 13:20:21.333: INFO: Got endpoints: latency-svc-8cttx [750.128158ms]
Feb 21 13:20:21.345: INFO: Created: latency-svc-hc5q8
Feb 21 13:20:21.381: INFO: Got endpoints: latency-svc-mksxg [750.30825ms]
Feb 21 13:20:21.392: INFO: Created: latency-svc-z5mpj
Feb 21 13:20:21.433: INFO: Got endpoints: latency-svc-bm92v [747.879366ms]
Feb 21 13:20:21.444: INFO: Created: latency-svc-bqwg2
Feb 21 13:20:21.483: INFO: Got endpoints: latency-svc-mpvqd [749.795485ms]
Feb 21 13:20:21.493: INFO: Created: latency-svc-9cnxr
Feb 21 13:20:21.532: INFO: Got endpoints: latency-svc-5h9fh [749.552503ms]
Feb 21 13:20:21.544: INFO: Created: latency-svc-pvcc9
Feb 21 13:20:21.582: INFO: Got endpoints: latency-svc-r6jsb [748.902537ms]
Feb 21 13:20:21.597: INFO: Created: latency-svc-k4qxp
Feb 21 13:20:21.632: INFO: Got endpoints: latency-svc-ldqng [748.939918ms]
Feb 21 13:20:21.642: INFO: Created: latency-svc-7p9kp
Feb 21 13:20:21.681: INFO: Got endpoints: latency-svc-wp4xr [747.797198ms]
Feb 21 13:20:21.693: INFO: Created: latency-svc-nds6h
Feb 21 13:20:21.733: INFO: Got endpoints: latency-svc-7m2jq [749.526111ms]
Feb 21 13:20:21.744: INFO: Created: latency-svc-5w47f
Feb 21 13:20:21.783: INFO: Got endpoints: latency-svc-7hkd2 [750.828223ms]
Feb 21 13:20:21.794: INFO: Created: latency-svc-lpcgk
Feb 21 13:20:21.834: INFO: Got endpoints: latency-svc-tkxfn [752.356476ms]
Feb 21 13:20:21.845: INFO: Created: latency-svc-t8qc4
Feb 21 13:20:21.882: INFO: Got endpoints: latency-svc-fm695 [747.839548ms]
Feb 21 13:20:21.892: INFO: Created: latency-svc-jfj9c
Feb 21 13:20:21.933: INFO: Got endpoints: latency-svc-rrm7k [750.133464ms]
Feb 21 13:20:21.943: INFO: Created: latency-svc-lfkd5
Feb 21 13:20:21.982: INFO: Got endpoints: latency-svc-whr9m [748.388912ms]
Feb 21 13:20:21.992: INFO: Created: latency-svc-9hzdf
Feb 21 13:20:22.032: INFO: Got endpoints: latency-svc-tb8zc [748.548273ms]
Feb 21 13:20:22.045: INFO: Created: latency-svc-xtwjq
Feb 21 13:20:22.083: INFO: Got endpoints: latency-svc-hc5q8 [750.034475ms]
Feb 21 13:20:22.094: INFO: Created: latency-svc-72h6d
Feb 21 13:20:22.132: INFO: Got endpoints: latency-svc-z5mpj [750.393265ms]
Feb 21 13:20:22.143: INFO: Created: latency-svc-xzklv
Feb 21 13:20:22.182: INFO: Got endpoints: latency-svc-bqwg2 [748.900953ms]
Feb 21 13:20:22.193: INFO: Created: latency-svc-xzkx6
Feb 21 13:20:22.234: INFO: Got endpoints: latency-svc-9cnxr [750.899146ms]
Feb 21 13:20:22.245: INFO: Created: latency-svc-7pd29
Feb 21 13:20:22.283: INFO: Got endpoints: latency-svc-pvcc9 [751.28615ms]
Feb 21 13:20:22.295: INFO: Created: latency-svc-s7sg8
Feb 21 13:20:22.332: INFO: Got endpoints: latency-svc-k4qxp [749.78019ms]
Feb 21 13:20:22.344: INFO: Created: latency-svc-qh6v6
Feb 21 13:20:22.383: INFO: Got endpoints: latency-svc-7p9kp [750.390918ms]
Feb 21 13:20:22.393: INFO: Created: latency-svc-7gq84
Feb 21 13:20:22.431: INFO: Got endpoints: latency-svc-nds6h [750.401148ms]
Feb 21 13:20:22.441: INFO: Created: latency-svc-wv277
Feb 21 13:20:22.482: INFO: Got endpoints: latency-svc-5w47f [749.419302ms]
Feb 21 13:20:22.495: INFO: Created: latency-svc-mdwk8
Feb 21 13:20:22.532: INFO: Got endpoints: latency-svc-lpcgk [749.102621ms]
Feb 21 13:20:22.543: INFO: Created: latency-svc-5g2s4
Feb 21 13:20:22.582: INFO: Got endpoints: latency-svc-t8qc4 [747.610516ms]
Feb 21 13:20:22.592: INFO: Created: latency-svc-6s4l4
Feb 21 13:20:22.631: INFO: Got endpoints: latency-svc-jfj9c [749.480416ms]
Feb 21 13:20:22.647: INFO: Created: latency-svc-z8zff
Feb 21 13:20:22.682: INFO: Got endpoints: latency-svc-lfkd5 [749.540357ms]
Feb 21 13:20:22.693: INFO: Created: latency-svc-k86h5
Feb 21 13:20:22.732: INFO: Got endpoints: latency-svc-9hzdf [750.423944ms]
Feb 21 13:20:22.742: INFO: Created: latency-svc-dnc6q
Feb 21 13:20:22.781: INFO: Got endpoints: latency-svc-xtwjq [749.042674ms]
Feb 21 13:20:22.793: INFO: Created: latency-svc-gq5mm
Feb 21 13:20:22.832: INFO: Got endpoints: latency-svc-72h6d [748.511613ms]
Feb 21 13:20:22.846: INFO: Created: latency-svc-7b7rf
Feb 21 13:20:22.882: INFO: Got endpoints: latency-svc-xzklv [750.440577ms]
Feb 21 13:20:22.893: INFO: Created: latency-svc-sskvz
Feb 21 13:20:22.932: INFO: Got endpoints: latency-svc-xzkx6 [750.114237ms]
Feb 21 13:20:22.943: INFO: Created: latency-svc-4qblg
Feb 21 13:20:22.985: INFO: Got endpoints: latency-svc-7pd29 [750.735037ms]
Feb 21 13:20:23.000: INFO: Created: latency-svc-5p7fp
Feb 21 13:20:23.033: INFO: Got endpoints: latency-svc-s7sg8 [749.402956ms]
Feb 21 13:20:23.044: INFO: Created: latency-svc-ntm8x
Feb 21 13:20:23.081: INFO: Got endpoints: latency-svc-qh6v6 [749.150206ms]
Feb 21 13:20:23.093: INFO: Created: latency-svc-rs42b
Feb 21 13:20:23.132: INFO: Got endpoints: latency-svc-7gq84 [748.81605ms]
Feb 21 13:20:23.143: INFO: Created: latency-svc-x8nkk
Feb 21 13:20:23.183: INFO: Got endpoints: latency-svc-wv277 [751.580601ms]
Feb 21 13:20:23.194: INFO: Created: latency-svc-qffhd
Feb 21 13:20:23.231: INFO: Got endpoints: latency-svc-mdwk8 [749.06357ms]
Feb 21 13:20:23.244: INFO: Created: latency-svc-n9fjn
Feb 21 13:20:23.284: INFO: Got endpoints: latency-svc-5g2s4 [751.831764ms]
Feb 21 13:20:23.295: INFO: Created: latency-svc-n6kjl
Feb 21 13:20:23.333: INFO: Got endpoints: latency-svc-6s4l4 [751.485056ms]
Feb 21 13:20:23.344: INFO: Created: latency-svc-28vqr
Feb 21 13:20:23.382: INFO: Got endpoints: latency-svc-z8zff [750.443007ms]
Feb 21 13:20:23.394: INFO: Created: latency-svc-hqrqz
Feb 21 13:20:23.432: INFO: Got endpoints: latency-svc-k86h5 [749.494164ms]
Feb 21 13:20:23.442: INFO: Created: latency-svc-gnm54
Feb 21 13:20:23.483: INFO: Got endpoints: latency-svc-dnc6q [750.491608ms]
Feb 21 13:20:23.494: INFO: Created: latency-svc-55tsn
Feb 21 13:20:23.531: INFO: Got endpoints: latency-svc-gq5mm [749.557403ms]
Feb 21 13:20:23.542: INFO: Created: latency-svc-wtl89
Feb 21 13:20:23.582: INFO: Got endpoints: latency-svc-7b7rf [750.306334ms]
Feb 21 13:20:23.592: INFO: Created: latency-svc-nb695
Feb 21 13:20:23.633: INFO: Got endpoints: latency-svc-sskvz [750.418737ms]
Feb 21 13:20:23.644: INFO: Created: latency-svc-2ggnd
Feb 21 13:20:23.683: INFO: Got endpoints: latency-svc-4qblg [751.227432ms]
Feb 21 13:20:23.696: INFO: Created: latency-svc-rb6zc
Feb 21 13:20:23.732: INFO: Got endpoints: latency-svc-5p7fp [747.281508ms]
Feb 21 13:20:23.743: INFO: Created: latency-svc-ztpvs
Feb 21 13:20:23.782: INFO: Got endpoints: latency-svc-ntm8x [748.987438ms]
Feb 21 13:20:23.794: INFO: Created: latency-svc-2tpgx
Feb 21 13:20:23.832: INFO: Got endpoints: latency-svc-rs42b [751.326299ms]
Feb 21 13:20:23.845: INFO: Created: latency-svc-ww667
Feb 21 13:20:23.883: INFO: Got endpoints: latency-svc-x8nkk [751.413585ms]
Feb 21 13:20:23.894: INFO: Created: latency-svc-bc6mm
Feb 21 13:20:23.932: INFO: Got endpoints: latency-svc-qffhd [748.750217ms]
Feb 21 13:20:23.943: INFO: Created: latency-svc-klz24
Feb 21 13:20:23.981: INFO: Got endpoints: latency-svc-n9fjn [749.882959ms]
Feb 21 13:20:23.993: INFO: Created: latency-svc-rf42d
Feb 21 13:20:24.033: INFO: Got endpoints: latency-svc-n6kjl [748.906344ms]
Feb 21 13:20:24.044: INFO: Created: latency-svc-pbqkc
Feb 21 13:20:24.082: INFO: Got endpoints: latency-svc-28vqr [748.410764ms]
Feb 21 13:20:24.092: INFO: Created: latency-svc-4h7lf
Feb 21 13:20:24.133: INFO: Got endpoints: latency-svc-hqrqz [751.199198ms]
Feb 21 13:20:24.144: INFO: Created: latency-svc-rgtds
Feb 21 13:20:24.183: INFO: Got endpoints: latency-svc-gnm54 [750.895231ms]
Feb 21 13:20:24.194: INFO: Created: latency-svc-h648g
Feb 21 13:20:24.232: INFO: Got endpoints: latency-svc-55tsn [749.77287ms]
Feb 21 13:20:24.244: INFO: Created: latency-svc-6qrcg
Feb 21 13:20:24.282: INFO: Got endpoints: latency-svc-wtl89 [750.892645ms]
Feb 21 13:20:24.293: INFO: Created: latency-svc-bhqn5
Feb 21 13:20:24.333: INFO: Got endpoints: latency-svc-nb695 [750.506431ms]
Feb 21 13:20:24.343: INFO: Created: latency-svc-kd8v4
Feb 21 13:20:24.383: INFO: Got endpoints: latency-svc-2ggnd [749.991495ms]
Feb 21 13:20:24.393: INFO: Created: latency-svc-cjnxz
Feb 21 13:20:24.431: INFO: Got endpoints: latency-svc-rb6zc [748.12629ms]
Feb 21 13:20:24.443: INFO: Created: latency-svc-w5hb2
Feb 21 13:20:24.482: INFO: Got endpoints: latency-svc-ztpvs [750.344433ms]
Feb 21 13:20:24.494: INFO: Created: latency-svc-sthxd
Feb 21 13:20:24.534: INFO: Got endpoints: latency-svc-2tpgx [751.876783ms]
Feb 21 13:20:24.544: INFO: Created: latency-svc-nkw97
Feb 21 13:20:24.584: INFO: Got endpoints: latency-svc-ww667 [751.197572ms]
Feb 21 13:20:24.594: INFO: Created: latency-svc-5wfhs
Feb 21 13:20:24.633: INFO: Got endpoints: latency-svc-bc6mm [750.104366ms]
Feb 21 13:20:24.645: INFO: Created: latency-svc-bxt5j
Feb 21 13:20:24.682: INFO: Got endpoints: latency-svc-klz24 [750.213542ms]
Feb 21 13:20:24.692: INFO: Created: latency-svc-mg2nl
Feb 21 13:20:24.732: INFO: Got endpoints: latency-svc-rf42d [750.351944ms]
Feb 21 13:20:24.744: INFO: Created: latency-svc-hsxkw
Feb 21 13:20:24.783: INFO: Got endpoints: latency-svc-pbqkc [749.20221ms]
Feb 21 13:20:24.794: INFO: Created: latency-svc-nsv8h
Feb 21 13:20:24.833: INFO: Got endpoints: latency-svc-4h7lf [750.874015ms]
Feb 21 13:20:24.851: INFO: Created: latency-svc-kbhrz
Feb 21 13:20:24.882: INFO: Got endpoints: latency-svc-rgtds [748.810406ms]
Feb 21 13:20:24.893: INFO: Created: latency-svc-99hqc
Feb 21 13:20:24.932: INFO: Got endpoints: latency-svc-h648g [749.337067ms]
Feb 21 13:20:24.943: INFO: Created: latency-svc-s5ghm
Feb 21 13:20:24.982: INFO: Got endpoints: latency-svc-6qrcg [749.209594ms]
Feb 21 13:20:24.993: INFO: Created: latency-svc-s74mb
Feb 21 13:20:25.034: INFO: Got endpoints: latency-svc-bhqn5 [752.062612ms]
Feb 21 13:20:25.047: INFO: Created: latency-svc-wn4b8
Feb 21 13:20:25.083: INFO: Got endpoints: latency-svc-kd8v4 [749.956943ms]
Feb 21 13:20:25.096: INFO: Created: latency-svc-7h6km
Feb 21 13:20:25.132: INFO: Got endpoints: latency-svc-cjnxz [749.020072ms]
Feb 21 13:20:25.143: INFO: Created: latency-svc-sgdch
Feb 21 13:20:25.182: INFO: Got endpoints: latency-svc-w5hb2 [751.09363ms]
Feb 21 13:20:25.193: INFO: Created: latency-svc-jsbp4
Feb 21 13:20:25.233: INFO: Got endpoints: latency-svc-sthxd [750.731752ms]
Feb 21 13:20:25.246: INFO: Created: latency-svc-n95zq
Feb 21 13:20:25.282: INFO: Got endpoints: latency-svc-nkw97 [748.343656ms]
Feb 21 13:20:25.292: INFO: Created: latency-svc-9bhnq
Feb 21 13:20:25.332: INFO: Got endpoints: latency-svc-5wfhs [748.486687ms]
Feb 21 13:20:25.343: INFO: Created: latency-svc-7sr85
Feb 21 13:20:25.383: INFO: Got endpoints: latency-svc-bxt5j [749.932553ms]
Feb 21 13:20:25.394: INFO: Created: latency-svc-vqcz6
Feb 21 13:20:25.432: INFO: Got endpoints: latency-svc-mg2nl [749.598969ms]
Feb 21 13:20:25.444: INFO: Created: latency-svc-4v2ts
Feb 21 13:20:25.481: INFO: Got endpoints: latency-svc-hsxkw [749.482397ms]
Feb 21 13:20:25.492: INFO: Created: latency-svc-w9fwm
Feb 21 13:20:25.533: INFO: Got endpoints: latency-svc-nsv8h [750.469352ms]
Feb 21 13:20:25.544: INFO: Created: latency-svc-lg5gv
Feb 21 13:20:25.581: INFO: Got endpoints: latency-svc-kbhrz [748.443462ms]
Feb 21 13:20:25.592: INFO: Created: latency-svc-nfxbb
Feb 21 13:20:25.633: INFO: Got endpoints: latency-svc-99hqc [750.921157ms]
Feb 21 13:20:25.643: INFO: Created: latency-svc-zpsqm
Feb 21 13:20:25.683: INFO: Got endpoints: latency-svc-s5ghm [751.043078ms]
Feb 21 13:20:25.694: INFO: Created: latency-svc-j25bz
Feb 21 13:20:25.732: INFO: Got endpoints: latency-svc-s74mb [749.968728ms]
Feb 21 13:20:25.743: INFO: Created: latency-svc-mv6q2
Feb 21 13:20:25.782: INFO: Got endpoints: latency-svc-wn4b8 [747.967668ms]
Feb 21 13:20:25.793: INFO: Created: latency-svc-bvbtc
Feb 21 13:20:25.832: INFO: Got endpoints: latency-svc-7h6km [749.552908ms]
Feb 21 13:20:25.881: INFO: Got endpoints: latency-svc-sgdch [749.221878ms]
Feb 21 13:20:25.932: INFO: Got endpoints: latency-svc-jsbp4 [750.139431ms]
Feb 21 13:20:25.981: INFO: Got endpoints: latency-svc-n95zq [748.027412ms]
Feb 21 13:20:26.034: INFO: Got endpoints: latency-svc-9bhnq [751.574197ms]
Feb 21 13:20:26.083: INFO: Got endpoints: latency-svc-7sr85 [750.577223ms]
Feb 21 13:20:26.132: INFO: Got endpoints: latency-svc-vqcz6 [748.189326ms]
Feb 21 13:20:26.182: INFO: Got endpoints: latency-svc-4v2ts [749.945631ms]
Feb 21 13:20:26.232: INFO: Got endpoints: latency-svc-w9fwm [750.386276ms]
Feb 21 13:20:26.283: INFO: Got endpoints: latency-svc-lg5gv [750.420952ms]
Feb 21 13:20:26.331: INFO: Got endpoints: latency-svc-nfxbb [749.578922ms]
Feb 21 13:20:26.383: INFO: Got endpoints: latency-svc-zpsqm [750.756098ms]
Feb 21 13:20:26.432: INFO: Got endpoints: latency-svc-j25bz [748.577459ms]
Feb 21 13:20:26.482: INFO: Got endpoints: latency-svc-mv6q2 [749.933955ms]
Feb 21 13:20:26.533: INFO: Got endpoints: latency-svc-bvbtc [751.069351ms]
Feb 21 13:20:26.533: INFO: Latencies: [16.551212ms 23.765824ms 31.35694ms 39.416387ms 49.882904ms 56.859562ms 65.527865ms 76.549337ms 83.523522ms 92.490992ms 100.149025ms 110.387979ms 118.284596ms 127.63334ms 129.013643ms 132.727505ms 133.499922ms 134.844594ms 135.068019ms 136.299088ms 136.778919ms 137.286352ms 137.782134ms 137.991852ms 138.80654ms 140.460881ms 141.741281ms 141.954748ms 142.221585ms 142.427716ms 142.616544ms 142.66678ms 143.402337ms 146.338985ms 152.749087ms 156.869752ms 197.799664ms 240.715065ms 294.738271ms 314.998077ms 355.786077ms 396.215149ms 439.218966ms 479.62905ms 516.958332ms 551.054235ms 601.162446ms 642.300212ms 687.493339ms 724.745594ms 727.920211ms 746.108718ms 747.281508ms 747.610516ms 747.797198ms 747.839548ms 747.879366ms 747.930809ms 747.933232ms 747.967668ms 748.027412ms 748.12629ms 748.189326ms 748.343656ms 748.388912ms 748.410764ms 748.443462ms 748.486687ms 748.511613ms 748.548273ms 748.570958ms 748.577459ms 748.585228ms 748.638188ms 748.740691ms 748.750217ms 748.810406ms 748.81605ms 748.849723ms 748.864891ms 748.900953ms 748.902537ms 748.906344ms 748.939918ms 748.957528ms 748.987438ms 749.020072ms 749.042674ms 749.061568ms 749.06357ms 749.102621ms 749.114003ms 749.150206ms 749.154463ms 749.182031ms 749.20221ms 749.209594ms 749.221878ms 749.337067ms 749.355888ms 749.402956ms 749.414643ms 749.419302ms 749.455115ms 749.480416ms 749.482397ms 749.494164ms 749.514503ms 749.526111ms 749.540357ms 749.544071ms 749.552503ms 749.552908ms 749.557403ms 749.578922ms 749.598969ms 749.766479ms 749.77287ms 749.78019ms 749.795485ms 749.817232ms 749.822016ms 749.882887ms 749.882959ms 749.93228ms 749.932553ms 749.933955ms 749.945631ms 749.956943ms 749.968728ms 749.991495ms 750.034475ms 750.083229ms 750.104366ms 750.114237ms 750.128158ms 750.133464ms 750.139431ms 750.210166ms 750.213542ms 750.234341ms 750.235123ms 750.263662ms 750.306334ms 750.30825ms 750.344433ms 750.351944ms 750.386276ms 750.390918ms 750.393265ms 750.401148ms 750.418737ms 750.420952ms 750.423944ms 750.440577ms 750.443007ms 750.469352ms 750.491608ms 750.506431ms 750.539069ms 750.577223ms 750.594492ms 750.715448ms 750.731752ms 750.735037ms 750.756098ms 750.828223ms 750.874015ms 750.892645ms 750.895231ms 750.899146ms 750.921157ms 751.018641ms 751.026535ms 751.043078ms 751.057705ms 751.069351ms 751.09363ms 751.169149ms 751.197572ms 751.199198ms 751.22253ms 751.227432ms 751.28615ms 751.32236ms 751.326299ms 751.328009ms 751.379479ms 751.413585ms 751.485056ms 751.574197ms 751.580601ms 751.659565ms 751.831764ms 751.864282ms 751.87502ms 751.876783ms 752.062612ms 752.356476ms 753.734939ms]
Feb 21 13:20:26.533: INFO: 50 %ile: 749.402956ms
Feb 21 13:20:26.533: INFO: 90 %ile: 751.199198ms
Feb 21 13:20:26.533: INFO: 99 %ile: 752.356476ms
Feb 21 13:20:26.533: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:20:26.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1689" for this suite.

• [SLOW TEST:9.870 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":22,"skipped":459,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:20:26.546: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5937
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-5937
Feb 21 13:20:26.693: INFO: Found 0 stateful pods, waiting for 1
Feb 21 13:20:36.704: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 13:20:36.728: INFO: Deleting all statefulset in ns statefulset-5937
Feb 21 13:20:36.730: INFO: Scaling statefulset ss to 0
Feb 21 13:20:46.748: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:20:46.751: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:20:46.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5937" for this suite.

• [SLOW TEST:20.227 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":23,"skipped":464,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:20:46.774: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3966
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:03.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3966" for this suite.

• [SLOW TEST:16.267 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":24,"skipped":475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:03.041: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-9400
STEP: creating replication controller nodeport-test in namespace services-9400
I0221 13:21:03.196457      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9400, replica count: 2
Feb 21 13:21:06.248: INFO: Creating new exec pod
I0221 13:21:06.247970      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 13:21:09.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9400 exec execpod4wlj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 21 13:21:09.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 21 13:21:09.389: INFO: stdout: "nodeport-test-qwpkf"
Feb 21 13:21:09.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9400 exec execpod4wlj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.94.167 80'
Feb 21 13:21:09.514: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.94.167 80\nConnection to 10.105.94.167 80 port [tcp/http] succeeded!\n"
Feb 21 13:21:09.514: INFO: stdout: ""
Feb 21 13:21:10.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9400 exec execpod4wlj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.94.167 80'
Feb 21 13:21:10.634: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.94.167 80\nConnection to 10.105.94.167 80 port [tcp/http] succeeded!\n"
Feb 21 13:21:10.634: INFO: stdout: "nodeport-test-b4vkk"
Feb 21 13:21:10.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9400 exec execpod4wlj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.54 30949'
Feb 21 13:21:10.729: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.54 30949\nConnection to 10.0.0.54 30949 port [tcp/*] succeeded!\n"
Feb 21 13:21:10.729: INFO: stdout: "nodeport-test-qwpkf"
Feb 21 13:21:10.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9400 exec execpod4wlj8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.211 30949'
Feb 21 13:21:10.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.211 30949\nConnection to 10.0.0.211 30949 port [tcp/*] succeeded!\n"
Feb 21 13:21:10.828: INFO: stdout: "nodeport-test-b4vkk"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:10.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9400" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.798 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":25,"skipped":514,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:10.839: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1737
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 21 13:21:15.511: INFO: Successfully updated pod "adopt-release-8lc67"
STEP: Checking that the Job readopts the Pod
Feb 21 13:21:15.511: INFO: Waiting up to 15m0s for pod "adopt-release-8lc67" in namespace "job-1737" to be "adopted"
Feb 21 13:21:15.514: INFO: Pod "adopt-release-8lc67": Phase="Running", Reason="", readiness=true. Elapsed: 2.690335ms
Feb 21 13:21:17.519: INFO: Pod "adopt-release-8lc67": Phase="Running", Reason="", readiness=true. Elapsed: 2.008120325s
Feb 21 13:21:17.519: INFO: Pod "adopt-release-8lc67" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 21 13:21:18.036: INFO: Successfully updated pod "adopt-release-8lc67"
STEP: Checking that the Job releases the Pod
Feb 21 13:21:18.036: INFO: Waiting up to 15m0s for pod "adopt-release-8lc67" in namespace "job-1737" to be "released"
Feb 21 13:21:18.038: INFO: Pod "adopt-release-8lc67": Phase="Running", Reason="", readiness=true. Elapsed: 2.402768ms
Feb 21 13:21:20.045: INFO: Pod "adopt-release-8lc67": Phase="Running", Reason="", readiness=true. Elapsed: 2.009281853s
Feb 21 13:21:20.045: INFO: Pod "adopt-release-8lc67" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:20.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1737" for this suite.

• [SLOW TEST:9.215 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":26,"skipped":517,"failed":0}
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:20.055: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9989
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Feb 21 13:21:20.199: INFO: Waiting up to 5m0s for pod "client-containers-9a5da620-b894-4f73-b200-cc68d3be6640" in namespace "containers-9989" to be "Succeeded or Failed"
Feb 21 13:21:20.203: INFO: Pod "client-containers-9a5da620-b894-4f73-b200-cc68d3be6640": Phase="Pending", Reason="", readiness=false. Elapsed: 4.239922ms
Feb 21 13:21:22.210: INFO: Pod "client-containers-9a5da620-b894-4f73-b200-cc68d3be6640": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011012082s
STEP: Saw pod success
Feb 21 13:21:22.210: INFO: Pod "client-containers-9a5da620-b894-4f73-b200-cc68d3be6640" satisfied condition "Succeeded or Failed"
Feb 21 13:21:22.213: INFO: Trying to get logs from node aksh-cncf-3 pod client-containers-9a5da620-b894-4f73-b200-cc68d3be6640 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:21:22.235: INFO: Waiting for pod client-containers-9a5da620-b894-4f73-b200-cc68d3be6640 to disappear
Feb 21 13:21:22.237: INFO: Pod client-containers-9a5da620-b894-4f73-b200-cc68d3be6640 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:22.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9989" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":517,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:22.245: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-d5f4c76d-3be2-4bce-a738-cffacc008f5e
STEP: Creating a pod to test consume secrets
Feb 21 13:21:22.395: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a34d227-cd64-4fde-9756-28d79ac502da" in namespace "projected-3404" to be "Succeeded or Failed"
Feb 21 13:21:22.397: INFO: Pod "pod-projected-secrets-4a34d227-cd64-4fde-9756-28d79ac502da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.222005ms
Feb 21 13:21:24.403: INFO: Pod "pod-projected-secrets-4a34d227-cd64-4fde-9756-28d79ac502da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008492763s
STEP: Saw pod success
Feb 21 13:21:24.403: INFO: Pod "pod-projected-secrets-4a34d227-cd64-4fde-9756-28d79ac502da" satisfied condition "Succeeded or Failed"
Feb 21 13:21:24.405: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-secrets-4a34d227-cd64-4fde-9756-28d79ac502da container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 13:21:24.423: INFO: Waiting for pod pod-projected-secrets-4a34d227-cd64-4fde-9756-28d79ac502da to disappear
Feb 21 13:21:24.425: INFO: Pod pod-projected-secrets-4a34d227-cd64-4fde-9756-28d79ac502da no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:24.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3404" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":534,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:24.432: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9103
STEP: creating service affinity-clusterip in namespace services-9103
STEP: creating replication controller affinity-clusterip in namespace services-9103
I0221 13:21:24.583492      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9103, replica count: 3
I0221 13:21:27.634527      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 13:21:30.634924      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 13:21:30.640: INFO: Creating new exec pod
Feb 21 13:21:33.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9103 exec execpod-affinityh69vc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb 21 13:21:33.756: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 21 13:21:33.756: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:21:33.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9103 exec execpod-affinityh69vc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.3.170 80'
Feb 21 13:21:33.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.3.170 80\nConnection to 10.106.3.170 80 port [tcp/http] succeeded!\n"
Feb 21 13:21:33.860: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:21:33.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-9103 exec execpod-affinityh69vc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.3.170:80/ ; done'
Feb 21 13:21:33.995: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.3.170:80/\n"
Feb 21 13:21:33.995: INFO: stdout: "\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4\naffinity-clusterip-p48c4"
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Received response from host: affinity-clusterip-p48c4
Feb 21 13:21:33.995: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9103, will wait for the garbage collector to delete the pods
Feb 21 13:21:34.068: INFO: Deleting ReplicationController affinity-clusterip took: 5.864709ms
Feb 21 13:21:34.168: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.1635ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:38.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9103" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.662 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":29,"skipped":543,"failed":0}
SSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:38.094: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Feb 21 13:21:40.251: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Feb 21 13:21:44.330: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:46.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2289" for this suite.

• [SLOW TEST:8.279 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":30,"skipped":547,"failed":0}
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:46.374: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9018
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-20922e36-72a4-4a02-a88b-1e27edc293e6
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:48.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9018" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":547,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:48.547: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-9699
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Feb 21 13:21:48.703: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Feb 21 13:21:48.720: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:48.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9699" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":32,"skipped":563,"failed":0}
S
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:48.748: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Feb 21 13:21:48.895: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:21:50.902: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Feb 21 13:21:50.914: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:21:52.919: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:21:54.922: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:21:56.920: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 21 13:21:56.922: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:56.922: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:56.923: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:56.923: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:56.996: INFO: Exec stderr: ""
Feb 21 13:21:56.996: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:56.996: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:56.996: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:56.996: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.034: INFO: Exec stderr: ""
Feb 21 13:21:57.035: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.035: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.035: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.035: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.073: INFO: Exec stderr: ""
Feb 21 13:21:57.073: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.073: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.074: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.074: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.144: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 21 13:21:57.144: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.144: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.144: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.144: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.206: INFO: Exec stderr: ""
Feb 21 13:21:57.206: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.206: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.206: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.242: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 21 13:21:57.242: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.242: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.243: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.243: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.298: INFO: Exec stderr: ""
Feb 21 13:21:57.298: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.298: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.298: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.298: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.351: INFO: Exec stderr: ""
Feb 21 13:21:57.351: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.351: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.352: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.352: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.391: INFO: Exec stderr: ""
Feb 21 13:21:57.391: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9005 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:21:57.391: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:21:57.391: INFO: ExecWithOptions: Clientset creation
Feb 21 13:21:57.391: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9005/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:21:57.429: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:21:57.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9005" for this suite.

• [SLOW TEST:8.694 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":564,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:21:57.442: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9350
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 13:21:57.585: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 21 13:21:57.630: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:21:59.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:01.635: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:03.636: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:05.634: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:07.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:09.635: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:11.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:13.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:15.634: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:22:17.633: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 21 13:22:17.640: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 21 13:22:19.644: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 21 13:22:19.649: INFO: The status of Pod netserver-2 is Running (Ready = true)
Feb 21 13:22:19.653: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Feb 21 13:22:21.669: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 21 13:22:21.669: INFO: Breadth first check of 10.244.0.22 on host 10.0.0.54...
Feb 21 13:22:21.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.34:9080/dial?request=hostname&protocol=udp&host=10.244.0.22&port=8081&tries=1'] Namespace:pod-network-test-9350 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:22:21.671: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:22:21.671: INFO: ExecWithOptions: Clientset creation
Feb 21 13:22:21.671: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9350/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.22%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:22:21.744: INFO: Waiting for responses: map[]
Feb 21 13:22:21.744: INFO: reached 10.244.0.22 after 0/1 tries
Feb 21 13:22:21.744: INFO: Breadth first check of 10.244.2.11 on host 10.0.0.211...
Feb 21 13:22:21.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.34:9080/dial?request=hostname&protocol=udp&host=10.244.2.11&port=8081&tries=1'] Namespace:pod-network-test-9350 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:22:21.746: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:22:21.747: INFO: ExecWithOptions: Clientset creation
Feb 21 13:22:21.747: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9350/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.11%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:22:21.782: INFO: Waiting for responses: map[]
Feb 21 13:22:21.782: INFO: reached 10.244.2.11 after 0/1 tries
Feb 21 13:22:21.782: INFO: Breadth first check of 10.244.1.33 on host 10.0.0.131...
Feb 21 13:22:21.784: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.34:9080/dial?request=hostname&protocol=udp&host=10.244.1.33&port=8081&tries=1'] Namespace:pod-network-test-9350 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:22:21.784: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:22:21.785: INFO: ExecWithOptions: Clientset creation
Feb 21 13:22:21.785: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9350/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.33%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:22:21.845: INFO: Waiting for responses: map[]
Feb 21 13:22:21.845: INFO: reached 10.244.1.33 after 0/1 tries
Feb 21 13:22:21.845: INFO: Breadth first check of 10.244.3.17 on host 10.0.0.114...
Feb 21 13:22:21.847: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.34:9080/dial?request=hostname&protocol=udp&host=10.244.3.17&port=8081&tries=1'] Namespace:pod-network-test-9350 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:22:21.847: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:22:21.848: INFO: ExecWithOptions: Clientset creation
Feb 21 13:22:21.848: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9350/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.34%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.17%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:22:21.887: INFO: Waiting for responses: map[]
Feb 21 13:22:21.887: INFO: reached 10.244.3.17 after 0/1 tries
Feb 21 13:22:21.887: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:21.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9350" for this suite.

• [SLOW TEST:24.454 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":589,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:21.896: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4377
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 21 13:22:22.041: INFO: Waiting up to 5m0s for pod "pod-a270130b-6b07-4f20-8b05-9ae6d67fa9b5" in namespace "emptydir-4377" to be "Succeeded or Failed"
Feb 21 13:22:22.043: INFO: Pod "pod-a270130b-6b07-4f20-8b05-9ae6d67fa9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035049ms
Feb 21 13:22:24.049: INFO: Pod "pod-a270130b-6b07-4f20-8b05-9ae6d67fa9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008258863s
STEP: Saw pod success
Feb 21 13:22:24.049: INFO: Pod "pod-a270130b-6b07-4f20-8b05-9ae6d67fa9b5" satisfied condition "Succeeded or Failed"
Feb 21 13:22:24.052: INFO: Trying to get logs from node aksh-cncf-1 pod pod-a270130b-6b07-4f20-8b05-9ae6d67fa9b5 container test-container: <nil>
STEP: delete the pod
Feb 21 13:22:24.074: INFO: Waiting for pod pod-a270130b-6b07-4f20-8b05-9ae6d67fa9b5 to disappear
Feb 21 13:22:24.076: INFO: Pod pod-a270130b-6b07-4f20-8b05-9ae6d67fa9b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:24.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4377" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":35,"skipped":599,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:24.084: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Feb 21 13:22:24.225: INFO: Waiting up to 5m0s for pod "var-expansion-05b6998c-ec5c-4e3c-953c-355887413f7f" in namespace "var-expansion-3206" to be "Succeeded or Failed"
Feb 21 13:22:24.227: INFO: Pod "var-expansion-05b6998c-ec5c-4e3c-953c-355887413f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.934438ms
Feb 21 13:22:26.232: INFO: Pod "var-expansion-05b6998c-ec5c-4e3c-953c-355887413f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007578815s
STEP: Saw pod success
Feb 21 13:22:26.232: INFO: Pod "var-expansion-05b6998c-ec5c-4e3c-953c-355887413f7f" satisfied condition "Succeeded or Failed"
Feb 21 13:22:26.235: INFO: Trying to get logs from node aksh-cncf-1 pod var-expansion-05b6998c-ec5c-4e3c-953c-355887413f7f container dapi-container: <nil>
STEP: delete the pod
Feb 21 13:22:26.248: INFO: Waiting for pod var-expansion-05b6998c-ec5c-4e3c-953c-355887413f7f to disappear
Feb 21 13:22:26.250: INFO: Pod var-expansion-05b6998c-ec5c-4e3c-953c-355887413f7f no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:26.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3206" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":606,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:26.257: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 21 13:22:26.404: INFO: Waiting up to 5m0s for pod "pod-ad607e93-583e-4c06-92ba-d8e50df6891c" in namespace "emptydir-4690" to be "Succeeded or Failed"
Feb 21 13:22:26.406: INFO: Pod "pod-ad607e93-583e-4c06-92ba-d8e50df6891c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.952992ms
Feb 21 13:22:28.410: INFO: Pod "pod-ad607e93-583e-4c06-92ba-d8e50df6891c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006574683s
STEP: Saw pod success
Feb 21 13:22:28.410: INFO: Pod "pod-ad607e93-583e-4c06-92ba-d8e50df6891c" satisfied condition "Succeeded or Failed"
Feb 21 13:22:28.413: INFO: Trying to get logs from node aksh-cncf-1 pod pod-ad607e93-583e-4c06-92ba-d8e50df6891c container test-container: <nil>
STEP: delete the pod
Feb 21 13:22:28.427: INFO: Waiting for pod pod-ad607e93-583e-4c06-92ba-d8e50df6891c to disappear
Feb 21 13:22:28.429: INFO: Pod pod-ad607e93-583e-4c06-92ba-d8e50df6891c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:28.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4690" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:28.437: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8988
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-90030637-b749-42ce-83cd-c418a4dff6a2
STEP: Creating the pod
Feb 21 13:22:28.593: INFO: The status of Pod pod-projected-configmaps-40fb6111-217a-45d7-b586-7d69e79f2391 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:22:30.598: INFO: The status of Pod pod-projected-configmaps-40fb6111-217a-45d7-b586-7d69e79f2391 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-90030637-b749-42ce-83cd-c418a4dff6a2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:32.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8988" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:32.634: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Feb 21 13:22:43.257: INFO: 73 pods remaining
Feb 21 13:22:43.257: INFO: 73 pods has nil DeletionTimestamp
Feb 21 13:22:43.257: INFO: 
STEP: Gathering metrics
Feb 21 13:22:48.266: INFO: The status of Pod kube-controller-manager-aksh-cncf-3 is Running (Ready = true)
Feb 21 13:22:48.298: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 21 13:22:48.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-2frsl" in namespace "gc-9055"
Feb 21 13:22:48.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qkkc" in namespace "gc-9055"
Feb 21 13:22:48.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tdt4" in namespace "gc-9055"
Feb 21 13:22:48.328: INFO: Deleting pod "simpletest-rc-to-be-deleted-4z6gk" in namespace "gc-9055"
Feb 21 13:22:48.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-56pkz" in namespace "gc-9055"
Feb 21 13:22:48.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-592vn" in namespace "gc-9055"
Feb 21 13:22:48.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hqtw" in namespace "gc-9055"
Feb 21 13:22:48.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jhvn" in namespace "gc-9055"
Feb 21 13:22:48.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ks2l" in namespace "gc-9055"
Feb 21 13:22:48.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ttk5" in namespace "gc-9055"
Feb 21 13:22:48.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xlv4" in namespace "gc-9055"
Feb 21 13:22:48.437: INFO: Deleting pod "simpletest-rc-to-be-deleted-697nf" in namespace "gc-9055"
Feb 21 13:22:48.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cb7x" in namespace "gc-9055"
Feb 21 13:22:48.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gb4j" in namespace "gc-9055"
Feb 21 13:22:48.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lngp" in namespace "gc-9055"
Feb 21 13:22:48.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zgt6" in namespace "gc-9055"
Feb 21 13:22:48.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-757v9" in namespace "gc-9055"
Feb 21 13:22:48.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dwgm" in namespace "gc-9055"
Feb 21 13:22:48.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nhdn" in namespace "gc-9055"
Feb 21 13:22:48.545: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pfr8" in namespace "gc-9055"
Feb 21 13:22:48.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-7s9xp" in namespace "gc-9055"
Feb 21 13:22:48.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kfhf" in namespace "gc-9055"
Feb 21 13:22:48.582: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ltxv" in namespace "gc-9055"
Feb 21 13:22:48.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ncxr" in namespace "gc-9055"
Feb 21 13:22:48.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zp4d" in namespace "gc-9055"
Feb 21 13:22:48.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hjsm" in namespace "gc-9055"
Feb 21 13:22:48.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j5kg" in namespace "gc-9055"
Feb 21 13:22:48.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6rc7" in namespace "gc-9055"
Feb 21 13:22:48.663: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw6dp" in namespace "gc-9055"
Feb 21 13:22:48.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx9nc" in namespace "gc-9055"
Feb 21 13:22:48.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-c98xq" in namespace "gc-9055"
Feb 21 13:22:48.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqgff" in namespace "gc-9055"
Feb 21 13:22:48.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsmrs" in namespace "gc-9055"
Feb 21 13:22:48.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbrmv" in namespace "gc-9055"
Feb 21 13:22:48.745: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvqhd" in namespace "gc-9055"
Feb 21 13:22:48.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-g55hd" in namespace "gc-9055"
Feb 21 13:22:48.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbrwx" in namespace "gc-9055"
Feb 21 13:22:48.781: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdchj" in namespace "gc-9055"
Feb 21 13:22:48.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-grdtm" in namespace "gc-9055"
Feb 21 13:22:48.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtjlm" in namespace "gc-9055"
Feb 21 13:22:48.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxnzm" in namespace "gc-9055"
Feb 21 13:22:48.832: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4xtp" in namespace "gc-9055"
Feb 21 13:22:48.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6srx" in namespace "gc-9055"
Feb 21 13:22:48.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9g85" in namespace "gc-9055"
Feb 21 13:22:48.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-hptc6" in namespace "gc-9055"
Feb 21 13:22:48.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpwv7" in namespace "gc-9055"
Feb 21 13:22:48.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvgtk" in namespace "gc-9055"
Feb 21 13:22:48.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-jn62q" in namespace "gc-9055"
Feb 21 13:22:48.936: INFO: Deleting pod "simpletest-rc-to-be-deleted-jx88p" in namespace "gc-9055"
Feb 21 13:22:48.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-k7qbf" in namespace "gc-9055"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:48.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9055" for this suite.

• [SLOW TEST:16.341 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":39,"skipped":694,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:48.975: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 13:22:49.549: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 21 13:22:51.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 13, 22, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 22, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 22, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 22, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 13:22:54.578: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 21 13:22:56.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=webhook-4622 attach --namespace=webhook-4622 to-be-attached-pod -i -c=container1'
Feb 21 13:22:56.662: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:56.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4622" for this suite.
STEP: Destroying namespace "webhook-4622-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.738 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":40,"skipped":704,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:56.714: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3097/configmap-test-24469e89-30ab-49cd-ad5e-f0c8f5507533
STEP: Creating a pod to test consume configMaps
Feb 21 13:22:56.863: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e510777-7fc5-4742-aea5-05c1365705f9" in namespace "configmap-3097" to be "Succeeded or Failed"
Feb 21 13:22:56.868: INFO: Pod "pod-configmaps-2e510777-7fc5-4742-aea5-05c1365705f9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.271892ms
Feb 21 13:22:58.875: INFO: Pod "pod-configmaps-2e510777-7fc5-4742-aea5-05c1365705f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011958867s
STEP: Saw pod success
Feb 21 13:22:58.875: INFO: Pod "pod-configmaps-2e510777-7fc5-4742-aea5-05c1365705f9" satisfied condition "Succeeded or Failed"
Feb 21 13:22:58.876: INFO: Trying to get logs from node aksh-cncf-3 pod pod-configmaps-2e510777-7fc5-4742-aea5-05c1365705f9 container env-test: <nil>
STEP: delete the pod
Feb 21 13:22:58.894: INFO: Waiting for pod pod-configmaps-2e510777-7fc5-4742-aea5-05c1365705f9 to disappear
Feb 21 13:22:58.896: INFO: Pod pod-configmaps-2e510777-7fc5-4742-aea5-05c1365705f9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:22:58.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3097" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":41,"skipped":708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:22:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Feb 21 13:22:59.054: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:23:01.060: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 21 13:23:01.074: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:23:03.082: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 21 13:23:03.104: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 13:23:03.106: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 13:23:05.107: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 13:23:05.113: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:05.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2644" for this suite.

• [SLOW TEST:6.214 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:05.122: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Feb 21 13:23:05.272: INFO: Waiting up to 5m0s for pod "test-pod-f50fea6d-f80b-46ab-be67-296403b4876b" in namespace "svcaccounts-9376" to be "Succeeded or Failed"
Feb 21 13:23:05.273: INFO: Pod "test-pod-f50fea6d-f80b-46ab-be67-296403b4876b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827573ms
Feb 21 13:23:07.280: INFO: Pod "test-pod-f50fea6d-f80b-46ab-be67-296403b4876b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008344723s
STEP: Saw pod success
Feb 21 13:23:07.280: INFO: Pod "test-pod-f50fea6d-f80b-46ab-be67-296403b4876b" satisfied condition "Succeeded or Failed"
Feb 21 13:23:07.282: INFO: Trying to get logs from node aksh-cncf-3 pod test-pod-f50fea6d-f80b-46ab-be67-296403b4876b container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:23:07.297: INFO: Waiting for pod test-pod-f50fea6d-f80b-46ab-be67-296403b4876b to disappear
Feb 21 13:23:07.299: INFO: Pod test-pod-f50fea6d-f80b-46ab-be67-296403b4876b no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:07.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9376" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":43,"skipped":781,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:07.307: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1571
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Feb 21 13:23:07.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-720 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 21 13:23:07.491: INFO: stderr: ""
Feb 21 13:23:07.491: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 21 13:23:12.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-720 get pod e2e-test-httpd-pod -o json'
Feb 21 13:23:12.582: INFO: stderr: ""
Feb 21 13:23:12.582: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.1.64/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.1.64/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2022-02-21T13:23:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-720\",\n        \"resourceVersion\": \"15142\",\n        \"uid\": \"464c9603-e040-43df-97da-bde0d00fb3c2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-shzz2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"aksh-cncf-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-shzz2\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-21T13:23:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-21T13:23:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-21T13:23:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-02-21T13:23:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://6226c50d7414586d0028ab7aab5861b27844b7f4bae8fa455b2f7da0387586f7\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-02-21T13:23:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.131\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.64\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.64\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-02-21T13:23:07Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 21 13:23:12.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-720 replace -f -'
Feb 21 13:23:13.231: INFO: stderr: ""
Feb 21 13:23:13.231: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1575
Feb 21 13:23:13.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-720 delete pods e2e-test-httpd-pod'
Feb 21 13:23:15.303: INFO: stderr: ""
Feb 21 13:23:15.303: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:15.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-720" for this suite.

• [SLOW TEST:8.009 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1568
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":44,"skipped":792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:15.316: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6395
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 21 13:23:15.461: INFO: Waiting up to 5m0s for pod "pod-f6986b8d-8efd-44e7-9a97-e587f65a8d59" in namespace "emptydir-6395" to be "Succeeded or Failed"
Feb 21 13:23:15.463: INFO: Pod "pod-f6986b8d-8efd-44e7-9a97-e587f65a8d59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864514ms
Feb 21 13:23:17.470: INFO: Pod "pod-f6986b8d-8efd-44e7-9a97-e587f65a8d59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008727693s
STEP: Saw pod success
Feb 21 13:23:17.470: INFO: Pod "pod-f6986b8d-8efd-44e7-9a97-e587f65a8d59" satisfied condition "Succeeded or Failed"
Feb 21 13:23:17.473: INFO: Trying to get logs from node aksh-cncf-3 pod pod-f6986b8d-8efd-44e7-9a97-e587f65a8d59 container test-container: <nil>
STEP: delete the pod
Feb 21 13:23:17.490: INFO: Waiting for pod pod-f6986b8d-8efd-44e7-9a97-e587f65a8d59 to disappear
Feb 21 13:23:17.492: INFO: Pod pod-f6986b8d-8efd-44e7-9a97-e587f65a8d59 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:17.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6395" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:17.499: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Feb 21 13:23:17.642: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:23:19.648: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:20.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2913" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":46,"skipped":859,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 13:23:20.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1c076f0-9250-4819-8e2f-2c7699613f95" in namespace "downward-api-4797" to be "Succeeded or Failed"
Feb 21 13:23:20.819: INFO: Pod "downwardapi-volume-e1c076f0-9250-4819-8e2f-2c7699613f95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174632ms
Feb 21 13:23:22.827: INFO: Pod "downwardapi-volume-e1c076f0-9250-4819-8e2f-2c7699613f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009367392s
STEP: Saw pod success
Feb 21 13:23:22.827: INFO: Pod "downwardapi-volume-e1c076f0-9250-4819-8e2f-2c7699613f95" satisfied condition "Succeeded or Failed"
Feb 21 13:23:22.829: INFO: Trying to get logs from node aksh-cncf-2 pod downwardapi-volume-e1c076f0-9250-4819-8e2f-2c7699613f95 container client-container: <nil>
STEP: delete the pod
Feb 21 13:23:22.847: INFO: Waiting for pod downwardapi-volume-e1c076f0-9250-4819-8e2f-2c7699613f95 to disappear
Feb 21 13:23:22.849: INFO: Pod downwardapi-volume-e1c076f0-9250-4819-8e2f-2c7699613f95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:22.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4797" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:22.858: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 21 13:23:22.999: INFO: Waiting up to 5m0s for pod "pod-35776ff6-9d99-49ad-abe6-1605dd466535" in namespace "emptydir-8356" to be "Succeeded or Failed"
Feb 21 13:23:23.003: INFO: Pod "pod-35776ff6-9d99-49ad-abe6-1605dd466535": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524779ms
Feb 21 13:23:25.009: INFO: Pod "pod-35776ff6-9d99-49ad-abe6-1605dd466535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009789041s
STEP: Saw pod success
Feb 21 13:23:25.009: INFO: Pod "pod-35776ff6-9d99-49ad-abe6-1605dd466535" satisfied condition "Succeeded or Failed"
Feb 21 13:23:25.011: INFO: Trying to get logs from node aksh-cncf-1 pod pod-35776ff6-9d99-49ad-abe6-1605dd466535 container test-container: <nil>
STEP: delete the pod
Feb 21 13:23:25.026: INFO: Waiting for pod pod-35776ff6-9d99-49ad-abe6-1605dd466535 to disappear
Feb 21 13:23:25.028: INFO: Pod pod-35776ff6-9d99-49ad-abe6-1605dd466535 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:25.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8356" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":911,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:25.035: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-b1fc93f5-a76f-494d-b051-221594014a30
STEP: Creating a pod to test consume configMaps
Feb 21 13:23:25.185: INFO: Waiting up to 5m0s for pod "pod-configmaps-b862ce9e-6830-417b-a2a6-60fcb58d94b9" in namespace "configmap-7059" to be "Succeeded or Failed"
Feb 21 13:23:25.187: INFO: Pod "pod-configmaps-b862ce9e-6830-417b-a2a6-60fcb58d94b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374954ms
Feb 21 13:23:27.193: INFO: Pod "pod-configmaps-b862ce9e-6830-417b-a2a6-60fcb58d94b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00851096s
STEP: Saw pod success
Feb 21 13:23:27.193: INFO: Pod "pod-configmaps-b862ce9e-6830-417b-a2a6-60fcb58d94b9" satisfied condition "Succeeded or Failed"
Feb 21 13:23:27.196: INFO: Trying to get logs from node aksh-cncf-1 pod pod-configmaps-b862ce9e-6830-417b-a2a6-60fcb58d94b9 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:23:27.213: INFO: Waiting for pod pod-configmaps-b862ce9e-6830-417b-a2a6-60fcb58d94b9 to disappear
Feb 21 13:23:27.215: INFO: Pod pod-configmaps-b862ce9e-6830-417b-a2a6-60fcb58d94b9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:27.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7059" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:27.222: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2708
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-d1769045-34c5-48a8-8d9d-2dc67289b36c
STEP: Creating secret with name s-test-opt-upd-1c9526e4-2acc-4ef6-a439-2c9d64b7f25a
STEP: Creating the pod
Feb 21 13:23:27.383: INFO: The status of Pod pod-projected-secrets-e343bd35-cdfe-4cf4-b785-ea1514107fd1 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:23:29.390: INFO: The status of Pod pod-projected-secrets-e343bd35-cdfe-4cf4-b785-ea1514107fd1 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-d1769045-34c5-48a8-8d9d-2dc67289b36c
STEP: Updating secret s-test-opt-upd-1c9526e4-2acc-4ef6-a439-2c9d64b7f25a
STEP: Creating secret with name s-test-opt-create-b9ffcecf-331e-4c22-ab39-a79f6d3cc448
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:23:31.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2708" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:23:31.452: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4365
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-4365
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4365
Feb 21 13:23:31.598: INFO: Found 0 stateful pods, waiting for 1
Feb 21 13:23:41.606: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 21 13:23:41.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4365 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:23:41.740: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:23:41.740: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:23:41.740: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:23:41.743: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 21 13:23:51.750: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:23:51.751: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:23:51.766: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Feb 21 13:23:51.766: INFO: ss-0  aksh-cncf-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:31 +0000 UTC  }]
Feb 21 13:23:51.766: INFO: 
Feb 21 13:23:51.766: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 21 13:23:52.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997715799s
Feb 21 13:23:53.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992322203s
Feb 21 13:23:54.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986348547s
Feb 21 13:23:55.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979361008s
Feb 21 13:23:56.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972396818s
Feb 21 13:23:57.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967734023s
Feb 21 13:23:58.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96126986s
Feb 21 13:23:59.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956677416s
Feb 21 13:24:00.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.708998ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4365
Feb 21 13:24:01.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4365 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 13:24:01.929: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 21 13:24:01.929: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 13:24:01.929: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 13:24:01.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4365 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 13:24:02.020: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 21 13:24:02.020: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 13:24:02.020: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 13:24:02.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4365 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 13:24:02.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 21 13:24:02.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 13:24:02.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 13:24:02.137: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 21 13:24:12.146: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:24:12.146: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:24:12.146: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 21 13:24:12.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4365 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:24:12.258: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:24:12.259: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:24:12.259: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:24:12.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4365 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:24:12.374: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:24:12.374: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:24:12.374: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:24:12.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-4365 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 13:24:12.491: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 13:24:12.491: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 13:24:12.491: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 13:24:12.491: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:24:12.494: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 21 13:24:22.505: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:24:22.505: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:24:22.505: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 13:24:22.516: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Feb 21 13:24:22.516: INFO: ss-0  aksh-cncf-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:31 +0000 UTC  }]
Feb 21 13:24:22.516: INFO: ss-1  aksh-cncf-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:51 +0000 UTC  }]
Feb 21 13:24:22.516: INFO: ss-2  aksh-cncf-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:51 +0000 UTC  }]
Feb 21 13:24:22.516: INFO: 
Feb 21 13:24:22.516: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 13:24:23.522: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Feb 21 13:24:23.522: INFO: ss-1  aksh-cncf-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:24:12 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:23:51 +0000 UTC  }]
Feb 21 13:24:23.522: INFO: 
Feb 21 13:24:23.522: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 21 13:24:24.527: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.991763661s
Feb 21 13:24:25.533: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986619142s
Feb 21 13:24:26.537: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.980818238s
Feb 21 13:24:27.545: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976412995s
Feb 21 13:24:28.548: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.968034418s
Feb 21 13:24:29.554: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.965185088s
Feb 21 13:24:30.558: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.959474023s
Feb 21 13:24:31.564: INFO: Verifying statefulset ss doesn't scale past 0 for another 955.646599ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4365
Feb 21 13:24:32.568: INFO: Scaling statefulset ss to 0
Feb 21 13:24:32.577: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 13:24:32.580: INFO: Deleting all statefulset in ns statefulset-4365
Feb 21 13:24:32.582: INFO: Scaling statefulset ss to 0
Feb 21 13:24:32.590: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:24:32.591: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:24:32.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4365" for this suite.

• [SLOW TEST:61.156 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":51,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:24:32.609: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-6d52e69f-efed-4d8e-9134-a21c5b1adafb
STEP: Creating a pod to test consume configMaps
Feb 21 13:24:32.753: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ead8a3e-771d-48e2-a26d-7b86592901b1" in namespace "configmap-7163" to be "Succeeded or Failed"
Feb 21 13:24:32.755: INFO: Pod "pod-configmaps-8ead8a3e-771d-48e2-a26d-7b86592901b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223283ms
Feb 21 13:24:34.759: INFO: Pod "pod-configmaps-8ead8a3e-771d-48e2-a26d-7b86592901b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006212255s
STEP: Saw pod success
Feb 21 13:24:34.759: INFO: Pod "pod-configmaps-8ead8a3e-771d-48e2-a26d-7b86592901b1" satisfied condition "Succeeded or Failed"
Feb 21 13:24:34.762: INFO: Trying to get logs from node aksh-cncf-3 pod pod-configmaps-8ead8a3e-771d-48e2-a26d-7b86592901b1 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:24:34.775: INFO: Waiting for pod pod-configmaps-8ead8a3e-771d-48e2-a26d-7b86592901b1 to disappear
Feb 21 13:24:34.777: INFO: Pod pod-configmaps-8ead8a3e-771d-48e2-a26d-7b86592901b1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:24:34.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7163" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":1002,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:24:34.785: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 21 13:24:34.923: INFO: PodSpec: initContainers in spec.initContainers
Feb 21 13:25:19.508: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-51a3abc7-8752-4ab6-9201-88d47e888846", GenerateName:"", Namespace:"init-container-4659", SelfLink:"", UID:"d375e5e5-cf99-4107-983c-6f9acc203bbc", ResourceVersion:"16181", Generation:0, CreationTimestamp:time.Date(2022, time.February, 21, 13, 24, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"923428710"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.1.70/32", "cni.projectcalico.org/podIPs":"10.244.1.70/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.February, 21, 13, 24, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0018680a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.February, 21, 13, 24, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0018680d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.February, 21, 13, 24, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001868120), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zp49d", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004474000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zp49d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zp49d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zp49d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004890120), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"aksh-cncf-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003a94000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0048901d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0048901f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0048901f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0048901fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0047120e0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.February, 21, 13, 24, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.February, 21, 13, 24, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.February, 21, 13, 24, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.February, 21, 13, 24, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.131", PodIP:"10.244.1.70", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.70"}}, StartTime:time.Date(2022, time.February, 21, 13, 24, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a940e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003a94150)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://37ebf53dd03e60a1c54f4ce75dc6653ab47d664180091b3226329a39822a9c5b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004474080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004474060), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc00489027f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:25:19.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4659" for this suite.

• [SLOW TEST:44.734 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":53,"skipped":1009,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:25:19.519: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9907
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 21 13:25:19.667: INFO: The status of Pod pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:25:21.672: INFO: The status of Pod pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 21 13:25:22.192: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce"
Feb 21 13:25:22.192: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce" in namespace "pods-9907" to be "terminated due to deadline exceeded"
Feb 21 13:25:22.194: INFO: Pod "pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.228622ms
Feb 21 13:25:24.201: INFO: Pod "pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.008766782s
Feb 21 13:25:26.206: INFO: Pod "pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014148041s
Feb 21 13:25:26.206: INFO: Pod "pod-update-activedeadlineseconds-bab7667c-87d9-47a7-bb64-15d1652345ce" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:25:26.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9907" for this suite.

• [SLOW TEST:6.697 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":1027,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:25:26.216: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 21 13:25:26.624: INFO: Pod name wrapped-volume-race-459f2dfd-802d-4fbd-9842-4e0b96700215: Found 3 pods out of 5
Feb 21 13:25:31.632: INFO: Pod name wrapped-volume-race-459f2dfd-802d-4fbd-9842-4e0b96700215: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-459f2dfd-802d-4fbd-9842-4e0b96700215 in namespace emptydir-wrapper-5018, will wait for the garbage collector to delete the pods
Feb 21 13:25:41.717: INFO: Deleting ReplicationController wrapped-volume-race-459f2dfd-802d-4fbd-9842-4e0b96700215 took: 8.396684ms
Feb 21 13:25:41.818: INFO: Terminating ReplicationController wrapped-volume-race-459f2dfd-802d-4fbd-9842-4e0b96700215 pods took: 100.696525ms
STEP: Creating RC which spawns configmap-volume pods
Feb 21 13:25:45.139: INFO: Pod name wrapped-volume-race-ec71ff3e-32d9-4865-b3ce-c8907742dda5: Found 0 pods out of 5
Feb 21 13:25:50.147: INFO: Pod name wrapped-volume-race-ec71ff3e-32d9-4865-b3ce-c8907742dda5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ec71ff3e-32d9-4865-b3ce-c8907742dda5 in namespace emptydir-wrapper-5018, will wait for the garbage collector to delete the pods
Feb 21 13:26:00.234: INFO: Deleting ReplicationController wrapped-volume-race-ec71ff3e-32d9-4865-b3ce-c8907742dda5 took: 7.159243ms
Feb 21 13:26:00.335: INFO: Terminating ReplicationController wrapped-volume-race-ec71ff3e-32d9-4865-b3ce-c8907742dda5 pods took: 100.998019ms
STEP: Creating RC which spawns configmap-volume pods
Feb 21 13:26:03.252: INFO: Pod name wrapped-volume-race-3e5e62f7-23f9-4bb6-873b-9911f06c0901: Found 0 pods out of 5
Feb 21 13:26:08.261: INFO: Pod name wrapped-volume-race-3e5e62f7-23f9-4bb6-873b-9911f06c0901: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3e5e62f7-23f9-4bb6-873b-9911f06c0901 in namespace emptydir-wrapper-5018, will wait for the garbage collector to delete the pods
Feb 21 13:26:18.340: INFO: Deleting ReplicationController wrapped-volume-race-3e5e62f7-23f9-4bb6-873b-9911f06c0901 took: 7.238282ms
Feb 21 13:26:18.440: INFO: Terminating ReplicationController wrapped-volume-race-3e5e62f7-23f9-4bb6-873b-9911f06c0901 pods took: 100.425234ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:21.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5018" for this suite.

• [SLOW TEST:55.546 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":55,"skipped":1030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:21.763: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-3033
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3033 to expose endpoints map[]
Feb 21 13:26:21.913: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 21 13:26:22.923: INFO: successfully validated that service multi-endpoint-test in namespace services-3033 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3033
Feb 21 13:26:22.933: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:26:24.941: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3033 to expose endpoints map[pod1:[100]]
Feb 21 13:26:24.951: INFO: successfully validated that service multi-endpoint-test in namespace services-3033 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3033
Feb 21 13:26:24.963: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:26:26.971: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3033 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 21 13:26:26.983: INFO: successfully validated that service multi-endpoint-test in namespace services-3033 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Feb 21 13:26:26.983: INFO: Creating new exec pod
Feb 21 13:26:29.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3033 exec execpodq45qw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb 21 13:26:30.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 21 13:26:30.101: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:26:30.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3033 exec execpodq45qw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.251.87 80'
Feb 21 13:26:30.195: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.251.87 80\nConnection to 10.101.251.87 80 port [tcp/http] succeeded!\n"
Feb 21 13:26:30.195: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:26:30.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3033 exec execpodq45qw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb 21 13:26:30.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 21 13:26:30.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:26:30.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3033 exec execpodq45qw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.251.87 81'
Feb 21 13:26:30.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.251.87 81\nConnection to 10.101.251.87 81 port [tcp/*] succeeded!\n"
Feb 21 13:26:30.389: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3033
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3033 to expose endpoints map[pod2:[101]]
Feb 21 13:26:30.412: INFO: successfully validated that service multi-endpoint-test in namespace services-3033 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3033
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3033 to expose endpoints map[]
Feb 21 13:26:31.442: INFO: successfully validated that service multi-endpoint-test in namespace services-3033 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:31.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3033" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.704 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":56,"skipped":1071,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:31.467: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1917
STEP: creating service affinity-clusterip-transition in namespace services-1917
STEP: creating replication controller affinity-clusterip-transition in namespace services-1917
I0221 13:26:31.616747      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1917, replica count: 3
I0221 13:26:34.667424      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 13:26:34.676: INFO: Creating new exec pod
Feb 21 13:26:37.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-1917 exec execpod-affinityvr8wq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb 21 13:26:37.898: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 21 13:26:37.898: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:26:37.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-1917 exec execpod-affinityvr8wq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.134.122 80'
Feb 21 13:26:38.008: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.134.122 80\nConnection to 10.103.134.122 80 port [tcp/http] succeeded!\n"
Feb 21 13:26:38.008: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:26:38.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-1917 exec execpod-affinityvr8wq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.134.122:80/ ; done'
Feb 21 13:26:38.170: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n"
Feb 21 13:26:38.170: INFO: stdout: "\naffinity-clusterip-transition-m9c8t\naffinity-clusterip-transition-xpwpl\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-m9c8t\naffinity-clusterip-transition-m9c8t\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-xpwpl\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-xpwpl\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-xpwpl\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-m9c8t\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-m9c8t"
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-m9c8t
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-xpwpl
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-m9c8t
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-m9c8t
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-xpwpl
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-xpwpl
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-xpwpl
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-m9c8t
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.170: INFO: Received response from host: affinity-clusterip-transition-m9c8t
Feb 21 13:26:38.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-1917 exec execpod-affinityvr8wq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.134.122:80/ ; done'
Feb 21 13:26:38.316: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.134.122:80/\n"
Feb 21 13:26:38.316: INFO: stdout: "\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8\naffinity-clusterip-transition-skkp8"
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Received response from host: affinity-clusterip-transition-skkp8
Feb 21 13:26:38.316: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1917, will wait for the garbage collector to delete the pods
Feb 21 13:26:38.388: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.645795ms
Feb 21 13:26:38.489: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.021357ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:41.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1917" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.248 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":57,"skipped":1076,"failed":0}
SS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:41.715: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a collection of services
Feb 21 13:26:41.846: INFO: Creating e2e-svc-a-ld5pr
Feb 21 13:26:41.858: INFO: Creating e2e-svc-b-rnzf8
Feb 21 13:26:41.868: INFO: Creating e2e-svc-c-5hwqn
STEP: deleting service collection
Feb 21 13:26:41.907: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:41.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6763" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":58,"skipped":1078,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:41.915: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:42.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-413" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":59,"skipped":1082,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:42.058: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 13:26:42.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 13:26:45.479: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:26:45.483: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:48.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5944" for this suite.
STEP: Destroying namespace "webhook-5944-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.574 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":60,"skipped":1083,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:48.632: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5603
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:26:48.771: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 21 13:26:51.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-5603 --namespace=crd-publish-openapi-5603 create -f -'
Feb 21 13:26:52.549: INFO: stderr: ""
Feb 21 13:26:52.549: INFO: stdout: "e2e-test-crd-publish-openapi-7833-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 21 13:26:52.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-5603 --namespace=crd-publish-openapi-5603 delete e2e-test-crd-publish-openapi-7833-crds test-cr'
Feb 21 13:26:52.620: INFO: stderr: ""
Feb 21 13:26:52.620: INFO: stdout: "e2e-test-crd-publish-openapi-7833-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 21 13:26:52.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-5603 --namespace=crd-publish-openapi-5603 apply -f -'
Feb 21 13:26:53.295: INFO: stderr: ""
Feb 21 13:26:53.295: INFO: stdout: "e2e-test-crd-publish-openapi-7833-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 21 13:26:53.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-5603 --namespace=crd-publish-openapi-5603 delete e2e-test-crd-publish-openapi-7833-crds test-cr'
Feb 21 13:26:53.363: INFO: stderr: ""
Feb 21 13:26:53.363: INFO: stdout: "e2e-test-crd-publish-openapi-7833-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 21 13:26:53.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-5603 explain e2e-test-crd-publish-openapi-7833-crds'
Feb 21 13:26:53.466: INFO: stderr: ""
Feb 21 13:26:53.466: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7833-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:55.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5603" for this suite.

• [SLOW TEST:7.242 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":61,"skipped":1085,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:55.875: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 21 13:26:56.008: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:26:58.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7189" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":62,"skipped":1097,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:26:58.710: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Feb 21 13:26:58.852: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:27:00.858: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 21 13:27:00.868: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:27:02.874: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Feb 21 13:27:02.883: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 13:27:02.885: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 13:27:04.886: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 13:27:04.891: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 13:27:06.886: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 13:27:06.890: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:27:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2288" for this suite.

• [SLOW TEST:8.195 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1116,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:27:06.906: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-11879962-d7ee-471a-ab59-08c347a19cd7
STEP: Creating a pod to test consume configMaps
Feb 21 13:27:07.050: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-677f3815-4cca-4633-ae88-7db111c390fb" in namespace "projected-4747" to be "Succeeded or Failed"
Feb 21 13:27:07.052: INFO: Pod "pod-projected-configmaps-677f3815-4cca-4633-ae88-7db111c390fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579919ms
Feb 21 13:27:09.056: INFO: Pod "pod-projected-configmaps-677f3815-4cca-4633-ae88-7db111c390fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006333447s
STEP: Saw pod success
Feb 21 13:27:09.056: INFO: Pod "pod-projected-configmaps-677f3815-4cca-4633-ae88-7db111c390fb" satisfied condition "Succeeded or Failed"
Feb 21 13:27:09.059: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-configmaps-677f3815-4cca-4633-ae88-7db111c390fb container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:27:09.078: INFO: Waiting for pod pod-projected-configmaps-677f3815-4cca-4633-ae88-7db111c390fb to disappear
Feb 21 13:27:09.080: INFO: Pod pod-projected-configmaps-677f3815-4cca-4633-ae88-7db111c390fb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:27:09.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4747" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1133,"failed":0}
SSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:27:09.087: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-5147
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Feb 21 13:27:09.220: INFO: Major version: 1
STEP: Confirm minor version
Feb 21 13:27:09.220: INFO: cleanMinorVersion: 23
Feb 21 13:27:09.220: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:27:09.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5147" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":65,"skipped":1137,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:27:09.228: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 13:27:09.723: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 13:27:12.740: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:27:12.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4131" for this suite.
STEP: Destroying namespace "webhook-4131-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":66,"skipped":1138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:27:12.835: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-1285d8e5-23e4-473c-aac6-635b00951218
STEP: Creating a pod to test consume secrets
Feb 21 13:27:12.976: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9572489b-4f1d-4c61-8366-9962b9119b24" in namespace "projected-5818" to be "Succeeded or Failed"
Feb 21 13:27:12.978: INFO: Pod "pod-projected-secrets-9572489b-4f1d-4c61-8366-9962b9119b24": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572485ms
Feb 21 13:27:14.983: INFO: Pod "pod-projected-secrets-9572489b-4f1d-4c61-8366-9962b9119b24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006851527s
STEP: Saw pod success
Feb 21 13:27:14.983: INFO: Pod "pod-projected-secrets-9572489b-4f1d-4c61-8366-9962b9119b24" satisfied condition "Succeeded or Failed"
Feb 21 13:27:14.985: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-secrets-9572489b-4f1d-4c61-8366-9962b9119b24 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 13:27:15.001: INFO: Waiting for pod pod-projected-secrets-9572489b-4f1d-4c61-8366-9962b9119b24 to disappear
Feb 21 13:27:15.005: INFO: Pod pod-projected-secrets-9572489b-4f1d-4c61-8366-9962b9119b24 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:27:15.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5818" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:27:15.012: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 21 13:27:15.152: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 21 13:28:15.182: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Feb 21 13:28:15.207: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 21 13:28:15.212: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 21 13:28:15.229: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 21 13:28:15.235: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 21 13:28:15.249: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 21 13:28:15.256: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Feb 21 13:28:15.270: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Feb 21 13:28:15.277: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:28:29.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9937" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.385 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":68,"skipped":1191,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:28:29.398: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Feb 21 13:28:29.541: INFO: observed Pod pod-test in namespace pods-4853 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 21 13:28:29.548: INFO: observed Pod pod-test in namespace pods-4853 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC  }]
Feb 21 13:28:29.560: INFO: observed Pod pod-test in namespace pods-4853 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC  }]
Feb 21 13:28:29.971: INFO: observed Pod pod-test in namespace pods-4853 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC  }]
Feb 21 13:28:30.218: INFO: Found Pod pod-test in namespace pods-4853 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-02-21 13:28:29 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Feb 21 13:28:30.230: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Feb 21 13:28:30.253: INFO: observed event type ADDED
Feb 21 13:28:30.253: INFO: observed event type MODIFIED
Feb 21 13:28:30.253: INFO: observed event type MODIFIED
Feb 21 13:28:30.253: INFO: observed event type MODIFIED
Feb 21 13:28:30.253: INFO: observed event type MODIFIED
Feb 21 13:28:30.253: INFO: observed event type MODIFIED
Feb 21 13:28:30.253: INFO: observed event type MODIFIED
Feb 21 13:28:30.253: INFO: observed event type MODIFIED
Feb 21 13:28:32.224: INFO: observed event type MODIFIED
Feb 21 13:28:32.371: INFO: observed event type MODIFIED
Feb 21 13:28:33.227: INFO: observed event type MODIFIED
Feb 21 13:28:33.233: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:28:33.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4853" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":69,"skipped":1205,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:28:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 21 13:28:33.392: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2565  80a21349-32d1-4c3a-8955-fa2f6542400c 18597 0 2022-02-21 13:28:33 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2022-02-21 13:28:33 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbghb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbghb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 13:28:33.395: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:28:35.401: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 21 13:28:35.401: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2565 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:28:35.401: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:28:35.402: INFO: ExecWithOptions: Clientset creation
Feb 21 13:28:35.402: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2565/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Feb 21 13:28:35.480: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2565 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:28:35.480: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:28:35.481: INFO: ExecWithOptions: Clientset creation
Feb 21 13:28:35.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2565/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:28:35.522: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:28:35.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2565" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":70,"skipped":1217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:28:35.544: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:28:35.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1044" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":71,"skipped":1267,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:28:35.687: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Feb 21 13:28:35.828: INFO: Waiting up to 5m0s for pod "client-containers-9ccfdd45-5a91-4fe8-97b7-c82583df117d" in namespace "containers-9740" to be "Succeeded or Failed"
Feb 21 13:28:35.829: INFO: Pod "client-containers-9ccfdd45-5a91-4fe8-97b7-c82583df117d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.587001ms
Feb 21 13:28:37.835: INFO: Pod "client-containers-9ccfdd45-5a91-4fe8-97b7-c82583df117d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006802446s
STEP: Saw pod success
Feb 21 13:28:37.835: INFO: Pod "client-containers-9ccfdd45-5a91-4fe8-97b7-c82583df117d" satisfied condition "Succeeded or Failed"
Feb 21 13:28:37.837: INFO: Trying to get logs from node aksh-cncf-2 pod client-containers-9ccfdd45-5a91-4fe8-97b7-c82583df117d container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:28:37.861: INFO: Waiting for pod client-containers-9ccfdd45-5a91-4fe8-97b7-c82583df117d to disappear
Feb 21 13:28:37.863: INFO: Pod client-containers-9ccfdd45-5a91-4fe8-97b7-c82583df117d no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:28:37.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9740" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1300,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:28:37.871: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:28:51.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4471" for this suite.

• [SLOW TEST:13.198 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":73,"skipped":1312,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:28:51.069: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6027
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:28:51.211: INFO: Found 0 stateful pods, waiting for 1
Feb 21 13:29:01.216: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Feb 21 13:29:01.233: INFO: Found 1 stateful pods, waiting for 2
Feb 21 13:29:11.238: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:29:11.238: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 13:29:11.252: INFO: Deleting all statefulset in ns statefulset-6027
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:29:11.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6027" for this suite.

• [SLOW TEST:20.198 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":74,"skipped":1326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:29:11.267: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:29:39.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8325" for this suite.

• [SLOW TEST:28.192 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":75,"skipped":1348,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:29:39.459: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9833
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 13:29:39.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-389709bc-76c0-465a-9912-58f5e1e9d5e5" in namespace "projected-9833" to be "Succeeded or Failed"
Feb 21 13:29:39.600: INFO: Pod "downwardapi-volume-389709bc-76c0-465a-9912-58f5e1e9d5e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.730253ms
Feb 21 13:29:41.605: INFO: Pod "downwardapi-volume-389709bc-76c0-465a-9912-58f5e1e9d5e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006746673s
STEP: Saw pod success
Feb 21 13:29:41.605: INFO: Pod "downwardapi-volume-389709bc-76c0-465a-9912-58f5e1e9d5e5" satisfied condition "Succeeded or Failed"
Feb 21 13:29:41.607: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-389709bc-76c0-465a-9912-58f5e1e9d5e5 container client-container: <nil>
STEP: delete the pod
Feb 21 13:29:41.625: INFO: Waiting for pod downwardapi-volume-389709bc-76c0-465a-9912-58f5e1e9d5e5 to disappear
Feb 21 13:29:41.627: INFO: Pod downwardapi-volume-389709bc-76c0-465a-9912-58f5e1e9d5e5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:29:41.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9833" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1348,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:29:41.634: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-82c153dd-fb07-4acd-b46e-a538097effe2
STEP: Creating a pod to test consume secrets
Feb 21 13:29:41.778: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2aa56143-f1d0-4ecc-aef2-8dbda21be3ec" in namespace "projected-3713" to be "Succeeded or Failed"
Feb 21 13:29:41.780: INFO: Pod "pod-projected-secrets-2aa56143-f1d0-4ecc-aef2-8dbda21be3ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090405ms
Feb 21 13:29:43.785: INFO: Pod "pod-projected-secrets-2aa56143-f1d0-4ecc-aef2-8dbda21be3ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007007366s
STEP: Saw pod success
Feb 21 13:29:43.785: INFO: Pod "pod-projected-secrets-2aa56143-f1d0-4ecc-aef2-8dbda21be3ec" satisfied condition "Succeeded or Failed"
Feb 21 13:29:43.787: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-secrets-2aa56143-f1d0-4ecc-aef2-8dbda21be3ec container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 13:29:43.802: INFO: Waiting for pod pod-projected-secrets-2aa56143-f1d0-4ecc-aef2-8dbda21be3ec to disappear
Feb 21 13:29:43.803: INFO: Pod pod-projected-secrets-2aa56143-f1d0-4ecc-aef2-8dbda21be3ec no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:29:43.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3713" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":77,"skipped":1356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:29:43.811: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:29:43.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5518 create -f -'
Feb 21 13:29:44.436: INFO: stderr: ""
Feb 21 13:29:44.437: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 21 13:29:44.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5518 create -f -'
Feb 21 13:29:44.558: INFO: stderr: ""
Feb 21 13:29:44.558: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 21 13:29:45.561: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 13:29:45.561: INFO: Found 0 / 1
Feb 21 13:29:46.562: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 13:29:46.562: INFO: Found 1 / 1
Feb 21 13:29:46.562: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 13:29:46.564: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 13:29:46.564: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 13:29:46.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5518 describe pod agnhost-primary-cl2bl'
Feb 21 13:29:46.625: INFO: stderr: ""
Feb 21 13:29:46.625: INFO: stdout: "Name:         agnhost-primary-cl2bl\nNamespace:    kubectl-5518\nPriority:     0\nNode:         aksh-cncf-3/10.0.0.131\nStart Time:   Mon, 21 Feb 2022 13:29:44 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.244.1.91/32\n              cni.projectcalico.org/podIPs: 10.244.1.91/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.244.1.91\nIPs:\n  IP:           10.244.1.91\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4b54fa91d5d5f3df74406e71f619415432c1dbfa3153c3afd43862b37dbcd24a\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 21 Feb 2022 13:29:45 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g2kj2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-g2kj2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5518/agnhost-primary-cl2bl to aksh-cncf-3\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Feb 21 13:29:46.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5518 describe rc agnhost-primary'
Feb 21 13:29:46.677: INFO: stderr: ""
Feb 21 13:29:46.677: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5518\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-cl2bl\n"
Feb 21 13:29:46.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5518 describe service agnhost-primary'
Feb 21 13:29:46.728: INFO: stderr: ""
Feb 21 13:29:46.728: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5518\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.187.95\nIPs:               10.105.187.95\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.91:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 21 13:29:46.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5518 describe node aksh-cncf-1'
Feb 21 13:29:46.801: INFO: stderr: ""
Feb 21 13:29:46.801: INFO: stdout: "Name:               aksh-cncf-1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=aksh-cncf-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"5e:33:52:7a:0e:df\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.0.54\n                    kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.54/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    rafay.dev/object-hash: 2cc94b19cd1913ed21230718a13656db067d6dfb61bc1f67a074f33ac3a725a4\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 21 Feb 2022 12:49:29 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  aksh-cncf-1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 21 Feb 2022 13:29:37 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 21 Feb 2022 12:50:01 +0000   Mon, 21 Feb 2022 12:50:01 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 21 Feb 2022 13:29:43 +0000   Mon, 21 Feb 2022 12:49:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 21 Feb 2022 13:29:43 +0000   Mon, 21 Feb 2022 12:49:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 21 Feb 2022 13:29:43 +0000   Mon, 21 Feb 2022 12:49:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 21 Feb 2022 13:29:43 +0000   Mon, 21 Feb 2022 12:54:49 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.0.54\n  Hostname:    aksh-cncf-1\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      50633164Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8140916Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    7910m\n  ephemeral-storage:      46663523866\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 6199412Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 4fc01482413a4ff9b92f3b4047bdaedf\n  System UUID:                4fc01482-413a-4ff9-b92f-3b4047bdaedf\n  Boot ID:                    5ef99553-3759-4c39-93a6-4db8d121b65e\n  Kernel Version:             5.4.0-1061-oracle\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.4\n  Kubelet Version:            v1.23.3\n  Kube-Proxy Version:         v1.23.3\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6d9dfd554f-x2lk5                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  kube-system                 canal-vnh8t                                                250m (3%)     0 (0%)      0 (0%)           0 (0%)         40m\n  kube-system                 kube-apiserver-aksh-cncf-1                                 150m (1%)     0 (0%)      0 (0%)           0 (0%)         37m\n  kube-system                 kube-controller-manager-aksh-cncf-1                        100m (1%)     0 (0%)      0 (0%)           0 (0%)         34m\n  kube-system                 kube-proxy-r42th                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  kube-system                 kube-scheduler-aksh-cncf-1                                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         37m\n  openebs                     openebs-ndm-lqth6                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-6psfh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests   Limits\n  --------               --------   ------\n  cpu                    600m (7%)  0 (0%)\n  memory                 0 (0%)     0 (0%)\n  ephemeral-storage      0 (0%)     0 (0%)\n  hugepages-1Gi          0 (0%)     0 (0%)\n  hugepages-2Mi          0 (0%)     0 (0%)\n  scheduling.k8s.io/foo  0          0\nEvents:\n  Type     Reason                                            Age                From        Message\n  ----     ------                                            ----               ----        -------\n  Warning  listen tcp4 :30949: bind: address already in use  8m43s              kube-proxy  can't open port \"nodePort for services-9400/nodeport-test:http\" (:30949/tcp4), skipping it\n  Warning  listen tcp4 :31909: bind: address already in use  13m                kube-proxy  can't open port \"nodePort for services-6029/nodeport-service\" (:31909/tcp4), skipping it\n  Normal   Starting                                          40m                kube-proxy  \n  Normal   NodeAllocatableEnforced                           40m                kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory                           40m                kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure                             40m                kubelet     Node aksh-cncf-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID                              40m                kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientPID\n  Warning  InvalidDiskCapacity                               40m                kubelet     invalid capacity 0 on image filesystem\n  Normal   Starting                                          40m                kubelet     Starting kubelet.\n  Normal   NodeReady                                         39m                kubelet     Node aksh-cncf-1 status is now: NodeReady\n  Normal   NodeHasSufficientMemory                           38m (x8 over 38m)  kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure                             38m (x7 over 38m)  kubelet     Node aksh-cncf-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID                              38m (x7 over 38m)  kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced                           38m                kubelet     Updated Node Allocatable limit across pods\n  Normal   Starting                                          38m                kubelet     Starting kubelet.\n  Warning  InvalidDiskCapacity                               38m                kubelet     invalid capacity 0 on image filesystem\n  Normal   Starting                                          35m                kubelet     Starting kubelet.\n  Normal   NodeAllocatableEnforced                           35m                kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory                           35m                kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure                             35m                kubelet     Node aksh-cncf-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID                              35m                kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientPID\n  Warning  InvalidDiskCapacity                               35m                kubelet     invalid capacity 0 on image filesystem\n  Warning  InvalidDiskCapacity                               35m                kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced                           35m                kubelet     Updated Node Allocatable limit across pods\n  Normal   NodeHasNoDiskPressure                             35m                kubelet     Node aksh-cncf-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID                              35m                kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady                                      35m                kubelet     Node aksh-cncf-1 status is now: NodeNotReady\n  Normal   Starting                                          35m                kubelet     Starting kubelet.\n  Normal   NodeHasSufficientMemory                           35m                kubelet     Node aksh-cncf-1 status is now: NodeHasSufficientMemory\n  Normal   NodeReady                                         34m                kubelet     Node aksh-cncf-1 status is now: NodeReady\n"
Feb 21 13:29:46.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5518 describe namespace kubectl-5518'
Feb 21 13:29:46.849: INFO: stderr: ""
Feb 21 13:29:46.849: INFO: stdout: "Name:         kubectl-5518\nLabels:       e2e-framework=kubectl\n              e2e-run=c0328a2c-4c81-4b6e-a827-8f7fdb76bf2b\n              kubernetes.io/metadata.name=kubectl-5518\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:29:46.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5518" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":78,"skipped":1394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:29:46.856: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 21 13:29:46.992: INFO: Waiting up to 5m0s for pod "downward-api-2f63474a-8946-43ea-bfa7-c7884697bec4" in namespace "downward-api-801" to be "Succeeded or Failed"
Feb 21 13:29:46.995: INFO: Pod "downward-api-2f63474a-8946-43ea-bfa7-c7884697bec4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.241388ms
Feb 21 13:29:49.000: INFO: Pod "downward-api-2f63474a-8946-43ea-bfa7-c7884697bec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008049829s
STEP: Saw pod success
Feb 21 13:29:49.000: INFO: Pod "downward-api-2f63474a-8946-43ea-bfa7-c7884697bec4" satisfied condition "Succeeded or Failed"
Feb 21 13:29:49.005: INFO: Trying to get logs from node aksh-cncf-1 pod downward-api-2f63474a-8946-43ea-bfa7-c7884697bec4 container dapi-container: <nil>
STEP: delete the pod
Feb 21 13:29:49.025: INFO: Waiting for pod downward-api-2f63474a-8946-43ea-bfa7-c7884697bec4 to disappear
Feb 21 13:29:49.027: INFO: Pod downward-api-2f63474a-8946-43ea-bfa7-c7884697bec4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:29:49.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-801" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1417,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:29:49.036: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:29:56.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2740" for this suite.

• [SLOW TEST:7.156 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":80,"skipped":1420,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:29:56.192: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9793
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-7d3235d1-385f-48d4-b9f5-5f4bc7b6a315
STEP: Creating the pod
Feb 21 13:29:56.343: INFO: The status of Pod pod-configmaps-c43aa089-8fc4-4fd0-ba81-18edfd868abb is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:29:58.346: INFO: The status of Pod pod-configmaps-c43aa089-8fc4-4fd0-ba81-18edfd868abb is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-7d3235d1-385f-48d4-b9f5-5f4bc7b6a315
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:31:20.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9793" for this suite.

• [SLOW TEST:84.387 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:31:20.580: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd in namespace container-probe-6521
Feb 21 13:31:22.725: INFO: Started pod liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd in namespace container-probe-6521
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 13:31:22.727: INFO: Initial restart count of pod liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd is 0
Feb 21 13:31:42.764: INFO: Restart count of pod container-probe-6521/liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd is now 1 (20.037200657s elapsed)
Feb 21 13:32:02.813: INFO: Restart count of pod container-probe-6521/liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd is now 2 (40.085484332s elapsed)
Feb 21 13:32:22.870: INFO: Restart count of pod container-probe-6521/liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd is now 3 (1m0.143248267s elapsed)
Feb 21 13:32:42.927: INFO: Restart count of pod container-probe-6521/liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd is now 4 (1m20.200226134s elapsed)
Feb 21 13:33:45.102: INFO: Restart count of pod container-probe-6521/liveness-056dcefa-edb0-469b-8ce5-dab4dba0cccd is now 5 (2m22.374841301s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:33:45.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6521" for this suite.

• [SLOW TEST:144.541 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":82,"skipped":1498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:33:45.121: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:34:01.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8023" for this suite.

• [SLOW TEST:16.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":83,"skipped":1530,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:34:01.333: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 21 13:34:01.471: INFO: Waiting up to 5m0s for pod "pod-64025d25-1713-458d-b280-f6e5e257b13f" in namespace "emptydir-186" to be "Succeeded or Failed"
Feb 21 13:34:01.474: INFO: Pod "pod-64025d25-1713-458d-b280-f6e5e257b13f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514092ms
Feb 21 13:34:03.478: INFO: Pod "pod-64025d25-1713-458d-b280-f6e5e257b13f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006681737s
STEP: Saw pod success
Feb 21 13:34:03.478: INFO: Pod "pod-64025d25-1713-458d-b280-f6e5e257b13f" satisfied condition "Succeeded or Failed"
Feb 21 13:34:03.480: INFO: Trying to get logs from node aksh-cncf-3 pod pod-64025d25-1713-458d-b280-f6e5e257b13f container test-container: <nil>
STEP: delete the pod
Feb 21 13:34:03.499: INFO: Waiting for pod pod-64025d25-1713-458d-b280-f6e5e257b13f to disappear
Feb 21 13:34:03.501: INFO: Pod pod-64025d25-1713-458d-b280-f6e5e257b13f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:34:03.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-186" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1538,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:34:03.508: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8992
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:34:03.646: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 21 13:34:08.650: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Feb 21 13:34:08.658: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Feb 21 13:34:08.664: INFO: observed ReplicaSet test-rs in namespace replicaset-8992 with ReadyReplicas 1, AvailableReplicas 1
Feb 21 13:34:08.674: INFO: observed ReplicaSet test-rs in namespace replicaset-8992 with ReadyReplicas 1, AvailableReplicas 1
Feb 21 13:34:08.688: INFO: observed ReplicaSet test-rs in namespace replicaset-8992 with ReadyReplicas 1, AvailableReplicas 1
Feb 21 13:34:08.701: INFO: observed ReplicaSet test-rs in namespace replicaset-8992 with ReadyReplicas 1, AvailableReplicas 1
Feb 21 13:34:09.691: INFO: observed ReplicaSet test-rs in namespace replicaset-8992 with ReadyReplicas 2, AvailableReplicas 2
Feb 21 13:34:09.951: INFO: observed Replicaset test-rs in namespace replicaset-8992 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:34:09.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8992" for this suite.

• [SLOW TEST:6.452 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":85,"skipped":1540,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:34:09.961: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3573
Feb 21 13:34:10.103: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:34:12.108: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 21 13:34:12.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 21 13:34:12.221: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 21 13:34:12.221: INFO: stdout: "iptables"
Feb 21 13:34:12.221: INFO: proxyMode: iptables
Feb 21 13:34:12.230: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 21 13:34:12.232: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3573
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3573
I0221 13:34:12.249427      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3573, replica count: 3
I0221 13:34:15.300816      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 13:34:15.310: INFO: Creating new exec pod
Feb 21 13:34:18.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb 21 13:34:18.432: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 21 13:34:18.432: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:34:18.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.158.145 80'
Feb 21 13:34:18.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.158.145 80\nConnection to 10.103.158.145 80 port [tcp/http] succeeded!\n"
Feb 21 13:34:18.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:34:18.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.131 31311'
Feb 21 13:34:18.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.131 31311\nConnection to 10.0.0.131 31311 port [tcp/*] succeeded!\n"
Feb 21 13:34:18.615: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:34:18.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.54 31311'
Feb 21 13:34:18.711: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.54 31311\nConnection to 10.0.0.54 31311 port [tcp/*] succeeded!\n"
Feb 21 13:34:18.711: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:34:18.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.54:31311/ ; done'
Feb 21 13:34:18.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n"
Feb 21 13:34:18.869: INFO: stdout: "\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r\naffinity-nodeport-timeout-bsr8r"
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Received response from host: affinity-nodeport-timeout-bsr8r
Feb 21 13:34:18.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.54:31311/'
Feb 21 13:34:18.990: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n"
Feb 21 13:34:18.990: INFO: stdout: "affinity-nodeport-timeout-bsr8r"
Feb 21 13:34:38.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.54:31311/'
Feb 21 13:34:39.091: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n"
Feb 21 13:34:39.091: INFO: stdout: "affinity-nodeport-timeout-bsr8r"
Feb 21 13:34:59.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.54:31311/'
Feb 21 13:34:59.198: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n"
Feb 21 13:34:59.198: INFO: stdout: "affinity-nodeport-timeout-bsr8r"
Feb 21 13:35:19.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-3573 exec execpod-affinityj9vgv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.54:31311/'
Feb 21 13:35:19.310: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.54:31311/\n"
Feb 21 13:35:19.310: INFO: stdout: "affinity-nodeport-timeout-xh7kc"
Feb 21 13:35:19.310: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3573, will wait for the garbage collector to delete the pods
Feb 21 13:35:19.380: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.852545ms
Feb 21 13:35:19.481: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.930222ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:35:21.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3573" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:71.651 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":86,"skipped":1544,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:35:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 13:35:21.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:35:21.764: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 13:35:22.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 21 13:35:22.771: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 13:35:23.770: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 13:35:23.770: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 21 13:35:23.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 21 13:35:23.790: INFO: Node aksh-cncf-4 is running 0 daemon pod, expected 1
Feb 21 13:35:24.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 21 13:35:24.798: INFO: Node aksh-cncf-4 is running 0 daemon pod, expected 1
Feb 21 13:35:25.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 13:35:25.796: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1426, will wait for the garbage collector to delete the pods
Feb 21 13:35:25.859: INFO: Deleting DaemonSet.extensions daemon-set took: 5.399124ms
Feb 21 13:35:25.959: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.305887ms
Feb 21 13:35:28.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 13:35:28.864: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 21 13:35:28.866: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21104"},"items":null}

Feb 21 13:35:28.868: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21104"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:35:28.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1426" for this suite.

• [SLOW TEST:7.274 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":87,"skipped":1551,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:35:28.886: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-093b0431-047b-454b-b1ec-23e1d5de662e in namespace container-probe-208
Feb 21 13:35:31.036: INFO: Started pod busybox-093b0431-047b-454b-b1ec-23e1d5de662e in namespace container-probe-208
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 13:35:31.038: INFO: Initial restart count of pod busybox-093b0431-047b-454b-b1ec-23e1d5de662e is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:39:31.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-208" for this suite.

• [SLOW TEST:242.720 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:39:31.606: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8385
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Feb 21 13:39:31.748: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 21 13:39:36.754: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:39:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8385" for this suite.

• [SLOW TEST:5.179 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":89,"skipped":1621,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:39:36.785: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Feb 21 13:39:36.927: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 21 13:39:41.933: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Feb 21 13:39:41.935: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:39:41.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2170" for this suite.

• [SLOW TEST:5.171 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":90,"skipped":1631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:39:41.956: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-2487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 21 13:39:42.115: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 21 13:40:42.146: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:40:42.149: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-5086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Feb 21 13:40:44.308: INFO: found a healthy node: aksh-cncf-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:40:52.366: INFO: pods created so far: [1 1 1]
Feb 21 13:40:52.366: INFO: length of pods created so far: 3
Feb 21 13:40:54.376: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:41:01.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5086" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:41:01.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2487" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:79.510 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":91,"skipped":1668,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:41:01.467: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-l55k
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 13:41:01.611: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-l55k" in namespace "subpath-5922" to be "Succeeded or Failed"
Feb 21 13:41:01.612: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Pending", Reason="", readiness=false. Elapsed: 1.607732ms
Feb 21 13:41:03.617: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 2.006112193s
Feb 21 13:41:05.621: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 4.010229236s
Feb 21 13:41:07.624: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 6.01351185s
Feb 21 13:41:09.629: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 8.018224852s
Feb 21 13:41:11.633: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 10.022067666s
Feb 21 13:41:13.637: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 12.026108516s
Feb 21 13:41:15.641: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 14.029833021s
Feb 21 13:41:17.644: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 16.033665828s
Feb 21 13:41:19.648: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 18.037013628s
Feb 21 13:41:21.651: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Running", Reason="", readiness=true. Elapsed: 20.040344749s
Feb 21 13:41:23.656: INFO: Pod "pod-subpath-test-projected-l55k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.044776964s
STEP: Saw pod success
Feb 21 13:41:23.656: INFO: Pod "pod-subpath-test-projected-l55k" satisfied condition "Succeeded or Failed"
Feb 21 13:41:23.657: INFO: Trying to get logs from node aksh-cncf-1 pod pod-subpath-test-projected-l55k container test-container-subpath-projected-l55k: <nil>
STEP: delete the pod
Feb 21 13:41:23.682: INFO: Waiting for pod pod-subpath-test-projected-l55k to disappear
Feb 21 13:41:23.683: INFO: Pod pod-subpath-test-projected-l55k no longer exists
STEP: Deleting pod pod-subpath-test-projected-l55k
Feb 21 13:41:23.683: INFO: Deleting pod "pod-subpath-test-projected-l55k" in namespace "subpath-5922"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:41:23.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5922" for this suite.

• [SLOW TEST:22.225 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":92,"skipped":1673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:41:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 21 13:41:23.833: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22803 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:41:23.833: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22803 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 21 13:41:23.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22804 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:41:23.838: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22804 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 21 13:41:23.844: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22805 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:41:23.844: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22805 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 21 13:41:23.849: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22806 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:41:23.849: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7205  defd50a9-a832-46ed-81c3-855fd67571c1 22806 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 21 13:41:23.852: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7205  a5c28250-a8d2-41df-a658-fa0354e6ba88 22807 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:41:23.852: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7205  a5c28250-a8d2-41df-a658-fa0354e6ba88 22807 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 21 13:41:33.858: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7205  a5c28250-a8d2-41df-a658-fa0354e6ba88 22860 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:41:33.858: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7205  a5c28250-a8d2-41df-a658-fa0354e6ba88 22860 0 2022-02-21 13:41:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-02-21 13:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:41:43.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7205" for this suite.

• [SLOW TEST:20.185 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":93,"skipped":1700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:41:43.877: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-da67b865-227e-4f20-aa9d-cc81682cecbf
STEP: Creating a pod to test consume configMaps
Feb 21 13:41:44.025: INFO: Waiting up to 5m0s for pod "pod-configmaps-553ef2f1-acee-454d-a5bb-d49b306c6019" in namespace "configmap-2270" to be "Succeeded or Failed"
Feb 21 13:41:44.027: INFO: Pod "pod-configmaps-553ef2f1-acee-454d-a5bb-d49b306c6019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041418ms
Feb 21 13:41:46.031: INFO: Pod "pod-configmaps-553ef2f1-acee-454d-a5bb-d49b306c6019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006022766s
STEP: Saw pod success
Feb 21 13:41:46.031: INFO: Pod "pod-configmaps-553ef2f1-acee-454d-a5bb-d49b306c6019" satisfied condition "Succeeded or Failed"
Feb 21 13:41:46.034: INFO: Trying to get logs from node aksh-cncf-3 pod pod-configmaps-553ef2f1-acee-454d-a5bb-d49b306c6019 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:41:46.052: INFO: Waiting for pod pod-configmaps-553ef2f1-acee-454d-a5bb-d49b306c6019 to disappear
Feb 21 13:41:46.054: INFO: Pod pod-configmaps-553ef2f1-acee-454d-a5bb-d49b306c6019 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:41:46.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2270" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:41:46.062: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 13:41:48.221: INFO: DNS probes using dns-8936/dns-test-c688f02f-f0bb-4a1d-b558-fefa3e6713bb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:41:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8936" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":95,"skipped":1800,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:41:48.242: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8052
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Feb 21 13:41:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:02.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8052" for this suite.

• [SLOW TEST:14.549 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":96,"skipped":1802,"failed":0}
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:02.791: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:02.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5620" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":97,"skipped":1804,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:02.940: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4350
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:42:03.071: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 21 13:42:05.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-4350 --namespace=crd-publish-openapi-4350 create -f -'
Feb 21 13:42:06.140: INFO: stderr: ""
Feb 21 13:42:06.140: INFO: stdout: "e2e-test-crd-publish-openapi-8286-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 21 13:42:06.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-4350 --namespace=crd-publish-openapi-4350 delete e2e-test-crd-publish-openapi-8286-crds test-cr'
Feb 21 13:42:06.206: INFO: stderr: ""
Feb 21 13:42:06.206: INFO: stdout: "e2e-test-crd-publish-openapi-8286-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 21 13:42:06.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-4350 --namespace=crd-publish-openapi-4350 apply -f -'
Feb 21 13:42:06.323: INFO: stderr: ""
Feb 21 13:42:06.323: INFO: stdout: "e2e-test-crd-publish-openapi-8286-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 21 13:42:06.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-4350 --namespace=crd-publish-openapi-4350 delete e2e-test-crd-publish-openapi-8286-crds test-cr'
Feb 21 13:42:06.390: INFO: stderr: ""
Feb 21 13:42:06.390: INFO: stdout: "e2e-test-crd-publish-openapi-8286-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 21 13:42:06.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-4350 explain e2e-test-crd-publish-openapi-8286-crds'
Feb 21 13:42:06.765: INFO: stderr: ""
Feb 21 13:42:06.765: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8286-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:09.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4350" for this suite.

• [SLOW TEST:6.325 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":98,"skipped":1812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:09.266: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8224.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8224.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 13:42:11.425: INFO: DNS probes using dns-test-9a01eb70-c731-489a-a960-5ae55f5c7922 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8224.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8224.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 13:42:21.476: INFO: File wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:21.479: INFO: File jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:21.479: INFO: Lookups using dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 failed for: [wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local]

Feb 21 13:42:26.482: INFO: File wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:26.485: INFO: File jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:26.485: INFO: Lookups using dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 failed for: [wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local]

Feb 21 13:42:31.483: INFO: File wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:31.486: INFO: File jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:31.486: INFO: Lookups using dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 failed for: [wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local]

Feb 21 13:42:36.482: INFO: File wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:36.485: INFO: File jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:36.485: INFO: Lookups using dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 failed for: [wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local]

Feb 21 13:42:41.483: INFO: File wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:41.486: INFO: File jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local from pod  dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 21 13:42:41.486: INFO: Lookups using dns-8224/dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 failed for: [wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local]

Feb 21 13:42:46.486: INFO: DNS probes using dns-test-8ad1c507-0f8b-479f-817d-31b3fefdf568 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8224.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8224.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8224.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8224.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 13:42:48.563: INFO: DNS probes using dns-test-1c69e666-a618-48aa-afc0-54e8064e4e6f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:48.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8224" for this suite.

• [SLOW TEST:39.336 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":99,"skipped":1843,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:48.603: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 13:42:48.742: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af33a6fd-4b6c-480a-ad23-75ce9edb333f" in namespace "projected-1772" to be "Succeeded or Failed"
Feb 21 13:42:48.744: INFO: Pod "downwardapi-volume-af33a6fd-4b6c-480a-ad23-75ce9edb333f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298256ms
Feb 21 13:42:50.749: INFO: Pod "downwardapi-volume-af33a6fd-4b6c-480a-ad23-75ce9edb333f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007077607s
STEP: Saw pod success
Feb 21 13:42:50.749: INFO: Pod "downwardapi-volume-af33a6fd-4b6c-480a-ad23-75ce9edb333f" satisfied condition "Succeeded or Failed"
Feb 21 13:42:50.751: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-af33a6fd-4b6c-480a-ad23-75ce9edb333f container client-container: <nil>
STEP: delete the pod
Feb 21 13:42:50.771: INFO: Waiting for pod downwardapi-volume-af33a6fd-4b6c-480a-ad23-75ce9edb333f to disappear
Feb 21 13:42:50.773: INFO: Pod downwardapi-volume-af33a6fd-4b6c-480a-ad23-75ce9edb333f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:50.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1772" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1845,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:50.781: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 13:42:50.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3f0b3d9-3758-4948-9550-42b3bc396cbe" in namespace "projected-9388" to be "Succeeded or Failed"
Feb 21 13:42:50.925: INFO: Pod "downwardapi-volume-b3f0b3d9-3758-4948-9550-42b3bc396cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.746906ms
Feb 21 13:42:52.931: INFO: Pod "downwardapi-volume-b3f0b3d9-3758-4948-9550-42b3bc396cbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007774406s
STEP: Saw pod success
Feb 21 13:42:52.931: INFO: Pod "downwardapi-volume-b3f0b3d9-3758-4948-9550-42b3bc396cbe" satisfied condition "Succeeded or Failed"
Feb 21 13:42:52.933: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-b3f0b3d9-3758-4948-9550-42b3bc396cbe container client-container: <nil>
STEP: delete the pod
Feb 21 13:42:52.950: INFO: Waiting for pod downwardapi-volume-b3f0b3d9-3758-4948-9550-42b3bc396cbe to disappear
Feb 21 13:42:52.951: INFO: Pod downwardapi-volume-b3f0b3d9-3758-4948-9550-42b3bc396cbe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9388" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:52.959: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 21 13:42:54.140: INFO: The status of Pod kube-controller-manager-aksh-cncf-3 is Running (Ready = true)
Feb 21 13:42:54.178: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:54.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2938" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":102,"skipped":1972,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:54.187: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 13:42:54.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d825fd2f-ad83-4ad6-b656-a424d1c3247b" in namespace "projected-6525" to be "Succeeded or Failed"
Feb 21 13:42:54.327: INFO: Pod "downwardapi-volume-d825fd2f-ad83-4ad6-b656-a424d1c3247b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.884497ms
Feb 21 13:42:56.331: INFO: Pod "downwardapi-volume-d825fd2f-ad83-4ad6-b656-a424d1c3247b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006366984s
STEP: Saw pod success
Feb 21 13:42:56.331: INFO: Pod "downwardapi-volume-d825fd2f-ad83-4ad6-b656-a424d1c3247b" satisfied condition "Succeeded or Failed"
Feb 21 13:42:56.333: INFO: Trying to get logs from node aksh-cncf-1 pod downwardapi-volume-d825fd2f-ad83-4ad6-b656-a424d1c3247b container client-container: <nil>
STEP: delete the pod
Feb 21 13:42:56.351: INFO: Waiting for pod downwardapi-volume-d825fd2f-ad83-4ad6-b656-a424d1c3247b to disappear
Feb 21 13:42:56.353: INFO: Pod downwardapi-volume-d825fd2f-ad83-4ad6-b656-a424d1c3247b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:42:56.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6525" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":103,"skipped":1983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:42:56.362: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:42:56.500: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 21 13:43:01.504: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 13:43:01.504: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 21 13:43:03.508: INFO: Creating deployment "test-rollover-deployment"
Feb 21 13:43:03.515: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 21 13:43:05.521: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 21 13:43:05.524: INFO: Ensure that both replica sets have 1 created replica
Feb 21 13:43:05.529: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 21 13:43:05.536: INFO: Updating deployment test-rollover-deployment
Feb 21 13:43:05.536: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 21 13:43:07.543: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 21 13:43:07.546: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 21 13:43:07.551: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 13:43:07.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 13:43:09.559: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 13:43:09.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 13:43:11.556: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 13:43:11.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 13:43:13.557: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 13:43:13.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 13:43:15.558: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 13:43:15.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 13, 43, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 13, 43, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 13:43:17.558: INFO: 
Feb 21 13:43:17.558: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 13:43:17.565: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2226  0b308483-8a83-4802-ab44-a53e4b832f98 23744 2 2022-02-21 13:43:03 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-21 13:43:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049b30a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-21 13:43:03 +0000 UTC,LastTransitionTime:2022-02-21 13:43:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-02-21 13:43:16 +0000 UTC,LastTransitionTime:2022-02-21 13:43:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 13:43:17.567: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-2226  d9426984-8305-4bc4-9817-b22d39584287 23734 2 2022-02-21 13:43:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0b308483-8a83-4802-ab44-a53e4b832f98 0xc004a79857 0xc004a79858}] []  [{kube-controller-manager Update apps/v1 2022-02-21 13:43:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b308483-8a83-4802-ab44-a53e4b832f98\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:43:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a79908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 21 13:43:17.567: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 21 13:43:17.567: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2226  7239712a-30b9-41fc-b282-215ee2fdf0ab 23743 2 2022-02-21 13:42:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0b308483-8a83-4802-ab44-a53e4b832f98 0xc004a79727 0xc004a79728}] []  [{e2e.test Update apps/v1 2022-02-21 13:42:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b308483-8a83-4802-ab44-a53e4b832f98\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:43:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a797e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 21 13:43:17.567: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-2226  4e5a4877-710f-467c-9860-37bba7948e42 23681 2 2022-02-21 13:43:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0b308483-8a83-4802-ab44-a53e4b832f98 0xc004a79977 0xc004a79978}] []  [{kube-controller-manager Update apps/v1 2022-02-21 13:43:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b308483-8a83-4802-ab44-a53e4b832f98\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:43:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a79a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 21 13:43:17.570: INFO: Pod "test-rollover-deployment-668b7f667d-55rk8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-55rk8 test-rollover-deployment-668b7f667d- deployment-2226  5a27abed-e117-40c5-8387-276eca9260a9 23696 0 2022-02-21 13:43:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[cni.projectcalico.org/podIP:10.244.0.82/32 cni.projectcalico.org/podIPs:10.244.0.82/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d d9426984-8305-4bc4-9817-b22d39584287 0xc004a79fe7 0xc004a79fe8}] []  [{kube-controller-manager Update v1 2022-02-21 13:43:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9426984-8305-4bc4-9817-b22d39584287\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 13:43:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-02-21 13:43:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njm2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njm2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:43:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:43:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:43:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:43:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.54,PodIP:10.244.0.82,StartTime:2022-02-21 13:43:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 13:43:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://06a1345cf4ef590c63523bd35da443b1617211a1d23340f22e8d0992c1355e28,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:17.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2226" for this suite.

• [SLOW TEST:21.216 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":104,"skipped":2010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:17.578: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 13:43:19.734: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:19.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4423" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":2036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:19.754: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 13:43:20.085: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 13:43:23.105: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:23.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1850" for this suite.
STEP: Destroying namespace "webhook-1850-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":106,"skipped":2109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:23.185: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-3625c912-0608-4a58-830b-793f183d95e7
STEP: Creating a pod to test consume secrets
Feb 21 13:43:23.332: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6c60387-d90a-4d81-a633-0a99d8d7daa3" in namespace "projected-3766" to be "Succeeded or Failed"
Feb 21 13:43:23.334: INFO: Pod "pod-projected-secrets-d6c60387-d90a-4d81-a633-0a99d8d7daa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026566ms
Feb 21 13:43:25.338: INFO: Pod "pod-projected-secrets-d6c60387-d90a-4d81-a633-0a99d8d7daa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006415249s
STEP: Saw pod success
Feb 21 13:43:25.338: INFO: Pod "pod-projected-secrets-d6c60387-d90a-4d81-a633-0a99d8d7daa3" satisfied condition "Succeeded or Failed"
Feb 21 13:43:25.340: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-secrets-d6c60387-d90a-4d81-a633-0a99d8d7daa3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 13:43:25.358: INFO: Waiting for pod pod-projected-secrets-d6c60387-d90a-4d81-a633-0a99d8d7daa3 to disappear
Feb 21 13:43:25.360: INFO: Pod pod-projected-secrets-d6c60387-d90a-4d81-a633-0a99d8d7daa3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:25.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3766" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":2138,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:25.368: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9893
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-5f2cacf3-8bd6-4a34-bb1b-61fe1942c7a4
STEP: Creating a pod to test consume configMaps
Feb 21 13:43:25.517: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b91c5794-3566-4632-9323-9d966a75d2a2" in namespace "projected-9893" to be "Succeeded or Failed"
Feb 21 13:43:25.519: INFO: Pod "pod-projected-configmaps-b91c5794-3566-4632-9323-9d966a75d2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06467ms
Feb 21 13:43:27.523: INFO: Pod "pod-projected-configmaps-b91c5794-3566-4632-9323-9d966a75d2a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006440604s
STEP: Saw pod success
Feb 21 13:43:27.523: INFO: Pod "pod-projected-configmaps-b91c5794-3566-4632-9323-9d966a75d2a2" satisfied condition "Succeeded or Failed"
Feb 21 13:43:27.525: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-configmaps-b91c5794-3566-4632-9323-9d966a75d2a2 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:43:27.542: INFO: Waiting for pod pod-projected-configmaps-b91c5794-3566-4632-9323-9d966a75d2a2 to disappear
Feb 21 13:43:27.544: INFO: Pod pod-projected-configmaps-b91c5794-3566-4632-9323-9d966a75d2a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:27.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9893" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":108,"skipped":2138,"failed":0}
S
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:27.551: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Feb 21 13:43:27.698: INFO: Found Service test-service-827nh in namespace services-4925 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 21 13:43:27.698: INFO: Service test-service-827nh created
STEP: Getting /status
Feb 21 13:43:27.701: INFO: Service test-service-827nh has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Feb 21 13:43:27.706: INFO: observed Service test-service-827nh in namespace services-4925 with annotations: map[] & LoadBalancer: {[]}
Feb 21 13:43:27.706: INFO: Found Service test-service-827nh in namespace services-4925 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 21 13:43:27.706: INFO: Service test-service-827nh has service status patched
STEP: updating the ServiceStatus
Feb 21 13:43:27.714: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Feb 21 13:43:27.714: INFO: Observed Service test-service-827nh in namespace services-4925 with annotations: map[] & Conditions: {[]}
Feb 21 13:43:27.714: INFO: Observed event: &Service{ObjectMeta:{test-service-827nh  services-4925  b3e9b92a-883e-4249-a6be-c85795134727 23969 0 2022-02-21 13:43:27 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-02-21 13:43:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-02-21 13:43:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.107.75.175,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.107.75.175],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 21 13:43:27.715: INFO: Found Service test-service-827nh in namespace services-4925 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 21 13:43:27.715: INFO: Service test-service-827nh has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Feb 21 13:43:27.721: INFO: observed Service test-service-827nh in namespace services-4925 with labels: map[test-service-static:true]
Feb 21 13:43:27.721: INFO: observed Service test-service-827nh in namespace services-4925 with labels: map[test-service-static:true]
Feb 21 13:43:27.721: INFO: observed Service test-service-827nh in namespace services-4925 with labels: map[test-service-static:true]
Feb 21 13:43:27.721: INFO: Found Service test-service-827nh in namespace services-4925 with labels: map[test-service:patched test-service-static:true]
Feb 21 13:43:27.721: INFO: Service test-service-827nh patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Feb 21 13:43:27.735: INFO: Observed event: ADDED
Feb 21 13:43:27.736: INFO: Observed event: MODIFIED
Feb 21 13:43:27.736: INFO: Observed event: MODIFIED
Feb 21 13:43:27.736: INFO: Observed event: MODIFIED
Feb 21 13:43:27.736: INFO: Found Service test-service-827nh in namespace services-4925 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 21 13:43:27.736: INFO: Service test-service-827nh deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:27.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4925" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":109,"skipped":2139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:27.743: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5346
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:27.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5346" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":110,"skipped":2163,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:27.904: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-e54ac28a-ca02-404a-b307-0e8982d6643b
STEP: Creating secret with name secret-projected-all-test-volume-21460ec8-0b17-4c7a-900f-2b5a76305ef2
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 21 13:43:28.054: INFO: Waiting up to 5m0s for pod "projected-volume-d17e5beb-60cb-441a-bbe6-6cedd1795adc" in namespace "projected-9424" to be "Succeeded or Failed"
Feb 21 13:43:28.057: INFO: Pod "projected-volume-d17e5beb-60cb-441a-bbe6-6cedd1795adc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.316458ms
Feb 21 13:43:30.062: INFO: Pod "projected-volume-d17e5beb-60cb-441a-bbe6-6cedd1795adc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007464992s
STEP: Saw pod success
Feb 21 13:43:30.062: INFO: Pod "projected-volume-d17e5beb-60cb-441a-bbe6-6cedd1795adc" satisfied condition "Succeeded or Failed"
Feb 21 13:43:30.064: INFO: Trying to get logs from node aksh-cncf-3 pod projected-volume-d17e5beb-60cb-441a-bbe6-6cedd1795adc container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 21 13:43:30.081: INFO: Waiting for pod projected-volume-d17e5beb-60cb-441a-bbe6-6cedd1795adc to disappear
Feb 21 13:43:30.083: INFO: Pod projected-volume-d17e5beb-60cb-441a-bbe6-6cedd1795adc no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:30.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9424" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:30.090: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-jlvr
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 13:43:30.236: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jlvr" in namespace "subpath-9318" to be "Succeeded or Failed"
Feb 21 13:43:30.238: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115216ms
Feb 21 13:43:32.242: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005468825s
Feb 21 13:43:34.247: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 4.010349615s
Feb 21 13:43:36.251: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 6.01435642s
Feb 21 13:43:38.255: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 8.019099214s
Feb 21 13:43:40.260: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 10.023446458s
Feb 21 13:43:42.263: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 12.027000011s
Feb 21 13:43:44.268: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 14.031417846s
Feb 21 13:43:46.273: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 16.036583548s
Feb 21 13:43:48.278: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 18.04179778s
Feb 21 13:43:50.283: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Running", Reason="", readiness=true. Elapsed: 20.046167142s
Feb 21 13:43:52.286: INFO: Pod "pod-subpath-test-secret-jlvr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049719969s
STEP: Saw pod success
Feb 21 13:43:52.286: INFO: Pod "pod-subpath-test-secret-jlvr" satisfied condition "Succeeded or Failed"
Feb 21 13:43:52.288: INFO: Trying to get logs from node aksh-cncf-3 pod pod-subpath-test-secret-jlvr container test-container-subpath-secret-jlvr: <nil>
STEP: delete the pod
Feb 21 13:43:52.306: INFO: Waiting for pod pod-subpath-test-secret-jlvr to disappear
Feb 21 13:43:52.307: INFO: Pod pod-subpath-test-secret-jlvr no longer exists
STEP: Deleting pod pod-subpath-test-secret-jlvr
Feb 21 13:43:52.307: INFO: Deleting pod "pod-subpath-test-secret-jlvr" in namespace "subpath-9318"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:43:52.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9318" for this suite.

• [SLOW TEST:22.227 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":112,"skipped":2218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:43:52.319: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-918c6a14-71c2-44f6-853b-5f00abdeaf5d in namespace container-probe-9969
Feb 21 13:43:54.462: INFO: Started pod test-webserver-918c6a14-71c2-44f6-853b-5f00abdeaf5d in namespace container-probe-9969
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 13:43:54.465: INFO: Initial restart count of pod test-webserver-918c6a14-71c2-44f6-853b-5f00abdeaf5d is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:47:55.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9969" for this suite.

• [SLOW TEST:242.716 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:47:55.035: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Feb 21 13:47:55.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 create -f -'
Feb 21 13:47:55.616: INFO: stderr: ""
Feb 21 13:47:55.616: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 13:47:55.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 21 13:47:55.660: INFO: stderr: ""
Feb 21 13:47:55.660: INFO: stdout: "update-demo-nautilus-6xwzr update-demo-nautilus-b5q65 "
Feb 21 13:47:55.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:47:55.701: INFO: stderr: ""
Feb 21 13:47:55.701: INFO: stdout: ""
Feb 21 13:47:55.701: INFO: update-demo-nautilus-6xwzr is created but not running
Feb 21 13:48:00.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 21 13:48:00.747: INFO: stderr: ""
Feb 21 13:48:00.747: INFO: stdout: "update-demo-nautilus-6xwzr update-demo-nautilus-b5q65 "
Feb 21 13:48:00.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:48:00.785: INFO: stderr: ""
Feb 21 13:48:00.785: INFO: stdout: "true"
Feb 21 13:48:00.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 13:48:00.823: INFO: stderr: ""
Feb 21 13:48:00.823: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 13:48:00.823: INFO: validating pod update-demo-nautilus-6xwzr
Feb 21 13:48:00.828: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 13:48:00.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 13:48:00.828: INFO: update-demo-nautilus-6xwzr is verified up and running
Feb 21 13:48:00.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-b5q65 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:48:00.869: INFO: stderr: ""
Feb 21 13:48:00.869: INFO: stdout: "true"
Feb 21 13:48:00.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-b5q65 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 13:48:00.911: INFO: stderr: ""
Feb 21 13:48:00.911: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 13:48:00.911: INFO: validating pod update-demo-nautilus-b5q65
Feb 21 13:48:00.914: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 13:48:00.914: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 13:48:00.914: INFO: update-demo-nautilus-b5q65 is verified up and running
STEP: scaling down the replication controller
Feb 21 13:48:00.915: INFO: scanned /root for discovery docs: <nil>
Feb 21 13:48:00.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 21 13:48:01.970: INFO: stderr: ""
Feb 21 13:48:01.970: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 13:48:01.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 21 13:48:02.014: INFO: stderr: ""
Feb 21 13:48:02.014: INFO: stdout: "update-demo-nautilus-6xwzr "
Feb 21 13:48:02.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:48:02.053: INFO: stderr: ""
Feb 21 13:48:02.053: INFO: stdout: "true"
Feb 21 13:48:02.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 13:48:02.092: INFO: stderr: ""
Feb 21 13:48:02.092: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 13:48:02.092: INFO: validating pod update-demo-nautilus-6xwzr
Feb 21 13:48:02.096: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 13:48:02.096: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 13:48:02.096: INFO: update-demo-nautilus-6xwzr is verified up and running
STEP: scaling up the replication controller
Feb 21 13:48:02.096: INFO: scanned /root for discovery docs: <nil>
Feb 21 13:48:02.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 21 13:48:03.154: INFO: stderr: ""
Feb 21 13:48:03.154: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 13:48:03.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 21 13:48:03.198: INFO: stderr: ""
Feb 21 13:48:03.198: INFO: stdout: "update-demo-nautilus-6xwzr update-demo-nautilus-n4299 "
Feb 21 13:48:03.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:48:03.238: INFO: stderr: ""
Feb 21 13:48:03.238: INFO: stdout: "true"
Feb 21 13:48:03.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 13:48:03.277: INFO: stderr: ""
Feb 21 13:48:03.277: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 13:48:03.277: INFO: validating pod update-demo-nautilus-6xwzr
Feb 21 13:48:03.280: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 13:48:03.280: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 13:48:03.280: INFO: update-demo-nautilus-6xwzr is verified up and running
Feb 21 13:48:03.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-n4299 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:48:03.328: INFO: stderr: ""
Feb 21 13:48:03.328: INFO: stdout: ""
Feb 21 13:48:03.328: INFO: update-demo-nautilus-n4299 is created but not running
Feb 21 13:48:08.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 21 13:48:08.375: INFO: stderr: ""
Feb 21 13:48:08.375: INFO: stdout: "update-demo-nautilus-6xwzr update-demo-nautilus-n4299 "
Feb 21 13:48:08.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:48:08.418: INFO: stderr: ""
Feb 21 13:48:08.418: INFO: stdout: "true"
Feb 21 13:48:08.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-6xwzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 13:48:08.461: INFO: stderr: ""
Feb 21 13:48:08.461: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 13:48:08.461: INFO: validating pod update-demo-nautilus-6xwzr
Feb 21 13:48:08.464: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 13:48:08.464: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 13:48:08.464: INFO: update-demo-nautilus-6xwzr is verified up and running
Feb 21 13:48:08.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-n4299 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 13:48:08.505: INFO: stderr: ""
Feb 21 13:48:08.505: INFO: stdout: "true"
Feb 21 13:48:08.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods update-demo-nautilus-n4299 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 13:48:08.543: INFO: stderr: ""
Feb 21 13:48:08.543: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 13:48:08.543: INFO: validating pod update-demo-nautilus-n4299
Feb 21 13:48:08.546: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 13:48:08.546: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 13:48:08.546: INFO: update-demo-nautilus-n4299 is verified up and running
STEP: using delete to clean up resources
Feb 21 13:48:08.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 delete --grace-period=0 --force -f -'
Feb 21 13:48:08.588: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 13:48:08.588: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 21 13:48:08.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get rc,svc -l name=update-demo --no-headers'
Feb 21 13:48:08.635: INFO: stderr: "No resources found in kubectl-5273 namespace.\n"
Feb 21 13:48:08.635: INFO: stdout: ""
Feb 21 13:48:08.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5273 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 13:48:08.681: INFO: stderr: ""
Feb 21 13:48:08.681: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:48:08.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5273" for this suite.

• [SLOW TEST:13.654 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":114,"skipped":2307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:48:08.689: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-602
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 13:48:08.828: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 21 13:48:08.866: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 13:48:10.870: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:12.870: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:14.869: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:16.869: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:18.869: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:20.870: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:22.869: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:24.869: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:26.870: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:28.870: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 13:48:30.869: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 21 13:48:30.874: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 21 13:48:30.878: INFO: The status of Pod netserver-2 is Running (Ready = true)
Feb 21 13:48:30.881: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Feb 21 13:48:32.906: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 21 13:48:32.906: INFO: Going to poll 10.244.0.84 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 21 13:48:32.907: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.84:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-602 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:48:32.907: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:48:32.908: INFO: ExecWithOptions: Clientset creation
Feb 21 13:48:32.908: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-602/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.84%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:48:32.963: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 21 13:48:32.963: INFO: Going to poll 10.244.2.51 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 21 13:48:32.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.51:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-602 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:48:32.966: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:48:32.966: INFO: ExecWithOptions: Clientset creation
Feb 21 13:48:32.966: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-602/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.51%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:48:33.004: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 21 13:48:33.004: INFO: Going to poll 10.244.1.123 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 21 13:48:33.007: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.123:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-602 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:48:33.007: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:48:33.007: INFO: ExecWithOptions: Clientset creation
Feb 21 13:48:33.007: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-602/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.123%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:48:33.058: INFO: Found all 1 expected endpoints: [netserver-2]
Feb 21 13:48:33.058: INFO: Going to poll 10.244.3.50 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 21 13:48:33.060: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.50:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-602 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 13:48:33.060: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 13:48:33.061: INFO: ExecWithOptions: Clientset creation
Feb 21 13:48:33.061: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-602/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.50%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 13:48:33.095: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:48:33.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-602" for this suite.

• [SLOW TEST:24.415 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":2350,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:48:33.105: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4052
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:48:50.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4052" for this suite.

• [SLOW TEST:17.189 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":116,"skipped":2365,"failed":0}
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:48:50.294: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7742
STEP: creating service affinity-nodeport in namespace services-7742
STEP: creating replication controller affinity-nodeport in namespace services-7742
I0221 13:48:50.447552      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7742, replica count: 3
I0221 13:48:53.498329      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 13:48:53.506: INFO: Creating new exec pod
Feb 21 13:48:56.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-7742 exec execpod-affinityxsx5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb 21 13:48:56.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 21 13:48:56.619: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:48:56.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-7742 exec execpod-affinityxsx5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.196.84 80'
Feb 21 13:48:56.712: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.196.84 80\nConnection to 10.97.196.84 80 port [tcp/http] succeeded!\n"
Feb 21 13:48:56.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:48:56.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-7742 exec execpod-affinityxsx5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.54 31903'
Feb 21 13:48:56.813: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.54 31903\nConnection to 10.0.0.54 31903 port [tcp/*] succeeded!\n"
Feb 21 13:48:56.813: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:48:56.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-7742 exec execpod-affinityxsx5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.131 31903'
Feb 21 13:48:56.929: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.131 31903\nConnection to 10.0.0.131 31903 port [tcp/*] succeeded!\n"
Feb 21 13:48:56.929: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 13:48:56.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-7742 exec execpod-affinityxsx5w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.54:31903/ ; done'
Feb 21 13:48:57.066: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:31903/\n"
Feb 21 13:48:57.066: INFO: stdout: "\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5\naffinity-nodeport-7vlq5"
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Received response from host: affinity-nodeport-7vlq5
Feb 21 13:48:57.066: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7742, will wait for the garbage collector to delete the pods
Feb 21 13:48:57.139: INFO: Deleting ReplicationController affinity-nodeport took: 5.743669ms
Feb 21 13:48:57.239: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.756012ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:00.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7742" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.672 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":117,"skipped":2365,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:00.966: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 21 13:49:01.104: INFO: Waiting up to 5m0s for pod "pod-b4e13000-85a1-4b20-ad3d-d11e29074b80" in namespace "emptydir-5917" to be "Succeeded or Failed"
Feb 21 13:49:01.106: INFO: Pod "pod-b4e13000-85a1-4b20-ad3d-d11e29074b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336267ms
Feb 21 13:49:03.110: INFO: Pod "pod-b4e13000-85a1-4b20-ad3d-d11e29074b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006246797s
STEP: Saw pod success
Feb 21 13:49:03.110: INFO: Pod "pod-b4e13000-85a1-4b20-ad3d-d11e29074b80" satisfied condition "Succeeded or Failed"
Feb 21 13:49:03.113: INFO: Trying to get logs from node aksh-cncf-3 pod pod-b4e13000-85a1-4b20-ad3d-d11e29074b80 container test-container: <nil>
STEP: delete the pod
Feb 21 13:49:03.134: INFO: Waiting for pod pod-b4e13000-85a1-4b20-ad3d-d11e29074b80 to disappear
Feb 21 13:49:03.136: INFO: Pod pod-b4e13000-85a1-4b20-ad3d-d11e29074b80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:03.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5917" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:03.142: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 13:49:03.282: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a2b532e-32ba-44c9-94f3-f56425ed3570" in namespace "downward-api-7838" to be "Succeeded or Failed"
Feb 21 13:49:03.284: INFO: Pod "downwardapi-volume-3a2b532e-32ba-44c9-94f3-f56425ed3570": Phase="Pending", Reason="", readiness=false. Elapsed: 1.728864ms
Feb 21 13:49:05.287: INFO: Pod "downwardapi-volume-3a2b532e-32ba-44c9-94f3-f56425ed3570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005040911s
STEP: Saw pod success
Feb 21 13:49:05.287: INFO: Pod "downwardapi-volume-3a2b532e-32ba-44c9-94f3-f56425ed3570" satisfied condition "Succeeded or Failed"
Feb 21 13:49:05.289: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-3a2b532e-32ba-44c9-94f3-f56425ed3570 container client-container: <nil>
STEP: delete the pod
Feb 21 13:49:05.301: INFO: Waiting for pod downwardapi-volume-3a2b532e-32ba-44c9-94f3-f56425ed3570 to disappear
Feb 21 13:49:05.304: INFO: Pod downwardapi-volume-3a2b532e-32ba-44c9-94f3-f56425ed3570 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:05.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7838" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2395,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:05.311: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:49:05.465: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"166228ac-42e9-4817-ac22-8469b16aef33", Controller:(*bool)(0xc003bad9e6), BlockOwnerDeletion:(*bool)(0xc003bad9e7)}}
Feb 21 13:49:05.470: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"85ee57e6-00d8-4703-82e6-7e619142752d", Controller:(*bool)(0xc004e049b6), BlockOwnerDeletion:(*bool)(0xc004e049b7)}}
Feb 21 13:49:05.477: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8fb16765-6d8f-4baa-b478-ea5177880d72", Controller:(*bool)(0xc004e04e7e), BlockOwnerDeletion:(*bool)(0xc004e04e7f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:10.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5148" for this suite.

• [SLOW TEST:5.183 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":120,"skipped":2408,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:10.494: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:49:10.632: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-bff54f63-041f-4f7c-a77a-881548529686" in namespace "security-context-test-6933" to be "Succeeded or Failed"
Feb 21 13:49:10.634: INFO: Pod "alpine-nnp-false-bff54f63-041f-4f7c-a77a-881548529686": Phase="Pending", Reason="", readiness=false. Elapsed: 1.850115ms
Feb 21 13:49:12.639: INFO: Pod "alpine-nnp-false-bff54f63-041f-4f7c-a77a-881548529686": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006738174s
Feb 21 13:49:14.642: INFO: Pod "alpine-nnp-false-bff54f63-041f-4f7c-a77a-881548529686": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009654845s
Feb 21 13:49:14.642: INFO: Pod "alpine-nnp-false-bff54f63-041f-4f7c-a77a-881548529686" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:14.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6933" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2418,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:14.655: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2703 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2703;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2703 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2703;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2703.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2703.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2703.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2703.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2703.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2703.svc;check="$$(dig +notcp +noall +answer +search 123.40.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.40.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.40.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.40.123_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2703 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2703;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2703 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2703;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2703.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2703.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2703.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2703.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2703.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2703.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2703.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2703.svc;check="$$(dig +notcp +noall +answer +search 123.40.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.40.123_udp@PTR;check="$$(dig +tcp +noall +answer +search 123.40.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.40.123_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 13:49:16.827: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.829: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.832: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.834: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.837: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.839: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.842: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.844: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.857: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.860: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.862: INFO: Unable to read jessie_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.865: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.867: INFO: Unable to read jessie_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.870: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.872: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.874: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:16.884: INFO: Lookups using dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2703 wheezy_tcp@dns-test-service.dns-2703 wheezy_udp@dns-test-service.dns-2703.svc wheezy_tcp@dns-test-service.dns-2703.svc wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2703 jessie_tcp@dns-test-service.dns-2703 jessie_udp@dns-test-service.dns-2703.svc jessie_tcp@dns-test-service.dns-2703.svc jessie_udp@_http._tcp.dns-test-service.dns-2703.svc jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc]

Feb 21 13:49:21.888: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.891: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.894: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.898: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.901: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.903: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.906: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.918: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.920: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.923: INFO: Unable to read jessie_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.925: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.927: INFO: Unable to read jessie_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.930: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.932: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.935: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:21.945: INFO: Lookups using dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2703 wheezy_tcp@dns-test-service.dns-2703 wheezy_udp@dns-test-service.dns-2703.svc wheezy_tcp@dns-test-service.dns-2703.svc wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2703 jessie_tcp@dns-test-service.dns-2703 jessie_udp@dns-test-service.dns-2703.svc jessie_tcp@dns-test-service.dns-2703.svc jessie_udp@_http._tcp.dns-test-service.dns-2703.svc jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc]

Feb 21 13:49:26.888: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.894: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.897: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.899: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.902: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.905: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.907: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.910: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.922: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.924: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.926: INFO: Unable to read jessie_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.929: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.931: INFO: Unable to read jessie_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.934: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.936: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.939: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:26.949: INFO: Lookups using dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2703 wheezy_tcp@dns-test-service.dns-2703 wheezy_udp@dns-test-service.dns-2703.svc wheezy_tcp@dns-test-service.dns-2703.svc wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2703 jessie_tcp@dns-test-service.dns-2703 jessie_udp@dns-test-service.dns-2703.svc jessie_tcp@dns-test-service.dns-2703.svc jessie_udp@_http._tcp.dns-test-service.dns-2703.svc jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc]

Feb 21 13:49:31.888: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.890: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.893: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.898: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.901: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.904: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.906: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.918: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.920: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.923: INFO: Unable to read jessie_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.925: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.927: INFO: Unable to read jessie_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.930: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.932: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.935: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:31.945: INFO: Lookups using dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2703 wheezy_tcp@dns-test-service.dns-2703 wheezy_udp@dns-test-service.dns-2703.svc wheezy_tcp@dns-test-service.dns-2703.svc wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2703 jessie_tcp@dns-test-service.dns-2703 jessie_udp@dns-test-service.dns-2703.svc jessie_tcp@dns-test-service.dns-2703.svc jessie_udp@_http._tcp.dns-test-service.dns-2703.svc jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc]

Feb 21 13:49:36.889: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.892: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.898: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.901: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.904: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.906: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.909: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.922: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.925: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.927: INFO: Unable to read jessie_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.930: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.932: INFO: Unable to read jessie_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.935: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.938: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.940: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:36.951: INFO: Lookups using dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2703 wheezy_tcp@dns-test-service.dns-2703 wheezy_udp@dns-test-service.dns-2703.svc wheezy_tcp@dns-test-service.dns-2703.svc wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2703 jessie_tcp@dns-test-service.dns-2703 jessie_udp@dns-test-service.dns-2703.svc jessie_tcp@dns-test-service.dns-2703.svc jessie_udp@_http._tcp.dns-test-service.dns-2703.svc jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc]

Feb 21 13:49:41.888: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.891: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.894: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.899: INFO: Unable to read wheezy_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.901: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.904: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.906: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.919: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.921: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.924: INFO: Unable to read jessie_udp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.927: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703 from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.929: INFO: Unable to read jessie_udp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.932: INFO: Unable to read jessie_tcp@dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.934: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.936: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc from pod dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc: the server could not find the requested resource (get pods dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc)
Feb 21 13:49:41.946: INFO: Lookups using dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2703 wheezy_tcp@dns-test-service.dns-2703 wheezy_udp@dns-test-service.dns-2703.svc wheezy_tcp@dns-test-service.dns-2703.svc wheezy_udp@_http._tcp.dns-test-service.dns-2703.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2703.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2703 jessie_tcp@dns-test-service.dns-2703 jessie_udp@dns-test-service.dns-2703.svc jessie_tcp@dns-test-service.dns-2703.svc jessie_udp@_http._tcp.dns-test-service.dns-2703.svc jessie_tcp@_http._tcp.dns-test-service.dns-2703.svc]

Feb 21 13:49:46.953: INFO: DNS probes using dns-2703/dns-test-9fb2cdb0-2834-4de4-9bf2-2e9c0e0076bc succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:47.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2703" for this suite.

• [SLOW TEST:32.363 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":122,"skipped":2421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9499
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 21 13:49:47.162: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9499  adf304bd-6b52-43e4-a774-5165ca29459d 26122 0 2022-02-21 13:49:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:49:47.162: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9499  adf304bd-6b52-43e4-a774-5165ca29459d 26123 0 2022-02-21 13:49:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 21 13:49:47.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9499  adf304bd-6b52-43e4-a774-5165ca29459d 26125 0 2022-02-21 13:49:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 13:49:47.172: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9499  adf304bd-6b52-43e4-a774-5165ca29459d 26126 0 2022-02-21 13:49:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:47.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9499" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":123,"skipped":2453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:47.181: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 13:49:47.314: INFO: Creating simple deployment test-new-deployment
Feb 21 13:49:47.323: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 13:49:49.355: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1055  9cd59880-3541-41bd-b469-7cca9e889491 26163 3 2022-02-21 13:49:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:49:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004dceae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-21 13:49:48 +0000 UTC,LastTransitionTime:2022-02-21 13:49:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-02-21 13:49:48 +0000 UTC,LastTransitionTime:2022-02-21 13:49:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 13:49:49.360: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-1055  d69bc7e2-e7b9-46fe-b3ef-7eb5d0fc89ad 26168 2 2022-02-21 13:49:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9cd59880-3541-41bd-b469-7cca9e889491 0xc002063967 0xc002063968}] []  [{kube-controller-manager Update apps/v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cd59880-3541-41bd-b469-7cca9e889491\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020639f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 21 13:49:49.363: INFO: Pod "test-new-deployment-5d9fdcc779-8zc8m" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-8zc8m test-new-deployment-5d9fdcc779- deployment-1055  c0e80a8f-acbf-4ac1-a56c-5746383e38e3 26153 0 2022-02-21 13:49:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.1.131/32 cni.projectcalico.org/podIPs:10.244.1.131/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 d69bc7e2-e7b9-46fe-b3ef-7eb5d0fc89ad 0xc003298167 0xc003298168}] []  [{Go-http-client Update v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-02-21 13:49:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d69bc7e2-e7b9-46fe-b3ef-7eb5d0fc89ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmctc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmctc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:49:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:49:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:49:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 13:49:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.131,PodIP:10.244.1.131,StartTime:2022-02-21 13:49:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 13:49:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dc8b1f715e4f5d15e64d314b3fbd78713eb121850be20d3f3eb44a8f47688672,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 13:49:49.363: INFO: Pod "test-new-deployment-5d9fdcc779-d79bm" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-d79bm test-new-deployment-5d9fdcc779- deployment-1055  f6bfa5c6-370c-4fce-bfd1-7275564d04c0 26165 0 2022-02-21 13:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 d69bc7e2-e7b9-46fe-b3ef-7eb5d0fc89ad 0xc003298370 0xc003298371}] []  [{kube-controller-manager Update v1 2022-02-21 13:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d69bc7e2-e7b9-46fe-b3ef-7eb5d0fc89ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmjn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmjn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:49.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1055" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":124,"skipped":2475,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 21 13:49:49.509: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 13:49:49.514: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 13:49:49.516: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-1 before test
Feb 21 13:49:49.521: INFO: test-new-deployment-5d9fdcc779-d79bm from deployment-1055 started at 2022-02-21 13:49:49 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container httpd ready: false, restart count 0
Feb 21 13:49:49.521: INFO: calico-kube-controllers-6d9dfd554f-x2lk5 from kube-system started at 2022-02-21 12:49:56 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 21 13:49:49.521: INFO: canal-vnh8t from kube-system started at 2022-02-21 12:49:43 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:49:49.521: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:49:49.521: INFO: kube-apiserver-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:49:49.521: INFO: kube-controller-manager-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:49:49.521: INFO: kube-proxy-r42th from kube-system started at 2022-02-21 12:49:43 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:49:49.521: INFO: kube-scheduler-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:49:49.521: INFO: openebs-ndm-lqth6 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:49:49.521: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-6psfh from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.521: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:49:49.521: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:49:49.521: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-2 before test
Feb 21 13:49:49.526: INFO: canal-9jnkf from kube-system started at 2022-02-21 12:51:19 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:49:49.526: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:49:49.526: INFO: coredns-599b888cfc-hvh9p from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:49:49.526: INFO: kube-apiserver-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:49:49.526: INFO: kube-controller-manager-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:49:49.526: INFO: kube-proxy-9s7vr from kube-system started at 2022-02-21 12:51:19 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:49:49.526: INFO: kube-scheduler-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:49:49.526: INFO: openebs-ndm-operator-5788658f97-r6nkb from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container node-disk-operator ready: true, restart count 0
Feb 21 13:49:49.526: INFO: openebs-ndm-zsg6k from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:49:49.526: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-8t6r2 from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.526: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:49:49.526: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:49:49.526: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-3 before test
Feb 21 13:49:49.531: INFO: test-new-deployment-5d9fdcc779-8zc8m from deployment-1055 started at 2022-02-21 13:49:47 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container httpd ready: true, restart count 0
Feb 21 13:49:49.531: INFO: canal-l6tfl from kube-system started at 2022-02-21 12:51:15 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:49:49.531: INFO: 	Container kube-flannel ready: true, restart count 1
Feb 21 13:49:49.531: INFO: coredns-599b888cfc-n8dbh from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:49:49.531: INFO: kube-apiserver-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:49:49.531: INFO: kube-controller-manager-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:49:49.531: INFO: kube-proxy-mjljh from kube-system started at 2022-02-21 12:51:15 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:49:49.531: INFO: kube-scheduler-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:49:49.531: INFO: kubelet-rubber-stamp-7f44cf9f9-stx2z from kube-system started at 2022-02-21 12:51:35 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 0
Feb 21 13:49:49.531: INFO: openebs-ndm-v6zq8 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:49:49.531: INFO: rafay-connector-v3-5ff84d7bb-sqhgm from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container connector ready: true, restart count 0
Feb 21 13:49:49.531: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-66hxs from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:49:49.531: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:49:49.531: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-4 before test
Feb 21 13:49:49.536: INFO: canal-x2v7q from kube-system started at 2022-02-21 12:53:38 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:49:49.536: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:49:49.536: INFO: kube-proxy-xzzph from kube-system started at 2022-02-21 12:53:38 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:49:49.536: INFO: openebs-localpv-provisioner-6867454bf5-wvwjv from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 0
Feb 21 13:49:49.536: INFO: openebs-ndm-pm2fl from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:49:49.536: INFO: controller-manager-v3-6b6c76796b-gv5np from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container manager ready: true, restart count 0
Feb 21 13:49:49.536: INFO: edge-client-86d55698bf-sx7hx from rafay-system started at 2022-02-21 12:57:39 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container edge-client ready: true, restart count 0
Feb 21 13:49:49.536: INFO: relay-agent-7694556d-48tl4 from rafay-system started at 2022-02-21 12:57:42 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container relay-agent ready: true, restart count 0
Feb 21 13:49:49.536: INFO: sonobuoy from sonobuoy started at 2022-02-21 13:15:07 +0000 UTC (1 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 21 13:49:49.536: INFO: sonobuoy-e2e-job-575d91b12b2744fe from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container e2e ready: true, restart count 0
Feb 21 13:49:49.536: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:49:49.536: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-98wsf from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:49:49.536: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:49:49.536: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-54b472d5-86f1-41ad-9e31-b06cc5762bb8 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-54b472d5-86f1-41ad-9e31-b06cc5762bb8 off the node aksh-cncf-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-54b472d5-86f1-41ad-9e31-b06cc5762bb8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:49:53.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4144" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":125,"skipped":2478,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:49:53.609: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Feb 21 13:49:53.741: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 21 13:49:53.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 create -f -'
Feb 21 13:49:54.161: INFO: stderr: ""
Feb 21 13:49:54.161: INFO: stdout: "service/agnhost-replica created\n"
Feb 21 13:49:54.161: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 21 13:49:54.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 create -f -'
Feb 21 13:49:54.640: INFO: stderr: ""
Feb 21 13:49:54.640: INFO: stdout: "service/agnhost-primary created\n"
Feb 21 13:49:54.640: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 21 13:49:54.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 create -f -'
Feb 21 13:49:54.783: INFO: stderr: ""
Feb 21 13:49:54.783: INFO: stdout: "service/frontend created\n"
Feb 21 13:49:54.783: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 21 13:49:54.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 create -f -'
Feb 21 13:49:54.894: INFO: stderr: ""
Feb 21 13:49:54.894: INFO: stdout: "deployment.apps/frontend created\n"
Feb 21 13:49:54.894: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 21 13:49:54.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 create -f -'
Feb 21 13:49:55.008: INFO: stderr: ""
Feb 21 13:49:55.008: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 21 13:49:55.009: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 21 13:49:55.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 create -f -'
Feb 21 13:49:55.122: INFO: stderr: ""
Feb 21 13:49:55.122: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Feb 21 13:49:55.122: INFO: Waiting for all frontend pods to be Running.
Feb 21 13:50:00.173: INFO: Waiting for frontend to serve content.
Feb 21 13:50:00.183: INFO: Trying to add a new entry to the guestbook.
Feb 21 13:50:00.190: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 21 13:50:00.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 delete --grace-period=0 --force -f -'
Feb 21 13:50:00.251: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 13:50:00.251: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 13:50:00.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 delete --grace-period=0 --force -f -'
Feb 21 13:50:00.306: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 13:50:00.306: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 13:50:00.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 delete --grace-period=0 --force -f -'
Feb 21 13:50:00.358: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 13:50:00.358: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 13:50:00.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 delete --grace-period=0 --force -f -'
Feb 21 13:50:00.401: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 13:50:00.401: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 13:50:00.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 delete --grace-period=0 --force -f -'
Feb 21 13:50:00.445: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 13:50:00.445: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 13:50:00.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1324 delete --grace-period=0 --force -f -'
Feb 21 13:50:00.490: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 13:50:00.490: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:50:00.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1324" for this suite.

• [SLOW TEST:6.892 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":126,"skipped":2491,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:50:00.501: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 13:50:00.948: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 13:50:03.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:50:05.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3257" for this suite.
STEP: Destroying namespace "webhook-3257-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":127,"skipped":2506,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:50:05.077: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 21 13:50:05.218: INFO: Waiting up to 5m0s for pod "pod-4a15bd86-dbfe-4882-8053-3e80538ea95f" in namespace "emptydir-3889" to be "Succeeded or Failed"
Feb 21 13:50:05.220: INFO: Pod "pod-4a15bd86-dbfe-4882-8053-3e80538ea95f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022499ms
Feb 21 13:50:07.227: INFO: Pod "pod-4a15bd86-dbfe-4882-8053-3e80538ea95f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008303957s
STEP: Saw pod success
Feb 21 13:50:07.227: INFO: Pod "pod-4a15bd86-dbfe-4882-8053-3e80538ea95f" satisfied condition "Succeeded or Failed"
Feb 21 13:50:07.229: INFO: Trying to get logs from node aksh-cncf-3 pod pod-4a15bd86-dbfe-4882-8053-3e80538ea95f container test-container: <nil>
STEP: delete the pod
Feb 21 13:50:07.242: INFO: Waiting for pod pod-4a15bd86-dbfe-4882-8053-3e80538ea95f to disappear
Feb 21 13:50:07.245: INFO: Pod pod-4a15bd86-dbfe-4882-8053-3e80538ea95f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:50:07.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3889" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2515,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:50:07.254: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 13:50:07.633: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 13:50:10.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:50:10.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1267" for this suite.
STEP: Destroying namespace "webhook-1267-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":129,"skipped":2536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:50:10.738: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-103640d4-79ee-411b-84ea-44787beff238
STEP: Creating a pod to test consume secrets
Feb 21 13:50:10.887: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5b94e5d9-63f0-42a4-8345-93641dc1a243" in namespace "projected-4611" to be "Succeeded or Failed"
Feb 21 13:50:10.889: INFO: Pod "pod-projected-secrets-5b94e5d9-63f0-42a4-8345-93641dc1a243": Phase="Pending", Reason="", readiness=false. Elapsed: 1.796381ms
Feb 21 13:50:12.894: INFO: Pod "pod-projected-secrets-5b94e5d9-63f0-42a4-8345-93641dc1a243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007098381s
STEP: Saw pod success
Feb 21 13:50:12.894: INFO: Pod "pod-projected-secrets-5b94e5d9-63f0-42a4-8345-93641dc1a243" satisfied condition "Succeeded or Failed"
Feb 21 13:50:12.897: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-secrets-5b94e5d9-63f0-42a4-8345-93641dc1a243 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 13:50:12.912: INFO: Waiting for pod pod-projected-secrets-5b94e5d9-63f0-42a4-8345-93641dc1a243 to disappear
Feb 21 13:50:12.914: INFO: Pod pod-projected-secrets-5b94e5d9-63f0-42a4-8345-93641dc1a243 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:50:12.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4611" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2575,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:50:12.922: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-1928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:56:01.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1928" for this suite.

• [SLOW TEST:348.170 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":131,"skipped":2581,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:56:01.092: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:56:01.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2144" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":132,"skipped":2609,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:56:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:56:01.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1750" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":133,"skipped":2614,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:56:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2726
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 13:56:02.584: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:56:02.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2726" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2634,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:56:02.607: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-d4cb31e2-fcb0-4ec3-8838-e828a140333f
STEP: Creating a pod to test consume configMaps
Feb 21 13:56:02.751: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86f6640d-e0de-4ad8-8391-301ea29a6d8c" in namespace "projected-7819" to be "Succeeded or Failed"
Feb 21 13:56:02.753: INFO: Pod "pod-projected-configmaps-86f6640d-e0de-4ad8-8391-301ea29a6d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081158ms
Feb 21 13:56:04.758: INFO: Pod "pod-projected-configmaps-86f6640d-e0de-4ad8-8391-301ea29a6d8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007115783s
STEP: Saw pod success
Feb 21 13:56:04.758: INFO: Pod "pod-projected-configmaps-86f6640d-e0de-4ad8-8391-301ea29a6d8c" satisfied condition "Succeeded or Failed"
Feb 21 13:56:04.760: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-configmaps-86f6640d-e0de-4ad8-8391-301ea29a6d8c container agnhost-container: <nil>
STEP: delete the pod
Feb 21 13:56:04.781: INFO: Waiting for pod pod-projected-configmaps-86f6640d-e0de-4ad8-8391-301ea29a6d8c to disappear
Feb 21 13:56:04.783: INFO: Pod pod-projected-configmaps-86f6640d-e0de-4ad8-8391-301ea29a6d8c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:56:04.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7819" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":135,"skipped":2734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:56:04.792: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-649
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Feb 21 13:56:04.936: INFO: Found 0 stateful pods, waiting for 3
Feb 21 13:56:14.940: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:56:14.941: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:56:14.941: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Feb 21 13:56:14.964: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 21 13:56:24.998: INFO: Updating stateful set ss2
Feb 21 13:56:25.002: INFO: Waiting for Pod statefulset-649/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Feb 21 13:56:35.043: INFO: Found 1 stateful pods, waiting for 3
Feb 21 13:56:45.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:56:45.048: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 13:56:45.048: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 21 13:56:45.071: INFO: Updating stateful set ss2
Feb 21 13:56:45.075: INFO: Waiting for Pod statefulset-649/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Feb 21 13:56:55.102: INFO: Updating stateful set ss2
Feb 21 13:56:55.107: INFO: Waiting for StatefulSet statefulset-649/ss2 to complete update
Feb 21 13:56:55.107: INFO: Waiting for Pod statefulset-649/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 13:57:05.115: INFO: Deleting all statefulset in ns statefulset-649
Feb 21 13:57:05.117: INFO: Scaling statefulset ss2 to 0
Feb 21 13:57:15.132: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:57:15.134: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:57:15.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-649" for this suite.

• [SLOW TEST:70.361 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":136,"skipped":2781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:57:15.153: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-7932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Feb 21 13:57:15.296: INFO: Waiting up to 5m0s for pod "security-context-bf4be56b-7ce1-41cb-8517-0922f3204ade" in namespace "security-context-7932" to be "Succeeded or Failed"
Feb 21 13:57:15.298: INFO: Pod "security-context-bf4be56b-7ce1-41cb-8517-0922f3204ade": Phase="Pending", Reason="", readiness=false. Elapsed: 1.610964ms
Feb 21 13:57:17.305: INFO: Pod "security-context-bf4be56b-7ce1-41cb-8517-0922f3204ade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008505282s
STEP: Saw pod success
Feb 21 13:57:17.305: INFO: Pod "security-context-bf4be56b-7ce1-41cb-8517-0922f3204ade" satisfied condition "Succeeded or Failed"
Feb 21 13:57:17.308: INFO: Trying to get logs from node aksh-cncf-3 pod security-context-bf4be56b-7ce1-41cb-8517-0922f3204ade container test-container: <nil>
STEP: delete the pod
Feb 21 13:57:17.325: INFO: Waiting for pod security-context-bf4be56b-7ce1-41cb-8517-0922f3204ade to disappear
Feb 21 13:57:17.327: INFO: Pod security-context-bf4be56b-7ce1-41cb-8517-0922f3204ade no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:57:17.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7932" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":137,"skipped":2811,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:57:17.335: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:57:17.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6066" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":138,"skipped":2820,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:57:17.498: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Feb 21 13:57:17.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-8515 create -f -'
Feb 21 13:57:17.855: INFO: stderr: ""
Feb 21 13:57:17.855: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 21 13:57:18.859: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 13:57:18.859: INFO: Found 1 / 1
Feb 21 13:57:18.859: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 21 13:57:18.862: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 13:57:18.862: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 13:57:18.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-8515 patch pod agnhost-primary-h7x6w -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 21 13:57:18.910: INFO: stderr: ""
Feb 21 13:57:18.910: INFO: stdout: "pod/agnhost-primary-h7x6w patched\n"
STEP: checking annotations
Feb 21 13:57:18.913: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 13:57:18.913: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:57:18.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8515" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":139,"skipped":2823,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:57:18.922: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-154
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-154
Feb 21 13:57:19.069: INFO: Found 0 stateful pods, waiting for 1
Feb 21 13:57:29.075: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Feb 21 13:57:29.090: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Feb 21 13:57:29.097: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Feb 21 13:57:29.098: INFO: Observed &StatefulSet event: ADDED
Feb 21 13:57:29.098: INFO: Found Statefulset ss in namespace statefulset-154 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 21 13:57:29.098: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Feb 21 13:57:29.098: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 21 13:57:29.103: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Feb 21 13:57:29.104: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 13:57:29.104: INFO: Deleting all statefulset in ns statefulset-154
Feb 21 13:57:29.106: INFO: Scaling statefulset ss to 0
Feb 21 13:57:39.121: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 13:57:39.124: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:57:39.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-154" for this suite.

• [SLOW TEST:20.219 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":140,"skipped":2840,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:57:39.141: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 21 13:57:49.307: INFO: The status of Pod kube-controller-manager-aksh-cncf-3 is Running (Ready = true)
Feb 21 13:57:49.352: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:57:49.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9098" for this suite.

• [SLOW TEST:10.219 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":141,"skipped":2841,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:57:49.360: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7034
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Feb 21 13:57:49.501: INFO: Waiting up to 5m0s for pod "var-expansion-ad59a72d-941c-4ca2-9226-e1b59a74b903" in namespace "var-expansion-7034" to be "Succeeded or Failed"
Feb 21 13:57:49.503: INFO: Pod "var-expansion-ad59a72d-941c-4ca2-9226-e1b59a74b903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02337ms
Feb 21 13:57:51.510: INFO: Pod "var-expansion-ad59a72d-941c-4ca2-9226-e1b59a74b903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009346077s
STEP: Saw pod success
Feb 21 13:57:51.510: INFO: Pod "var-expansion-ad59a72d-941c-4ca2-9226-e1b59a74b903" satisfied condition "Succeeded or Failed"
Feb 21 13:57:51.512: INFO: Trying to get logs from node aksh-cncf-3 pod var-expansion-ad59a72d-941c-4ca2-9226-e1b59a74b903 container dapi-container: <nil>
STEP: delete the pod
Feb 21 13:57:51.531: INFO: Waiting for pod var-expansion-ad59a72d-941c-4ca2-9226-e1b59a74b903 to disappear
Feb 21 13:57:51.532: INFO: Pod var-expansion-ad59a72d-941c-4ca2-9226-e1b59a74b903 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 13:57:51.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7034" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2854,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 13:57:51.540: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Feb 21 13:57:51.675: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 13:57:51.681: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 13:57:51.683: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-1 before test
Feb 21 13:57:51.688: INFO: calico-kube-controllers-6d9dfd554f-x2lk5 from kube-system started at 2022-02-21 12:49:56 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 21 13:57:51.688: INFO: canal-vnh8t from kube-system started at 2022-02-21 12:49:43 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:57:51.688: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:57:51.688: INFO: kube-apiserver-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:57:51.688: INFO: kube-controller-manager-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:57:51.688: INFO: kube-proxy-r42th from kube-system started at 2022-02-21 12:49:43 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:57:51.688: INFO: kube-scheduler-aksh-cncf-1 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:57:51.688: INFO: openebs-ndm-lqth6 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:57:51.688: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-6psfh from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.688: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:57:51.688: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:57:51.688: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-2 before test
Feb 21 13:57:51.693: INFO: canal-9jnkf from kube-system started at 2022-02-21 12:51:19 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:57:51.693: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:57:51.693: INFO: coredns-599b888cfc-hvh9p from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:57:51.693: INFO: kube-apiserver-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:57:51.693: INFO: kube-controller-manager-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:57:51.693: INFO: kube-proxy-9s7vr from kube-system started at 2022-02-21 12:51:19 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:57:51.693: INFO: kube-scheduler-aksh-cncf-2 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:57:51.693: INFO: openebs-ndm-operator-5788658f97-r6nkb from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container node-disk-operator ready: true, restart count 0
Feb 21 13:57:51.693: INFO: openebs-ndm-zsg6k from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:57:51.693: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-8t6r2 from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.693: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:57:51.693: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:57:51.693: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-3 before test
Feb 21 13:57:51.698: INFO: canal-l6tfl from kube-system started at 2022-02-21 12:51:15 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:57:51.698: INFO: 	Container kube-flannel ready: true, restart count 1
Feb 21 13:57:51.698: INFO: coredns-599b888cfc-n8dbh from kube-system started at 2022-02-21 12:52:18 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container coredns ready: true, restart count 0
Feb 21 13:57:51.698: INFO: kube-apiserver-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 21 13:57:51.698: INFO: kube-controller-manager-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container kube-controller-manager ready: true, restart count 0
Feb 21 13:57:51.698: INFO: kube-proxy-mjljh from kube-system started at 2022-02-21 12:51:15 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:57:51.698: INFO: kube-scheduler-aksh-cncf-3 from kube-system started at 2022-02-21 12:51:45 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container kube-scheduler ready: true, restart count 0
Feb 21 13:57:51.698: INFO: kubelet-rubber-stamp-7f44cf9f9-stx2z from kube-system started at 2022-02-21 12:51:35 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container kubelet-rubber-stamp ready: true, restart count 0
Feb 21 13:57:51.698: INFO: openebs-ndm-v6zq8 from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:57:51.698: INFO: rafay-connector-v3-5ff84d7bb-sqhgm from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container connector ready: true, restart count 0
Feb 21 13:57:51.698: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-66hxs from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:57:51.698: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 13:57:51.698: INFO: 
Logging pods the apiserver thinks is on node aksh-cncf-4 before test
Feb 21 13:57:51.703: INFO: canal-x2v7q from kube-system started at 2022-02-21 12:53:38 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 13:57:51.703: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 21 13:57:51.703: INFO: kube-proxy-xzzph from kube-system started at 2022-02-21 12:53:38 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 13:57:51.703: INFO: openebs-localpv-provisioner-6867454bf5-wvwjv from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container openebs-provisioner-hostpath ready: true, restart count 0
Feb 21 13:57:51.703: INFO: openebs-ndm-pm2fl from openebs started at 2022-02-21 12:58:41 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container node-disk-manager ready: true, restart count 0
Feb 21 13:57:51.703: INFO: controller-manager-v3-6b6c76796b-gv5np from rafay-system started at 2022-02-21 13:00:06 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container manager ready: true, restart count 0
Feb 21 13:57:51.703: INFO: edge-client-86d55698bf-sx7hx from rafay-system started at 2022-02-21 12:57:39 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container edge-client ready: true, restart count 0
Feb 21 13:57:51.703: INFO: relay-agent-7694556d-48tl4 from rafay-system started at 2022-02-21 12:57:42 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container relay-agent ready: true, restart count 0
Feb 21 13:57:51.703: INFO: sonobuoy from sonobuoy started at 2022-02-21 13:15:07 +0000 UTC (1 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 21 13:57:51.703: INFO: sonobuoy-e2e-job-575d91b12b2744fe from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container e2e ready: true, restart count 0
Feb 21 13:57:51.703: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:57:51.703: INFO: sonobuoy-systemd-logs-daemon-set-ac2a1f6af41c44c4-98wsf from sonobuoy started at 2022-02-21 13:15:10 +0000 UTC (2 container statuses recorded)
Feb 21 13:57:51.703: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 13:57:51.703: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-22501b75-bbd4-40e1-bc43-0bbd5a0de459 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.131 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-22501b75-bbd4-40e1-bc43-0bbd5a0de459 off the node aksh-cncf-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-22501b75-bbd4-40e1-bc43-0bbd5a0de459
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:02:55.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7694" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.257 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":143,"skipped":2862,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:02:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4589
STEP: creating service affinity-nodeport-transition in namespace services-4589
STEP: creating replication controller affinity-nodeport-transition in namespace services-4589
I0221 14:02:55.954957      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4589, replica count: 3
I0221 14:02:59.005541      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 14:02:59.015: INFO: Creating new exec pod
Feb 21 14:03:02.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-4589 exec execpod-affinitykkr4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb 21 14:03:02.150: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 21 14:03:02.150: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:03:02.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-4589 exec execpod-affinitykkr4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.169.139 80'
Feb 21 14:03:02.257: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.169.139 80\nConnection to 10.108.169.139 80 port [tcp/http] succeeded!\n"
Feb 21 14:03:02.257: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:03:02.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-4589 exec execpod-affinitykkr4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.54 30192'
Feb 21 14:03:02.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.54 30192\nConnection to 10.0.0.54 30192 port [tcp/*] succeeded!\n"
Feb 21 14:03:02.361: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:03:02.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-4589 exec execpod-affinitykkr4j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.131 30192'
Feb 21 14:03:02.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.131 30192\nConnection to 10.0.0.131 30192 port [tcp/*] succeeded!\n"
Feb 21 14:03:02.454: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:03:02.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-4589 exec execpod-affinitykkr4j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.54:30192/ ; done'
Feb 21 14:03:02.603: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n"
Feb 21 14:03:02.603: INFO: stdout: "\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-txnmv\naffinity-nodeport-transition-txnmv\naffinity-nodeport-transition-zmhrt\naffinity-nodeport-transition-txnmv\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-txnmv\naffinity-nodeport-transition-zmhrt\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-txnmv\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-zmhrt\naffinity-nodeport-transition-zmhrt\naffinity-nodeport-transition-zmhrt\naffinity-nodeport-transition-zmhrt\naffinity-nodeport-transition-zmhrt"
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-txnmv
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-txnmv
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-zmhrt
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-txnmv
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-txnmv
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-zmhrt
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-txnmv
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-zmhrt
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-zmhrt
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-zmhrt
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-zmhrt
Feb 21 14:03:02.603: INFO: Received response from host: affinity-nodeport-transition-zmhrt
Feb 21 14:03:02.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-4589 exec execpod-affinitykkr4j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.54:30192/ ; done'
Feb 21 14:03:02.733: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.54:30192/\n"
Feb 21 14:03:02.733: INFO: stdout: "\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw\naffinity-nodeport-transition-t27vw"
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Received response from host: affinity-nodeport-transition-t27vw
Feb 21 14:03:02.733: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4589, will wait for the garbage collector to delete the pods
Feb 21 14:03:02.807: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.33641ms
Feb 21 14:03:02.907: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.327551ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:06.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4589" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.639 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":144,"skipped":2872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:06.437: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-3b2330a1-e9e1-4ee9-a2e0-4d625f003f8d
STEP: Creating a pod to test consume configMaps
Feb 21 14:03:06.582: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f57d5cd3-e05c-4303-80b3-7debbb33def4" in namespace "projected-2198" to be "Succeeded or Failed"
Feb 21 14:03:06.585: INFO: Pod "pod-projected-configmaps-f57d5cd3-e05c-4303-80b3-7debbb33def4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37797ms
Feb 21 14:03:08.591: INFO: Pod "pod-projected-configmaps-f57d5cd3-e05c-4303-80b3-7debbb33def4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008939286s
STEP: Saw pod success
Feb 21 14:03:08.591: INFO: Pod "pod-projected-configmaps-f57d5cd3-e05c-4303-80b3-7debbb33def4" satisfied condition "Succeeded or Failed"
Feb 21 14:03:08.593: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-configmaps-f57d5cd3-e05c-4303-80b3-7debbb33def4 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 14:03:08.618: INFO: Waiting for pod pod-projected-configmaps-f57d5cd3-e05c-4303-80b3-7debbb33def4 to disappear
Feb 21 14:03:08.620: INFO: Pod pod-projected-configmaps-f57d5cd3-e05c-4303-80b3-7debbb33def4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:08.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2198" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2900,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:08.627: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:03:08.768: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f6f71f13-5fae-4ad1-8ff2-ad0d56ac7ae4" in namespace "security-context-test-7669" to be "Succeeded or Failed"
Feb 21 14:03:08.770: INFO: Pod "busybox-privileged-false-f6f71f13-5fae-4ad1-8ff2-ad0d56ac7ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.360596ms
Feb 21 14:03:10.775: INFO: Pod "busybox-privileged-false-f6f71f13-5fae-4ad1-8ff2-ad0d56ac7ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006975045s
Feb 21 14:03:10.775: INFO: Pod "busybox-privileged-false-f6f71f13-5fae-4ad1-8ff2-ad0d56ac7ae4" satisfied condition "Succeeded or Failed"
Feb 21 14:03:10.779: INFO: Got logs for pod "busybox-privileged-false-f6f71f13-5fae-4ad1-8ff2-ad0d56ac7ae4": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:10.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7669" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2902,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7028
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:03:10.921: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:11.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7028" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":147,"skipped":2912,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:11.470: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:03:11.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-3909 version'
Feb 21 14:03:11.671: INFO: stderr: ""
Feb 21 14:03:11.671: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:25:17Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:11.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3909" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":148,"skipped":2913,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:11.680: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3055, will wait for the garbage collector to delete the pods
Feb 21 14:03:13.882: INFO: Deleting Job.batch foo took: 4.694701ms
Feb 21 14:03:13.983: INFO: Terminating Job.batch foo pods took: 101.091668ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:47.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3055" for this suite.

• [SLOW TEST:35.515 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":149,"skipped":2920,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:47.196: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1636
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:03:47.341: INFO: The status of Pod server-envvars-d8750aad-be0e-43ce-a72c-f91991c89743 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:03:49.343: INFO: The status of Pod server-envvars-d8750aad-be0e-43ce-a72c-f91991c89743 is Running (Ready = true)
Feb 21 14:03:49.362: INFO: Waiting up to 5m0s for pod "client-envvars-0b60b428-158d-47a7-8c4b-0a76440efb0f" in namespace "pods-1636" to be "Succeeded or Failed"
Feb 21 14:03:49.365: INFO: Pod "client-envvars-0b60b428-158d-47a7-8c4b-0a76440efb0f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.322086ms
Feb 21 14:03:51.367: INFO: Pod "client-envvars-0b60b428-158d-47a7-8c4b-0a76440efb0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005795411s
STEP: Saw pod success
Feb 21 14:03:51.367: INFO: Pod "client-envvars-0b60b428-158d-47a7-8c4b-0a76440efb0f" satisfied condition "Succeeded or Failed"
Feb 21 14:03:51.370: INFO: Trying to get logs from node aksh-cncf-1 pod client-envvars-0b60b428-158d-47a7-8c4b-0a76440efb0f container env3cont: <nil>
STEP: delete the pod
Feb 21 14:03:51.387: INFO: Waiting for pod client-envvars-0b60b428-158d-47a7-8c4b-0a76440efb0f to disappear
Feb 21 14:03:51.389: INFO: Pod client-envvars-0b60b428-158d-47a7-8c4b-0a76440efb0f no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:51.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1636" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2940,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:51.396: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-7821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Feb 21 14:03:53.564: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:03:55.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7821" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":151,"skipped":2941,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:03:55.578: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Feb 21 14:05:56.242: INFO: Successfully updated pod "var-expansion-7cd00435-109b-4c61-a4d7-c10480117466"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Feb 21 14:05:58.248: INFO: Deleting pod "var-expansion-7cd00435-109b-4c61-a4d7-c10480117466" in namespace "var-expansion-1411"
Feb 21 14:05:58.254: INFO: Wait up to 5m0s for pod "var-expansion-7cd00435-109b-4c61-a4d7-c10480117466" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:06:32.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1411" for this suite.

• [SLOW TEST:156.691 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":152,"skipped":2952,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:06:32.270: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 21 14:06:32.414: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 21 14:07:32.444: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:07:32.446: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-5107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:07:32.591: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Feb 21 14:07:32.593: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:07:32.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5107" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:07:32.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7915" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.398 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":153,"skipped":2959,"failed":0}
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:07:32.668: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 21 14:07:32.809: INFO: The status of Pod annotationupdate3cd2466c-238a-4922-ace9-9d0536ec9b90 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:07:34.814: INFO: The status of Pod annotationupdate3cd2466c-238a-4922-ace9-9d0536ec9b90 is Running (Ready = true)
Feb 21 14:07:35.337: INFO: Successfully updated pod "annotationupdate3cd2466c-238a-4922-ace9-9d0536ec9b90"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:07:39.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7485" for this suite.

• [SLOW TEST:6.694 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":2959,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:07:39.362: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5090.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5090.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5090.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5090.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 14:07:41.536: INFO: DNS probes using dns-5090/dns-test-2c2f007f-59e6-4d17-b823-7dd60415648e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:07:41.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5090" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":155,"skipped":2961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:07:41.574: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7159
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:07:41.707: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:07:44.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7159" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":156,"skipped":2995,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:07:44.822: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1758
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 21 14:07:44.956: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 21 14:07:56.957: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:07:59.437: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:09.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1758" for this suite.

• [SLOW TEST:24.488 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":157,"skipped":3014,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:09.310: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:08:09.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1248fa5-1395-4bfd-8c6b-e28a02301498" in namespace "projected-3986" to be "Succeeded or Failed"
Feb 21 14:08:09.452: INFO: Pod "downwardapi-volume-f1248fa5-1395-4bfd-8c6b-e28a02301498": Phase="Pending", Reason="", readiness=false. Elapsed: 1.738456ms
Feb 21 14:08:11.457: INFO: Pod "downwardapi-volume-f1248fa5-1395-4bfd-8c6b-e28a02301498": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006463817s
STEP: Saw pod success
Feb 21 14:08:11.457: INFO: Pod "downwardapi-volume-f1248fa5-1395-4bfd-8c6b-e28a02301498" satisfied condition "Succeeded or Failed"
Feb 21 14:08:11.459: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-f1248fa5-1395-4bfd-8c6b-e28a02301498 container client-container: <nil>
STEP: delete the pod
Feb 21 14:08:11.476: INFO: Waiting for pod downwardapi-volume-f1248fa5-1395-4bfd-8c6b-e28a02301498 to disappear
Feb 21 14:08:11.478: INFO: Pod downwardapi-volume-f1248fa5-1395-4bfd-8c6b-e28a02301498 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:11.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3986" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":3016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:11.486: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:08:11.623: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68f6f613-dfa7-40de-a9ad-b96e8996d578" in namespace "downward-api-9711" to be "Succeeded or Failed"
Feb 21 14:08:11.625: INFO: Pod "downwardapi-volume-68f6f613-dfa7-40de-a9ad-b96e8996d578": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433558ms
Feb 21 14:08:13.630: INFO: Pod "downwardapi-volume-68f6f613-dfa7-40de-a9ad-b96e8996d578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007117895s
STEP: Saw pod success
Feb 21 14:08:13.630: INFO: Pod "downwardapi-volume-68f6f613-dfa7-40de-a9ad-b96e8996d578" satisfied condition "Succeeded or Failed"
Feb 21 14:08:13.632: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-68f6f613-dfa7-40de-a9ad-b96e8996d578 container client-container: <nil>
STEP: delete the pod
Feb 21 14:08:13.649: INFO: Waiting for pod downwardapi-volume-68f6f613-dfa7-40de-a9ad-b96e8996d578 to disappear
Feb 21 14:08:13.651: INFO: Pod downwardapi-volume-68f6f613-dfa7-40de-a9ad-b96e8996d578 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:13.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9711" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":3040,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:13.659: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:24.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2940" for this suite.

• [SLOW TEST:11.179 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":160,"skipped":3050,"failed":0}
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:24.838: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-329" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":161,"skipped":3054,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:24.984: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6229
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-42167f5e-c816-4a39-a6ab-e4f92ab1c83f
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:25.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6229" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":162,"skipped":3058,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:25.124: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-291
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-291 to expose endpoints map[]
Feb 21 14:08:25.265: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 21 14:08:26.276: INFO: successfully validated that service endpoint-test2 in namespace services-291 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-291
Feb 21 14:08:26.284: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:08:28.289: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-291 to expose endpoints map[pod1:[80]]
Feb 21 14:08:28.297: INFO: successfully validated that service endpoint-test2 in namespace services-291 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Feb 21 14:08:28.297: INFO: Creating new exec pod
Feb 21 14:08:31.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-291 exec execpod8b6r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 21 14:08:31.526: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 21 14:08:31.526: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:08:31.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-291 exec execpod8b6r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.241.91 80'
Feb 21 14:08:31.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.241.91 80\nConnection to 10.105.241.91 80 port [tcp/http] succeeded!\n"
Feb 21 14:08:31.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-291
Feb 21 14:08:31.657: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:08:33.663: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-291 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 21 14:08:33.673: INFO: successfully validated that service endpoint-test2 in namespace services-291 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Feb 21 14:08:34.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-291 exec execpod8b6r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 21 14:08:34.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 21 14:08:34.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:08:34.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-291 exec execpod8b6r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.241.91 80'
Feb 21 14:08:34.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.241.91 80\nConnection to 10.105.241.91 80 port [tcp/http] succeeded!\n"
Feb 21 14:08:34.862: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-291
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-291 to expose endpoints map[pod2:[80]]
Feb 21 14:08:34.888: INFO: successfully validated that service endpoint-test2 in namespace services-291 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Feb 21 14:08:35.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-291 exec execpod8b6r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 21 14:08:35.986: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 21 14:08:35.986: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:08:35.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-291 exec execpod8b6r2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.241.91 80'
Feb 21 14:08:36.093: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.241.91 80\nConnection to 10.105.241.91 80 port [tcp/http] succeeded!\n"
Feb 21 14:08:36.093: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-291
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-291 to expose endpoints map[]
Feb 21 14:08:36.113: INFO: successfully validated that service endpoint-test2 in namespace services-291 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:36.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-291" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.020 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":163,"skipped":3061,"failed":0}
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:36.144: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:08:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 21 14:08:36.304: INFO: The status of Pod pod-logs-websocket-c6787c16-83c7-4b28-bc79-7dff92627868 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:08:38.308: INFO: The status of Pod pod-logs-websocket-c6787c16-83c7-4b28-bc79-7dff92627868 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:38.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-978" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":3061,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:38.324: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 21 14:08:38.465: INFO: Waiting up to 5m0s for pod "pod-ab0decb3-184a-4152-ab35-1859501998bb" in namespace "emptydir-205" to be "Succeeded or Failed"
Feb 21 14:08:38.467: INFO: Pod "pod-ab0decb3-184a-4152-ab35-1859501998bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307321ms
Feb 21 14:08:40.472: INFO: Pod "pod-ab0decb3-184a-4152-ab35-1859501998bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006595007s
STEP: Saw pod success
Feb 21 14:08:40.472: INFO: Pod "pod-ab0decb3-184a-4152-ab35-1859501998bb" satisfied condition "Succeeded or Failed"
Feb 21 14:08:40.473: INFO: Trying to get logs from node aksh-cncf-3 pod pod-ab0decb3-184a-4152-ab35-1859501998bb container test-container: <nil>
STEP: delete the pod
Feb 21 14:08:40.489: INFO: Waiting for pod pod-ab0decb3-184a-4152-ab35-1859501998bb to disappear
Feb 21 14:08:40.491: INFO: Pod pod-ab0decb3-184a-4152-ab35-1859501998bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:40.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-205" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":3077,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:40.498: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
STEP: creating an pod
Feb 21 14:08:40.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 21 14:08:40.674: INFO: stderr: ""
Feb 21 14:08:40.674: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Feb 21 14:08:40.674: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 21 14:08:40.674: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5356" to be "running and ready, or succeeded"
Feb 21 14:08:40.676: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.457148ms
Feb 21 14:08:42.681: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006882945s
Feb 21 14:08:42.681: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 21 14:08:42.681: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 21 14:08:42.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 logs logs-generator logs-generator'
Feb 21 14:08:42.726: INFO: stderr: ""
Feb 21 14:08:42.726: INFO: stdout: "I0221 14:08:41.340154       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/w22 262\nI0221 14:08:41.540276       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ml6 390\nI0221 14:08:41.741220       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/xccr 286\nI0221 14:08:41.940530       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/ts8t 278\nI0221 14:08:42.140829       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/nkl 542\nI0221 14:08:42.341134       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/8zbv 361\nI0221 14:08:42.540373       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/xbj 361\n"
STEP: limiting log lines
Feb 21 14:08:42.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 logs logs-generator logs-generator --tail=1'
Feb 21 14:08:42.773: INFO: stderr: ""
Feb 21 14:08:42.773: INFO: stdout: "I0221 14:08:42.740676       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/l9sv 257\n"
Feb 21 14:08:42.773: INFO: got output "I0221 14:08:42.740676       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/l9sv 257\n"
STEP: limiting log bytes
Feb 21 14:08:42.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 logs logs-generator logs-generator --limit-bytes=1'
Feb 21 14:08:42.817: INFO: stderr: ""
Feb 21 14:08:42.818: INFO: stdout: "I"
Feb 21 14:08:42.818: INFO: got output "I"
STEP: exposing timestamps
Feb 21 14:08:42.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 21 14:08:42.861: INFO: stderr: ""
Feb 21 14:08:42.861: INFO: stdout: "2022-02-21T14:08:42.740744514Z I0221 14:08:42.740676       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/l9sv 257\n"
Feb 21 14:08:42.861: INFO: got output "2022-02-21T14:08:42.740744514Z I0221 14:08:42.740676       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/l9sv 257\n"
STEP: restricting to a time range
Feb 21 14:08:45.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 logs logs-generator logs-generator --since=1s'
Feb 21 14:08:45.414: INFO: stderr: ""
Feb 21 14:08:45.414: INFO: stdout: "I0221 14:08:44.540311       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/75kx 392\nI0221 14:08:44.740610       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/p6h 359\nI0221 14:08:44.940922       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/f65t 526\nI0221 14:08:45.141212       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/wsjf 434\nI0221 14:08:45.340525       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/jwhv 203\n"
Feb 21 14:08:45.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 logs logs-generator logs-generator --since=24h'
Feb 21 14:08:45.457: INFO: stderr: ""
Feb 21 14:08:45.457: INFO: stdout: "I0221 14:08:41.340154       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/w22 262\nI0221 14:08:41.540276       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ml6 390\nI0221 14:08:41.741220       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/xccr 286\nI0221 14:08:41.940530       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/ts8t 278\nI0221 14:08:42.140829       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/nkl 542\nI0221 14:08:42.341134       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/8zbv 361\nI0221 14:08:42.540373       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/xbj 361\nI0221 14:08:42.740676       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/l9sv 257\nI0221 14:08:42.940976       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/4pw 487\nI0221 14:08:43.140221       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/l75 527\nI0221 14:08:43.340531       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/rnbn 440\nI0221 14:08:43.540853       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/cxn 419\nI0221 14:08:43.741139       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/qjs 563\nI0221 14:08:43.940457       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vqp5 474\nI0221 14:08:44.140753       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/9dvx 305\nI0221 14:08:44.341063       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/q6d 463\nI0221 14:08:44.540311       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/75kx 392\nI0221 14:08:44.740610       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/p6h 359\nI0221 14:08:44.940922       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/f65t 526\nI0221 14:08:45.141212       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/wsjf 434\nI0221 14:08:45.340525       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/jwhv 203\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
Feb 21 14:08:45.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-5356 delete pod logs-generator'
Feb 21 14:08:45.901: INFO: stderr: ""
Feb 21 14:08:45.901: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:45.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5356" for this suite.

• [SLOW TEST:5.411 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1406
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":166,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:45.909: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-6cce78c3-5be6-4b59-9b75-89eeea53e78d
STEP: Creating a pod to test consume secrets
Feb 21 14:08:46.050: INFO: Waiting up to 5m0s for pod "pod-secrets-4b9f0cb1-a309-4731-8df2-e7397b5d938a" in namespace "secrets-1623" to be "Succeeded or Failed"
Feb 21 14:08:46.052: INFO: Pod "pod-secrets-4b9f0cb1-a309-4731-8df2-e7397b5d938a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153219ms
Feb 21 14:08:48.055: INFO: Pod "pod-secrets-4b9f0cb1-a309-4731-8df2-e7397b5d938a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00481548s
STEP: Saw pod success
Feb 21 14:08:48.055: INFO: Pod "pod-secrets-4b9f0cb1-a309-4731-8df2-e7397b5d938a" satisfied condition "Succeeded or Failed"
Feb 21 14:08:48.057: INFO: Trying to get logs from node aksh-cncf-1 pod pod-secrets-4b9f0cb1-a309-4731-8df2-e7397b5d938a container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 14:08:48.074: INFO: Waiting for pod pod-secrets-4b9f0cb1-a309-4731-8df2-e7397b5d938a to disappear
Feb 21 14:08:48.076: INFO: Pod pod-secrets-4b9f0cb1-a309-4731-8df2-e7397b5d938a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:48.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1623" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":3105,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:48.084: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 21 14:08:48.217: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:52.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5657" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":3118,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:52.555: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-8635/secret-test-2e8b34ce-4f89-4b5b-b916-0829c37ec5d9
STEP: Creating a pod to test consume secrets
Feb 21 14:08:52.697: INFO: Waiting up to 5m0s for pod "pod-configmaps-33f538b0-0607-4fb1-9d0a-8cf48704c517" in namespace "secrets-8635" to be "Succeeded or Failed"
Feb 21 14:08:52.699: INFO: Pod "pod-configmaps-33f538b0-0607-4fb1-9d0a-8cf48704c517": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085671ms
Feb 21 14:08:54.704: INFO: Pod "pod-configmaps-33f538b0-0607-4fb1-9d0a-8cf48704c517": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006728502s
STEP: Saw pod success
Feb 21 14:08:54.704: INFO: Pod "pod-configmaps-33f538b0-0607-4fb1-9d0a-8cf48704c517" satisfied condition "Succeeded or Failed"
Feb 21 14:08:54.706: INFO: Trying to get logs from node aksh-cncf-1 pod pod-configmaps-33f538b0-0607-4fb1-9d0a-8cf48704c517 container env-test: <nil>
STEP: delete the pod
Feb 21 14:08:54.722: INFO: Waiting for pod pod-configmaps-33f538b0-0607-4fb1-9d0a-8cf48704c517 to disappear
Feb 21 14:08:54.724: INFO: Pod pod-configmaps-33f538b0-0607-4fb1-9d0a-8cf48704c517 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:54.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8635" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":169,"skipped":3123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:54.731: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Feb 21 14:08:54.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-4363 api-versions'
Feb 21 14:08:54.906: INFO: stderr: ""
Feb 21 14:08:54.906: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncluster.rafay.dev/v2\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nopenebs.io/v1alpha1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:08:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4363" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":170,"skipped":3167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:08:54.914: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:08:55.059: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 14:08:55.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:08:55.068: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:08:56.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 21 14:08:56.074: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:08:57.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 14:08:57.074: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 21 14:08:57.098: INFO: Wrong image for pod: daemon-set-55cw4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:57.098: INFO: Wrong image for pod: daemon-set-6hrkl. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:57.098: INFO: Wrong image for pod: daemon-set-rcsnc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:57.098: INFO: Wrong image for pod: daemon-set-rnwsc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:58.105: INFO: Wrong image for pod: daemon-set-55cw4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:58.105: INFO: Wrong image for pod: daemon-set-rcsnc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:58.105: INFO: Wrong image for pod: daemon-set-rnwsc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:59.105: INFO: Pod daemon-set-4f68w is not available
Feb 21 14:08:59.105: INFO: Wrong image for pod: daemon-set-55cw4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:59.105: INFO: Wrong image for pod: daemon-set-rcsnc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:08:59.105: INFO: Wrong image for pod: daemon-set-rnwsc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:09:00.105: INFO: Wrong image for pod: daemon-set-rcsnc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:09:00.105: INFO: Wrong image for pod: daemon-set-rnwsc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:09:01.105: INFO: Pod daemon-set-g9gvf is not available
Feb 21 14:09:01.105: INFO: Wrong image for pod: daemon-set-rcsnc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:09:01.105: INFO: Wrong image for pod: daemon-set-rnwsc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:09:02.105: INFO: Wrong image for pod: daemon-set-rcsnc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:09:03.107: INFO: Pod daemon-set-g2sjk is not available
Feb 21 14:09:03.107: INFO: Wrong image for pod: daemon-set-rcsnc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Feb 21 14:09:07.106: INFO: Pod daemon-set-49xq6 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 21 14:09:07.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 21 14:09:07.113: INFO: Node aksh-cncf-2 is running 0 daemon pod, expected 1
Feb 21 14:09:08.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 14:09:08.119: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3937, will wait for the garbage collector to delete the pods
Feb 21 14:09:08.186: INFO: Deleting DaemonSet.extensions daemon-set took: 5.08035ms
Feb 21 14:09:08.287: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.58104ms
Feb 21 14:09:10.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:09:10.991: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 21 14:09:10.993: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33028"},"items":null}

Feb 21 14:09:10.995: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33028"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:09:11.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3937" for this suite.

• [SLOW TEST:16.098 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":171,"skipped":3222,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:09:11.012: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 21 14:09:11.149: INFO: Waiting up to 5m0s for pod "downward-api-f948f271-2c7d-47f1-9bfd-0751f7aab4eb" in namespace "downward-api-7331" to be "Succeeded or Failed"
Feb 21 14:09:11.151: INFO: Pod "downward-api-f948f271-2c7d-47f1-9bfd-0751f7aab4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073717ms
Feb 21 14:09:13.157: INFO: Pod "downward-api-f948f271-2c7d-47f1-9bfd-0751f7aab4eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007861844s
STEP: Saw pod success
Feb 21 14:09:13.157: INFO: Pod "downward-api-f948f271-2c7d-47f1-9bfd-0751f7aab4eb" satisfied condition "Succeeded or Failed"
Feb 21 14:09:13.158: INFO: Trying to get logs from node aksh-cncf-1 pod downward-api-f948f271-2c7d-47f1-9bfd-0751f7aab4eb container dapi-container: <nil>
STEP: delete the pod
Feb 21 14:09:13.172: INFO: Waiting for pod downward-api-f948f271-2c7d-47f1-9bfd-0751f7aab4eb to disappear
Feb 21 14:09:13.174: INFO: Pod downward-api-f948f271-2c7d-47f1-9bfd-0751f7aab4eb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:09:13.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7331" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":3240,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:09:13.181: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:09:13.320: INFO: The status of Pod busybox-readonly-fs7f06e05a-0942-40d8-978a-74c22b026976 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:09:15.325: INFO: The status of Pod busybox-readonly-fs7f06e05a-0942-40d8-978a-74c22b026976 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:09:15.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5455" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3252,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:09:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Feb 21 14:09:35.568: INFO: EndpointSlice for Service endpointslice-2064/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:09:45.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2064" for this suite.

• [SLOW TEST:30.251 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":174,"skipped":3271,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:09:45.589: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-786
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-786
I0221 14:09:45.747043      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-786, replica count: 2
I0221 14:09:48.798345      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 14:09:48.798: INFO: Creating new exec pod
Feb 21 14:09:51.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-786 exec execpodfr5ml -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 21 14:09:51.935: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 21 14:09:51.935: INFO: stdout: "externalname-service-kqgjz"
Feb 21 14:09:51.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-786 exec execpodfr5ml -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.252.215 80'
Feb 21 14:09:52.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.252.215 80\nConnection to 10.108.252.215 80 port [tcp/http] succeeded!\n"
Feb 21 14:09:52.049: INFO: stdout: "externalname-service-kqgjz"
Feb 21 14:09:52.049: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:09:52.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-786" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:6.487 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":175,"skipped":3281,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:09:52.076: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:09:52.417: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:09:55.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:10:06.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9643" for this suite.
STEP: Destroying namespace "webhook-9643-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.500 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":176,"skipped":3319,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:10:06.575: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-6e7ecc7c-f565-4bcf-a43b-4acca0a3e4e4
STEP: Creating a pod to test consume configMaps
Feb 21 14:10:06.726: INFO: Waiting up to 5m0s for pod "pod-configmaps-baf2a674-627b-455e-96f6-0c00479ba27c" in namespace "configmap-4234" to be "Succeeded or Failed"
Feb 21 14:10:06.728: INFO: Pod "pod-configmaps-baf2a674-627b-455e-96f6-0c00479ba27c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00232ms
Feb 21 14:10:08.731: INFO: Pod "pod-configmaps-baf2a674-627b-455e-96f6-0c00479ba27c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005139051s
STEP: Saw pod success
Feb 21 14:10:08.731: INFO: Pod "pod-configmaps-baf2a674-627b-455e-96f6-0c00479ba27c" satisfied condition "Succeeded or Failed"
Feb 21 14:10:08.733: INFO: Trying to get logs from node aksh-cncf-3 pod pod-configmaps-baf2a674-627b-455e-96f6-0c00479ba27c container agnhost-container: <nil>
STEP: delete the pod
Feb 21 14:10:08.749: INFO: Waiting for pod pod-configmaps-baf2a674-627b-455e-96f6-0c00479ba27c to disappear
Feb 21 14:10:08.750: INFO: Pod pod-configmaps-baf2a674-627b-455e-96f6-0c00479ba27c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:10:08.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4234" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3329,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:10:08.758: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:10:10.903: INFO: Deleting pod "var-expansion-039096b3-e921-4808-9270-f4f9400fb1aa" in namespace "var-expansion-9821"
Feb 21 14:10:10.911: INFO: Wait up to 5m0s for pod "var-expansion-039096b3-e921-4808-9270-f4f9400fb1aa" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:10:14.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9821" for this suite.

• [SLOW TEST:6.172 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":178,"skipped":3343,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:10:14.930: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:10:15.233: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:10:18.254: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:10:18.257: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:10:21.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7115" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.449 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":179,"skipped":3356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:10:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-80a605e5-28e0-404e-a4c5-df4865dd7545 in namespace container-probe-9398
Feb 21 14:10:23.530: INFO: Started pod busybox-80a605e5-28e0-404e-a4c5-df4865dd7545 in namespace container-probe-9398
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 14:10:23.532: INFO: Initial restart count of pod busybox-80a605e5-28e0-404e-a4c5-df4865dd7545 is 0
Feb 21 14:11:13.662: INFO: Restart count of pod container-probe-9398/busybox-80a605e5-28e0-404e-a4c5-df4865dd7545 is now 1 (50.130040461s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9398" for this suite.

• [SLOW TEST:52.302 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:13.682: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-1687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Feb 21 14:11:13.821: INFO: created test-podtemplate-1
Feb 21 14:11:13.825: INFO: created test-podtemplate-2
Feb 21 14:11:13.829: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Feb 21 14:11:13.831: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Feb 21 14:11:13.843: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:13.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1687" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":181,"skipped":3460,"failed":0}
SSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:13.853: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-9143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 21 14:11:14.002: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 21 14:11:14.005: INFO: starting watch
STEP: patching
STEP: updating
Feb 21 14:11:14.014: INFO: waiting for watch events with expected annotations
Feb 21 14:11:14.015: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:14.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9143" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":182,"skipped":3467,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:14.053: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-vm42
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 14:11:14.197: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vm42" in namespace "subpath-444" to be "Succeeded or Failed"
Feb 21 14:11:14.198: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612839ms
Feb 21 14:11:16.203: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 2.00670989s
Feb 21 14:11:18.210: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 4.013256767s
Feb 21 14:11:20.214: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 6.01762921s
Feb 21 14:11:22.220: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 8.023201607s
Feb 21 14:11:24.225: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 10.028596377s
Feb 21 14:11:26.232: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 12.034899934s
Feb 21 14:11:28.238: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 14.040933788s
Feb 21 14:11:30.243: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 16.046337709s
Feb 21 14:11:32.246: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 18.049623444s
Feb 21 14:11:34.251: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Running", Reason="", readiness=true. Elapsed: 20.054693688s
Feb 21 14:11:36.256: INFO: Pod "pod-subpath-test-configmap-vm42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059461175s
STEP: Saw pod success
Feb 21 14:11:36.256: INFO: Pod "pod-subpath-test-configmap-vm42" satisfied condition "Succeeded or Failed"
Feb 21 14:11:36.258: INFO: Trying to get logs from node aksh-cncf-3 pod pod-subpath-test-configmap-vm42 container test-container-subpath-configmap-vm42: <nil>
STEP: delete the pod
Feb 21 14:11:36.272: INFO: Waiting for pod pod-subpath-test-configmap-vm42 to disappear
Feb 21 14:11:36.274: INFO: Pod pod-subpath-test-configmap-vm42 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vm42
Feb 21 14:11:36.274: INFO: Deleting pod "pod-subpath-test-configmap-vm42" in namespace "subpath-444"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:36.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-444" for this suite.

• [SLOW TEST:22.230 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":183,"skipped":3481,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:36.282: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-875
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Feb 21 14:11:36.424: INFO: Waiting up to 5m0s for pod "security-context-e3607bf6-48f3-4976-b6fe-baba6b999069" in namespace "security-context-875" to be "Succeeded or Failed"
Feb 21 14:11:36.426: INFO: Pod "security-context-e3607bf6-48f3-4976-b6fe-baba6b999069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.28715ms
Feb 21 14:11:38.430: INFO: Pod "security-context-e3607bf6-48f3-4976-b6fe-baba6b999069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006694017s
STEP: Saw pod success
Feb 21 14:11:38.430: INFO: Pod "security-context-e3607bf6-48f3-4976-b6fe-baba6b999069" satisfied condition "Succeeded or Failed"
Feb 21 14:11:38.433: INFO: Trying to get logs from node aksh-cncf-3 pod security-context-e3607bf6-48f3-4976-b6fe-baba6b999069 container test-container: <nil>
STEP: delete the pod
Feb 21 14:11:38.446: INFO: Waiting for pod security-context-e3607bf6-48f3-4976-b6fe-baba6b999069 to disappear
Feb 21 14:11:38.448: INFO: Pod security-context-e3607bf6-48f3-4976-b6fe-baba6b999069 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:38.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-875" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":184,"skipped":3492,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:38.455: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Feb 21 14:11:38.584: INFO: namespace kubectl-6828
Feb 21 14:11:38.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6828 create -f -'
Feb 21 14:11:39.183: INFO: stderr: ""
Feb 21 14:11:39.183: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb 21 14:11:40.188: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 14:11:40.188: INFO: Found 0 / 1
Feb 21 14:11:41.186: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 14:11:41.186: INFO: Found 1 / 1
Feb 21 14:11:41.186: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 14:11:41.189: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 21 14:11:41.189: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 14:11:41.189: INFO: wait on agnhost-primary startup in kubectl-6828 
Feb 21 14:11:41.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6828 logs agnhost-primary-wq2nz agnhost-primary'
Feb 21 14:11:41.242: INFO: stderr: ""
Feb 21 14:11:41.242: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 21 14:11:41.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6828 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 21 14:11:41.300: INFO: stderr: ""
Feb 21 14:11:41.300: INFO: stdout: "service/rm2 exposed\n"
Feb 21 14:11:41.305: INFO: Service rm2 in namespace kubectl-6828 found.
STEP: exposing service
Feb 21 14:11:43.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6828 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 21 14:11:43.362: INFO: stderr: ""
Feb 21 14:11:43.362: INFO: stdout: "service/rm3 exposed\n"
Feb 21 14:11:43.366: INFO: Service rm3 in namespace kubectl-6828 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:45.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6828" for this suite.

• [SLOW TEST:6.924 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":185,"skipped":3532,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:45.380: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-9940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 21 14:11:45.771: INFO: starting watch
STEP: patching
STEP: updating
Feb 21 14:11:45.780: INFO: waiting for watch events with expected annotations
Feb 21 14:11:45.780: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:45.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9940" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":186,"skipped":3540,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:45.830: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:11:45.959: INFO: Creating deployment "webserver-deployment"
Feb 21 14:11:45.963: INFO: Waiting for observed generation 1
Feb 21 14:11:47.970: INFO: Waiting for all required pods to come up
Feb 21 14:11:47.974: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 21 14:11:47.974: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 21 14:11:47.978: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 21 14:11:47.985: INFO: Updating deployment webserver-deployment
Feb 21 14:11:47.985: INFO: Waiting for observed generation 2
Feb 21 14:11:49.993: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 21 14:11:49.995: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 21 14:11:49.997: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 21 14:11:50.002: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 21 14:11:50.003: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 21 14:11:50.006: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 21 14:11:50.010: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 21 14:11:50.010: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 21 14:11:50.019: INFO: Updating deployment webserver-deployment
Feb 21 14:11:50.019: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 21 14:11:50.025: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 21 14:11:50.027: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 14:11:50.038: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9320  8ec26051-3d5a-40b9-b8b0-ff4e57bafdc0 34554 3 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035c3a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-21 14:11:47 +0000 UTC,LastTransitionTime:2022-02-21 14:11:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-02-21 14:11:48 +0000 UTC,LastTransitionTime:2022-02-21 14:11:45 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 21 14:11:50.051: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-9320  add61993-a13c-46fe-a4e8-ff419731f95c 34558 3 2022-02-21 14:11:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8ec26051-3d5a-40b9-b8b0-ff4e57bafdc0 0xc0035c3e47 0xc0035c3e48}] []  [{kube-controller-manager Update apps/v1 2022-02-21 14:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ec26051-3d5a-40b9-b8b0-ff4e57bafdc0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035c3ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 21 14:11:50.051: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 21 14:11:50.051: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-9320  71362510-304e-472a-bbf3-6fa05f8c5992 34555 3 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8ec26051-3d5a-40b9-b8b0-ff4e57bafdc0 0xc0035c3f47 0xc0035c3f48}] []  [{kube-controller-manager Update apps/v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ec26051-3d5a-40b9-b8b0-ff4e57bafdc0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035c3fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 21 14:11:50.065: INFO: Pod "webserver-deployment-566f96c878-5rwkf" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-5rwkf webserver-deployment-566f96c878- deployment-9320  aa4a7fef-79d2-42a6-9546-7346f1797462 34522 0 2022-02-21 14:11:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:10.244.1.175/32 cni.projectcalico.org/podIPs:10.244.1.175/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fa367 0xc0049fa368}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7gcz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7gcz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.131,PodIP:,StartTime:2022-02-21 14:11:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.065: INFO: Pod "webserver-deployment-566f96c878-8kz5v" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-8kz5v webserver-deployment-566f96c878- deployment-9320  766f710d-df5f-4fa9-bee5-8f71190d7c60 34523 0 2022-02-21 14:11:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:10.244.2.66/32 cni.projectcalico.org/podIPs:10.244.2.66/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fa590 0xc0049fa591}] []  [{Go-http-client Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jtkbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtkbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.211,PodIP:,StartTime:2022-02-21 14:11:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.065: INFO: Pod "webserver-deployment-566f96c878-gchzx" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-gchzx webserver-deployment-566f96c878- deployment-9320  af0c0e02-8dc4-4299-970e-eea73d7d5a59 34548 0 2022-02-21 14:11:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:10.244.3.60/32 cni.projectcalico.org/podIPs:10.244.3.60/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fa7b0 0xc0049fa7b1}] []  [{calico Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 14:11:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqs68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqs68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.114,PodIP:10.244.3.60,StartTime:2022-02-21 14:11:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.065: INFO: Pod "webserver-deployment-566f96c878-mc587" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-mc587 webserver-deployment-566f96c878- deployment-9320  a25eede8-ada4-4989-8cdc-edbc3ee80e97 34573 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fa9f0 0xc0049fa9f1}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fmm6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fmm6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-566f96c878-p7ftv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-p7ftv webserver-deployment-566f96c878- deployment-9320  40016e08-edc5-418e-abed-b36a134f7984 34551 0 2022-02-21 14:11:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:10.244.0.112/32 cni.projectcalico.org/podIPs:10.244.0.112/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fab57 0xc0049fab58}] []  [{calico Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 14:11:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lqmr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lqmr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.54,PodIP:10.244.0.112,StartTime:2022-02-21 14:11:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-566f96c878-rn98p" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-rn98p webserver-deployment-566f96c878- deployment-9320  9f232eba-b6a0-426e-8362-18d5f74bd798 34530 0 2022-02-21 14:11:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/podIP:10.244.2.67/32 cni.projectcalico.org/podIPs:10.244.2.67/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fadb0 0xc0049fadb1}] []  [{Go-http-client Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-02-21 14:11:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hg975,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hg975,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.211,PodIP:,StartTime:2022-02-21 14:11:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-566f96c878-tfqdk" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-tfqdk webserver-deployment-566f96c878- deployment-9320  0cd59913-e415-462f-b5e1-c1f72356b8b8 34571 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fafb0 0xc0049fafb1}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hnqlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hnqlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-566f96c878-vfr6m" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-vfr6m webserver-deployment-566f96c878- deployment-9320  9711973f-fac8-420d-b259-d2bb95c976cd 34574 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 add61993-a13c-46fe-a4e8-ff419731f95c 0xc0049fb120 0xc0049fb121}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"add61993-a13c-46fe-a4e8-ff419731f95c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m542z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m542z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-5d9fdcc779-5sgn4" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5sgn4 webserver-deployment-5d9fdcc779- deployment-9320  b65d8026-70ce-432d-977d-d4f0189bca02 34575 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fb267 0xc0049fb268}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sv9n4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sv9n4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-5d9fdcc779-6vfpq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-6vfpq webserver-deployment-5d9fdcc779- deployment-9320  6a14adeb-a17b-40d9-a768-76c781feb18e 34566 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fb3d0 0xc0049fb3d1}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67s5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67s5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-5d9fdcc779-7cjjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7cjjk webserver-deployment-5d9fdcc779- deployment-9320  182c6793-3526-4b4c-a129-5dd97c1d278d 34568 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fb530 0xc0049fb531}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2gp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2gp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-5d9fdcc779-9tm92" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9tm92 webserver-deployment-5d9fdcc779- deployment-9320  b4f32d70-7147-4857-b723-c260e8635a4b 34446 0 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.0.111/32 cni.projectcalico.org/podIPs:10.244.0.111/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fb687 0xc0049fb688}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmg48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmg48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.54,PodIP:10.244.0.111,StartTime:2022-02-21 14:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20bbf930c8d555101871a74acf8155b5edddd44c1c0e90bb8efd761c0a6b16e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.066: INFO: Pod "webserver-deployment-5d9fdcc779-dr6df" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-dr6df webserver-deployment-5d9fdcc779- deployment-9320  8bd3d53e-a70e-47d2-a2d4-230a3676e706 34437 0 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.3.59/32 cni.projectcalico.org/podIPs:10.244.3.59/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fb910 0xc0049fb911}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x746s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x746s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.114,PodIP:10.244.3.59,StartTime:2022-02-21 14:11:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c6e139c09f7812f146261e57386f199fe2ed73918e9ada8f044499a5d0d60b21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-grjc7" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-grjc7 webserver-deployment-5d9fdcc779- deployment-9320  91c24703-91ff-4f81-b7fc-979677d70a6b 34456 0 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.1.173/32 cni.projectcalico.org/podIPs:10.244.1.173/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fbb50 0xc0049fbb51}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-02-21 14:11:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6trd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6trd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.131,PodIP:10.244.1.173,StartTime:2022-02-21 14:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://21c58d190659ce83283152cef425e2adad23df6b13ee72d0a5a7eede28c049fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-h2j47" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-h2j47 webserver-deployment-5d9fdcc779- deployment-9320  308acf86-064d-4338-a896-290f0f671ea5 34453 0 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.1.174/32 cni.projectcalico.org/podIPs:10.244.1.174/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fbd80 0xc0049fbd81}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-02-21 14:11:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmgh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmgh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.131,PodIP:10.244.1.174,StartTime:2022-02-21 14:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://09cd8f8d59a9a8d41a6019d3f7a5fa2bf3d538d151b957ae46efb9a59b28f6db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-k8qqt" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-k8qqt webserver-deployment-5d9fdcc779- deployment-9320  bcc3362b-2a38-4512-8f0b-948b27fee35d 34443 0 2022-02-21 14:11:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.0.110/32 cni.projectcalico.org/podIPs:10.244.0.110/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0049fbfc0 0xc0049fbfc1}] []  [{Go-http-client Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l8vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l8vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.54,PodIP:10.244.0.110,StartTime:2022-02-21 14:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://664248c5a7da4c8ba9865ee8be7f1f723e0228cc49f36e12032db6a5a75338c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-npw56" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-npw56 webserver-deployment-5d9fdcc779- deployment-9320  c12b877c-42ed-4706-9b5d-e4462a523fb1 34565 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0057e01d0 0xc0057e01d1}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-627rn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-627rn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-pnxdq" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-pnxdq webserver-deployment-5d9fdcc779- deployment-9320  86e73789-f164-43b9-9d28-efc17a05b0e3 34450 0 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.0.109/32 cni.projectcalico.org/podIPs:10.244.0.109/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0057e0360 0xc0057e0361}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-02-21 14:11:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gt2v5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gt2v5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.54,PodIP:10.244.0.109,StartTime:2022-02-21 14:11:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a05175f8890ccc9fb4f7073be900429fa7c3e558372ab4ef62e3d7a6fc49c32d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-qmxxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-qmxxq webserver-deployment-5d9fdcc779- deployment-9320  46dd9246-7b7c-4a22-a217-ff87048c99bd 34572 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0057e05c0 0xc0057e05c1}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5g98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5g98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-rng8r" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rng8r webserver-deployment-5d9fdcc779- deployment-9320  6c1f7c3c-2899-47cb-8f8f-4abb16d52b5b 34462 0 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.2.64/32 cni.projectcalico.org/podIPs:10.244.2.64/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0057e0727 0xc0057e0728}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-02-21 14:11:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvqkl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvqkl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.211,PodIP:10.244.2.64,StartTime:2022-02-21 14:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5c000bdf35f8d80ee978838fe4a23ab22aa87c958c70a6ee84d6510f79ad2212,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-sxd8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-sxd8p webserver-deployment-5d9fdcc779- deployment-9320  84dec16b-3543-4eb8-8bce-4d0060d1ab50 34567 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0057e0930 0xc0057e0931}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2pnhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2pnhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-w29kd" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-w29kd webserver-deployment-5d9fdcc779- deployment-9320  47a90d46-aa85-4a03-adff-5e019ecb54e0 34433 0 2022-02-21 14:11:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/podIP:10.244.3.58/32 cni.projectcalico.org/podIPs:10.244.3.58/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0057e0ab0 0xc0057e0ab1}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2022-02-21 14:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jk6h2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jk6h2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.114,PodIP:10.244.3.58,StartTime:2022-02-21 14:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:11:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3c7bf4cd7d4a3f7167146d627fa62fca44b72f9d5ab3a4d4a11fcc66b20d68e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 21 14:11:50.067: INFO: Pod "webserver-deployment-5d9fdcc779-zt8gf" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-zt8gf webserver-deployment-5d9fdcc779- deployment-9320  7b388e92-449b-4996-9b57-b76c7311ab6a 34570 0 2022-02-21 14:11:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 71362510-304e-472a-bbf3-6fa05f8c5992 0xc0057e0cc0 0xc0057e0cc1}] []  [{kube-controller-manager Update v1 2022-02-21 14:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71362510-304e-472a-bbf3-6fa05f8c5992\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6pz88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6pz88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:50.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9320" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":187,"skipped":3548,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:50.116: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:11:50.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7673" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":188,"skipped":3563,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:11:50.298: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4354
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:12:50.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4354" for this suite.

• [SLOW TEST:60.153 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3566,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:12:50.451: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3f83c679-b1ce-4beb-8534-ec60d640ae53
STEP: Creating a pod to test consume secrets
Feb 21 14:12:50.595: INFO: Waiting up to 5m0s for pod "pod-secrets-f3cbf35d-b118-4922-b2cd-90c0b4f038b3" in namespace "secrets-6014" to be "Succeeded or Failed"
Feb 21 14:12:50.597: INFO: Pod "pod-secrets-f3cbf35d-b118-4922-b2cd-90c0b4f038b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02341ms
Feb 21 14:12:52.602: INFO: Pod "pod-secrets-f3cbf35d-b118-4922-b2cd-90c0b4f038b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006686706s
STEP: Saw pod success
Feb 21 14:12:52.602: INFO: Pod "pod-secrets-f3cbf35d-b118-4922-b2cd-90c0b4f038b3" satisfied condition "Succeeded or Failed"
Feb 21 14:12:52.605: INFO: Trying to get logs from node aksh-cncf-1 pod pod-secrets-f3cbf35d-b118-4922-b2cd-90c0b4f038b3 container secret-env-test: <nil>
STEP: delete the pod
Feb 21 14:12:52.621: INFO: Waiting for pod pod-secrets-f3cbf35d-b118-4922-b2cd-90c0b4f038b3 to disappear
Feb 21 14:12:52.623: INFO: Pod pod-secrets-f3cbf35d-b118-4922-b2cd-90c0b4f038b3 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:12:52.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6014" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3570,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:12:52.630: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8892
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 14:12:52.761: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 21 14:12:52.797: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:12:54.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:12:56.802: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:12:58.800: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:13:00.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:13:02.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:13:04.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:13:06.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:13:08.800: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:13:10.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:13:12.802: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 21 14:13:12.806: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb 21 14:13:14.811: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 21 14:13:14.815: INFO: The status of Pod netserver-2 is Running (Ready = true)
Feb 21 14:13:14.819: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Feb 21 14:13:16.839: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 21 14:13:16.839: INFO: Breadth first check of 10.244.0.114 on host 10.0.0.54...
Feb 21 14:13:16.841: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.178:9080/dial?request=hostname&protocol=http&host=10.244.0.114&port=8083&tries=1'] Namespace:pod-network-test-8892 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:13:16.841: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:13:16.842: INFO: ExecWithOptions: Clientset creation
Feb 21 14:13:16.842: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8892/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.178%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.114%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:13:16.917: INFO: Waiting for responses: map[]
Feb 21 14:13:16.917: INFO: reached 10.244.0.114 after 0/1 tries
Feb 21 14:13:16.917: INFO: Breadth first check of 10.244.2.68 on host 10.0.0.211...
Feb 21 14:13:16.919: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.178:9080/dial?request=hostname&protocol=http&host=10.244.2.68&port=8083&tries=1'] Namespace:pod-network-test-8892 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:13:16.919: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:13:16.920: INFO: ExecWithOptions: Clientset creation
Feb 21 14:13:16.920: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8892/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.178%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.68%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:13:16.956: INFO: Waiting for responses: map[]
Feb 21 14:13:16.956: INFO: reached 10.244.2.68 after 0/1 tries
Feb 21 14:13:16.956: INFO: Breadth first check of 10.244.1.177 on host 10.0.0.131...
Feb 21 14:13:16.958: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.178:9080/dial?request=hostname&protocol=http&host=10.244.1.177&port=8083&tries=1'] Namespace:pod-network-test-8892 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:13:16.958: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:13:16.959: INFO: ExecWithOptions: Clientset creation
Feb 21 14:13:16.959: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8892/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.178%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.177%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:13:17.017: INFO: Waiting for responses: map[]
Feb 21 14:13:17.017: INFO: reached 10.244.1.177 after 0/1 tries
Feb 21 14:13:17.017: INFO: Breadth first check of 10.244.3.61 on host 10.0.0.114...
Feb 21 14:13:17.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.178:9080/dial?request=hostname&protocol=http&host=10.244.3.61&port=8083&tries=1'] Namespace:pod-network-test-8892 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:13:17.019: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:13:17.020: INFO: ExecWithOptions: Clientset creation
Feb 21 14:13:17.020: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8892/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.178%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.61%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:13:17.061: INFO: Waiting for responses: map[]
Feb 21 14:13:17.061: INFO: reached 10.244.3.61 after 0/1 tries
Feb 21 14:13:17.061: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:13:17.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8892" for this suite.

• [SLOW TEST:24.437 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:13:17.068: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-3573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Feb 21 14:13:17.204: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Feb 21 14:13:17.209: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 21 14:13:17.209: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Feb 21 14:13:17.216: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 21 14:13:17.217: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Feb 21 14:13:17.223: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 21 14:13:17.223: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Feb 21 14:13:24.248: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:13:24.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3573" for this suite.

• [SLOW TEST:7.200 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":192,"skipped":3602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:13:24.268: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-d4j6w in namespace proxy-6868
I0221 14:13:24.417380      22 runners.go:193] Created replication controller with name: proxy-service-d4j6w, namespace: proxy-6868, replica count: 1
I0221 14:13:25.468687      22 runners.go:193] proxy-service-d4j6w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 14:13:26.469467      22 runners.go:193] proxy-service-d4j6w Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 14:13:26.473: INFO: setup took 2.07471708s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 21 14:13:26.480: INFO: (0) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 6.40269ms)
Feb 21 14:13:26.480: INFO: (0) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.988505ms)
Feb 21 14:13:26.481: INFO: (0) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 7.35359ms)
Feb 21 14:13:26.481: INFO: (0) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 7.722371ms)
Feb 21 14:13:26.481: INFO: (0) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 7.752359ms)
Feb 21 14:13:26.482: INFO: (0) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 8.201223ms)
Feb 21 14:13:26.482: INFO: (0) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 8.48982ms)
Feb 21 14:13:26.482: INFO: (0) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 8.460595ms)
Feb 21 14:13:26.482: INFO: (0) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 8.431619ms)
Feb 21 14:13:26.496: INFO: (0) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 22.777011ms)
Feb 21 14:13:26.497: INFO: (0) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 23.043199ms)
Feb 21 14:13:26.497: INFO: (0) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 23.05434ms)
Feb 21 14:13:26.497: INFO: (0) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 23.724495ms)
Feb 21 14:13:26.497: INFO: (0) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 24.040656ms)
Feb 21 14:13:26.497: INFO: (0) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 23.878016ms)
Feb 21 14:13:26.497: INFO: (0) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 23.915429ms)
Feb 21 14:13:26.503: INFO: (1) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.593732ms)
Feb 21 14:13:26.504: INFO: (1) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.999473ms)
Feb 21 14:13:26.504: INFO: (1) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 6.118379ms)
Feb 21 14:13:26.504: INFO: (1) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 6.044509ms)
Feb 21 14:13:26.504: INFO: (1) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 6.270649ms)
Feb 21 14:13:26.504: INFO: (1) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 6.690138ms)
Feb 21 14:13:26.504: INFO: (1) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 6.705767ms)
Feb 21 14:13:26.505: INFO: (1) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 7.204246ms)
Feb 21 14:13:26.505: INFO: (1) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 7.508203ms)
Feb 21 14:13:26.505: INFO: (1) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 8.007532ms)
Feb 21 14:13:26.506: INFO: (1) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 8.221983ms)
Feb 21 14:13:26.506: INFO: (1) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 8.212544ms)
Feb 21 14:13:26.506: INFO: (1) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 8.192155ms)
Feb 21 14:13:26.506: INFO: (1) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 8.495472ms)
Feb 21 14:13:26.506: INFO: (1) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 8.653122ms)
Feb 21 14:13:26.506: INFO: (1) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 8.768551ms)
Feb 21 14:13:26.513: INFO: (2) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.925294ms)
Feb 21 14:13:26.513: INFO: (2) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.920245ms)
Feb 21 14:13:26.513: INFO: (2) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 6.966072ms)
Feb 21 14:13:26.513: INFO: (2) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 6.85452ms)
Feb 21 14:13:26.513: INFO: (2) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 6.869348ms)
Feb 21 14:13:26.514: INFO: (2) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 7.090658ms)
Feb 21 14:13:26.514: INFO: (2) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 7.317291ms)
Feb 21 14:13:26.514: INFO: (2) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 7.431788ms)
Feb 21 14:13:26.514: INFO: (2) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 7.569672ms)
Feb 21 14:13:26.514: INFO: (2) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 7.536217ms)
Feb 21 14:13:26.514: INFO: (2) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 7.644923ms)
Feb 21 14:13:26.514: INFO: (2) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 7.771274ms)
Feb 21 14:13:26.515: INFO: (2) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 8.225187ms)
Feb 21 14:13:26.515: INFO: (2) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 8.552792ms)
Feb 21 14:13:26.515: INFO: (2) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 8.618036ms)
Feb 21 14:13:26.515: INFO: (2) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 8.883741ms)
Feb 21 14:13:26.519: INFO: (3) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 3.902564ms)
Feb 21 14:13:26.520: INFO: (3) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.297877ms)
Feb 21 14:13:26.520: INFO: (3) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.756169ms)
Feb 21 14:13:26.521: INFO: (3) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.144587ms)
Feb 21 14:13:26.522: INFO: (3) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 6.03425ms)
Feb 21 14:13:26.522: INFO: (3) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 6.419483ms)
Feb 21 14:13:26.522: INFO: (3) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 6.826566ms)
Feb 21 14:13:26.522: INFO: (3) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 6.738028ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 7.180602ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 7.084488ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 7.448731ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 7.322581ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 7.276493ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 7.351306ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 7.540204ms)
Feb 21 14:13:26.523: INFO: (3) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 7.576884ms)
Feb 21 14:13:26.528: INFO: (4) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 4.850487ms)
Feb 21 14:13:26.528: INFO: (4) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.060397ms)
Feb 21 14:13:26.529: INFO: (4) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 5.570288ms)
Feb 21 14:13:26.529: INFO: (4) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.649979ms)
Feb 21 14:13:26.529: INFO: (4) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.649918ms)
Feb 21 14:13:26.529: INFO: (4) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.881219ms)
Feb 21 14:13:26.529: INFO: (4) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 6.157334ms)
Feb 21 14:13:26.529: INFO: (4) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.148367ms)
Feb 21 14:13:26.530: INFO: (4) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.561001ms)
Feb 21 14:13:26.530: INFO: (4) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 7.130346ms)
Feb 21 14:13:26.530: INFO: (4) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 7.052878ms)
Feb 21 14:13:26.530: INFO: (4) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 7.039101ms)
Feb 21 14:13:26.530: INFO: (4) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 7.190239ms)
Feb 21 14:13:26.531: INFO: (4) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 7.347137ms)
Feb 21 14:13:26.531: INFO: (4) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 7.324755ms)
Feb 21 14:13:26.531: INFO: (4) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 7.940065ms)
Feb 21 14:13:26.536: INFO: (5) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.116864ms)
Feb 21 14:13:26.537: INFO: (5) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 5.47703ms)
Feb 21 14:13:26.537: INFO: (5) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 5.745101ms)
Feb 21 14:13:26.537: INFO: (5) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 5.682622ms)
Feb 21 14:13:26.537: INFO: (5) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.853015ms)
Feb 21 14:13:26.537: INFO: (5) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 6.095797ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.209683ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 6.400789ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 6.613232ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 6.51738ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 6.612109ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 6.912672ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 7.018663ms)
Feb 21 14:13:26.538: INFO: (5) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 6.919725ms)
Feb 21 14:13:26.539: INFO: (5) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 7.635817ms)
Feb 21 14:13:26.539: INFO: (5) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 7.629134ms)
Feb 21 14:13:26.545: INFO: (6) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 5.15697ms)
Feb 21 14:13:26.545: INFO: (6) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 6.196067ms)
Feb 21 14:13:26.545: INFO: (6) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.983613ms)
Feb 21 14:13:26.546: INFO: (6) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 6.175779ms)
Feb 21 14:13:26.546: INFO: (6) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 6.131925ms)
Feb 21 14:13:26.546: INFO: (6) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 6.683304ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 7.153028ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 6.937478ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 7.329885ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 7.078536ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 7.287605ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 7.620578ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 8.130267ms)
Feb 21 14:13:26.547: INFO: (6) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 7.866907ms)
Feb 21 14:13:26.548: INFO: (6) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 8.285752ms)
Feb 21 14:13:26.548: INFO: (6) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 7.767537ms)
Feb 21 14:13:26.551: INFO: (7) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 3.182824ms)
Feb 21 14:13:26.551: INFO: (7) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 3.296732ms)
Feb 21 14:13:26.552: INFO: (7) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.666719ms)
Feb 21 14:13:26.552: INFO: (7) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 4.627214ms)
Feb 21 14:13:26.553: INFO: (7) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.362291ms)
Feb 21 14:13:26.553: INFO: (7) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 5.26675ms)
Feb 21 14:13:26.553: INFO: (7) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.311205ms)
Feb 21 14:13:26.553: INFO: (7) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 5.341361ms)
Feb 21 14:13:26.553: INFO: (7) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.594283ms)
Feb 21 14:13:26.553: INFO: (7) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 5.669986ms)
Feb 21 14:13:26.554: INFO: (7) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.73524ms)
Feb 21 14:13:26.554: INFO: (7) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 6.190798ms)
Feb 21 14:13:26.554: INFO: (7) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.67584ms)
Feb 21 14:13:26.554: INFO: (7) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 6.621266ms)
Feb 21 14:13:26.555: INFO: (7) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 7.10694ms)
Feb 21 14:13:26.555: INFO: (7) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 7.545305ms)
Feb 21 14:13:26.559: INFO: (8) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 3.839103ms)
Feb 21 14:13:26.559: INFO: (8) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 3.854983ms)
Feb 21 14:13:26.560: INFO: (8) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 4.567719ms)
Feb 21 14:13:26.561: INFO: (8) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.292489ms)
Feb 21 14:13:26.561: INFO: (8) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 5.415633ms)
Feb 21 14:13:26.561: INFO: (8) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.567793ms)
Feb 21 14:13:26.561: INFO: (8) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 5.837605ms)
Feb 21 14:13:26.561: INFO: (8) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 5.852544ms)
Feb 21 14:13:26.562: INFO: (8) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 6.255621ms)
Feb 21 14:13:26.562: INFO: (8) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.193103ms)
Feb 21 14:13:26.562: INFO: (8) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 6.255861ms)
Feb 21 14:13:26.562: INFO: (8) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 6.469047ms)
Feb 21 14:13:26.562: INFO: (8) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 6.606258ms)
Feb 21 14:13:26.562: INFO: (8) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 6.515485ms)
Feb 21 14:13:26.562: INFO: (8) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 6.748459ms)
Feb 21 14:13:26.563: INFO: (8) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 7.178016ms)
Feb 21 14:13:26.570: INFO: (9) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 7.266182ms)
Feb 21 14:13:26.570: INFO: (9) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 7.28498ms)
Feb 21 14:13:26.570: INFO: (9) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 7.14372ms)
Feb 21 14:13:26.570: INFO: (9) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 7.242218ms)
Feb 21 14:13:26.570: INFO: (9) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 7.641017ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 7.878769ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 7.743572ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 7.781974ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 7.828042ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 7.838953ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 8.127853ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 8.518356ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 8.493819ms)
Feb 21 14:13:26.571: INFO: (9) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 8.576366ms)
Feb 21 14:13:26.572: INFO: (9) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 9.260967ms)
Feb 21 14:13:26.572: INFO: (9) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 9.218237ms)
Feb 21 14:13:26.575: INFO: (10) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 2.710307ms)
Feb 21 14:13:26.577: INFO: (10) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 4.423595ms)
Feb 21 14:13:26.577: INFO: (10) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 4.682269ms)
Feb 21 14:13:26.577: INFO: (10) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.782378ms)
Feb 21 14:13:26.577: INFO: (10) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 5.147693ms)
Feb 21 14:13:26.577: INFO: (10) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 5.32994ms)
Feb 21 14:13:26.577: INFO: (10) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.252874ms)
Feb 21 14:13:26.578: INFO: (10) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 5.069926ms)
Feb 21 14:13:26.578: INFO: (10) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 5.334969ms)
Feb 21 14:13:26.578: INFO: (10) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 5.462432ms)
Feb 21 14:13:26.578: INFO: (10) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.396225ms)
Feb 21 14:13:26.578: INFO: (10) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 5.933268ms)
Feb 21 14:13:26.578: INFO: (10) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.00291ms)
Feb 21 14:13:26.579: INFO: (10) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 6.37068ms)
Feb 21 14:13:26.579: INFO: (10) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 6.536847ms)
Feb 21 14:13:26.579: INFO: (10) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 6.807832ms)
Feb 21 14:13:26.582: INFO: (11) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 3.269169ms)
Feb 21 14:13:26.583: INFO: (11) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 3.509397ms)
Feb 21 14:13:26.583: INFO: (11) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 3.839615ms)
Feb 21 14:13:26.583: INFO: (11) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 3.594559ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 4.482547ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 3.786914ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 4.542222ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 4.419247ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.295563ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 4.245537ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 5.297137ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 4.622324ms)
Feb 21 14:13:26.584: INFO: (11) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 4.982077ms)
Feb 21 14:13:26.585: INFO: (11) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 5.105103ms)
Feb 21 14:13:26.585: INFO: (11) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 4.980075ms)
Feb 21 14:13:26.586: INFO: (11) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 6.470189ms)
Feb 21 14:13:26.596: INFO: (12) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 10.494415ms)
Feb 21 14:13:26.597: INFO: (12) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 10.9278ms)
Feb 21 14:13:26.597: INFO: (12) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 10.999275ms)
Feb 21 14:13:26.597: INFO: (12) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 11.338301ms)
Feb 21 14:13:26.598: INFO: (12) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 11.664471ms)
Feb 21 14:13:26.598: INFO: (12) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 12.048111ms)
Feb 21 14:13:26.598: INFO: (12) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 11.93229ms)
Feb 21 14:13:26.598: INFO: (12) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 12.168319ms)
Feb 21 14:13:26.598: INFO: (12) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 12.606262ms)
Feb 21 14:13:26.598: INFO: (12) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 12.31565ms)
Feb 21 14:13:26.598: INFO: (12) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 12.444416ms)
Feb 21 14:13:26.599: INFO: (12) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 12.67363ms)
Feb 21 14:13:26.599: INFO: (12) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 12.989662ms)
Feb 21 14:13:26.599: INFO: (12) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 13.114971ms)
Feb 21 14:13:26.599: INFO: (12) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 13.410603ms)
Feb 21 14:13:26.599: INFO: (12) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 13.441091ms)
Feb 21 14:13:26.603: INFO: (13) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 3.252869ms)
Feb 21 14:13:26.604: INFO: (13) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 4.329997ms)
Feb 21 14:13:26.604: INFO: (13) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 4.413316ms)
Feb 21 14:13:26.605: INFO: (13) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 4.857672ms)
Feb 21 14:13:26.605: INFO: (13) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.50386ms)
Feb 21 14:13:26.606: INFO: (13) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.593571ms)
Feb 21 14:13:26.606: INFO: (13) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.75603ms)
Feb 21 14:13:26.606: INFO: (13) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 5.818569ms)
Feb 21 14:13:26.606: INFO: (13) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 6.548467ms)
Feb 21 14:13:26.606: INFO: (13) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.890266ms)
Feb 21 14:13:26.606: INFO: (13) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 6.685298ms)
Feb 21 14:13:26.606: INFO: (13) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 6.512109ms)
Feb 21 14:13:26.608: INFO: (13) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 8.451038ms)
Feb 21 14:13:26.610: INFO: (13) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 9.991057ms)
Feb 21 14:13:26.610: INFO: (13) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 9.667401ms)
Feb 21 14:13:26.610: INFO: (13) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 10.308722ms)
Feb 21 14:13:26.614: INFO: (14) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 3.540225ms)
Feb 21 14:13:26.614: INFO: (14) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 3.671273ms)
Feb 21 14:13:26.614: INFO: (14) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 4.376707ms)
Feb 21 14:13:26.615: INFO: (14) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 4.781737ms)
Feb 21 14:13:26.615: INFO: (14) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 4.700472ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.152442ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 5.032183ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.376738ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 5.656922ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 5.45613ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 5.738646ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 5.853986ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 6.095877ms)
Feb 21 14:13:26.616: INFO: (14) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 5.96057ms)
Feb 21 14:13:26.617: INFO: (14) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 6.398693ms)
Feb 21 14:13:26.617: INFO: (14) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 6.569107ms)
Feb 21 14:13:26.620: INFO: (15) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 3.137479ms)
Feb 21 14:13:26.621: INFO: (15) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 3.801102ms)
Feb 21 14:13:26.621: INFO: (15) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 3.95326ms)
Feb 21 14:13:26.621: INFO: (15) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 4.263411ms)
Feb 21 14:13:26.621: INFO: (15) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 4.288789ms)
Feb 21 14:13:26.621: INFO: (15) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 4.241921ms)
Feb 21 14:13:26.622: INFO: (15) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.428214ms)
Feb 21 14:13:26.622: INFO: (15) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 4.851669ms)
Feb 21 14:13:26.622: INFO: (15) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 5.281959ms)
Feb 21 14:13:26.622: INFO: (15) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 5.223968ms)
Feb 21 14:13:26.622: INFO: (15) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 5.112867ms)
Feb 21 14:13:26.623: INFO: (15) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 5.19343ms)
Feb 21 14:13:26.623: INFO: (15) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.308881ms)
Feb 21 14:13:26.623: INFO: (15) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 5.974216ms)
Feb 21 14:13:26.623: INFO: (15) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 5.563724ms)
Feb 21 14:13:26.623: INFO: (15) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 6.017708ms)
Feb 21 14:13:26.627: INFO: (16) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 3.197193ms)
Feb 21 14:13:26.627: INFO: (16) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 3.536588ms)
Feb 21 14:13:26.628: INFO: (16) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.22645ms)
Feb 21 14:13:26.628: INFO: (16) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 4.08979ms)
Feb 21 14:13:26.628: INFO: (16) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 4.782921ms)
Feb 21 14:13:26.628: INFO: (16) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 4.220881ms)
Feb 21 14:13:26.628: INFO: (16) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 4.827464ms)
Feb 21 14:13:26.628: INFO: (16) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 4.48371ms)
Feb 21 14:13:26.628: INFO: (16) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 4.886387ms)
Feb 21 14:13:26.629: INFO: (16) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.032224ms)
Feb 21 14:13:26.629: INFO: (16) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 5.516557ms)
Feb 21 14:13:26.629: INFO: (16) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.386538ms)
Feb 21 14:13:26.629: INFO: (16) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 5.575827ms)
Feb 21 14:13:26.630: INFO: (16) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 5.717878ms)
Feb 21 14:13:26.630: INFO: (16) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 6.165709ms)
Feb 21 14:13:26.630: INFO: (16) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 6.659631ms)
Feb 21 14:13:26.634: INFO: (17) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 3.93247ms)
Feb 21 14:13:26.636: INFO: (17) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.826334ms)
Feb 21 14:13:26.636: INFO: (17) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.85582ms)
Feb 21 14:13:26.636: INFO: (17) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.802478ms)
Feb 21 14:13:26.636: INFO: (17) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 5.79754ms)
Feb 21 14:13:26.636: INFO: (17) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 5.868915ms)
Feb 21 14:13:26.637: INFO: (17) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 6.124763ms)
Feb 21 14:13:26.637: INFO: (17) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 6.467483ms)
Feb 21 14:13:26.637: INFO: (17) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 6.83359ms)
Feb 21 14:13:26.637: INFO: (17) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 6.759679ms)
Feb 21 14:13:26.637: INFO: (17) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 6.685129ms)
Feb 21 14:13:26.637: INFO: (17) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 6.803773ms)
Feb 21 14:13:26.638: INFO: (17) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 6.960471ms)
Feb 21 14:13:26.638: INFO: (17) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 7.033921ms)
Feb 21 14:13:26.638: INFO: (17) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 7.170752ms)
Feb 21 14:13:26.639: INFO: (17) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 8.162427ms)
Feb 21 14:13:26.644: INFO: (18) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 5.125091ms)
Feb 21 14:13:26.644: INFO: (18) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 5.223257ms)
Feb 21 14:13:26.645: INFO: (18) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 5.827676ms)
Feb 21 14:13:26.645: INFO: (18) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 5.842295ms)
Feb 21 14:13:26.645: INFO: (18) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 6.267192ms)
Feb 21 14:13:26.645: INFO: (18) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 6.301348ms)
Feb 21 14:13:26.645: INFO: (18) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 6.31304ms)
Feb 21 14:13:26.646: INFO: (18) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.659188ms)
Feb 21 14:13:26.646: INFO: (18) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 6.629573ms)
Feb 21 14:13:26.646: INFO: (18) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 6.742267ms)
Feb 21 14:13:26.646: INFO: (18) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 7.067666ms)
Feb 21 14:13:26.646: INFO: (18) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 7.591111ms)
Feb 21 14:13:26.647: INFO: (18) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 7.675773ms)
Feb 21 14:13:26.647: INFO: (18) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 7.889328ms)
Feb 21 14:13:26.647: INFO: (18) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 7.941338ms)
Feb 21 14:13:26.647: INFO: (18) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 7.916881ms)
Feb 21 14:13:26.654: INFO: (19) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">... (200; 6.820064ms)
Feb 21 14:13:26.654: INFO: (19) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:460/proxy/: tls baz (200; 7.162315ms)
Feb 21 14:13:26.655: INFO: (19) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:1080/proxy/rewriteme">test<... (200; 7.507522ms)
Feb 21 14:13:26.655: INFO: (19) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:462/proxy/: tls qux (200; 7.671343ms)
Feb 21 14:13:26.655: INFO: (19) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn/proxy/rewriteme">test</a> (200; 7.993928ms)
Feb 21 14:13:26.655: INFO: (19) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname1/proxy/: tls baz (200; 8.082256ms)
Feb 21 14:13:26.655: INFO: (19) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 8.054403ms)
Feb 21 14:13:26.655: INFO: (19) /api/v1/namespaces/proxy-6868/pods/proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 8.222573ms)
Feb 21 14:13:26.656: INFO: (19) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname2/proxy/: bar (200; 8.751459ms)
Feb 21 14:13:26.656: INFO: (19) /api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/: <a href="/api/v1/namespaces/proxy-6868/pods/https:proxy-service-d4j6w-vf9pn:443/proxy/tlsrewritem... (200; 8.68904ms)
Feb 21 14:13:26.656: INFO: (19) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:160/proxy/: foo (200; 9.079854ms)
Feb 21 14:13:26.656: INFO: (19) /api/v1/namespaces/proxy-6868/pods/http:proxy-service-d4j6w-vf9pn:162/proxy/: bar (200; 9.045198ms)
Feb 21 14:13:26.657: INFO: (19) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname2/proxy/: bar (200; 9.632787ms)
Feb 21 14:13:26.657: INFO: (19) /api/v1/namespaces/proxy-6868/services/https:proxy-service-d4j6w:tlsportname2/proxy/: tls qux (200; 9.636273ms)
Feb 21 14:13:26.657: INFO: (19) /api/v1/namespaces/proxy-6868/services/http:proxy-service-d4j6w:portname1/proxy/: foo (200; 9.264133ms)
Feb 21 14:13:26.658: INFO: (19) /api/v1/namespaces/proxy-6868/services/proxy-service-d4j6w:portname1/proxy/: foo (200; 11.262907ms)
STEP: deleting ReplicationController proxy-service-d4j6w in namespace proxy-6868, will wait for the garbage collector to delete the pods
Feb 21 14:13:26.717: INFO: Deleting ReplicationController proxy-service-d4j6w took: 5.313027ms
Feb 21 14:13:26.817: INFO: Terminating ReplicationController proxy-service-d4j6w pods took: 100.087841ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:13:30.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6868" for this suite.

• [SLOW TEST:6.260 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":193,"skipped":3684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:13:30.529: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7494
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:13:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 21 14:13:33.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-7494 --namespace=crd-publish-openapi-7494 create -f -'
Feb 21 14:13:33.700: INFO: stderr: ""
Feb 21 14:13:33.700: INFO: stdout: "e2e-test-crd-publish-openapi-2204-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 21 14:13:33.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-7494 --namespace=crd-publish-openapi-7494 delete e2e-test-crd-publish-openapi-2204-crds test-cr'
Feb 21 14:13:33.773: INFO: stderr: ""
Feb 21 14:13:33.773: INFO: stdout: "e2e-test-crd-publish-openapi-2204-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 21 14:13:33.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-7494 --namespace=crd-publish-openapi-7494 apply -f -'
Feb 21 14:13:34.207: INFO: stderr: ""
Feb 21 14:13:34.207: INFO: stdout: "e2e-test-crd-publish-openapi-2204-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 21 14:13:34.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-7494 --namespace=crd-publish-openapi-7494 delete e2e-test-crd-publish-openapi-2204-crds test-cr'
Feb 21 14:13:34.252: INFO: stderr: ""
Feb 21 14:13:34.252: INFO: stdout: "e2e-test-crd-publish-openapi-2204-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 21 14:13:34.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-7494 explain e2e-test-crd-publish-openapi-2204-crds'
Feb 21 14:13:34.650: INFO: stderr: ""
Feb 21 14:13:34.650: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2204-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:13:37.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7494" for this suite.

• [SLOW TEST:6.531 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":194,"skipped":3712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:13:37.060: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-5873
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5873
STEP: Deleting pre-stop pod
Feb 21 14:13:46.244: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:13:46.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5873" for this suite.

• [SLOW TEST:9.215 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":195,"skipped":3766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:13:46.276: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 21 14:13:46.411: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:13:48.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7698" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":196,"skipped":3792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:13:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:13:48.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6374" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":197,"skipped":3839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:13:48.640: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:13:49.061: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:13:52.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:14:04.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6344" for this suite.
STEP: Destroying namespace "webhook-6344-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.588 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":198,"skipped":3863,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:14:04.227: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-3898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:04.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3898" for this suite.

• [SLOW TEST:300.176 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":199,"skipped":3879,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:04.404: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9353
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-aac37c89-b5ba-40f0-9ea3-a2885111af2e
STEP: Creating a pod to test consume configMaps
Feb 21 14:19:04.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-55807219-964c-4830-87b2-abdff188ab53" in namespace "configmap-9353" to be "Succeeded or Failed"
Feb 21 14:19:04.556: INFO: Pod "pod-configmaps-55807219-964c-4830-87b2-abdff188ab53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034744ms
Feb 21 14:19:06.565: INFO: Pod "pod-configmaps-55807219-964c-4830-87b2-abdff188ab53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010236394s
STEP: Saw pod success
Feb 21 14:19:06.565: INFO: Pod "pod-configmaps-55807219-964c-4830-87b2-abdff188ab53" satisfied condition "Succeeded or Failed"
Feb 21 14:19:06.566: INFO: Trying to get logs from node aksh-cncf-3 pod pod-configmaps-55807219-964c-4830-87b2-abdff188ab53 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 14:19:06.587: INFO: Waiting for pod pod-configmaps-55807219-964c-4830-87b2-abdff188ab53 to disappear
Feb 21 14:19:06.591: INFO: Pod pod-configmaps-55807219-964c-4830-87b2-abdff188ab53 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:06.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9353" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3895,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:06.599: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-550
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:17.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-550" for this suite.

• [SLOW TEST:11.195 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":201,"skipped":3895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:17.795: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-4192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:19.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4192" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":202,"skipped":3931,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:19.993: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-522
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:19:20.127: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 21 14:19:22.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 create -f -'
Feb 21 14:19:23.251: INFO: stderr: ""
Feb 21 14:19:23.251: INFO: stdout: "e2e-test-crd-publish-openapi-8572-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 21 14:19:23.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 delete e2e-test-crd-publish-openapi-8572-crds test-foo'
Feb 21 14:19:23.296: INFO: stderr: ""
Feb 21 14:19:23.296: INFO: stdout: "e2e-test-crd-publish-openapi-8572-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 21 14:19:23.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 apply -f -'
Feb 21 14:19:23.757: INFO: stderr: ""
Feb 21 14:19:23.757: INFO: stdout: "e2e-test-crd-publish-openapi-8572-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 21 14:19:23.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 delete e2e-test-crd-publish-openapi-8572-crds test-foo'
Feb 21 14:19:23.802: INFO: stderr: ""
Feb 21 14:19:23.802: INFO: stdout: "e2e-test-crd-publish-openapi-8572-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Feb 21 14:19:23.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 create -f -'
Feb 21 14:19:24.238: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 21 14:19:24.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 create -f -'
Feb 21 14:19:24.336: INFO: rc: 1
Feb 21 14:19:24.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 apply -f -'
Feb 21 14:19:24.438: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 21 14:19:24.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 create -f -'
Feb 21 14:19:24.538: INFO: rc: 1
Feb 21 14:19:24.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 --namespace=crd-publish-openapi-522 apply -f -'
Feb 21 14:19:24.643: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 21 14:19:24.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 explain e2e-test-crd-publish-openapi-8572-crds'
Feb 21 14:19:24.746: INFO: stderr: ""
Feb 21 14:19:24.746: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8572-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 21 14:19:24.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 explain e2e-test-crd-publish-openapi-8572-crds.metadata'
Feb 21 14:19:24.853: INFO: stderr: ""
Feb 21 14:19:24.853: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8572-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 21 14:19:24.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 explain e2e-test-crd-publish-openapi-8572-crds.spec'
Feb 21 14:19:24.958: INFO: stderr: ""
Feb 21 14:19:24.958: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8572-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 21 14:19:24.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 explain e2e-test-crd-publish-openapi-8572-crds.spec.bars'
Feb 21 14:19:25.064: INFO: stderr: ""
Feb 21 14:19:25.064: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8572-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 21 14:19:25.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=crd-publish-openapi-522 explain e2e-test-crd-publish-openapi-8572-crds.spec.bars2'
Feb 21 14:19:25.164: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:27.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-522" for this suite.

• [SLOW TEST:7.633 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":203,"skipped":3931,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:27.625: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-d0638d10-564c-4ebf-8391-8431eac14c12
STEP: Creating a pod to test consume secrets
Feb 21 14:19:27.776: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1874e5e7-fca9-464b-ba49-c5571c4f8718" in namespace "projected-7685" to be "Succeeded or Failed"
Feb 21 14:19:27.779: INFO: Pod "pod-projected-secrets-1874e5e7-fca9-464b-ba49-c5571c4f8718": Phase="Pending", Reason="", readiness=false. Elapsed: 2.341617ms
Feb 21 14:19:29.784: INFO: Pod "pod-projected-secrets-1874e5e7-fca9-464b-ba49-c5571c4f8718": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007371891s
STEP: Saw pod success
Feb 21 14:19:29.784: INFO: Pod "pod-projected-secrets-1874e5e7-fca9-464b-ba49-c5571c4f8718" satisfied condition "Succeeded or Failed"
Feb 21 14:19:29.786: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-secrets-1874e5e7-fca9-464b-ba49-c5571c4f8718 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 14:19:29.801: INFO: Waiting for pod pod-projected-secrets-1874e5e7-fca9-464b-ba49-c5571c4f8718 to disappear
Feb 21 14:19:29.802: INFO: Pod pod-projected-secrets-1874e5e7-fca9-464b-ba49-c5571c4f8718 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:29.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7685" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3949,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:29.811: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 21 14:19:29.955: INFO: Waiting up to 5m0s for pod "pod-4b241a10-eeb6-4bbe-bcde-721d5e93be6c" in namespace "emptydir-6576" to be "Succeeded or Failed"
Feb 21 14:19:29.958: INFO: Pod "pod-4b241a10-eeb6-4bbe-bcde-721d5e93be6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904187ms
Feb 21 14:19:31.965: INFO: Pod "pod-4b241a10-eeb6-4bbe-bcde-721d5e93be6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009528655s
STEP: Saw pod success
Feb 21 14:19:31.965: INFO: Pod "pod-4b241a10-eeb6-4bbe-bcde-721d5e93be6c" satisfied condition "Succeeded or Failed"
Feb 21 14:19:31.967: INFO: Trying to get logs from node aksh-cncf-3 pod pod-4b241a10-eeb6-4bbe-bcde-721d5e93be6c container test-container: <nil>
STEP: delete the pod
Feb 21 14:19:31.983: INFO: Waiting for pod pod-4b241a10-eeb6-4bbe-bcde-721d5e93be6c to disappear
Feb 21 14:19:31.985: INFO: Pod pod-4b241a10-eeb6-4bbe-bcde-721d5e93be6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:31.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6576" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":205,"skipped":4023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:31.996: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5331" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":206,"skipped":4059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:34.170: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:19:34.305: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 21 14:19:36.339: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:37.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8244" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":207,"skipped":4089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:37.356: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Feb 21 14:19:37.527: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:19:39.533: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 21 14:19:39.544: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:19:41.549: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 21 14:19:41.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 14:19:41.566: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 14:19:43.567: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 14:19:43.571: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:43.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6938" for this suite.

• [SLOW TEST:6.226 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":208,"skipped":4127,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:43.582: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd
Feb 21 14:19:43.724: INFO: Pod name my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd: Found 0 pods out of 1
Feb 21 14:19:48.730: INFO: Pod name my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd: Found 1 pods out of 1
Feb 21 14:19:48.730: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd" are running
Feb 21 14:19:48.733: INFO: Pod "my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd-xn2qv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:19:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:19:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:19:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:19:43 +0000 UTC Reason: Message:}])
Feb 21 14:19:48.733: INFO: Trying to dial the pod
Feb 21 14:19:53.744: INFO: Controller my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd: Got expected result from replica 1 [my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd-xn2qv]: "my-hostname-basic-59d8b5f2-09f0-4dd2-9fb2-40ab01d933bd-xn2qv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:53.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3094" for this suite.

• [SLOW TEST:10.171 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":209,"skipped":4131,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:53.753: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 21 14:19:53.898: INFO: Waiting up to 5m0s for pod "downward-api-a8cfe8f6-9dd0-4ad0-8ba7-7d869cc7ea33" in namespace "downward-api-9961" to be "Succeeded or Failed"
Feb 21 14:19:53.901: INFO: Pod "downward-api-a8cfe8f6-9dd0-4ad0-8ba7-7d869cc7ea33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484678ms
Feb 21 14:19:55.906: INFO: Pod "downward-api-a8cfe8f6-9dd0-4ad0-8ba7-7d869cc7ea33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007743222s
STEP: Saw pod success
Feb 21 14:19:55.906: INFO: Pod "downward-api-a8cfe8f6-9dd0-4ad0-8ba7-7d869cc7ea33" satisfied condition "Succeeded or Failed"
Feb 21 14:19:55.908: INFO: Trying to get logs from node aksh-cncf-3 pod downward-api-a8cfe8f6-9dd0-4ad0-8ba7-7d869cc7ea33 container dapi-container: <nil>
STEP: delete the pod
Feb 21 14:19:55.925: INFO: Waiting for pod downward-api-a8cfe8f6-9dd0-4ad0-8ba7-7d869cc7ea33 to disappear
Feb 21 14:19:55.926: INFO: Pod downward-api-a8cfe8f6-9dd0-4ad0-8ba7-7d869cc7ea33 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:55.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9961" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":4147,"failed":0}
S
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:55.934: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-1848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:19:56.228: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 21 14:19:56.229: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 21 14:19:56.229: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 21 14:19:56.229: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 21 14:19:56.229: INFO: Checking APIGroup: apps
Feb 21 14:19:56.230: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 21 14:19:56.230: INFO: Versions found [{apps/v1 v1}]
Feb 21 14:19:56.230: INFO: apps/v1 matches apps/v1
Feb 21 14:19:56.230: INFO: Checking APIGroup: events.k8s.io
Feb 21 14:19:56.231: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 21 14:19:56.231: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Feb 21 14:19:56.231: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 21 14:19:56.231: INFO: Checking APIGroup: authentication.k8s.io
Feb 21 14:19:56.232: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 21 14:19:56.232: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 21 14:19:56.232: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 21 14:19:56.232: INFO: Checking APIGroup: authorization.k8s.io
Feb 21 14:19:56.232: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 21 14:19:56.232: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 21 14:19:56.232: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 21 14:19:56.232: INFO: Checking APIGroup: autoscaling
Feb 21 14:19:56.233: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 21 14:19:56.233: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Feb 21 14:19:56.233: INFO: autoscaling/v2 matches autoscaling/v2
Feb 21 14:19:56.233: INFO: Checking APIGroup: batch
Feb 21 14:19:56.234: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 21 14:19:56.234: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Feb 21 14:19:56.234: INFO: batch/v1 matches batch/v1
Feb 21 14:19:56.234: INFO: Checking APIGroup: certificates.k8s.io
Feb 21 14:19:56.234: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 21 14:19:56.234: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 21 14:19:56.234: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 21 14:19:56.234: INFO: Checking APIGroup: networking.k8s.io
Feb 21 14:19:56.235: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 21 14:19:56.235: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 21 14:19:56.235: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 21 14:19:56.235: INFO: Checking APIGroup: policy
Feb 21 14:19:56.235: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 21 14:19:56.235: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Feb 21 14:19:56.235: INFO: policy/v1 matches policy/v1
Feb 21 14:19:56.235: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 21 14:19:56.236: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 21 14:19:56.236: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 21 14:19:56.236: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 21 14:19:56.236: INFO: Checking APIGroup: storage.k8s.io
Feb 21 14:19:56.237: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 21 14:19:56.237: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 21 14:19:56.237: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 21 14:19:56.237: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 21 14:19:56.238: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 21 14:19:56.238: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 21 14:19:56.238: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 21 14:19:56.238: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 21 14:19:56.238: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 21 14:19:56.238: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 21 14:19:56.238: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 21 14:19:56.238: INFO: Checking APIGroup: scheduling.k8s.io
Feb 21 14:19:56.239: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 21 14:19:56.239: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 21 14:19:56.239: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 21 14:19:56.239: INFO: Checking APIGroup: coordination.k8s.io
Feb 21 14:19:56.240: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 21 14:19:56.240: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 21 14:19:56.240: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 21 14:19:56.240: INFO: Checking APIGroup: node.k8s.io
Feb 21 14:19:56.241: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 21 14:19:56.241: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Feb 21 14:19:56.241: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 21 14:19:56.241: INFO: Checking APIGroup: discovery.k8s.io
Feb 21 14:19:56.241: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 21 14:19:56.241: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Feb 21 14:19:56.241: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 21 14:19:56.241: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 21 14:19:56.242: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Feb 21 14:19:56.242: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 21 14:19:56.242: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Feb 21 14:19:56.242: INFO: Checking APIGroup: crd.projectcalico.org
Feb 21 14:19:56.243: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 21 14:19:56.243: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 21 14:19:56.243: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 21 14:19:56.243: INFO: Checking APIGroup: openebs.io
Feb 21 14:19:56.244: INFO: PreferredVersion.GroupVersion: openebs.io/v1alpha1
Feb 21 14:19:56.244: INFO: Versions found [{openebs.io/v1alpha1 v1alpha1}]
Feb 21 14:19:56.244: INFO: openebs.io/v1alpha1 matches openebs.io/v1alpha1
Feb 21 14:19:56.244: INFO: Checking APIGroup: cluster.rafay.dev
Feb 21 14:19:56.245: INFO: PreferredVersion.GroupVersion: cluster.rafay.dev/v2
Feb 21 14:19:56.245: INFO: Versions found [{cluster.rafay.dev/v2 v2}]
Feb 21 14:19:56.245: INFO: cluster.rafay.dev/v2 matches cluster.rafay.dev/v2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:56.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1848" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":211,"skipped":4148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:56.254: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1579
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Feb 21 14:19:56.396: INFO: Waiting up to 5m0s for pod "client-containers-0a2d4a70-0129-4a93-8311-ab4fc3bf73f3" in namespace "containers-1579" to be "Succeeded or Failed"
Feb 21 14:19:56.398: INFO: Pod "client-containers-0a2d4a70-0129-4a93-8311-ab4fc3bf73f3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.920977ms
Feb 21 14:19:58.406: INFO: Pod "client-containers-0a2d4a70-0129-4a93-8311-ab4fc3bf73f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01017717s
STEP: Saw pod success
Feb 21 14:19:58.406: INFO: Pod "client-containers-0a2d4a70-0129-4a93-8311-ab4fc3bf73f3" satisfied condition "Succeeded or Failed"
Feb 21 14:19:58.408: INFO: Trying to get logs from node aksh-cncf-3 pod client-containers-0a2d4a70-0129-4a93-8311-ab4fc3bf73f3 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 14:19:58.424: INFO: Waiting for pod client-containers-0a2d4a70-0129-4a93-8311-ab4fc3bf73f3 to disappear
Feb 21 14:19:58.426: INFO: Pod client-containers-0a2d4a70-0129-4a93-8311-ab4fc3bf73f3 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:58.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1579" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":212,"skipped":4171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:58.435: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2095
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:19:58.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2095" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":213,"skipped":4281,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:19:58.616: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 21 14:19:58.762: INFO: The status of Pod pod-update-cab3ec7f-1f9b-4ac5-a2d6-a267903a15e7 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:20:00.768: INFO: The status of Pod pod-update-cab3ec7f-1f9b-4ac5-a2d6-a267903a15e7 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 21 14:20:01.283: INFO: Successfully updated pod "pod-update-cab3ec7f-1f9b-4ac5-a2d6-a267903a15e7"
STEP: verifying the updated pod is in kubernetes
Feb 21 14:20:01.288: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:01.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6966" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:01.298: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9729
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9729.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9729.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9729.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9729.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9729.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 14:20:03.462: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local from pod dns-9729/dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2: the server could not find the requested resource (get pods dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2)
Feb 21 14:20:03.471: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9729.svc.cluster.local from pod dns-9729/dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2: the server could not find the requested resource (get pods dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2)
Feb 21 14:20:03.477: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local from pod dns-9729/dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2: the server could not find the requested resource (get pods dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2)
Feb 21 14:20:03.480: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9729.svc.cluster.local from pod dns-9729/dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2: the server could not find the requested resource (get pods dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2)
Feb 21 14:20:03.484: INFO: Lookups using dns-9729/dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9729.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local jessie_udp@dns-test-service-2.dns-9729.svc.cluster.local]

Feb 21 14:20:08.509: INFO: DNS probes using dns-9729/dns-test-28254b47-4cd5-4552-91b7-737fc7104ea2 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:08.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9729" for this suite.

• [SLOW TEST:7.260 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":215,"skipped":4320,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:08.559: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9831
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9891
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:21.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-435" for this suite.
STEP: Destroying namespace "nsdeletetest-9831" for this suite.
Feb 21 14:20:22.003: INFO: Namespace nsdeletetest-9831 was already deleted
STEP: Destroying namespace "nsdeletetest-9891" for this suite.

• [SLOW TEST:13.449 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":216,"skipped":4333,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:22.008: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-ccced26a-ee56-4dc4-830e-47d5fbfcfa69
STEP: Creating a pod to test consume configMaps
Feb 21 14:20:22.152: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ce33fe2-828e-4646-8f39-372aa199deba" in namespace "configmap-5504" to be "Succeeded or Failed"
Feb 21 14:20:22.154: INFO: Pod "pod-configmaps-0ce33fe2-828e-4646-8f39-372aa199deba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078916ms
Feb 21 14:20:24.164: INFO: Pod "pod-configmaps-0ce33fe2-828e-4646-8f39-372aa199deba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011403511s
STEP: Saw pod success
Feb 21 14:20:24.164: INFO: Pod "pod-configmaps-0ce33fe2-828e-4646-8f39-372aa199deba" satisfied condition "Succeeded or Failed"
Feb 21 14:20:24.166: INFO: Trying to get logs from node aksh-cncf-3 pod pod-configmaps-0ce33fe2-828e-4646-8f39-372aa199deba container agnhost-container: <nil>
STEP: delete the pod
Feb 21 14:20:24.181: INFO: Waiting for pod pod-configmaps-0ce33fe2-828e-4646-8f39-372aa199deba to disappear
Feb 21 14:20:24.183: INFO: Pod pod-configmaps-0ce33fe2-828e-4646-8f39-372aa199deba no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:24.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5504" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":4352,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:24.190: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:20:24.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4aae248-e913-4ce4-ab3f-d36ae48f393c" in namespace "downward-api-1628" to be "Succeeded or Failed"
Feb 21 14:20:24.337: INFO: Pod "downwardapi-volume-a4aae248-e913-4ce4-ab3f-d36ae48f393c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288275ms
Feb 21 14:20:26.343: INFO: Pod "downwardapi-volume-a4aae248-e913-4ce4-ab3f-d36ae48f393c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007618261s
STEP: Saw pod success
Feb 21 14:20:26.343: INFO: Pod "downwardapi-volume-a4aae248-e913-4ce4-ab3f-d36ae48f393c" satisfied condition "Succeeded or Failed"
Feb 21 14:20:26.344: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-a4aae248-e913-4ce4-ab3f-d36ae48f393c container client-container: <nil>
STEP: delete the pod
Feb 21 14:20:26.359: INFO: Waiting for pod downwardapi-volume-a4aae248-e913-4ce4-ab3f-d36ae48f393c to disappear
Feb 21 14:20:26.361: INFO: Pod downwardapi-volume-a4aae248-e913-4ce4-ab3f-d36ae48f393c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:26.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1628" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":4360,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:26.370: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Feb 21 14:20:26.511: INFO: created test-event-1
Feb 21 14:20:26.514: INFO: created test-event-2
Feb 21 14:20:26.517: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Feb 21 14:20:26.520: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Feb 21 14:20:26.534: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:26.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9303" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":219,"skipped":4369,"failed":0}
S
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:26.543: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Feb 21 14:20:26.689: INFO: The status of Pod pod-hostip-8e4b7d53-12c6-4152-ab08-1439953c3e6e is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:20:28.696: INFO: The status of Pod pod-hostip-8e4b7d53-12c6-4152-ab08-1439953c3e6e is Running (Ready = true)
Feb 21 14:20:28.701: INFO: Pod pod-hostip-8e4b7d53-12c6-4152-ab08-1439953c3e6e has hostIP: 10.0.0.131
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:28.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3034" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":220,"skipped":4370,"failed":0}
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:28.712: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4631
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:20:28.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4631" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4376,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:20:28.873: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2605
Feb 21 14:20:29.015: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:20:31.022: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 21 14:20:31.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2605 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 21 14:20:31.132: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 21 14:20:31.132: INFO: stdout: "iptables"
Feb 21 14:20:31.132: INFO: proxyMode: iptables
Feb 21 14:20:31.143: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 21 14:20:31.145: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2605
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2605
I0221 14:20:31.161104      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2605, replica count: 3
I0221 14:20:34.212015      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 14:20:34.219: INFO: Creating new exec pod
Feb 21 14:20:37.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2605 exec execpod-affinitym9rbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb 21 14:20:37.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 21 14:20:37.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:20:37.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2605 exec execpod-affinitym9rbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.63.188 80'
Feb 21 14:20:37.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.63.188 80\nConnection to 10.105.63.188 80 port [tcp/http] succeeded!\n"
Feb 21 14:20:37.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 21 14:20:37.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2605 exec execpod-affinitym9rbk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.63.188:80/ ; done'
Feb 21 14:20:37.595: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n"
Feb 21 14:20:37.595: INFO: stdout: "\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp\naffinity-clusterip-timeout-cz6pp"
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.595: INFO: Received response from host: affinity-clusterip-timeout-cz6pp
Feb 21 14:20:37.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2605 exec execpod-affinitym9rbk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.63.188:80/'
Feb 21 14:20:37.702: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n"
Feb 21 14:20:37.702: INFO: stdout: "affinity-clusterip-timeout-cz6pp"
Feb 21 14:20:57.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2605 exec execpod-affinitym9rbk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.63.188:80/'
Feb 21 14:20:57.817: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n"
Feb 21 14:20:57.817: INFO: stdout: "affinity-clusterip-timeout-cz6pp"
Feb 21 14:21:17.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2605 exec execpod-affinitym9rbk -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.63.188:80/'
Feb 21 14:21:17.928: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.63.188:80/\n"
Feb 21 14:21:17.928: INFO: stdout: "affinity-clusterip-timeout-rwjdd"
Feb 21 14:21:17.928: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2605, will wait for the garbage collector to delete the pods
Feb 21 14:21:18.001: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.339901ms
Feb 21 14:21:18.102: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.035216ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:21:22.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2605" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:53.455 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":222,"skipped":4383,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:21:22.328: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:21:22.463: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 21 14:21:22.471: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 21 14:21:27.485: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 14:21:27.485: INFO: Creating deployment "test-rolling-update-deployment"
Feb 21 14:21:27.491: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 21 14:21:27.496: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 21 14:21:29.504: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 21 14:21:29.506: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 14:21:29.513: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2060  c5336ec0-9938-4612-b389-764cd56e75a0 38501 1 2022-02-21 14:21:27 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-02-21 14:21:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:21:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057107d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-02-21 14:21:27 +0000 UTC,LastTransitionTime:2022-02-21 14:21:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-02-21 14:21:28 +0000 UTC,LastTransitionTime:2022-02-21 14:21:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 14:21:29.515: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-2060  0b791498-b907-4d75-bcb3-a8a9db1a126a 38491 1 2022-02-21 14:21:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c5336ec0-9938-4612-b389-764cd56e75a0 0xc005710cc7 0xc005710cc8}] []  [{kube-controller-manager Update apps/v1 2022-02-21 14:21:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5336ec0-9938-4612-b389-764cd56e75a0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:21:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005710d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 21 14:21:29.515: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 21 14:21:29.515: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2060  7f0adb9d-6201-41db-96ab-55c421707f50 38500 2 2022-02-21 14:21:22 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c5336ec0-9938-4612-b389-764cd56e75a0 0xc005710b97 0xc005710b98}] []  [{e2e.test Update apps/v1 2022-02-21 14:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:21:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5336ec0-9938-4612-b389-764cd56e75a0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:21:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005710c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 21 14:21:29.518: INFO: Pod "test-rolling-update-deployment-796dbc4547-ckkfq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-ckkfq test-rolling-update-deployment-796dbc4547- deployment-2060  3e853295-8933-4fea-b2f0-34c0a3e1377f 38490 0 2022-02-21 14:21:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[cni.projectcalico.org/podIP:10.244.0.122/32 cni.projectcalico.org/podIPs:10.244.0.122/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 0b791498-b907-4d75-bcb3-a8a9db1a126a 0xc005711217 0xc005711218}] []  [{calico Update v1 2022-02-21 14:21:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-02-21 14:21:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b791498-b907-4d75-bcb3-a8a9db1a126a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-02-21 14:21:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swkx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swkx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:21:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:21:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:21:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:21:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.54,PodIP:10.244.0.122,StartTime:2022-02-21 14:21:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:21:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://6f23ba848cc9bdff105129b1fc0354d0e414ec4291b9c6c06fff599731de3dd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:21:29.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2060" for this suite.

• [SLOW TEST:7.199 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":223,"skipped":4389,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:21:29.527: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 21 14:21:29.670: INFO: Waiting up to 5m0s for pod "pod-e97e1788-995e-417f-93e6-bde00c83db6f" in namespace "emptydir-2400" to be "Succeeded or Failed"
Feb 21 14:21:29.672: INFO: Pod "pod-e97e1788-995e-417f-93e6-bde00c83db6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13391ms
Feb 21 14:21:31.681: INFO: Pod "pod-e97e1788-995e-417f-93e6-bde00c83db6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011004411s
STEP: Saw pod success
Feb 21 14:21:31.681: INFO: Pod "pod-e97e1788-995e-417f-93e6-bde00c83db6f" satisfied condition "Succeeded or Failed"
Feb 21 14:21:31.683: INFO: Trying to get logs from node aksh-cncf-3 pod pod-e97e1788-995e-417f-93e6-bde00c83db6f container test-container: <nil>
STEP: delete the pod
Feb 21 14:21:31.703: INFO: Waiting for pod pod-e97e1788-995e-417f-93e6-bde00c83db6f to disappear
Feb 21 14:21:31.705: INFO: Pod pod-e97e1788-995e-417f-93e6-bde00c83db6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:21:31.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2400" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":224,"skipped":4396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:21:31.713: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-61
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5c7d7c92-e5ea-4b42-80ab-bee6f20084d2
STEP: Creating a pod to test consume secrets
Feb 21 14:21:31.857: INFO: Waiting up to 5m0s for pod "pod-secrets-a917f315-074c-4e80-b822-5f5a4de2b535" in namespace "secrets-61" to be "Succeeded or Failed"
Feb 21 14:21:31.859: INFO: Pod "pod-secrets-a917f315-074c-4e80-b822-5f5a4de2b535": Phase="Pending", Reason="", readiness=false. Elapsed: 1.987239ms
Feb 21 14:21:33.862: INFO: Pod "pod-secrets-a917f315-074c-4e80-b822-5f5a4de2b535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004922151s
STEP: Saw pod success
Feb 21 14:21:33.862: INFO: Pod "pod-secrets-a917f315-074c-4e80-b822-5f5a4de2b535" satisfied condition "Succeeded or Failed"
Feb 21 14:21:33.864: INFO: Trying to get logs from node aksh-cncf-3 pod pod-secrets-a917f315-074c-4e80-b822-5f5a4de2b535 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 14:21:33.879: INFO: Waiting for pod pod-secrets-a917f315-074c-4e80-b822-5f5a4de2b535 to disappear
Feb 21 14:21:33.881: INFO: Pod pod-secrets-a917f315-074c-4e80-b822-5f5a4de2b535 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:21:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-61" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4437,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:21:33.890: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Feb 21 14:21:36.042: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6339 PodName:var-expansion-073f1a93-fce4-48b7-b6b1-d0625cee2758 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:21:36.042: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:21:36.043: INFO: ExecWithOptions: Clientset creation
Feb 21 14:21:36.043: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6339/pods/var-expansion-073f1a93-fce4-48b7-b6b1-d0625cee2758/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Feb 21 14:21:36.103: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6339 PodName:var-expansion-073f1a93-fce4-48b7-b6b1-d0625cee2758 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:21:36.103: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:21:36.104: INFO: ExecWithOptions: Clientset creation
Feb 21 14:21:36.104: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6339/pods/var-expansion-073f1a93-fce4-48b7-b6b1-d0625cee2758/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Feb 21 14:21:36.660: INFO: Successfully updated pod "var-expansion-073f1a93-fce4-48b7-b6b1-d0625cee2758"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Feb 21 14:21:36.663: INFO: Deleting pod "var-expansion-073f1a93-fce4-48b7-b6b1-d0625cee2758" in namespace "var-expansion-6339"
Feb 21 14:21:36.669: INFO: Wait up to 5m0s for pod "var-expansion-073f1a93-fce4-48b7-b6b1-d0625cee2758" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:10.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6339" for this suite.

• [SLOW TEST:36.797 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":226,"skipped":4453,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:10.687: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 14:22:10.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:22:10.856: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:22:11.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 21 14:22:11.865: INFO: Node aksh-cncf-2 is running 0 daemon pod, expected 1
Feb 21 14:22:12.865: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 14:22:12.865: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 21 14:22:12.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 21 14:22:12.882: INFO: Node aksh-cncf-4 is running 0 daemon pod, expected 1
Feb 21 14:22:13.893: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 21 14:22:13.893: INFO: Node aksh-cncf-4 is running 0 daemon pod, expected 1
Feb 21 14:22:14.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 21 14:22:14.892: INFO: Node aksh-cncf-4 is running 0 daemon pod, expected 1
Feb 21 14:22:15.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 14:22:15.891: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1935, will wait for the garbage collector to delete the pods
Feb 21 14:22:15.952: INFO: Deleting DaemonSet.extensions daemon-set took: 7.180733ms
Feb 21 14:22:16.053: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.994533ms
Feb 21 14:22:18.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:22:18.858: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 21 14:22:18.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38919"},"items":null}

Feb 21 14:22:18.862: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38919"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:18.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1935" for this suite.

• [SLOW TEST:8.196 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":227,"skipped":4460,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:18.886: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 21 14:22:19.030: INFO: The status of Pod labelsupdate515e6b22-3bae-428d-b22d-7c59764104f1 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:22:21.036: INFO: The status of Pod labelsupdate515e6b22-3bae-428d-b22d-7c59764104f1 is Running (Ready = true)
Feb 21 14:22:21.560: INFO: Successfully updated pod "labelsupdate515e6b22-3bae-428d-b22d-7c59764104f1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:25.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7242" for this suite.

• [SLOW TEST:6.702 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":228,"skipped":4465,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:25.588: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:29.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5802" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4476,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:29.753: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-1634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:29.888: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-7752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1634
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:36.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7752" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:36.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1634" for this suite.

• [SLOW TEST:6.363 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":230,"skipped":4485,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:36.117: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:36.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6026" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":231,"skipped":4496,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:36.286: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:22:36.748: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:22:39.770: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:39.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2796" for this suite.
STEP: Destroying namespace "webhook-2796-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":232,"skipped":4504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:39.937: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Feb 21 14:22:40.085: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:22:42.093: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.54 on the node which pod1 resides and expect scheduled
Feb 21 14:22:42.104: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:22:44.114: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.54 but use UDP protocol on the node which pod2 resides
Feb 21 14:22:44.125: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:22:46.134: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:22:48.131: INFO: The status of Pod pod3 is Running (Ready = true)
Feb 21 14:22:48.140: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:22:50.151: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Feb 21 14:22:50.153: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.54 http://127.0.0.1:54323/hostname] Namespace:hostport-990 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:22:50.153: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:22:50.153: INFO: ExecWithOptions: Clientset creation
Feb 21 14:22:50.153: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-990/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.54+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.54, port: 54323
Feb 21 14:22:50.215: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.54:54323/hostname] Namespace:hostport-990 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:22:50.215: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:22:50.215: INFO: ExecWithOptions: Clientset creation
Feb 21 14:22:50.215: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-990/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.54%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.54, port: 54323 UDP
Feb 21 14:22:50.249: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.54 54323] Namespace:hostport-990 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:22:50.249: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:22:50.249: INFO: ExecWithOptions: Clientset creation
Feb 21 14:22:50.249: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-990/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.0.0.54+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:22:55.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-990" for this suite.

• [SLOW TEST:15.382 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":233,"skipped":4566,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:22:55.319: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6395
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Feb 21 14:22:55.458: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:12.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6395" for this suite.

• [SLOW TEST:16.759 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":234,"skipped":4566,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:12.078: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 21 14:23:12.233: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1420  df7469b3-8aff-40ba-8292-acafc431defc 39482 0 2022-02-21 14:23:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-02-21 14:23:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 14:23:12.233: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1420  df7469b3-8aff-40ba-8292-acafc431defc 39483 0 2022-02-21 14:23:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-02-21 14:23:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:12.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1420" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":235,"skipped":4567,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:12.242: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-4f199db4-e28c-4ab6-8fc5-f2383c88a515
STEP: Creating a pod to test consume configMaps
Feb 21 14:23:12.392: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc72235c-4e30-462c-affc-0142d29385e7" in namespace "projected-1867" to be "Succeeded or Failed"
Feb 21 14:23:12.395: INFO: Pod "pod-projected-configmaps-cc72235c-4e30-462c-affc-0142d29385e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.401417ms
Feb 21 14:23:14.404: INFO: Pod "pod-projected-configmaps-cc72235c-4e30-462c-affc-0142d29385e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012271984s
STEP: Saw pod success
Feb 21 14:23:14.404: INFO: Pod "pod-projected-configmaps-cc72235c-4e30-462c-affc-0142d29385e7" satisfied condition "Succeeded or Failed"
Feb 21 14:23:14.407: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-configmaps-cc72235c-4e30-462c-affc-0142d29385e7 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 14:23:14.423: INFO: Waiting for pod pod-projected-configmaps-cc72235c-4e30-462c-affc-0142d29385e7 to disappear
Feb 21 14:23:14.425: INFO: Pod pod-projected-configmaps-cc72235c-4e30-462c-affc-0142d29385e7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:14.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1867" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4581,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:14.433: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:23:14.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-820d49c8-e430-4666-b40e-3a4bd7199876" in namespace "downward-api-2525" to be "Succeeded or Failed"
Feb 21 14:23:14.578: INFO: Pod "downwardapi-volume-820d49c8-e430-4666-b40e-3a4bd7199876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.149075ms
Feb 21 14:23:16.581: INFO: Pod "downwardapi-volume-820d49c8-e430-4666-b40e-3a4bd7199876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005328584s
STEP: Saw pod success
Feb 21 14:23:16.581: INFO: Pod "downwardapi-volume-820d49c8-e430-4666-b40e-3a4bd7199876" satisfied condition "Succeeded or Failed"
Feb 21 14:23:16.583: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-820d49c8-e430-4666-b40e-3a4bd7199876 container client-container: <nil>
STEP: delete the pod
Feb 21 14:23:16.596: INFO: Waiting for pod downwardapi-volume-820d49c8-e430-4666-b40e-3a4bd7199876 to disappear
Feb 21 14:23:16.598: INFO: Pod downwardapi-volume-820d49c8-e430-4666-b40e-3a4bd7199876 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:16.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2525" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4585,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:16.606: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Feb 21 14:23:16.749: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:23:18.755: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 21 14:23:19.775: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:20.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8144" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":238,"skipped":4600,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:20.804: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4164
STEP: Creating secret with name secret-test-a366f7a5-8109-4228-9f11-f32839266ec9
STEP: Creating a pod to test consume secrets
Feb 21 14:23:21.099: INFO: Waiting up to 5m0s for pod "pod-secrets-eb1deb4d-3ed9-4674-b508-fb4e7073cb31" in namespace "secrets-326" to be "Succeeded or Failed"
Feb 21 14:23:21.101: INFO: Pod "pod-secrets-eb1deb4d-3ed9-4674-b508-fb4e7073cb31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464025ms
Feb 21 14:23:23.108: INFO: Pod "pod-secrets-eb1deb4d-3ed9-4674-b508-fb4e7073cb31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008964439s
STEP: Saw pod success
Feb 21 14:23:23.108: INFO: Pod "pod-secrets-eb1deb4d-3ed9-4674-b508-fb4e7073cb31" satisfied condition "Succeeded or Failed"
Feb 21 14:23:23.111: INFO: Trying to get logs from node aksh-cncf-3 pod pod-secrets-eb1deb4d-3ed9-4674-b508-fb4e7073cb31 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 14:23:23.127: INFO: Waiting for pod pod-secrets-eb1deb4d-3ed9-4674-b508-fb4e7073cb31 to disappear
Feb 21 14:23:23.129: INFO: Pod pod-secrets-eb1deb4d-3ed9-4674-b508-fb4e7073cb31 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:23.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-326" for this suite.
STEP: Destroying namespace "secret-namespace-4164" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4615,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:23.142: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:23:23.287: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9da06d7f-6677-45bb-8e25-eb3784ae495a" in namespace "security-context-test-4361" to be "Succeeded or Failed"
Feb 21 14:23:23.289: INFO: Pod "busybox-user-65534-9da06d7f-6677-45bb-8e25-eb3784ae495a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.801523ms
Feb 21 14:23:25.293: INFO: Pod "busybox-user-65534-9da06d7f-6677-45bb-8e25-eb3784ae495a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005438474s
Feb 21 14:23:25.293: INFO: Pod "busybox-user-65534-9da06d7f-6677-45bb-8e25-eb3784ae495a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:25.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4361" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4633,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:25.302: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:23:25.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-913eb6a1-a481-4440-b685-777356ba0efb" in namespace "projected-7930" to be "Succeeded or Failed"
Feb 21 14:23:25.447: INFO: Pod "downwardapi-volume-913eb6a1-a481-4440-b685-777356ba0efb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343346ms
Feb 21 14:23:27.453: INFO: Pod "downwardapi-volume-913eb6a1-a481-4440-b685-777356ba0efb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008060772s
STEP: Saw pod success
Feb 21 14:23:27.453: INFO: Pod "downwardapi-volume-913eb6a1-a481-4440-b685-777356ba0efb" satisfied condition "Succeeded or Failed"
Feb 21 14:23:27.455: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-913eb6a1-a481-4440-b685-777356ba0efb container client-container: <nil>
STEP: delete the pod
Feb 21 14:23:27.476: INFO: Waiting for pod downwardapi-volume-913eb6a1-a481-4440-b685-777356ba0efb to disappear
Feb 21 14:23:27.478: INFO: Pod downwardapi-volume-913eb6a1-a481-4440-b685-777356ba0efb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:27.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7930" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4648,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:27.487: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3c785bc3-0b61-4ad5-9517-7f89dd34910b
STEP: Creating a pod to test consume secrets
Feb 21 14:23:27.638: INFO: Waiting up to 5m0s for pod "pod-secrets-b8da1c58-120a-44bb-b16b-18961fae5431" in namespace "secrets-5357" to be "Succeeded or Failed"
Feb 21 14:23:27.640: INFO: Pod "pod-secrets-b8da1c58-120a-44bb-b16b-18961fae5431": Phase="Pending", Reason="", readiness=false. Elapsed: 2.471788ms
Feb 21 14:23:29.647: INFO: Pod "pod-secrets-b8da1c58-120a-44bb-b16b-18961fae5431": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009503616s
STEP: Saw pod success
Feb 21 14:23:29.647: INFO: Pod "pod-secrets-b8da1c58-120a-44bb-b16b-18961fae5431" satisfied condition "Succeeded or Failed"
Feb 21 14:23:29.650: INFO: Trying to get logs from node aksh-cncf-3 pod pod-secrets-b8da1c58-120a-44bb-b16b-18961fae5431 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 14:23:29.668: INFO: Waiting for pod pod-secrets-b8da1c58-120a-44bb-b16b-18961fae5431 to disappear
Feb 21 14:23:29.671: INFO: Pod pod-secrets-b8da1c58-120a-44bb-b16b-18961fae5431 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:29.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5357" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4663,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:29.678: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:23:30.048: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:23:33.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:33.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-231" for this suite.
STEP: Destroying namespace "webhook-231-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":243,"skipped":4665,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:33.265: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 21 14:23:35.932: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1324 pod-service-account-65d203c8-d55a-4753-a021-e090f5e18a54 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 21 14:23:36.041: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1324 pod-service-account-65d203c8-d55a-4753-a021-e090f5e18a54 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 21 14:23:36.137: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1324 pod-service-account-65d203c8-d55a-4753-a021-e090f5e18a54 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:36.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1324" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":244,"skipped":4672,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:36.264: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:23:36.407: INFO: Got root ca configmap in namespace "svcaccounts-8559"
Feb 21 14:23:36.411: INFO: Deleted root ca configmap in namespace "svcaccounts-8559"
STEP: waiting for a new root ca configmap created
Feb 21 14:23:36.916: INFO: Recreated root ca configmap in namespace "svcaccounts-8559"
Feb 21 14:23:36.922: INFO: Updated root ca configmap in namespace "svcaccounts-8559"
STEP: waiting for the root ca configmap reconciled
Feb 21 14:23:37.427: INFO: Reconciled root ca configmap in namespace "svcaccounts-8559"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:37.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8559" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":245,"skipped":4674,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:37.437: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:23:37.586: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:23:39.592: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:41.592: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:43.597: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:45.593: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:47.593: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:49.592: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:51.592: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:53.593: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:55.593: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:57.592: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = false)
Feb 21 14:23:59.593: INFO: The status of Pod test-webserver-73a62b91-435c-4c7b-8e61-afd8e1a73d8e is Running (Ready = true)
Feb 21 14:23:59.595: INFO: Container started at 2022-02-21 14:23:38 +0000 UTC, pod became ready at 2022-02-21 14:23:57 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:23:59.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4792" for this suite.

• [SLOW TEST:22.168 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":246,"skipped":4685,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:23:59.604: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:23:59.743: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: creating the pod
STEP: submitting the pod to kubernetes
Feb 21 14:23:59.754: INFO: The status of Pod pod-exec-websocket-49dd8ea0-e828-49a2-bf1a-b7cc6e210a9b is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:24:01.761: INFO: The status of Pod pod-exec-websocket-49dd8ea0-e828-49a2-bf1a-b7cc6e210a9b is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:24:01.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7498" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:24:01.840: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:24:13.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4335" for this suite.

• [SLOW TEST:11.268 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":248,"skipped":4721,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:24:13.108: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:24:13.414: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:24:16.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:24:16.444: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-261-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:24:19.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7144" for this suite.
STEP: Destroying namespace "webhook-7144-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.467 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":249,"skipped":4742,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:24:19.575: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8030
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8030
I0221 14:24:19.747355      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8030, replica count: 2
I0221 14:24:22.797729      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 14:24:22.797: INFO: Creating new exec pod
Feb 21 14:24:25.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 21 14:24:25.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 21 14:24:25.938: INFO: stdout: ""
Feb 21 14:24:26.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 21 14:24:27.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 21 14:24:27.042: INFO: stdout: ""
Feb 21 14:24:27.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 21 14:24:28.030: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 21 14:24:28.030: INFO: stdout: ""
Feb 21 14:24:28.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 21 14:24:29.041: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 21 14:24:29.041: INFO: stdout: ""
Feb 21 14:24:29.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 21 14:24:30.062: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 21 14:24:30.062: INFO: stdout: "externalname-service-jw6jx"
Feb 21 14:24:30.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.6.105 80'
Feb 21 14:24:30.180: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.6.105 80\nConnection to 10.111.6.105 80 port [tcp/http] succeeded!\n"
Feb 21 14:24:30.180: INFO: stdout: "externalname-service-jw6jx"
Feb 21 14:24:30.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.211 32591'
Feb 21 14:24:30.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.211 32591\nConnection to 10.0.0.211 32591 port [tcp/*] succeeded!\n"
Feb 21 14:24:30.298: INFO: stdout: "externalname-service-jw6jx"
Feb 21 14:24:30.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.114 32591'
Feb 21 14:24:30.405: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.114 32591\nConnection to 10.0.0.114 32591 port [tcp/*] succeeded!\n"
Feb 21 14:24:30.405: INFO: stdout: ""
Feb 21 14:24:31.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-8030 exec execpodzpkfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.114 32591'
Feb 21 14:24:31.501: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.114 32591\nConnection to 10.0.0.114 32591 port [tcp/*] succeeded!\n"
Feb 21 14:24:31.501: INFO: stdout: "externalname-service-92qxd"
Feb 21 14:24:31.501: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:24:31.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8030" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.961 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":250,"skipped":4743,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:24:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:24:32.020: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:24:35.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:24:35.047: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2360-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:24:38.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4763" for this suite.
STEP: Destroying namespace "webhook-4763-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.645 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":251,"skipped":4761,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:24:38.182: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 21 14:24:38.373: INFO: The status of Pod annotationupdatebd342489-a7c7-4ebe-af6d-799385b03b7e is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:24:40.377: INFO: The status of Pod annotationupdatebd342489-a7c7-4ebe-af6d-799385b03b7e is Running (Ready = true)
Feb 21 14:24:40.905: INFO: Successfully updated pod "annotationupdatebd342489-a7c7-4ebe-af6d-799385b03b7e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:24:44.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7370" for this suite.

• [SLOW TEST:6.756 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:24:44.938: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-552.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-552.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-552.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-552.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-552.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-552.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 231.178.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.178.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.178.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.178.231_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-552.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-552.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-552.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-552.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-552.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-552.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-552.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 231.178.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.178.231_udp@PTR;check="$$(dig +tcp +noall +answer +search 231.178.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.178.231_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 14:24:47.118: INFO: Unable to read wheezy_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.122: INFO: Unable to read wheezy_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.125: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.128: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.143: INFO: Unable to read jessie_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.145: INFO: Unable to read jessie_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.148: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.151: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:47.163: INFO: Lookups using dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702 failed for: [wheezy_udp@dns-test-service.dns-552.svc.cluster.local wheezy_tcp@dns-test-service.dns-552.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_udp@dns-test-service.dns-552.svc.cluster.local jessie_tcp@dns-test-service.dns-552.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local]

Feb 21 14:24:52.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.174: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.177: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.191: INFO: Unable to read jessie_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.194: INFO: Unable to read jessie_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.196: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.199: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:52.210: INFO: Lookups using dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702 failed for: [wheezy_udp@dns-test-service.dns-552.svc.cluster.local wheezy_tcp@dns-test-service.dns-552.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_udp@dns-test-service.dns-552.svc.cluster.local jessie_tcp@dns-test-service.dns-552.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local]

Feb 21 14:24:57.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.189: INFO: Unable to read jessie_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.192: INFO: Unable to read jessie_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.195: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.198: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:24:57.208: INFO: Lookups using dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702 failed for: [wheezy_udp@dns-test-service.dns-552.svc.cluster.local wheezy_tcp@dns-test-service.dns-552.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_udp@dns-test-service.dns-552.svc.cluster.local jessie_tcp@dns-test-service.dns-552.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local]

Feb 21 14:25:02.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.174: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.177: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.191: INFO: Unable to read jessie_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.194: INFO: Unable to read jessie_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.197: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.200: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:02.212: INFO: Lookups using dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702 failed for: [wheezy_udp@dns-test-service.dns-552.svc.cluster.local wheezy_tcp@dns-test-service.dns-552.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_udp@dns-test-service.dns-552.svc.cluster.local jessie_tcp@dns-test-service.dns-552.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local]

Feb 21 14:25:07.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.191: INFO: Unable to read jessie_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.194: INFO: Unable to read jessie_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.197: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.199: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:07.211: INFO: Lookups using dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702 failed for: [wheezy_udp@dns-test-service.dns-552.svc.cluster.local wheezy_tcp@dns-test-service.dns-552.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_udp@dns-test-service.dns-552.svc.cluster.local jessie_tcp@dns-test-service.dns-552.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local]

Feb 21 14:25:12.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.170: INFO: Unable to read wheezy_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.174: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.177: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.190: INFO: Unable to read jessie_udp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.193: INFO: Unable to read jessie_tcp@dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.196: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.199: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local from pod dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702: the server could not find the requested resource (get pods dns-test-71edde55-083a-475f-ad22-664a792b9702)
Feb 21 14:25:12.209: INFO: Lookups using dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702 failed for: [wheezy_udp@dns-test-service.dns-552.svc.cluster.local wheezy_tcp@dns-test-service.dns-552.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_udp@dns-test-service.dns-552.svc.cluster.local jessie_tcp@dns-test-service.dns-552.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-552.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-552.svc.cluster.local]

Feb 21 14:25:17.212: INFO: DNS probes using dns-552/dns-test-71edde55-083a-475f-ad22-664a792b9702 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:25:17.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-552" for this suite.

• [SLOW TEST:32.342 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":253,"skipped":4797,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:25:17.281: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7549
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-75c95bd7-65f2-4dde-9d1d-e312a688e556
STEP: Creating configMap with name cm-test-opt-upd-fa2b228c-d012-4a3f-97ef-c267dc79ec1d
STEP: Creating the pod
Feb 21 14:25:17.437: INFO: The status of Pod pod-configmaps-a1d786fd-3541-4ae5-bbdf-dde8fb406ca5 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:25:19.442: INFO: The status of Pod pod-configmaps-a1d786fd-3541-4ae5-bbdf-dde8fb406ca5 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-75c95bd7-65f2-4dde-9d1d-e312a688e556
STEP: Updating configmap cm-test-opt-upd-fa2b228c-d012-4a3f-97ef-c267dc79ec1d
STEP: Creating configMap with name cm-test-opt-create-87a680f7-583d-4e7e-bf42-3e95c0f1507c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:25:21.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7549" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:25:21.501: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:25:21.654: INFO: Create a RollingUpdate DaemonSet
Feb 21 14:25:21.660: INFO: Check that daemon pods launch on every node of the cluster
Feb 21 14:25:21.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:25:21.665: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:25:22.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:25:22.675: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:25:23.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 14:25:23.675: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Feb 21 14:25:23.675: INFO: Update the DaemonSet to trigger a rollout
Feb 21 14:25:23.683: INFO: Updating DaemonSet daemon-set
Feb 21 14:25:26.700: INFO: Roll back the DaemonSet before rollout is complete
Feb 21 14:25:26.708: INFO: Updating DaemonSet daemon-set
Feb 21 14:25:26.708: INFO: Make sure DaemonSet rollback is complete
Feb 21 14:25:26.710: INFO: Wrong image for pod: daemon-set-gk6z7. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Feb 21 14:25:26.710: INFO: Pod daemon-set-gk6z7 is not available
Feb 21 14:25:29.719: INFO: Pod daemon-set-qc2hm is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3761, will wait for the garbage collector to delete the pods
Feb 21 14:25:29.787: INFO: Deleting DaemonSet.extensions daemon-set took: 7.025988ms
Feb 21 14:25:29.888: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.208156ms
Feb 21 14:25:32.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:25:32.093: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 21 14:25:32.095: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41086"},"items":null}

Feb 21 14:25:32.097: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41086"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:25:32.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3761" for this suite.

• [SLOW TEST:10.616 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":255,"skipped":4869,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:25:32.118: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 21 14:25:32.260: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 21 14:25:37.265: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:25:38.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4730" for this suite.

• [SLOW TEST:6.176 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":256,"skipped":4886,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:25:38.294: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Feb 21 14:25:44.472: INFO: 80 pods remaining
Feb 21 14:25:44.472: INFO: 80 pods has nil DeletionTimestamp
Feb 21 14:25:44.472: INFO: 
Feb 21 14:25:45.472: INFO: 71 pods remaining
Feb 21 14:25:45.472: INFO: 70 pods has nil DeletionTimestamp
Feb 21 14:25:45.472: INFO: 
Feb 21 14:25:46.464: INFO: 60 pods remaining
Feb 21 14:25:46.464: INFO: 60 pods has nil DeletionTimestamp
Feb 21 14:25:46.464: INFO: 
Feb 21 14:25:47.464: INFO: 40 pods remaining
Feb 21 14:25:47.464: INFO: 40 pods has nil DeletionTimestamp
Feb 21 14:25:47.464: INFO: 
Feb 21 14:25:48.478: INFO: 30 pods remaining
Feb 21 14:25:48.478: INFO: 30 pods has nil DeletionTimestamp
Feb 21 14:25:48.478: INFO: 
Feb 21 14:25:49.463: INFO: 20 pods remaining
Feb 21 14:25:49.463: INFO: 20 pods has nil DeletionTimestamp
Feb 21 14:25:49.463: INFO: 
STEP: Gathering metrics
Feb 21 14:25:50.478: INFO: The status of Pod kube-controller-manager-aksh-cncf-3 is Running (Ready = true)
Feb 21 14:25:50.516: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:25:50.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7782" for this suite.

• [SLOW TEST:12.231 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":257,"skipped":4889,"failed":0}
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:25:50.525: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Feb 21 14:25:50.666: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 21 14:26:50.700: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:26:50.702: INFO: Starting informer...
STEP: Starting pod...
Feb 21 14:26:50.917: INFO: Pod is running on aksh-cncf-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 21 14:26:50.932: INFO: Pod wasn't evicted. Proceeding
Feb 21 14:26:50.932: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 21 14:28:05.952: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:28:05.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3259" for this suite.

• [SLOW TEST:135.441 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":258,"skipped":4892,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:28:05.966: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb 21 14:28:06.118: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 21 14:29:06.152: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Feb 21 14:29:06.177: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 21 14:29:06.182: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 21 14:29:06.199: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 21 14:29:06.206: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 21 14:29:06.223: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 21 14:29:06.230: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Feb 21 14:29:06.245: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Feb 21 14:29:06.251: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:22.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9643" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.410 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":259,"skipped":4907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:22.377: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6473
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-80da3a74-9184-4f3a-b0b0-403c8814c840
STEP: Creating configMap with name cm-test-opt-upd-7a1d68af-d949-42c7-b949-13ebba828086
STEP: Creating the pod
Feb 21 14:29:22.540: INFO: The status of Pod pod-projected-configmaps-d26a02f1-0cb7-4415-aa1c-50ed0b69f1e3 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:29:24.548: INFO: The status of Pod pod-projected-configmaps-d26a02f1-0cb7-4415-aa1c-50ed0b69f1e3 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-80da3a74-9184-4f3a-b0b0-403c8814c840
STEP: Updating configmap cm-test-opt-upd-7a1d68af-d949-42c7-b949-13ebba828086
STEP: Creating configMap with name cm-test-opt-create-cca6f388-c6fc-4cb5-bc6d-47936d27c52e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:28.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6473" for this suite.

• [SLOW TEST:6.241 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":4936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:28.618: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4193
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-ca2c3ffb-d282-4bc2-9de4-8666a406c333
STEP: Creating a pod to test consume configMaps
Feb 21 14:29:28.764: INFO: Waiting up to 5m0s for pod "pod-configmaps-42ec0de0-b8cf-4f61-b389-47127c2925ce" in namespace "configmap-4193" to be "Succeeded or Failed"
Feb 21 14:29:28.766: INFO: Pod "pod-configmaps-42ec0de0-b8cf-4f61-b389-47127c2925ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223326ms
Feb 21 14:29:30.774: INFO: Pod "pod-configmaps-42ec0de0-b8cf-4f61-b389-47127c2925ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010621957s
STEP: Saw pod success
Feb 21 14:29:30.774: INFO: Pod "pod-configmaps-42ec0de0-b8cf-4f61-b389-47127c2925ce" satisfied condition "Succeeded or Failed"
Feb 21 14:29:30.777: INFO: Trying to get logs from node aksh-cncf-1 pod pod-configmaps-42ec0de0-b8cf-4f61-b389-47127c2925ce container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 14:29:30.798: INFO: Waiting for pod pod-configmaps-42ec0de0-b8cf-4f61-b389-47127c2925ce to disappear
Feb 21 14:29:30.800: INFO: Pod pod-configmaps-42ec0de0-b8cf-4f61-b389-47127c2925ce no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:30.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4193" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":4971,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:30.808: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:29:30.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00cc8787-6387-4ead-8c38-897d6287c226" in namespace "projected-7458" to be "Succeeded or Failed"
Feb 21 14:29:30.952: INFO: Pod "downwardapi-volume-00cc8787-6387-4ead-8c38-897d6287c226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396503ms
Feb 21 14:29:32.958: INFO: Pod "downwardapi-volume-00cc8787-6387-4ead-8c38-897d6287c226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008437393s
STEP: Saw pod success
Feb 21 14:29:32.958: INFO: Pod "downwardapi-volume-00cc8787-6387-4ead-8c38-897d6287c226" satisfied condition "Succeeded or Failed"
Feb 21 14:29:32.960: INFO: Trying to get logs from node aksh-cncf-4 pod downwardapi-volume-00cc8787-6387-4ead-8c38-897d6287c226 container client-container: <nil>
STEP: delete the pod
Feb 21 14:29:32.986: INFO: Waiting for pod downwardapi-volume-00cc8787-6387-4ead-8c38-897d6287c226 to disappear
Feb 21 14:29:32.988: INFO: Pod downwardapi-volume-00cc8787-6387-4ead-8c38-897d6287c226 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:32.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7458" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":4975,"failed":0}
SSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:32.997: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-9045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 21 14:29:33.154: INFO: starting watch
STEP: patching
STEP: updating
Feb 21 14:29:33.163: INFO: waiting for watch events with expected annotations
Feb 21 14:29:33.163: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:33.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9045" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":263,"skipped":4980,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:33.191: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:29:33.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e550455d-0d68-4e7c-8e93-6fffe79001b1" in namespace "projected-7000" to be "Succeeded or Failed"
Feb 21 14:29:33.339: INFO: Pod "downwardapi-volume-e550455d-0d68-4e7c-8e93-6fffe79001b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.453843ms
Feb 21 14:29:35.344: INFO: Pod "downwardapi-volume-e550455d-0d68-4e7c-8e93-6fffe79001b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007746312s
STEP: Saw pod success
Feb 21 14:29:35.345: INFO: Pod "downwardapi-volume-e550455d-0d68-4e7c-8e93-6fffe79001b1" satisfied condition "Succeeded or Failed"
Feb 21 14:29:35.347: INFO: Trying to get logs from node aksh-cncf-1 pod downwardapi-volume-e550455d-0d68-4e7c-8e93-6fffe79001b1 container client-container: <nil>
STEP: delete the pod
Feb 21 14:29:35.366: INFO: Waiting for pod downwardapi-volume-e550455d-0d68-4e7c-8e93-6fffe79001b1 to disappear
Feb 21 14:29:35.368: INFO: Pod downwardapi-volume-e550455d-0d68-4e7c-8e93-6fffe79001b1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:35.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7000" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:35.375: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Feb 21 14:29:35.523: INFO: The status of Pod labelsupdateaa9d581c-9da5-4725-b6af-6690438dd3b1 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:29:37.527: INFO: The status of Pod labelsupdateaa9d581c-9da5-4725-b6af-6690438dd3b1 is Running (Ready = true)
Feb 21 14:29:38.049: INFO: Successfully updated pod "labelsupdateaa9d581c-9da5-4725-b6af-6690438dd3b1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:42.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4936" for this suite.

• [SLOW TEST:6.710 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":5002,"failed":0}
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:42.085: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Feb 21 14:29:42.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-4031 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 21 14:29:42.402: INFO: stderr: ""
Feb 21 14:29:42.402: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Feb 21 14:29:42.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-4031 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Feb 21 14:29:43.139: INFO: stderr: ""
Feb 21 14:29:43.139: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Feb 21 14:29:43.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-4031 delete pods e2e-test-httpd-pod'
Feb 21 14:29:45.277: INFO: stderr: ""
Feb 21 14:29:45.277: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:45.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4031" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":266,"skipped":5002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:45.291: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1537
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Feb 21 14:29:45.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1637 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Feb 21 14:29:45.468: INFO: stderr: ""
Feb 21 14:29:45.468: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1541
Feb 21 14:29:45.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-1637 delete pods e2e-test-httpd-pod'
Feb 21 14:29:47.289: INFO: stderr: ""
Feb 21 14:29:47.289: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:47.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1637" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":267,"skipped":5030,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:47.301: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 21 14:29:47.453: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4224  7a3b95e8-8db5-4596-bffb-172e52373620 44136 0 2022-02-21 14:29:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-21 14:29:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 14:29:47.453: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4224  7a3b95e8-8db5-4596-bffb-172e52373620 44137 0 2022-02-21 14:29:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-21 14:29:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 14:29:47.454: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4224  7a3b95e8-8db5-4596-bffb-172e52373620 44138 0 2022-02-21 14:29:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-21 14:29:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 21 14:29:57.488: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4224  7a3b95e8-8db5-4596-bffb-172e52373620 44203 0 2022-02-21 14:29:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-21 14:29:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 14:29:57.488: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4224  7a3b95e8-8db5-4596-bffb-172e52373620 44204 0 2022-02-21 14:29:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-21 14:29:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 21 14:29:57.488: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4224  7a3b95e8-8db5-4596-bffb-172e52373620 44206 0 2022-02-21 14:29:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-02-21 14:29:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:29:57.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4224" for this suite.

• [SLOW TEST:10.196 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":268,"skipped":5043,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:29:57.497: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3417
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 21 14:29:57.636: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:30:00.100: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:30:10.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3417" for this suite.

• [SLOW TEST:12.598 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":269,"skipped":5053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:30:10.096: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:30:10.607: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:30:13.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:30:13.633: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4607-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:30:16.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2896" for this suite.
STEP: Destroying namespace "webhook-2896-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.748 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":270,"skipped":5101,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:30:16.844: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Feb 21 14:30:17.509: INFO: created pod pod-service-account-defaultsa
Feb 21 14:30:17.509: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 21 14:30:17.515: INFO: created pod pod-service-account-mountsa
Feb 21 14:30:17.515: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 21 14:30:17.522: INFO: created pod pod-service-account-nomountsa
Feb 21 14:30:17.522: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 21 14:30:17.529: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 21 14:30:17.529: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 21 14:30:17.536: INFO: created pod pod-service-account-mountsa-mountspec
Feb 21 14:30:17.536: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 21 14:30:17.545: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 21 14:30:17.545: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 21 14:30:17.552: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 21 14:30:17.552: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 21 14:30:17.559: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 21 14:30:17.559: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 21 14:30:17.564: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 21 14:30:17.564: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:30:17.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2127" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":271,"skipped":5108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:30:17.578: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:30:17.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40f7b327-6162-4251-aab8-a8bc052a923d" in namespace "downward-api-8004" to be "Succeeded or Failed"
Feb 21 14:30:17.730: INFO: Pod "downwardapi-volume-40f7b327-6162-4251-aab8-a8bc052a923d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21586ms
Feb 21 14:30:19.734: INFO: Pod "downwardapi-volume-40f7b327-6162-4251-aab8-a8bc052a923d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005831123s
STEP: Saw pod success
Feb 21 14:30:19.734: INFO: Pod "downwardapi-volume-40f7b327-6162-4251-aab8-a8bc052a923d" satisfied condition "Succeeded or Failed"
Feb 21 14:30:19.737: INFO: Trying to get logs from node aksh-cncf-2 pod downwardapi-volume-40f7b327-6162-4251-aab8-a8bc052a923d container client-container: <nil>
STEP: delete the pod
Feb 21 14:30:19.760: INFO: Waiting for pod downwardapi-volume-40f7b327-6162-4251-aab8-a8bc052a923d to disappear
Feb 21 14:30:19.763: INFO: Pod downwardapi-volume-40f7b327-6162-4251-aab8-a8bc052a923d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:30:19.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8004" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":5194,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:30:19.771: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Feb 21 14:30:19.906: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-656 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:30:19.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-656" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":273,"skipped":5209,"failed":0}

------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:30:19.949: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1527
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:30:20.097: INFO: created pod
Feb 21 14:30:20.097: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1527" to be "Succeeded or Failed"
Feb 21 14:30:20.100: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.651287ms
Feb 21 14:30:22.104: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006991689s
STEP: Saw pod success
Feb 21 14:30:22.104: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 21 14:30:52.104: INFO: polling logs
Feb 21 14:30:52.110: INFO: Pod logs: 
2022/02/21 14:30:20 OK: Got token
2022/02/21 14:30:20 validating with in-cluster discovery
2022/02/21 14:30:20 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/02/21 14:30:20 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1527:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1645454420, NotBefore:1645453820, IssuedAt:1645453820, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1527", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"55fcfeb3-ca97-4d57-baf6-b02cb64df6a2"}}}
2022/02/21 14:30:20 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/02/21 14:30:20 OK: Validated signature on JWT
2022/02/21 14:30:20 OK: Got valid claims from token!
2022/02/21 14:30:20 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1527:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1645454420, NotBefore:1645453820, IssuedAt:1645453820, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1527", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"55fcfeb3-ca97-4d57-baf6-b02cb64df6a2"}}}

Feb 21 14:30:52.110: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:30:52.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1527" for this suite.

• [SLOW TEST:32.176 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":274,"skipped":5209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:30:52.126: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-fbbcc9f5-8d16-4fc0-b738-32d5d450c126 in namespace container-probe-4984
Feb 21 14:30:54.278: INFO: Started pod liveness-fbbcc9f5-8d16-4fc0-b738-32d5d450c126 in namespace container-probe-4984
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 14:30:54.281: INFO: Initial restart count of pod liveness-fbbcc9f5-8d16-4fc0-b738-32d5d450c126 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:34:55.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4984" for this suite.

• [SLOW TEST:242.930 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5241,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:34:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:34:55.431: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:34:58.452: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 21 14:34:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:34:58.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6008" for this suite.
STEP: Destroying namespace "webhook-6008-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":276,"skipped":5247,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:34:58.523: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 21 14:34:58.669: INFO: Waiting up to 5m0s for pod "downward-api-bab4bea3-d9a8-460a-bb63-be3292e29f07" in namespace "downward-api-6831" to be "Succeeded or Failed"
Feb 21 14:34:58.672: INFO: Pod "downward-api-bab4bea3-d9a8-460a-bb63-be3292e29f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.231809ms
Feb 21 14:35:00.678: INFO: Pod "downward-api-bab4bea3-d9a8-460a-bb63-be3292e29f07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008606063s
STEP: Saw pod success
Feb 21 14:35:00.678: INFO: Pod "downward-api-bab4bea3-d9a8-460a-bb63-be3292e29f07" satisfied condition "Succeeded or Failed"
Feb 21 14:35:00.683: INFO: Trying to get logs from node aksh-cncf-3 pod downward-api-bab4bea3-d9a8-460a-bb63-be3292e29f07 container dapi-container: <nil>
STEP: delete the pod
Feb 21 14:35:00.703: INFO: Waiting for pod downward-api-bab4bea3-d9a8-460a-bb63-be3292e29f07 to disappear
Feb 21 14:35:00.706: INFO: Pod downward-api-bab4bea3-d9a8-460a-bb63-be3292e29f07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6831" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":5259,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7181
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 14:35:00.846: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 21 14:35:00.894: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:35:02.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:04.903: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:06.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:08.899: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:10.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:12.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:14.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:16.903: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:18.900: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb 21 14:35:20.902: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb 21 14:35:20.908: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb 21 14:35:20.912: INFO: The status of Pod netserver-2 is Running (Ready = true)
Feb 21 14:35:20.916: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Feb 21 14:35:22.946: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 21 14:35:22.946: INFO: Going to poll 10.244.0.173 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 21 14:35:22.948: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.173 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:35:22.948: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:35:22.948: INFO: ExecWithOptions: Clientset creation
Feb 21 14:35:22.948: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.173+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:35:24.020: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 21 14:35:24.020: INFO: Going to poll 10.244.2.102 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 21 14:35:24.026: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.102 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:35:24.026: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:35:24.027: INFO: ExecWithOptions: Clientset creation
Feb 21 14:35:24.027: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.102+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:35:25.098: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 21 14:35:25.098: INFO: Going to poll 10.244.1.254 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 21 14:35:25.106: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.254 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:35:25.106: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:35:25.106: INFO: ExecWithOptions: Clientset creation
Feb 21 14:35:25.106: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.254+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:35:26.174: INFO: Found all 1 expected endpoints: [netserver-2]
Feb 21 14:35:26.174: INFO: Going to poll 10.244.3.93 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 21 14:35:26.179: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.93 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7181 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:35:26.179: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:35:26.179: INFO: ExecWithOptions: Clientset creation
Feb 21 14:35:26.179: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7181/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.93+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:35:27.229: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:27.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7181" for this suite.

• [SLOW TEST:26.529 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":5265,"failed":0}
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:27.243: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:33.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2737" for this suite.

• [SLOW TEST:6.200 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":279,"skipped":5265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:33.444: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:35:33.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f51e4a7d-da50-4e2a-82c0-5a63838874b9" in namespace "projected-8057" to be "Succeeded or Failed"
Feb 21 14:35:33.589: INFO: Pod "downwardapi-volume-f51e4a7d-da50-4e2a-82c0-5a63838874b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587443ms
Feb 21 14:35:35.592: INFO: Pod "downwardapi-volume-f51e4a7d-da50-4e2a-82c0-5a63838874b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005930646s
STEP: Saw pod success
Feb 21 14:35:35.592: INFO: Pod "downwardapi-volume-f51e4a7d-da50-4e2a-82c0-5a63838874b9" satisfied condition "Succeeded or Failed"
Feb 21 14:35:35.594: INFO: Trying to get logs from node aksh-cncf-1 pod downwardapi-volume-f51e4a7d-da50-4e2a-82c0-5a63838874b9 container client-container: <nil>
STEP: delete the pod
Feb 21 14:35:35.617: INFO: Waiting for pod downwardapi-volume-f51e4a7d-da50-4e2a-82c0-5a63838874b9 to disappear
Feb 21 14:35:35.619: INFO: Pod downwardapi-volume-f51e4a7d-da50-4e2a-82c0-5a63838874b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:35.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8057" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":5331,"failed":0}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:35.628: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:35.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3643" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":281,"skipped":5341,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:35.791: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8659
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:35:35.945: INFO: The status of Pod pod-secrets-cd668b3a-1336-4620-8d49-368175faf633 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:35:37.951: INFO: The status of Pod pod-secrets-cd668b3a-1336-4620-8d49-368175faf633 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:37.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8659" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":282,"skipped":5346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:37.990: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 21 14:35:38.129: INFO: Waiting up to 5m0s for pod "pod-d161ad0f-0cb3-4786-95e0-6fbcc29e208d" in namespace "emptydir-6048" to be "Succeeded or Failed"
Feb 21 14:35:38.133: INFO: Pod "pod-d161ad0f-0cb3-4786-95e0-6fbcc29e208d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.505627ms
Feb 21 14:35:40.139: INFO: Pod "pod-d161ad0f-0cb3-4786-95e0-6fbcc29e208d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008044144s
STEP: Saw pod success
Feb 21 14:35:40.139: INFO: Pod "pod-d161ad0f-0cb3-4786-95e0-6fbcc29e208d" satisfied condition "Succeeded or Failed"
Feb 21 14:35:40.141: INFO: Trying to get logs from node aksh-cncf-3 pod pod-d161ad0f-0cb3-4786-95e0-6fbcc29e208d container test-container: <nil>
STEP: delete the pod
Feb 21 14:35:40.156: INFO: Waiting for pod pod-d161ad0f-0cb3-4786-95e0-6fbcc29e208d to disappear
Feb 21 14:35:40.157: INFO: Pod pod-d161ad0f-0cb3-4786-95e0-6fbcc29e208d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:40.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6048" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5383,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:40.165: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:35:40.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f5c3f2d-18a3-4cc9-bc4e-cc6bf6f9524b" in namespace "downward-api-7816" to be "Succeeded or Failed"
Feb 21 14:35:40.310: INFO: Pod "downwardapi-volume-5f5c3f2d-18a3-4cc9-bc4e-cc6bf6f9524b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.47567ms
Feb 21 14:35:42.316: INFO: Pod "downwardapi-volume-5f5c3f2d-18a3-4cc9-bc4e-cc6bf6f9524b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008531428s
STEP: Saw pod success
Feb 21 14:35:42.316: INFO: Pod "downwardapi-volume-5f5c3f2d-18a3-4cc9-bc4e-cc6bf6f9524b" satisfied condition "Succeeded or Failed"
Feb 21 14:35:42.319: INFO: Trying to get logs from node aksh-cncf-3 pod downwardapi-volume-5f5c3f2d-18a3-4cc9-bc4e-cc6bf6f9524b container client-container: <nil>
STEP: delete the pod
Feb 21 14:35:42.334: INFO: Waiting for pod downwardapi-volume-5f5c3f2d-18a3-4cc9-bc4e-cc6bf6f9524b to disappear
Feb 21 14:35:42.336: INFO: Pod downwardapi-volume-5f5c3f2d-18a3-4cc9-bc4e-cc6bf6f9524b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:42.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7816" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5389,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:42.344: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9312
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Feb 21 14:35:42.492: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 21 14:35:47.499: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Feb 21 14:35:47.502: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Feb 21 14:35:47.510: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Feb 21 14:35:47.512: INFO: Observed &ReplicaSet event: ADDED
Feb 21 14:35:47.512: INFO: Observed &ReplicaSet event: MODIFIED
Feb 21 14:35:47.512: INFO: Observed &ReplicaSet event: MODIFIED
Feb 21 14:35:47.512: INFO: Observed &ReplicaSet event: MODIFIED
Feb 21 14:35:47.512: INFO: Found replicaset test-rs in namespace replicaset-9312 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 21 14:35:47.512: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Feb 21 14:35:47.512: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 21 14:35:47.519: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Feb 21 14:35:47.521: INFO: Observed &ReplicaSet event: ADDED
Feb 21 14:35:47.521: INFO: Observed &ReplicaSet event: MODIFIED
Feb 21 14:35:47.521: INFO: Observed &ReplicaSet event: MODIFIED
Feb 21 14:35:47.521: INFO: Observed &ReplicaSet event: MODIFIED
Feb 21 14:35:47.521: INFO: Observed replicaset test-rs in namespace replicaset-9312 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 21 14:35:47.521: INFO: Observed &ReplicaSet event: MODIFIED
Feb 21 14:35:47.521: INFO: Found replicaset test-rs in namespace replicaset-9312 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 21 14:35:47.521: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:35:47.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9312" for this suite.

• [SLOW TEST:5.185 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":285,"skipped":5400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:35:47.530: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9759
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:35:47.661: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Creating first CR 
Feb 21 14:35:50.234: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-21T14:35:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-21T14:35:50Z]] name:name1 resourceVersion:46259 uid:2307d2b1-d47e-4eb1-8007-bc9c77cd8cab] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 21 14:36:00.244: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-21T14:36:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-21T14:36:00Z]] name:name2 resourceVersion:46311 uid:555095de-af4c-4939-9696-2d1a70924357] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 21 14:36:10.254: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-21T14:35:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-21T14:36:10Z]] name:name1 resourceVersion:46348 uid:2307d2b1-d47e-4eb1-8007-bc9c77cd8cab] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 21 14:36:20.264: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-21T14:36:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-21T14:36:20Z]] name:name2 resourceVersion:46385 uid:555095de-af4c-4939-9696-2d1a70924357] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 21 14:36:30.273: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-21T14:35:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-21T14:36:10Z]] name:name1 resourceVersion:46422 uid:2307d2b1-d47e-4eb1-8007-bc9c77cd8cab] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 21 14:36:40.284: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-02-21T14:36:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-02-21T14:36:20Z]] name:name2 resourceVersion:46458 uid:555095de-af4c-4939-9696-2d1a70924357] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:36:50.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9759" for this suite.

• [SLOW TEST:63.278 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":286,"skipped":5428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:36:50.808: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5c8abdc4-95ae-4111-bebd-065e2df31cea
STEP: Creating a pod to test consume secrets
Feb 21 14:36:50.967: INFO: Waiting up to 5m0s for pod "pod-secrets-47525980-6472-499d-88e2-259fd877c695" in namespace "secrets-1511" to be "Succeeded or Failed"
Feb 21 14:36:50.969: INFO: Pod "pod-secrets-47525980-6472-499d-88e2-259fd877c695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057506ms
Feb 21 14:36:52.974: INFO: Pod "pod-secrets-47525980-6472-499d-88e2-259fd877c695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00729996s
STEP: Saw pod success
Feb 21 14:36:52.974: INFO: Pod "pod-secrets-47525980-6472-499d-88e2-259fd877c695" satisfied condition "Succeeded or Failed"
Feb 21 14:36:52.976: INFO: Trying to get logs from node aksh-cncf-3 pod pod-secrets-47525980-6472-499d-88e2-259fd877c695 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 14:36:52.992: INFO: Waiting for pod pod-secrets-47525980-6472-499d-88e2-259fd877c695 to disappear
Feb 21 14:36:52.994: INFO: Pod pod-secrets-47525980-6472-499d-88e2-259fd877c695 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:36:52.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1511" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5463,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:36:53.002: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3111/configmap-test-47c43a9d-e4d5-4c60-a250-49b04be0c453
STEP: Creating a pod to test consume configMaps
Feb 21 14:36:53.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-9db771e3-f08f-41eb-a152-5c6de223d4d3" in namespace "configmap-3111" to be "Succeeded or Failed"
Feb 21 14:36:53.152: INFO: Pod "pod-configmaps-9db771e3-f08f-41eb-a152-5c6de223d4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105679ms
Feb 21 14:36:55.159: INFO: Pod "pod-configmaps-9db771e3-f08f-41eb-a152-5c6de223d4d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008599578s
STEP: Saw pod success
Feb 21 14:36:55.159: INFO: Pod "pod-configmaps-9db771e3-f08f-41eb-a152-5c6de223d4d3" satisfied condition "Succeeded or Failed"
Feb 21 14:36:55.161: INFO: Trying to get logs from node aksh-cncf-3 pod pod-configmaps-9db771e3-f08f-41eb-a152-5c6de223d4d3 container env-test: <nil>
STEP: delete the pod
Feb 21 14:36:55.176: INFO: Waiting for pod pod-configmaps-9db771e3-f08f-41eb-a152-5c6de223d4d3 to disappear
Feb 21 14:36:55.179: INFO: Pod pod-configmaps-9db771e3-f08f-41eb-a152-5c6de223d4d3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:36:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3111" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":288,"skipped":5480,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:36:55.187: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Feb 21 14:36:55.360: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:36:57.368: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Feb 21 14:36:57.379: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:36:59.386: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Feb 21 14:36:59.395: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 14:36:59.397: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 14:37:01.398: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 14:37:01.404: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 14:37:03.397: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 14:37:03.403: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:03.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4318" for this suite.

• [SLOW TEST:8.239 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5482,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:03.426: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 21 14:37:03.570: INFO: Waiting up to 5m0s for pod "pod-3f33dee2-e556-4814-a629-0ad563655f2c" in namespace "emptydir-7441" to be "Succeeded or Failed"
Feb 21 14:37:03.572: INFO: Pod "pod-3f33dee2-e556-4814-a629-0ad563655f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.823271ms
Feb 21 14:37:05.581: INFO: Pod "pod-3f33dee2-e556-4814-a629-0ad563655f2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010627227s
STEP: Saw pod success
Feb 21 14:37:05.581: INFO: Pod "pod-3f33dee2-e556-4814-a629-0ad563655f2c" satisfied condition "Succeeded or Failed"
Feb 21 14:37:05.583: INFO: Trying to get logs from node aksh-cncf-3 pod pod-3f33dee2-e556-4814-a629-0ad563655f2c container test-container: <nil>
STEP: delete the pod
Feb 21 14:37:05.598: INFO: Waiting for pod pod-3f33dee2-e556-4814-a629-0ad563655f2c to disappear
Feb 21 14:37:05.600: INFO: Pod pod-3f33dee2-e556-4814-a629-0ad563655f2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:05.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7441" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":290,"skipped":5498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:05.610: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Feb 21 14:37:05.747: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Feb 21 14:37:06.063: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 21 14:37:08.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 14:37:10.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 14:37:12.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 14:37:14.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 14:37:16.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.February, 21, 14, 37, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 14:37:18.225: INFO: Waited 115.166106ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Feb 21 14:37:18.275: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:18.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3390" for this suite.

• [SLOW TEST:13.310 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":291,"skipped":5553,"failed":0}
S
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:18.920: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-2894
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:19.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2894" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":292,"skipped":5554,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:19.107: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-kwfc
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 14:37:19.255: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kwfc" in namespace "subpath-1940" to be "Succeeded or Failed"
Feb 21 14:37:19.257: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18603ms
Feb 21 14:37:21.265: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009411521s
Feb 21 14:37:23.272: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 4.017047413s
Feb 21 14:37:25.281: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 6.02617593s
Feb 21 14:37:27.288: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 8.032963646s
Feb 21 14:37:29.293: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 10.038226289s
Feb 21 14:37:31.298: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 12.042604927s
Feb 21 14:37:33.305: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 14.049975108s
Feb 21 14:37:35.313: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 16.057612119s
Feb 21 14:37:37.323: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 18.067760029s
Feb 21 14:37:39.329: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Running", Reason="", readiness=true. Elapsed: 20.073496202s
Feb 21 14:37:41.333: INFO: Pod "pod-subpath-test-downwardapi-kwfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.077950811s
STEP: Saw pod success
Feb 21 14:37:41.333: INFO: Pod "pod-subpath-test-downwardapi-kwfc" satisfied condition "Succeeded or Failed"
Feb 21 14:37:41.336: INFO: Trying to get logs from node aksh-cncf-3 pod pod-subpath-test-downwardapi-kwfc container test-container-subpath-downwardapi-kwfc: <nil>
STEP: delete the pod
Feb 21 14:37:41.349: INFO: Waiting for pod pod-subpath-test-downwardapi-kwfc to disappear
Feb 21 14:37:41.351: INFO: Pod pod-subpath-test-downwardapi-kwfc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kwfc
Feb 21 14:37:41.351: INFO: Deleting pod "pod-subpath-test-downwardapi-kwfc" in namespace "subpath-1940"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:41.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1940" for this suite.

• [SLOW TEST:22.255 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":293,"skipped":5572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:41.362: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:37:41.508: INFO: The status of Pod busybox-host-aliases1e10da62-76c9-4edb-939c-e597ebc4e86e is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:37:43.515: INFO: The status of Pod busybox-host-aliases1e10da62-76c9-4edb-939c-e597ebc4e86e is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:43.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9145" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5608,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7683
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Feb 21 14:37:45.685: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7683 PodName:pod-sharedvolume-64079e42-1474-4ae0-b975-59a3fb2ae6bc ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 21 14:37:45.685: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:37:45.685: INFO: ExecWithOptions: Clientset creation
Feb 21 14:37:45.685: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-7683/pods/pod-sharedvolume-64079e42-1474-4ae0-b975-59a3fb2ae6bc/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Feb 21 14:37:45.743: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:45.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7683" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":295,"skipped":5617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:45.754: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-df2cb1f8-28e7-4f5b-bc9e-f460f96259b1
STEP: Creating a pod to test consume secrets
Feb 21 14:37:45.898: INFO: Waiting up to 5m0s for pod "pod-secrets-b2d7ef4c-f099-48fa-a169-0aef57e9dce0" in namespace "secrets-9104" to be "Succeeded or Failed"
Feb 21 14:37:45.900: INFO: Pod "pod-secrets-b2d7ef4c-f099-48fa-a169-0aef57e9dce0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295058ms
Feb 21 14:37:47.905: INFO: Pod "pod-secrets-b2d7ef4c-f099-48fa-a169-0aef57e9dce0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007327479s
STEP: Saw pod success
Feb 21 14:37:47.905: INFO: Pod "pod-secrets-b2d7ef4c-f099-48fa-a169-0aef57e9dce0" satisfied condition "Succeeded or Failed"
Feb 21 14:37:47.907: INFO: Trying to get logs from node aksh-cncf-1 pod pod-secrets-b2d7ef4c-f099-48fa-a169-0aef57e9dce0 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 14:37:47.925: INFO: Waiting for pod pod-secrets-b2d7ef4c-f099-48fa-a169-0aef57e9dce0 to disappear
Feb 21 14:37:47.927: INFO: Pod pod-secrets-b2d7ef4c-f099-48fa-a169-0aef57e9dce0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:47.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9104" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5640,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:47.935: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Feb 21 14:37:48.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 create -f -'
Feb 21 14:37:48.718: INFO: stderr: ""
Feb 21 14:37:48.718: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 14:37:48.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 21 14:37:48.761: INFO: stderr: ""
Feb 21 14:37:48.761: INFO: stdout: "update-demo-nautilus-b927z update-demo-nautilus-sv65m "
Feb 21 14:37:48.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods update-demo-nautilus-b927z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 14:37:48.802: INFO: stderr: ""
Feb 21 14:37:48.802: INFO: stdout: ""
Feb 21 14:37:48.802: INFO: update-demo-nautilus-b927z is created but not running
Feb 21 14:37:53.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 21 14:37:53.848: INFO: stderr: ""
Feb 21 14:37:53.849: INFO: stdout: "update-demo-nautilus-b927z update-demo-nautilus-sv65m "
Feb 21 14:37:53.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods update-demo-nautilus-b927z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 14:37:53.889: INFO: stderr: ""
Feb 21 14:37:53.889: INFO: stdout: "true"
Feb 21 14:37:53.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods update-demo-nautilus-b927z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 14:37:53.928: INFO: stderr: ""
Feb 21 14:37:53.928: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 14:37:53.928: INFO: validating pod update-demo-nautilus-b927z
Feb 21 14:37:53.932: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 14:37:53.932: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 14:37:53.932: INFO: update-demo-nautilus-b927z is verified up and running
Feb 21 14:37:53.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods update-demo-nautilus-sv65m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 21 14:37:53.972: INFO: stderr: ""
Feb 21 14:37:53.972: INFO: stdout: "true"
Feb 21 14:37:53.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods update-demo-nautilus-sv65m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 21 14:37:54.014: INFO: stderr: ""
Feb 21 14:37:54.014: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Feb 21 14:37:54.014: INFO: validating pod update-demo-nautilus-sv65m
Feb 21 14:37:54.017: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 14:37:54.017: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 14:37:54.017: INFO: update-demo-nautilus-sv65m is verified up and running
STEP: using delete to clean up resources
Feb 21 14:37:54.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 delete --grace-period=0 --force -f -'
Feb 21 14:37:54.062: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 14:37:54.062: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 21 14:37:54.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get rc,svc -l name=update-demo --no-headers'
Feb 21 14:37:54.110: INFO: stderr: "No resources found in kubectl-6901 namespace.\n"
Feb 21 14:37:54.110: INFO: stdout: ""
Feb 21 14:37:54.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-6901 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 14:37:54.153: INFO: stderr: ""
Feb 21 14:37:54.153: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:37:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6901" for this suite.

• [SLOW TEST:6.227 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":297,"skipped":5649,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:37:54.162: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Feb 21 14:37:54.300: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 21 14:38:54.331: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:38:54.333: INFO: Starting informer...
STEP: Starting pods...
Feb 21 14:38:54.552: INFO: Pod1 is running on aksh-cncf-3. Tainting Node
Feb 21 14:38:56.775: INFO: Pod2 is running on aksh-cncf-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 21 14:39:02.207: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 21 14:39:25.245: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:39:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5947" for this suite.

• [SLOW TEST:91.115 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":298,"skipped":5699,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:39:25.278: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:39:43.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4648" for this suite.

• [SLOW TEST:18.340 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5704,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:39:43.618: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6377
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Feb 21 14:39:43.758: INFO: Waiting up to 5m0s for pod "downward-api-53a081e5-dca5-40c7-a126-3ba67207cbe8" in namespace "downward-api-6377" to be "Succeeded or Failed"
Feb 21 14:39:43.761: INFO: Pod "downward-api-53a081e5-dca5-40c7-a126-3ba67207cbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291463ms
Feb 21 14:39:45.769: INFO: Pod "downward-api-53a081e5-dca5-40c7-a126-3ba67207cbe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010319465s
STEP: Saw pod success
Feb 21 14:39:45.769: INFO: Pod "downward-api-53a081e5-dca5-40c7-a126-3ba67207cbe8" satisfied condition "Succeeded or Failed"
Feb 21 14:39:45.771: INFO: Trying to get logs from node aksh-cncf-3 pod downward-api-53a081e5-dca5-40c7-a126-3ba67207cbe8 container dapi-container: <nil>
STEP: delete the pod
Feb 21 14:39:45.791: INFO: Waiting for pod downward-api-53a081e5-dca5-40c7-a126-3ba67207cbe8 to disappear
Feb 21 14:39:45.793: INFO: Pod downward-api-53a081e5-dca5-40c7-a126-3ba67207cbe8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:39:45.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6377" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5721,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:39:45.801: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-6633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:01.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6633" for this suite.

• [SLOW TEST:76.167 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":301,"skipped":5757,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:01.968: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8275
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:41:02.110: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:03.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8275" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":302,"skipped":5760,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:03.149: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Feb 21 14:41:03.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-4834 create -f -'
Feb 21 14:41:04.035: INFO: stderr: ""
Feb 21 14:41:04.035: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Feb 21 14:41:04.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-4834 diff -f -'
Feb 21 14:41:04.789: INFO: rc: 1
Feb 21 14:41:04.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-4834 delete -f -'
Feb 21 14:41:04.834: INFO: stderr: ""
Feb 21 14:41:04.834: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:04.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4834" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":303,"skipped":5760,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:04.844: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:41:04.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04ae1387-3aa5-40fa-8d64-64c8bfdba6a0" in namespace "downward-api-9525" to be "Succeeded or Failed"
Feb 21 14:41:04.989: INFO: Pod "downwardapi-volume-04ae1387-3aa5-40fa-8d64-64c8bfdba6a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.999078ms
Feb 21 14:41:06.993: INFO: Pod "downwardapi-volume-04ae1387-3aa5-40fa-8d64-64c8bfdba6a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005968352s
STEP: Saw pod success
Feb 21 14:41:06.993: INFO: Pod "downwardapi-volume-04ae1387-3aa5-40fa-8d64-64c8bfdba6a0" satisfied condition "Succeeded or Failed"
Feb 21 14:41:06.995: INFO: Trying to get logs from node aksh-cncf-4 pod downwardapi-volume-04ae1387-3aa5-40fa-8d64-64c8bfdba6a0 container client-container: <nil>
STEP: delete the pod
Feb 21 14:41:07.017: INFO: Waiting for pod downwardapi-volume-04ae1387-3aa5-40fa-8d64-64c8bfdba6a0 to disappear
Feb 21 14:41:07.018: INFO: Pod downwardapi-volume-04ae1387-3aa5-40fa-8d64-64c8bfdba6a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:07.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9525" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5764,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:07.025: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-09af71c3-95c7-4933-b10e-e30ddfba44bc
STEP: Creating a pod to test consume configMaps
Feb 21 14:41:07.176: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-38cb80fe-c37d-4292-bce5-4980cc025cc3" in namespace "projected-4362" to be "Succeeded or Failed"
Feb 21 14:41:07.179: INFO: Pod "pod-projected-configmaps-38cb80fe-c37d-4292-bce5-4980cc025cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.388768ms
Feb 21 14:41:09.184: INFO: Pod "pod-projected-configmaps-38cb80fe-c37d-4292-bce5-4980cc025cc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008019219s
STEP: Saw pod success
Feb 21 14:41:09.184: INFO: Pod "pod-projected-configmaps-38cb80fe-c37d-4292-bce5-4980cc025cc3" satisfied condition "Succeeded or Failed"
Feb 21 14:41:09.186: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-configmaps-38cb80fe-c37d-4292-bce5-4980cc025cc3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 14:41:09.207: INFO: Waiting for pod pod-projected-configmaps-38cb80fe-c37d-4292-bce5-4980cc025cc3 to disappear
Feb 21 14:41:09.209: INFO: Pod pod-projected-configmaps-38cb80fe-c37d-4292-bce5-4980cc025cc3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:09.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4362" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":305,"skipped":5777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:09.216: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:41:09.359: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-013b0503-5f7e-4573-8de0-b5bad4b89a0c" in namespace "security-context-test-1800" to be "Succeeded or Failed"
Feb 21 14:41:09.361: INFO: Pod "busybox-readonly-false-013b0503-5f7e-4573-8de0-b5bad4b89a0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032902ms
Feb 21 14:41:11.366: INFO: Pod "busybox-readonly-false-013b0503-5f7e-4573-8de0-b5bad4b89a0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007497683s
Feb 21 14:41:11.366: INFO: Pod "busybox-readonly-false-013b0503-5f7e-4573-8de0-b5bad4b89a0c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:11.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1800" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:11.378: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3376
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-ee0b4a18-aee2-4127-bb91-a42f05665413
STEP: Creating secret with name s-test-opt-upd-4438e8fd-15ed-4b2d-936e-d378097ed01d
STEP: Creating the pod
Feb 21 14:41:11.536: INFO: The status of Pod pod-secrets-a7a4a31a-a2e8-4ede-984a-be2f839474c0 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:41:13.544: INFO: The status of Pod pod-secrets-a7a4a31a-a2e8-4ede-984a-be2f839474c0 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-ee0b4a18-aee2-4127-bb91-a42f05665413
STEP: Updating secret s-test-opt-upd-4438e8fd-15ed-4b2d-936e-d378097ed01d
STEP: Creating secret with name s-test-opt-create-7baae0dd-82a1-4cd0-b6fe-d6ac815b300a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:15.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3376" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":5873,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:15.605: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:41:15.738: INFO: Creating ReplicaSet my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996
Feb 21 14:41:15.745: INFO: Pod name my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996: Found 0 pods out of 1
Feb 21 14:41:20.755: INFO: Pod name my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996: Found 1 pods out of 1
Feb 21 14:41:20.755: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996" is running
Feb 21 14:41:20.758: INFO: Pod "my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996-hgmqc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:41:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:41:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:41:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-02-21 14:41:15 +0000 UTC Reason: Message:}])
Feb 21 14:41:20.758: INFO: Trying to dial the pod
Feb 21 14:41:25.769: INFO: Controller my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996: Got expected result from replica 1 [my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996-hgmqc]: "my-hostname-basic-e8db1635-4437-4e76-bc69-6a4748140996-hgmqc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:41:25.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8531" for this suite.

• [SLOW TEST:10.173 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":308,"skipped":5875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:41:25.779: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-66
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-66
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Feb 21 14:41:25.927: INFO: Found 0 stateful pods, waiting for 3
Feb 21 14:41:35.934: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 14:41:35.934: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 14:41:35.934: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 14:41:35.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-66 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 14:41:36.055: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 14:41:36.055: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 14:41:36.055: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Feb 21 14:41:46.090: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 21 14:41:56.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-66 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 14:41:56.205: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 21 14:41:56.205: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 14:41:56.205: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Feb 21 14:42:06.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-66 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 21 14:42:06.346: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 21 14:42:06.346: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 21 14:42:06.346: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 21 14:42:16.381: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 21 14:42:26.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=statefulset-66 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 21 14:42:26.508: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 21 14:42:26.508: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 21 14:42:26.508: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 21 14:42:36.530: INFO: Waiting for StatefulSet statefulset-66/ss2 to complete update
Feb 21 14:42:36.530: INFO: Waiting for Pod statefulset-66/ss2-0 to have revision ss2-57bbdd95cb update revision ss2-5f8764d585
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 14:42:46.540: INFO: Deleting all statefulset in ns statefulset-66
Feb 21 14:42:46.542: INFO: Scaling statefulset ss2 to 0
Feb 21 14:42:56.560: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 14:42:56.563: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:42:56.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-66" for this suite.

• [SLOW TEST:90.805 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":309,"skipped":5908,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:42:56.584: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:42:56.924: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:42:59.947: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:42:59.953: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:43:03.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6839" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.569 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":310,"skipped":5928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:43:03.153: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 14:43:03.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:43:03.332: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:43:04.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 21 14:43:04.341: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:43:05.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 14:43:05.341: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Feb 21 14:43:05.366: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49414"},"items":null}

Feb 21 14:43:05.371: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49417"},"items":[{"metadata":{"name":"daemon-set-7kpgv","generateName":"daemon-set-","namespace":"daemonsets-3510","uid":"378a39c4-0d65-451b-b818-0b394abf8d48","resourceVersion":"49414","creationTimestamp":"2022-02-21T14:43:03Z","deletionTimestamp":"2022-02-21T14:43:35Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.244.1.28/32","cni.projectcalico.org/podIPs":"10.244.1.28/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"21fe4805-42e9-427d-9d4f-e64c3f958547","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21fe4805-42e9-427d-9d4f-e64c3f958547\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lhm9w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lhm9w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aksh-cncf-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aksh-cncf-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"}],"hostIP":"10.0.0.131","podIP":"10.244.1.28","podIPs":[{"ip":"10.244.1.28"}],"startTime":"2022-02-21T14:43:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-21T14:43:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://c43ebf1908599da6e8087196d36deb59c3b67a6995ec14e5a2fd5867875b9a5c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lm4sz","generateName":"daemon-set-","namespace":"daemonsets-3510","uid":"39fe808a-5f13-41e8-9e59-771592ac2a8b","resourceVersion":"49413","creationTimestamp":"2022-02-21T14:43:03Z","deletionTimestamp":"2022-02-21T14:43:35Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.244.3.100/32","cni.projectcalico.org/podIPs":"10.244.3.100/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"21fe4805-42e9-427d-9d4f-e64c3f958547","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21fe4805-42e9-427d-9d4f-e64c3f958547\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qwshg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qwshg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aksh-cncf-4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aksh-cncf-4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"}],"hostIP":"10.0.0.114","podIP":"10.244.3.100","podIPs":[{"ip":"10.244.3.100"}],"startTime":"2022-02-21T14:43:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-21T14:43:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://325325d7efa3b5db7071aa8e90cb137a22e290b9772e5e0ea539a4cab6578fef","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nrbvz","generateName":"daemon-set-","namespace":"daemonsets-3510","uid":"0457a342-25af-4822-bfe8-39a783747e63","resourceVersion":"49415","creationTimestamp":"2022-02-21T14:43:03Z","deletionTimestamp":"2022-02-21T14:43:35Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.244.0.181/32","cni.projectcalico.org/podIPs":"10.244.0.181/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"21fe4805-42e9-427d-9d4f-e64c3f958547","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21fe4805-42e9-427d-9d4f-e64c3f958547\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2d6dn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2d6dn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aksh-cncf-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aksh-cncf-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"}],"hostIP":"10.0.0.54","podIP":"10.244.0.181","podIPs":[{"ip":"10.244.0.181"}],"startTime":"2022-02-21T14:43:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-21T14:43:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f5deddc66c72b8ec5babad8581f06f327b473f50b0c80537800095bf9a37027e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ztjkj","generateName":"daemon-set-","namespace":"daemonsets-3510","uid":"b5b7311e-e8c5-43f7-83cb-8a1b2cadfcc3","resourceVersion":"49417","creationTimestamp":"2022-02-21T14:43:03Z","deletionTimestamp":"2022-02-21T14:43:35Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/podIP":"10.244.2.106/32","cni.projectcalico.org/podIPs":"10.244.2.106/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"21fe4805-42e9-427d-9d4f-e64c3f958547","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21fe4805-42e9-427d-9d4f-e64c3f958547\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-02-21T14:43:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qnfz9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qnfz9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"aksh-cncf-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["aksh-cncf-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-02-21T14:43:03Z"}],"hostIP":"10.0.0.211","podIP":"10.244.2.106","podIPs":[{"ip":"10.244.2.106"}],"startTime":"2022-02-21T14:43:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-02-21T14:43:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://af1eb2b69440e88605b3a2b86728b7c8ae1356c335459ccf2d54b6e79e7159cf","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:43:05.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3510" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":311,"skipped":5970,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:43:05.391: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 21 14:43:45.585: INFO: The status of Pod kube-controller-manager-aksh-cncf-3 is Running (Ready = true)
Feb 21 14:43:45.631: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 21 14:43:45.631: INFO: Deleting pod "simpletest.rc-28hsf" in namespace "gc-5200"
Feb 21 14:43:45.643: INFO: Deleting pod "simpletest.rc-2tgrv" in namespace "gc-5200"
Feb 21 14:43:45.659: INFO: Deleting pod "simpletest.rc-45f2j" in namespace "gc-5200"
Feb 21 14:43:45.669: INFO: Deleting pod "simpletest.rc-46vtw" in namespace "gc-5200"
Feb 21 14:43:45.680: INFO: Deleting pod "simpletest.rc-4b82c" in namespace "gc-5200"
Feb 21 14:43:45.692: INFO: Deleting pod "simpletest.rc-4h4zd" in namespace "gc-5200"
Feb 21 14:43:45.709: INFO: Deleting pod "simpletest.rc-4j8lw" in namespace "gc-5200"
Feb 21 14:43:45.723: INFO: Deleting pod "simpletest.rc-4lhhh" in namespace "gc-5200"
Feb 21 14:43:45.733: INFO: Deleting pod "simpletest.rc-56gfl" in namespace "gc-5200"
Feb 21 14:43:45.755: INFO: Deleting pod "simpletest.rc-59j7t" in namespace "gc-5200"
Feb 21 14:43:45.770: INFO: Deleting pod "simpletest.rc-5b82j" in namespace "gc-5200"
Feb 21 14:43:45.781: INFO: Deleting pod "simpletest.rc-5nt9k" in namespace "gc-5200"
Feb 21 14:43:45.799: INFO: Deleting pod "simpletest.rc-5vsbm" in namespace "gc-5200"
Feb 21 14:43:45.813: INFO: Deleting pod "simpletest.rc-6c4s9" in namespace "gc-5200"
Feb 21 14:43:45.834: INFO: Deleting pod "simpletest.rc-6vh2z" in namespace "gc-5200"
Feb 21 14:43:45.845: INFO: Deleting pod "simpletest.rc-7bm8x" in namespace "gc-5200"
Feb 21 14:43:45.859: INFO: Deleting pod "simpletest.rc-8gx87" in namespace "gc-5200"
Feb 21 14:43:45.875: INFO: Deleting pod "simpletest.rc-8kq99" in namespace "gc-5200"
Feb 21 14:43:45.888: INFO: Deleting pod "simpletest.rc-8t6vh" in namespace "gc-5200"
Feb 21 14:43:45.902: INFO: Deleting pod "simpletest.rc-8w5jp" in namespace "gc-5200"
Feb 21 14:43:45.915: INFO: Deleting pod "simpletest.rc-96kfm" in namespace "gc-5200"
Feb 21 14:43:45.925: INFO: Deleting pod "simpletest.rc-9bfcl" in namespace "gc-5200"
Feb 21 14:43:45.943: INFO: Deleting pod "simpletest.rc-9fk7k" in namespace "gc-5200"
Feb 21 14:43:45.960: INFO: Deleting pod "simpletest.rc-9wsjc" in namespace "gc-5200"
Feb 21 14:43:45.979: INFO: Deleting pod "simpletest.rc-bh2nl" in namespace "gc-5200"
Feb 21 14:43:45.994: INFO: Deleting pod "simpletest.rc-bpnrd" in namespace "gc-5200"
Feb 21 14:43:46.009: INFO: Deleting pod "simpletest.rc-brpcv" in namespace "gc-5200"
Feb 21 14:43:46.027: INFO: Deleting pod "simpletest.rc-btk5x" in namespace "gc-5200"
Feb 21 14:43:46.043: INFO: Deleting pod "simpletest.rc-c9cnt" in namespace "gc-5200"
Feb 21 14:43:46.079: INFO: Deleting pod "simpletest.rc-cvgdn" in namespace "gc-5200"
Feb 21 14:43:46.098: INFO: Deleting pod "simpletest.rc-d8gqx" in namespace "gc-5200"
Feb 21 14:43:46.113: INFO: Deleting pod "simpletest.rc-d95nk" in namespace "gc-5200"
Feb 21 14:43:46.127: INFO: Deleting pod "simpletest.rc-dcx5g" in namespace "gc-5200"
Feb 21 14:43:46.142: INFO: Deleting pod "simpletest.rc-dfjtb" in namespace "gc-5200"
Feb 21 14:43:46.154: INFO: Deleting pod "simpletest.rc-dkl6q" in namespace "gc-5200"
Feb 21 14:43:46.168: INFO: Deleting pod "simpletest.rc-ff9rm" in namespace "gc-5200"
Feb 21 14:43:46.181: INFO: Deleting pod "simpletest.rc-fhj76" in namespace "gc-5200"
Feb 21 14:43:46.193: INFO: Deleting pod "simpletest.rc-fnmld" in namespace "gc-5200"
Feb 21 14:43:46.205: INFO: Deleting pod "simpletest.rc-frtkm" in namespace "gc-5200"
Feb 21 14:43:46.219: INFO: Deleting pod "simpletest.rc-fz97s" in namespace "gc-5200"
Feb 21 14:43:46.233: INFO: Deleting pod "simpletest.rc-gj6zd" in namespace "gc-5200"
Feb 21 14:43:46.248: INFO: Deleting pod "simpletest.rc-gjcbt" in namespace "gc-5200"
Feb 21 14:43:46.265: INFO: Deleting pod "simpletest.rc-grmr6" in namespace "gc-5200"
Feb 21 14:43:46.281: INFO: Deleting pod "simpletest.rc-hvnz4" in namespace "gc-5200"
Feb 21 14:43:46.293: INFO: Deleting pod "simpletest.rc-j2f82" in namespace "gc-5200"
Feb 21 14:43:46.307: INFO: Deleting pod "simpletest.rc-j4w6v" in namespace "gc-5200"
Feb 21 14:43:46.321: INFO: Deleting pod "simpletest.rc-j4xwh" in namespace "gc-5200"
Feb 21 14:43:46.335: INFO: Deleting pod "simpletest.rc-jcpct" in namespace "gc-5200"
Feb 21 14:43:46.350: INFO: Deleting pod "simpletest.rc-jtgl9" in namespace "gc-5200"
Feb 21 14:43:46.364: INFO: Deleting pod "simpletest.rc-k82wj" in namespace "gc-5200"
Feb 21 14:43:46.379: INFO: Deleting pod "simpletest.rc-kgm4d" in namespace "gc-5200"
Feb 21 14:43:46.397: INFO: Deleting pod "simpletest.rc-l967z" in namespace "gc-5200"
Feb 21 14:43:46.412: INFO: Deleting pod "simpletest.rc-lftc6" in namespace "gc-5200"
Feb 21 14:43:46.423: INFO: Deleting pod "simpletest.rc-lm7fx" in namespace "gc-5200"
Feb 21 14:43:46.445: INFO: Deleting pod "simpletest.rc-lmr5p" in namespace "gc-5200"
Feb 21 14:43:46.459: INFO: Deleting pod "simpletest.rc-lqk6f" in namespace "gc-5200"
Feb 21 14:43:46.472: INFO: Deleting pod "simpletest.rc-lzl9f" in namespace "gc-5200"
Feb 21 14:43:46.484: INFO: Deleting pod "simpletest.rc-m8hpr" in namespace "gc-5200"
Feb 21 14:43:46.496: INFO: Deleting pod "simpletest.rc-ml4ph" in namespace "gc-5200"
Feb 21 14:43:46.513: INFO: Deleting pod "simpletest.rc-mq8bt" in namespace "gc-5200"
Feb 21 14:43:46.525: INFO: Deleting pod "simpletest.rc-mwkj5" in namespace "gc-5200"
Feb 21 14:43:46.535: INFO: Deleting pod "simpletest.rc-p558w" in namespace "gc-5200"
Feb 21 14:43:46.549: INFO: Deleting pod "simpletest.rc-p8wzp" in namespace "gc-5200"
Feb 21 14:43:46.567: INFO: Deleting pod "simpletest.rc-p9b8b" in namespace "gc-5200"
Feb 21 14:43:46.579: INFO: Deleting pod "simpletest.rc-phmp2" in namespace "gc-5200"
Feb 21 14:43:46.591: INFO: Deleting pod "simpletest.rc-pjwsm" in namespace "gc-5200"
Feb 21 14:43:46.606: INFO: Deleting pod "simpletest.rc-pn2s4" in namespace "gc-5200"
Feb 21 14:43:46.624: INFO: Deleting pod "simpletest.rc-px8vg" in namespace "gc-5200"
Feb 21 14:43:46.672: INFO: Deleting pod "simpletest.rc-q6x7d" in namespace "gc-5200"
Feb 21 14:43:46.722: INFO: Deleting pod "simpletest.rc-q7lvg" in namespace "gc-5200"
Feb 21 14:43:46.763: INFO: Deleting pod "simpletest.rc-qk76z" in namespace "gc-5200"
Feb 21 14:43:46.823: INFO: Deleting pod "simpletest.rc-qmwqx" in namespace "gc-5200"
Feb 21 14:43:46.863: INFO: Deleting pod "simpletest.rc-qrgvp" in namespace "gc-5200"
Feb 21 14:43:46.917: INFO: Deleting pod "simpletest.rc-rdjnw" in namespace "gc-5200"
Feb 21 14:43:46.968: INFO: Deleting pod "simpletest.rc-rg8s6" in namespace "gc-5200"
Feb 21 14:43:47.018: INFO: Deleting pod "simpletest.rc-rp9g7" in namespace "gc-5200"
Feb 21 14:43:47.073: INFO: Deleting pod "simpletest.rc-rsjkg" in namespace "gc-5200"
Feb 21 14:43:47.118: INFO: Deleting pod "simpletest.rc-rsm7f" in namespace "gc-5200"
Feb 21 14:43:47.168: INFO: Deleting pod "simpletest.rc-rvkhs" in namespace "gc-5200"
Feb 21 14:43:47.215: INFO: Deleting pod "simpletest.rc-s6kvz" in namespace "gc-5200"
Feb 21 14:43:47.265: INFO: Deleting pod "simpletest.rc-scgt9" in namespace "gc-5200"
Feb 21 14:43:47.318: INFO: Deleting pod "simpletest.rc-slzk8" in namespace "gc-5200"
Feb 21 14:43:47.365: INFO: Deleting pod "simpletest.rc-stjgl" in namespace "gc-5200"
Feb 21 14:43:47.417: INFO: Deleting pod "simpletest.rc-t4t9v" in namespace "gc-5200"
Feb 21 14:43:47.466: INFO: Deleting pod "simpletest.rc-t7wzh" in namespace "gc-5200"
Feb 21 14:43:47.517: INFO: Deleting pod "simpletest.rc-tc8l6" in namespace "gc-5200"
Feb 21 14:43:47.569: INFO: Deleting pod "simpletest.rc-v6km2" in namespace "gc-5200"
Feb 21 14:43:47.620: INFO: Deleting pod "simpletest.rc-vn8hg" in namespace "gc-5200"
Feb 21 14:43:47.665: INFO: Deleting pod "simpletest.rc-vsdp2" in namespace "gc-5200"
Feb 21 14:43:47.718: INFO: Deleting pod "simpletest.rc-vsq2b" in namespace "gc-5200"
Feb 21 14:43:47.766: INFO: Deleting pod "simpletest.rc-vzkck" in namespace "gc-5200"
Feb 21 14:43:47.816: INFO: Deleting pod "simpletest.rc-w2xnb" in namespace "gc-5200"
Feb 21 14:43:47.868: INFO: Deleting pod "simpletest.rc-w52hp" in namespace "gc-5200"
Feb 21 14:43:47.920: INFO: Deleting pod "simpletest.rc-w596z" in namespace "gc-5200"
Feb 21 14:43:47.966: INFO: Deleting pod "simpletest.rc-wmv4z" in namespace "gc-5200"
Feb 21 14:43:48.018: INFO: Deleting pod "simpletest.rc-x6z7q" in namespace "gc-5200"
Feb 21 14:43:48.070: INFO: Deleting pod "simpletest.rc-xnwx5" in namespace "gc-5200"
Feb 21 14:43:48.118: INFO: Deleting pod "simpletest.rc-xv7fv" in namespace "gc-5200"
Feb 21 14:43:48.164: INFO: Deleting pod "simpletest.rc-z888k" in namespace "gc-5200"
Feb 21 14:43:48.214: INFO: Deleting pod "simpletest.rc-zp228" in namespace "gc-5200"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:43:48.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5200" for this suite.

• [SLOW TEST:42.972 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":312,"skipped":5985,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:43:48.363: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-457
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-457
STEP: Waiting until pod test-pod will start running in namespace statefulset-457
STEP: Creating statefulset with conflicting port in namespace statefulset-457
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-457
Feb 21 14:43:52.555: INFO: Observed stateful pod in namespace: statefulset-457, name: ss-0, uid: 007a25cc-280f-4488-b098-bb2902e97e68, status phase: Pending. Waiting for statefulset controller to delete.
Feb 21 14:43:52.568: INFO: Observed stateful pod in namespace: statefulset-457, name: ss-0, uid: 007a25cc-280f-4488-b098-bb2902e97e68, status phase: Failed. Waiting for statefulset controller to delete.
Feb 21 14:43:52.574: INFO: Observed stateful pod in namespace: statefulset-457, name: ss-0, uid: 007a25cc-280f-4488-b098-bb2902e97e68, status phase: Failed. Waiting for statefulset controller to delete.
Feb 21 14:43:52.576: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-457
STEP: Removing pod with conflicting port in namespace statefulset-457
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-457 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Feb 21 14:43:56.603: INFO: Deleting all statefulset in ns statefulset-457
Feb 21 14:43:56.605: INFO: Scaling statefulset ss to 0
Feb 21 14:44:06.624: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 14:44:06.627: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:06.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-457" for this suite.

• [SLOW TEST:18.286 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":313,"skipped":5988,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:06.650: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4996
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 21 14:44:06.785: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
Feb 21 14:44:09.323: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:20.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4996" for this suite.

• [SLOW TEST:13.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":314,"skipped":6063,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:20.333: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 14:44:22.494: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:22.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-486" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":6065,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:22.517: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-8743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:44:22.657: INFO: Endpoints addresses: [10.0.0.131 10.0.0.211 10.0.0.54] , ports: [6443]
Feb 21 14:44:22.657: INFO: EndpointSlices addresses: [10.0.0.131 10.0.0.211 10.0.0.54] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:22.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8743" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":316,"skipped":6067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:22.664: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-5493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb 21 14:44:22.808: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb 21 14:44:22.812: INFO: starting watch
STEP: patching
STEP: updating
Feb 21 14:44:22.839: INFO: waiting for watch events with expected annotations
Feb 21 14:44:22.839: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:22.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5493" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":317,"skipped":6100,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:22.889: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-9098
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Feb 21 14:44:23.040: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Feb 21 14:44:25.057: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Feb 21 14:44:27.075: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:29.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9098" for this suite.

• [SLOW TEST:6.201 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":318,"skipped":6105,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:29.090: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:29.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8106" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":319,"skipped":6120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:29.258: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9822
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-85ed8959-4bff-4429-b14b-aed3a3118e7a
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:29.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9822" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":320,"skipped":6207,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:29.409: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:44:29.555: INFO: The status of Pod busybox-scheduling-cca74100-5f81-487f-a254-9aba684a9438 is Pending, waiting for it to be Running (with Ready = true)
Feb 21 14:44:31.560: INFO: The status of Pod busybox-scheduling-cca74100-5f81-487f-a254-9aba684a9438 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:31.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-539" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":6221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:31.577: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Feb 21 14:44:31.712: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-3973 proxy --unix-socket=/tmp/kubectl-proxy-unix3131276612/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:31.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3973" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":322,"skipped":6282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:31.749: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Feb 21 14:44:31.900: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:31.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6022" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":323,"skipped":6317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-0eb41067-5b94-4c59-8350-3053738482c3
STEP: Creating a pod to test consume configMaps
Feb 21 14:44:32.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-224da614-77a8-4f58-b220-58d4a6d1d0b8" in namespace "projected-1785" to be "Succeeded or Failed"
Feb 21 14:44:32.078: INFO: Pod "pod-projected-configmaps-224da614-77a8-4f58-b220-58d4a6d1d0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.33112ms
Feb 21 14:44:34.081: INFO: Pod "pod-projected-configmaps-224da614-77a8-4f58-b220-58d4a6d1d0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006506249s
STEP: Saw pod success
Feb 21 14:44:34.081: INFO: Pod "pod-projected-configmaps-224da614-77a8-4f58-b220-58d4a6d1d0b8" satisfied condition "Succeeded or Failed"
Feb 21 14:44:34.083: INFO: Trying to get logs from node aksh-cncf-3 pod pod-projected-configmaps-224da614-77a8-4f58-b220-58d4a6d1d0b8 container agnhost-container: <nil>
STEP: delete the pod
Feb 21 14:44:34.101: INFO: Waiting for pod pod-projected-configmaps-224da614-77a8-4f58-b220-58d4a6d1d0b8 to disappear
Feb 21 14:44:34.103: INFO: Pod pod-projected-configmaps-224da614-77a8-4f58-b220-58d4a6d1d0b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:34.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1785" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":324,"skipped":6350,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:34.112: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3720
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:44:34.248: INFO: Creating pod...
Feb 21 14:44:34.260: INFO: Pod Quantity: 1 Status: Pending
Feb 21 14:44:35.268: INFO: Pod Status: Running
Feb 21 14:44:35.268: INFO: Creating service...
Feb 21 14:44:35.281: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/pods/agnhost/proxy/some/path/with/DELETE
Feb 21 14:44:35.285: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 21 14:44:35.285: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/pods/agnhost/proxy/some/path/with/GET
Feb 21 14:44:35.289: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 21 14:44:35.289: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/pods/agnhost/proxy/some/path/with/HEAD
Feb 21 14:44:35.292: INFO: http.Client request:HEAD | StatusCode:200
Feb 21 14:44:35.292: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 21 14:44:35.295: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 21 14:44:35.295: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/pods/agnhost/proxy/some/path/with/PATCH
Feb 21 14:44:35.297: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 21 14:44:35.297: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/pods/agnhost/proxy/some/path/with/POST
Feb 21 14:44:35.300: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 21 14:44:35.300: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/pods/agnhost/proxy/some/path/with/PUT
Feb 21 14:44:35.303: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 21 14:44:35.303: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/services/test-service/proxy/some/path/with/DELETE
Feb 21 14:44:35.307: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 21 14:44:35.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/services/test-service/proxy/some/path/with/GET
Feb 21 14:44:35.312: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 21 14:44:35.312: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/services/test-service/proxy/some/path/with/HEAD
Feb 21 14:44:35.316: INFO: http.Client request:HEAD | StatusCode:200
Feb 21 14:44:35.316: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/services/test-service/proxy/some/path/with/OPTIONS
Feb 21 14:44:35.319: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 21 14:44:35.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/services/test-service/proxy/some/path/with/PATCH
Feb 21 14:44:35.323: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 21 14:44:35.323: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/services/test-service/proxy/some/path/with/POST
Feb 21 14:44:35.330: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 21 14:44:35.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3720/services/test-service/proxy/some/path/with/PUT
Feb 21 14:44:35.334: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:35.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3720" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":325,"skipped":6363,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:35.343: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Feb 21 14:44:35.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd99a31c-a27f-4f97-b728-fe107b5eb47a" in namespace "downward-api-9602" to be "Succeeded or Failed"
Feb 21 14:44:35.488: INFO: Pod "downwardapi-volume-fd99a31c-a27f-4f97-b728-fe107b5eb47a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160776ms
Feb 21 14:44:37.496: INFO: Pod "downwardapi-volume-fd99a31c-a27f-4f97-b728-fe107b5eb47a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009499509s
STEP: Saw pod success
Feb 21 14:44:37.496: INFO: Pod "downwardapi-volume-fd99a31c-a27f-4f97-b728-fe107b5eb47a" satisfied condition "Succeeded or Failed"
Feb 21 14:44:37.498: INFO: Trying to get logs from node aksh-cncf-2 pod downwardapi-volume-fd99a31c-a27f-4f97-b728-fe107b5eb47a container client-container: <nil>
STEP: delete the pod
Feb 21 14:44:37.522: INFO: Waiting for pod downwardapi-volume-fd99a31c-a27f-4f97-b728-fe107b5eb47a to disappear
Feb 21 14:44:37.524: INFO: Pod downwardapi-volume-fd99a31c-a27f-4f97-b728-fe107b5eb47a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:37.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9602" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6374,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:37.532: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Feb 21 14:44:37.675: INFO: Waiting up to 5m0s for pod "var-expansion-778486b9-929f-441c-a244-ae6886e1d9da" in namespace "var-expansion-5953" to be "Succeeded or Failed"
Feb 21 14:44:37.677: INFO: Pod "var-expansion-778486b9-929f-441c-a244-ae6886e1d9da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085731ms
Feb 21 14:44:39.685: INFO: Pod "var-expansion-778486b9-929f-441c-a244-ae6886e1d9da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009642071s
STEP: Saw pod success
Feb 21 14:44:39.685: INFO: Pod "var-expansion-778486b9-929f-441c-a244-ae6886e1d9da" satisfied condition "Succeeded or Failed"
Feb 21 14:44:39.687: INFO: Trying to get logs from node aksh-cncf-1 pod var-expansion-778486b9-929f-441c-a244-ae6886e1d9da container dapi-container: <nil>
STEP: delete the pod
Feb 21 14:44:39.712: INFO: Waiting for pod var-expansion-778486b9-929f-441c-a244-ae6886e1d9da to disappear
Feb 21 14:44:39.714: INFO: Pod var-expansion-778486b9-929f-441c-a244-ae6886e1d9da no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:39.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5953" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":6377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:39.722: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3949
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-b43faefb-39fb-4341-b48b-354377ad2ab2-4364
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:40.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3949" for this suite.
STEP: Destroying namespace "nspatchtest-b43faefb-39fb-4341-b48b-354377ad2ab2-4364" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":328,"skipped":6402,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:40.016: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 14:44:41.169: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:44:41.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9132" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":6412,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:44:41.191: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-020b425d-6a84-49ce-ab02-d11f736de7fb in namespace container-probe-254
Feb 21 14:44:43.344: INFO: Started pod liveness-020b425d-6a84-49ce-ab02-d11f736de7fb in namespace container-probe-254
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 14:44:43.347: INFO: Initial restart count of pod liveness-020b425d-6a84-49ce-ab02-d11f736de7fb is 0
Feb 21 14:45:03.411: INFO: Restart count of pod container-probe-254/liveness-020b425d-6a84-49ce-ab02-d11f736de7fb is now 1 (20.064222422s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:03.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-254" for this suite.

• [SLOW TEST:22.240 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":6423,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:03.431: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 21 14:45:03.581: INFO: Waiting up to 5m0s for pod "pod-aa319d2b-def9-463e-b28d-1d45fc040834" in namespace "emptydir-7802" to be "Succeeded or Failed"
Feb 21 14:45:03.583: INFO: Pod "pod-aa319d2b-def9-463e-b28d-1d45fc040834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351278ms
Feb 21 14:45:05.591: INFO: Pod "pod-aa319d2b-def9-463e-b28d-1d45fc040834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010753485s
STEP: Saw pod success
Feb 21 14:45:05.591: INFO: Pod "pod-aa319d2b-def9-463e-b28d-1d45fc040834" satisfied condition "Succeeded or Failed"
Feb 21 14:45:05.594: INFO: Trying to get logs from node aksh-cncf-3 pod pod-aa319d2b-def9-463e-b28d-1d45fc040834 container test-container: <nil>
STEP: delete the pod
Feb 21 14:45:05.612: INFO: Waiting for pod pod-aa319d2b-def9-463e-b28d-1d45fc040834 to disappear
Feb 21 14:45:05.614: INFO: Pod pod-aa319d2b-def9-463e-b28d-1d45fc040834 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:05.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7802" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":6426,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:05.623: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Feb 21 14:45:05.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-2865 cluster-info'
Feb 21 14:45:05.802: INFO: stderr: ""
Feb 21 14:45:05.802: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:05.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2865" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":332,"skipped":6436,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:05.811: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:45:05.944: INFO: Creating deployment "test-recreate-deployment"
Feb 21 14:45:05.949: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 21 14:45:05.954: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 21 14:45:07.966: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 21 14:45:07.967: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 21 14:45:07.979: INFO: Updating deployment test-recreate-deployment
Feb 21 14:45:07.979: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 14:45:08.042: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8986  dafa4a2d-0016-422a-ad9c-b48a235170b5 52348 2 2022-02-21 14:45:05 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-02-21 14:45:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00673f498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-02-21 14:45:08 +0000 UTC,LastTransitionTime:2022-02-21 14:45:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-02-21 14:45:08 +0000 UTC,LastTransitionTime:2022-02-21 14:45:05 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 21 14:45:08.045: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-8986  3d6bc68e-4e7c-45fe-a016-51f5b460c4ae 52346 1 2022-02-21 14:45:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment dafa4a2d-0016-422a-ad9c-b48a235170b5 0xc00676ac77 0xc00676ac78}] []  [{kube-controller-manager Update apps/v1 2022-02-21 14:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dafa4a2d-0016-422a-ad9c-b48a235170b5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:45:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00676ad58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 21 14:45:08.045: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 21 14:45:08.045: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-8986  cf534465-a877-43c0-b4ac-f7f8eaa60676 52336 2 2022-02-21 14:45:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment dafa4a2d-0016-422a-ad9c-b48a235170b5 0xc00676add7 0xc00676add8}] []  [{kube-controller-manager Update apps/v1 2022-02-21 14:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dafa4a2d-0016-422a-ad9c-b48a235170b5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:45:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00676aeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 21 14:45:08.047: INFO: Pod "test-recreate-deployment-5b99bd5487-bsmfj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-bsmfj test-recreate-deployment-5b99bd5487- deployment-8986  6416ea28-1127-40e3-a25a-da182fe741e3 52347 0 2022-02-21 14:45:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 3d6bc68e-4e7c-45fe-a016-51f5b460c4ae 0xc00673fae7 0xc00673fae8}] []  [{Go-http-client Update v1 2022-02-21 14:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {kube-controller-manager Update v1 2022-02-21 14:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d6bc68e-4e7c-45fe-a016-51f5b460c4ae\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lggr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lggr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:45:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:45:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:45:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.131,PodIP:,StartTime:2022-02-21 14:45:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:08.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8986" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":333,"skipped":6440,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:08.056: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:45:10.207: INFO: Deleting pod "var-expansion-a592eab1-7fd0-47b6-9d92-e85ba075b085" in namespace "var-expansion-3513"
Feb 21 14:45:10.216: INFO: Wait up to 5m0s for pod "var-expansion-a592eab1-7fd0-47b6-9d92-e85ba075b085" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:12.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3513" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":334,"skipped":6451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:12.240: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:45:12.647: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:45:15.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:15.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4246" for this suite.
STEP: Destroying namespace "webhook-4246-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":335,"skipped":6487,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:15.769: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:45:16.194: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:45:19.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:20.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1452" for this suite.
STEP: Destroying namespace "webhook-1452-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":336,"skipped":6496,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:20.331: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2793
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2793
STEP: creating replication controller externalsvc in namespace services-2793
I0221 14:45:20.492756      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2793, replica count: 2
I0221 14:45:23.544365      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 21 14:45:23.563: INFO: Creating new exec pod
Feb 21 14:45:25.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=services-2793 exec execpodff2vj -- /bin/sh -x -c nslookup clusterip-service.services-2793.svc.cluster.local'
Feb 21 14:45:25.706: INFO: stderr: "+ nslookup clusterip-service.services-2793.svc.cluster.local\n"
Feb 21 14:45:25.706: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2793.svc.cluster.local\tcanonical name = externalsvc.services-2793.svc.cluster.local.\nName:\texternalsvc.services-2793.svc.cluster.local\nAddress: 10.96.242.8\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2793, will wait for the garbage collector to delete the pods
Feb 21 14:45:25.768: INFO: Deleting ReplicationController externalsvc took: 8.589058ms
Feb 21 14:45:25.868: INFO: Terminating ReplicationController externalsvc pods took: 100.289683ms
Feb 21 14:45:28.295: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2793" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.982 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":337,"skipped":6500,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:28.313: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Feb 21 14:45:28.446: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:32.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-339" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":338,"skipped":6520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:32.039: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 14:45:32.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:45:32.205: INFO: Node aksh-cncf-1 is running 0 daemon pod, expected 1
Feb 21 14:45:33.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 21 14:45:33.215: INFO: Node aksh-cncf-2 is running 0 daemon pod, expected 1
Feb 21 14:45:34.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 21 14:45:34.215: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status
Feb 21 14:45:34.219: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Feb 21 14:45:34.228: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Feb 21 14:45:34.229: INFO: Observed &DaemonSet event: ADDED
Feb 21 14:45:34.229: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.229: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.229: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.230: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.230: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.230: INFO: Found daemon set daemon-set in namespace daemonsets-8723 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 21 14:45:34.230: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Feb 21 14:45:34.238: INFO: Observed &DaemonSet event: ADDED
Feb 21 14:45:34.238: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.238: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.238: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.238: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.239: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.239: INFO: Observed daemon set daemon-set in namespace daemonsets-8723 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 21 14:45:34.239: INFO: Observed &DaemonSet event: MODIFIED
Feb 21 14:45:34.239: INFO: Found daemon set daemon-set in namespace daemonsets-8723 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 21 14:45:34.239: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8723, will wait for the garbage collector to delete the pods
Feb 21 14:45:34.300: INFO: Deleting DaemonSet.extensions daemon-set took: 5.385651ms
Feb 21 14:45:34.400: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.433274ms
Feb 21 14:45:37.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 21 14:45:37.305: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 21 14:45:37.307: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52941"},"items":null}

Feb 21 14:45:37.309: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52941"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:37.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8723" for this suite.

• [SLOW TEST:5.295 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":339,"skipped":6570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:37.334: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-f8c4
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 14:45:37.484: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f8c4" in namespace "subpath-416" to be "Succeeded or Failed"
Feb 21 14:45:37.486: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.961477ms
Feb 21 14:45:39.492: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008461407s
Feb 21 14:45:41.498: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.014407578s
Feb 21 14:45:43.503: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 6.018840737s
Feb 21 14:45:45.509: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 8.025456427s
Feb 21 14:45:47.516: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 10.031613352s
Feb 21 14:45:49.522: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 12.038275269s
Feb 21 14:45:51.529: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 14.045346047s
Feb 21 14:45:53.535: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 16.050526655s
Feb 21 14:45:55.540: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 18.056246921s
Feb 21 14:45:57.548: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Running", Reason="", readiness=true. Elapsed: 20.063835449s
Feb 21 14:45:59.554: INFO: Pod "pod-subpath-test-configmap-f8c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.069687509s
STEP: Saw pod success
Feb 21 14:45:59.554: INFO: Pod "pod-subpath-test-configmap-f8c4" satisfied condition "Succeeded or Failed"
Feb 21 14:45:59.556: INFO: Trying to get logs from node aksh-cncf-3 pod pod-subpath-test-configmap-f8c4 container test-container-subpath-configmap-f8c4: <nil>
STEP: delete the pod
Feb 21 14:45:59.575: INFO: Waiting for pod pod-subpath-test-configmap-f8c4 to disappear
Feb 21 14:45:59.577: INFO: Pod pod-subpath-test-configmap-f8c4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f8c4
Feb 21 14:45:59.577: INFO: Deleting pod "pod-subpath-test-configmap-f8c4" in namespace "subpath-416"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:45:59.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-416" for this suite.

• [SLOW TEST:22.253 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":340,"skipped":6593,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:45:59.587: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Feb 21 14:45:59.732: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 21 14:46:04.736: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 14:46:04.736: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Feb 21 14:46:04.752: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4335  a2028dd2-792b-486c-b5e5-0a40f24b7bcc 53136 1 2022-02-21 14:46:04 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-02-21 14:46:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0066a94d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 21 14:46:04.756: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb 21 14:46:04.756: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 21 14:46:04.756: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4335  6545115c-2329-413b-a264-b4dbbed86acc 53138 1 2022-02-21 14:45:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment a2028dd2-792b-486c-b5e5-0a40f24b7bcc 0xc0066a9917 0xc0066a9918}] []  [{e2e.test Update apps/v1 2022-02-21 14:45:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-02-21 14:46:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-02-21 14:46:04 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"a2028dd2-792b-486c-b5e5-0a40f24b7bcc\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0066a9a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 21 14:46:04.763: INFO: Pod "test-cleanup-controller-bfnnr" is available:
&Pod{ObjectMeta:{test-cleanup-controller-bfnnr test-cleanup-controller- deployment-4335  4b5c33b3-0929-4987-8416-6d454d94c905 53109 0 2022-02-21 14:45:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.1.67/32 cni.projectcalico.org/podIPs:10.244.1.67/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 6545115c-2329-413b-a264-b4dbbed86acc 0xc0068bb917 0xc0068bb918}] []  [{kube-controller-manager Update v1 2022-02-21 14:45:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6545115c-2329-413b-a264-b4dbbed86acc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-02-21 14:46:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {Go-http-client Update v1 2022-02-21 14:46:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wkfrn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wkfrn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:aksh-cncf-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:45:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:46:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:46:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-02-21 14:45:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.131,PodIP:10.244.1.67,StartTime:2022-02-21 14:45:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-02-21 14:46:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://38e7d19aa540b26edecef4d0aaa644c8c9e45232a61b3fbf40af2b3da2c08d15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:46:04.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4335" for this suite.

• [SLOW TEST:5.191 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":341,"skipped":6595,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:46:04.779: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:46:10.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5113" for this suite.

• [SLOW TEST:6.157 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":342,"skipped":6596,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:46:10.936: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3869
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 21 14:46:11.083: INFO: Waiting up to 5m0s for pod "pod-6c5627a5-8f46-4804-89ef-bf07faa5f760" in namespace "emptydir-3869" to be "Succeeded or Failed"
Feb 21 14:46:11.087: INFO: Pod "pod-6c5627a5-8f46-4804-89ef-bf07faa5f760": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842469ms
Feb 21 14:46:13.091: INFO: Pod "pod-6c5627a5-8f46-4804-89ef-bf07faa5f760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008525149s
STEP: Saw pod success
Feb 21 14:46:13.091: INFO: Pod "pod-6c5627a5-8f46-4804-89ef-bf07faa5f760" satisfied condition "Succeeded or Failed"
Feb 21 14:46:13.093: INFO: Trying to get logs from node aksh-cncf-3 pod pod-6c5627a5-8f46-4804-89ef-bf07faa5f760 container test-container: <nil>
STEP: delete the pod
Feb 21 14:46:13.114: INFO: Waiting for pod pod-6c5627a5-8f46-4804-89ef-bf07faa5f760 to disappear
Feb 21 14:46:13.116: INFO: Pod pod-6c5627a5-8f46-4804-89ef-bf07faa5f760 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:46:13.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3869" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:46:13.124: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 21 14:46:13.513: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 21 14:46:16.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:46:16.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-783" for this suite.
STEP: Destroying namespace "webhook-783-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":344,"skipped":6642,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:46:16.595: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-6444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Feb 21 14:46:18.757: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:46:20.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6444" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":345,"skipped":6657,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Feb 21 14:46:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-129910592
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1331
STEP: creating the pod
Feb 21 14:46:20.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 create -f -'
Feb 21 14:46:21.335: INFO: stderr: ""
Feb 21 14:46:21.335: INFO: stdout: "pod/pause created\n"
Feb 21 14:46:21.335: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 21 14:46:21.335: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7582" to be "running and ready"
Feb 21 14:46:21.342: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.875001ms
Feb 21 14:46:23.347: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.012111537s
Feb 21 14:46:23.347: INFO: Pod "pause" satisfied condition "running and ready"
Feb 21 14:46:23.347: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 21 14:46:23.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 label pods pause testing-label=testing-label-value'
Feb 21 14:46:23.404: INFO: stderr: ""
Feb 21 14:46:23.404: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 21 14:46:23.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 get pod pause -L testing-label'
Feb 21 14:46:23.444: INFO: stderr: ""
Feb 21 14:46:23.444: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 21 14:46:23.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 label pods pause testing-label-'
Feb 21 14:46:23.492: INFO: stderr: ""
Feb 21 14:46:23.492: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 21 14:46:23.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 get pod pause -L testing-label'
Feb 21 14:46:23.534: INFO: stderr: ""
Feb 21 14:46:23.534: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1337
STEP: using delete to clean up resources
Feb 21 14:46:23.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 delete --grace-period=0 --force -f -'
Feb 21 14:46:23.586: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 14:46:23.586: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 21 14:46:23.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 get rc,svc -l name=pause --no-headers'
Feb 21 14:46:23.629: INFO: stderr: "No resources found in kubectl-7582 namespace.\n"
Feb 21 14:46:23.629: INFO: stdout: ""
Feb 21 14:46:23.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129910592 --namespace=kubectl-7582 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 14:46:23.670: INFO: stderr: ""
Feb 21 14:46:23.670: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Feb 21 14:46:23.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7582" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":346,"skipped":6687,"failed":0}
SSSSSSSSSFeb 21 14:46:23.679: INFO: Running AfterSuite actions on all nodes
Feb 21 14:46:23.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Feb 21 14:46:23.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Feb 21 14:46:23.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Feb 21 14:46:23.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb 21 14:46:23.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb 21 14:46:23.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb 21 14:46:23.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Feb 21 14:46:23.679: INFO: Running AfterSuite actions on node 1
Feb 21 14:46:23.679: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6696,"failed":0}

Ran 346 of 7042 Specs in 5452.234 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6696 Skipped
PASS

Ginkgo ran 1 suite in 1h30m53.649833148s
Test Suite Passed
